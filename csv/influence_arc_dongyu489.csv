2007.sigdial-1.18,J96-1002,0,0.0066209,"om different input modalities. Adding new modalities thus becomes simple and cheap. Extracting semantic information, however, is not trivial, especially since the output from the speech recognizer contains errors and users may convey multiple semantics in one utterance. The semantic information extracted includes the task classification, which is a generic call-routing problem, e.g. (Kuo, et al., 2002; Carpenter, et al., 1998), and task-specific semantic slots (e.g. origin city, destination city, time of day for weather forecast). Slot labeling is performed using a Maximum Entropy classifier (Berger et al., 1996) trained from the same LM training sentences. 2.3 Dialog manager The task of the dialog manager is to determine the appropriate actions to take, given the current dialog context and the newly extracted semantic information. Note that both the speech recognizer and the semantic parser are not certain about their results. The confidence from them needs to be taken into consideration when decision is to be made. The dialog management is based on a two-level state machine in our system: the turn level and the dialog level. The turn level state machines are prebuilt configurable and reusable dialog"
2020.acl-main.101,D19-1299,0,0.435857,"which can be easily extracted with existing tools such as NLTK (Loper and Bird, 2002). To match key words, a mis-matching loss should be defined. Such a mis-matching loss could be non-differentiable, e.g., when the loss is defined as the number of matched entities. In order to still be able to learn by gradient descent, one can adopt the policy gradient algorithm to deal with the non-differentiability. However, policy gradient is known to exhibit high variance. To overcome this issue, we instead propose to perform optimization via optimal transport (OT), inspired by the recent techniques in (Chen et al., 2019a). Optimal-Transport Distance In the context of text generation, a generated text sequence, y = (y1 , · · · , yn ), canP be represented as a discrete disn tribution µ = i=1 ui δyi (·), where ui ≥ 0 P and i ui = 1, δx (·) denotes a spike distribution located at x. Given two discrete distribuPn tions µ ν, written as µ = i=1 ui δxi and Pand m ν = v δ , respectively, the OT distance j y j j=1 between µ and ν is defined as the solution of the following maximum network-flow problem: n X m X LOT = min Uij · d(xi , yj ) , (4) U ∈Π(µ,ν) i=1 j=1 where d(x, y) is the cost of moving x to y (matching x an"
2020.acl-main.101,W18-6502,0,0.0834443,"are based on an encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., ∗ Zhenyi Wang was a research intern student at Tencent AI Lab in Bellevue, WA when doing this work. Figure 1: An example of table-to-text generation. This generation is unfaithful because there exists information in table not covered by generated text (marked in blue); At the same time, hallucinated information in text does not appear in table (marked in red). 2015), most of which are RNN-based Sequenceto-Sequence (Seq2Seq) models (Lebret et al., 2016b; Liu et al., 2018; Wiseman et al., 2018; Ma et al., 2019; Wang et al., 2018; Liu et al., 2019a). Though significant progress has been achieved, we advocate two key problems in existing methods. Firstly, because of the intrinsic shortage of RNN, RNN-based models are not able to capture longterm dependencies, which would lose important information reflected in a table. This drawback prevents them from being applied to larger tables, for example, a table describing a large Knowledge Base (Wang et al., 2018). Secondly, little work has focused on generating faithful text descriptions, which is defined, in this paper, as the level of matching between a generated text seque"
2020.acl-main.101,P17-2089,0,0.0267034,"ural networks (Beck et al., 2018) does not seem helpful. As a result, we only rely on the sequence representation in this paper. 2.2 The Base Objective Our base objective comes from the standard Transformer model, which is defined as the negative log-likelihood loss Lmle of a target sentence y given its input T , i.e., Lmle = − log P (y|T ; θ) (2) with P (y|T ; θ) defined in (1). 2.3 sequence, we propose to match the mean embeddings of both sequences. In fact, the mean embedding has been proved to be an effective representation for the whole sequence in machine translation (Yang et al., 2019; Wang et al., 2017). Let Vˆtable and Vˆtext be the mean embeddings of a table and the target text embeddings in our Transformerbased model, respectively. A table-target sentence disagreement loss Ldisagree is then defined as Faithfulness Modeling with a Table-Text Disagreement Constraint Loss One key element of our model is to enforce a generated text sequence to be consistent with (or faithful to) the table input. To achieve this, we propose to add some constraints so that a generated text sequence only contains information from the table. Our first idea is inspired by related work in machine translation (Yang"
2020.acl-main.101,D18-1356,0,0.0327722,"Missing"
2020.acl-main.101,P19-1296,0,0.0649213,"acted with graph neural networks (Beck et al., 2018) does not seem helpful. As a result, we only rely on the sequence representation in this paper. 2.2 The Base Objective Our base objective comes from the standard Transformer model, which is defined as the negative log-likelihood loss Lmle of a target sentence y given its input T , i.e., Lmle = − log P (y|T ; θ) (2) with P (y|T ; θ) defined in (1). 2.3 sequence, we propose to match the mean embeddings of both sequences. In fact, the mean embedding has been proved to be an effective representation for the whole sequence in machine translation (Yang et al., 2019; Wang et al., 2017). Let Vˆtable and Vˆtext be the mean embeddings of a table and the target text embeddings in our Transformerbased model, respectively. A table-target sentence disagreement loss Ldisagree is then defined as Faithfulness Modeling with a Table-Text Disagreement Constraint Loss One key element of our model is to enforce a generated text sequence to be consistent with (or faithful to) the table input. To achieve this, we propose to add some constraints so that a generated text sequence only contains information from the table. Our first idea is inspired by related work in machin"
2020.acl-main.101,H05-1042,0,\N,Missing
2020.acl-main.101,P02-1040,0,\N,Missing
2020.acl-main.101,P09-1011,0,\N,Missing
2020.acl-main.101,W14-3348,0,\N,Missing
2020.acl-main.101,W04-1013,0,\N,Missing
2020.acl-main.101,P17-4012,0,\N,Missing
2020.acl-main.101,P17-1099,0,\N,Missing
2020.acl-main.101,W18-6505,0,\N,Missing
2020.acl-main.101,P19-1600,0,\N,Missing
2020.acl-main.101,W02-0109,0,\N,Missing
2020.acl-main.101,D19-5615,0,\N,Missing
2020.acl-main.101,P18-1026,0,\N,Missing
2020.acl-main.101,D18-1422,0,\N,Missing
2020.acl-main.233,P11-1020,0,0.0143246,"ai et al., 2019). In particular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the gene"
2020.acl-main.233,P19-1285,0,0.11736,"et al. (2019) further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder. Recently, transformers (Vaswani et al., 2017) have proven to be more effective than RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc.), demonstrating superior performance in many sequential modeling tasks (Vaswani et al., 2017; Zhou et al., 2018; Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019). Zhou et al. (2018) first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations. This transformer captioning model is essentially the same as the original transformer (Vaswani et al., 2017) for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the context (i.e., previou"
2020.acl-main.233,W14-3348,0,0.0163482,"r than 100 for video and 20 for text and set the maximum number of video segments to 6 for ActivityNet Captions and 12 for YouCookII. Finally, we build vocabularies based on words that occur at least 5 times for ActivityNet Captions and 3 times for YouCookII. The resulting vocabulary contains 3,544 words for ActivityNet Captions and 992 words for YouCookII. Evaluation Metrics (Automatic and Human) We evaluate the captioning performance at paragraph-level, following (Park et al., 2019; Xiong et al., 2018), reporting numbers on standard metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDErD (Vedantam et al., 2015). Since these metrics mainly focus on whether the generated paragraph matches the ground-truth paragraph, they fail to evaluate the redundancy of these multi-sentence paragraphs. Thus, we follow previous works (Park et al., 2019; Xiong et al., 2018) to evaluate repetition using R@4. It measures the degree of N-gram (N=4) repetition in the descriptions. Besides the automated metrics, we also conduct human evaluations to provide additional comparisons between the methods. We consider two aspects in human evaluation, relevance (i.e., how related is a generated para"
2020.acl-main.233,N19-1423,0,0.511527,"coding process. Park et al. (2019) further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder. Recently, transformers (Vaswani et al., 2017) have proven to be more effective than RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc.), demonstrating superior performance in many sequential modeling tasks (Vaswani et al., 2017; Zhou et al., 2018; Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019). Zhou et al. (2018) first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations. This transformer captioning model is essentially the same as the original transformer (Vaswani et al., 2017) for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the cont"
2020.acl-main.233,D18-1117,0,0.0616968,"visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descriptions. Though individual sentences may precisely describe the corresponding event segments, when put together the sentences of"
2020.acl-main.233,P02-1040,0,0.107056,"6b). We truncate sequences longer than 100 for video and 20 for text and set the maximum number of video segments to 6 for ActivityNet Captions and 12 for YouCookII. Finally, we build vocabularies based on words that occur at least 5 times for ActivityNet Captions and 3 times for YouCookII. The resulting vocabulary contains 3,544 words for ActivityNet Captions and 992 words for YouCookII. Evaluation Metrics (Automatic and Human) We evaluate the captioning performance at paragraph-level, following (Park et al., 2019; Xiong et al., 2018), reporting numbers on standard metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDErD (Vedantam et al., 2015). Since these metrics mainly focus on whether the generated paragraph matches the ground-truth paragraph, they fail to evaluate the redundancy of these multi-sentence paragraphs. Thus, we follow previous works (Park et al., 2019; Xiong et al., 2018) to evaluate repetition using R@4. It measures the degree of N-gram (N=4) repetition in the descriptions. Besides the automated metrics, we also conduct human evaluations to provide additional comparisons between the methods. We consider two aspects in human evaluation, relevance (i"
2020.acl-main.233,P17-1117,1,0.916773,"articular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descr"
2020.acl-main.233,D17-1103,1,0.902834,"articular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descr"
2020.acl-main.233,D19-1514,1,0.865631,"is used as the basis of our approach. Different from RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc) that use recurrent structure to model long-term dependencies, transformer relies on self-attention to learn the dependencies between input words. Transformers have proven to be more efficient and powerful than RNNs, with superior performance in many sequential modeling tasks, including machine translation (Vaswani et al., 2017), language modeling/pre-training (Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019) and multi-modal representation learning (Tan and Bansal, 2019; Chen et al., 2019; Sun et al., 2019). Additionally, Zhou et al. (2018) have shown that a transformer model can generate better captions than the LSTM model. However, transformer architectures are still unable to model history information well. This problem is identified in the task of language modeling as context fragmentation (Dai et al., 2019), i.e., each language segment is modeled individually without knowing its surrounding context, leading to inefficient optimization and inferior performance. To resolve this issue, Transformer-XL (Dai et al., 2019) introduces the idea of recurrence to"
2020.acl-main.444,L18-1544,0,0.0141277,"lations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in"
2020.acl-main.444,D18-1514,0,0.0201036,"d relation extraction. However, trigger identification is perhaps as difficult as relation extraction, and it is labor-intensive to annotate large-scale datasets with triggers. Future research may explore how to identify triggers based on a small amount of human-annotated triggers as seeds (Bronstein et al., 2015; Yu and Ji, 2016). 6 Error Analysis and Limitations Related Work Cross-Sentence Relation Extraction Datasets Different from the sentence-level relation extraction (RE) datasets (Roth and Yih, 2004; Hendrickx et al., 2010; Riedel et al., 2010; Zhang and Wang, 2015; Zhang et al., 2017; Han et al., 2018), in which relations are between two arguments in the same sentence, we focus on cross-sentence RE tasks (Ji et al., 2011; Surdeanu, 2013; Surdeanu and Ji, 2014) and present the first dialogue-based RE dataset, in which dialogues serve as input contexts instead of formally written sentences or documents. 4934 Task style/source of doc # rel cross rate◦ # doc # triples• 4 96 353 75.2 n/a n/a 960,000 101,873 3 million 140,661 881,298 11 million 1 96 15 36 n/a 40.7 n/a 95.6 1,500 5,053 4,991 1,788 2,434 56,354 13,425 8,068 —– distant supervision —– Peng et al. (2017) DocRED (Yao et al., 2019) T-RE"
2020.acl-main.444,S10-1006,0,0.0512395,"Missing"
2020.acl-main.444,N15-1086,0,0.0249294,"in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods, and experimental re"
2020.acl-main.444,D17-1274,0,0.0276444,"Missing"
2020.acl-main.444,W16-3612,0,0.300656,"ank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid and Blanco, 2018; Yang and Choi, 2019). Altogether, we annotate 10,168 relational triples. For each (subject, relation type, object) triple, we also annotate the minimal contiguous text span that most clearly expresses the relation; this may enable researchers to explore relation extraction methods that provide fine-grained explanations along with evidence sentences. For example, the bolded text span “brother” in Table 1 indicates the PER : SIBLINGS relation (R1 and R2) between speaker 2 (S2) and “Frank”. Our analysis of DialogRE indicates that"
2020.acl-main.444,2020.tacl-1.5,0,0.0476439,"Missing"
2020.acl-main.444,C10-2065,0,0.0927781,"Missing"
2020.acl-main.444,K17-1034,0,0.0286461,"tiple sentences; •: not include no-relation argument pairs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et"
2020.acl-main.444,P16-1200,0,0.0219825,"not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dia"
2020.acl-main.444,D18-1360,0,0.0220502,"i et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (S"
2020.acl-main.444,W19-1505,0,0.158706,"irs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2"
2020.acl-main.444,N18-1185,0,0.0438555,"ters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational"
2020.acl-main.444,D19-1069,0,0.0132178,"sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid"
2020.acl-main.444,P09-1113,0,0.143159,"Missing"
2020.acl-main.444,I13-1189,0,0.0607292,"Missing"
2020.acl-main.444,W15-1506,0,0.0549726,"Missing"
2020.acl-main.444,Q17-1008,0,0.0624783,"Missing"
2020.acl-main.444,D14-1162,0,0.0834244,"2 ] are two newly-introduced special tokens. In addition, we define a ˆk (k ∈ {1, 2}) to be [Sk ] if ∃i(si = ak ), and ak otherwise. The modified input sequence to ˆ BERT is [CLS]d[SEP]ˆ a1 [SEP]ˆ a2 [SEP]. In Appendix A.4, we investigate in three alternative input sequences. It is worth mentioning that a modification that does not disambiguate speaker arguments from other arguments performs substantially worse than the above speaker-aware modification. 5 Experiment 5.1 Implementation Details CNN, LSTM, and BiLSTM Baselines: The CNN/LSTM/BiLSTM encoder takes as features GloVe word embeddings (Pennington et al., 2014), mention embeddings, and type embeddings. We assign the same mention embedding to mentions of the same argument and obtain the type embeddings based on named entity types of the two arguments. We use spaCy4 for entity typing. Language Model Fine-Tuning: We use the uncased base model of BERT released by Devlin et al. (2019). We truncate a document when the input sequence length exceeds 512 and fine-tune BERT using a batch size of 24 and a learning rate of 3×10−5 4 https://spacy.io/. for 20 epochs. Other parameters remain unchanged. The embeddings of newly-introduced special tokens (e.g., [S1 ]"
2020.acl-main.444,N18-1202,0,0.0184035,"likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language"
2020.acl-main.444,D19-1005,0,0.0144051,"which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018;"
2020.acl-main.444,W09-2418,0,0.0987858,"Missing"
2020.acl-main.444,E17-1110,0,0.0558289,"Missing"
2020.acl-main.444,D18-1470,0,0.0603447,"Missing"
2020.acl-main.444,W04-2401,0,0.111748,"ROOMMATE and PER : SPOUSE). These experimental results show the critical role of triggers in dialogue-based relation extraction. However, trigger identification is perhaps as difficult as relation extraction, and it is labor-intensive to annotate large-scale datasets with triggers. Future research may explore how to identify triggers based on a small amount of human-annotated triggers as seeds (Bronstein et al., 2015; Yu and Ji, 2016). 6 Error Analysis and Limitations Related Work Cross-Sentence Relation Extraction Datasets Different from the sentence-level relation extraction (RE) datasets (Roth and Yih, 2004; Hendrickx et al., 2010; Riedel et al., 2010; Zhang and Wang, 2015; Zhang et al., 2017; Han et al., 2018), in which relations are between two arguments in the same sentence, we focus on cross-sentence RE tasks (Ji et al., 2011; Surdeanu, 2013; Surdeanu and Ji, 2014) and present the first dialogue-based RE dataset, in which dialogues serve as input contexts instead of formally written sentences or documents. 4934 Task style/source of doc # rel cross rate◦ # doc # triples• 4 96 353 75.2 n/a n/a 960,000 101,873 3 million 140,661 881,298 11 million 1 96 15 36 n/a 40.7 n/a 95.6 1,500 5,053 4,991 1"
2020.acl-main.444,D18-1246,0,0.0131017,"elation argument pairs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldi"
2020.acl-main.444,Q19-1014,1,0.86041,"; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue"
2020.acl-main.444,swampillai-stevenson-2010-inter,0,0.168912,"ialogRE is available at https:// dataset.org/dialogre/. 1 R1 R2 R3 R4 Trigger brother brother none none Relation type per:siblings per:siblings per:alternate names unanswerable Table 1: A dialogue and its associated instances in DialogRE. S1, S2: anoymized speaker of each utterance. Introduction Cross-sentence relation extraction, which aims to identify relations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation"
2020.acl-main.444,W15-4631,0,0.0336045,"). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relati"
2020.acl-main.444,D19-1585,0,0.0200002,"sentation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang"
2020.acl-main.444,P11-1034,0,0.159616,"riples can be mapped to one of the 36 relation types in our relation schema (e.g., HUS BAND, EX - HUSBAND, and WIFE can be mapped to PER : SPOUSE ) except for the remaining relatively rare or implicit relation types such as PROM DATE and GENDER, and KISSED, demonstrating the relation schema we use for annotation is capable of covering most of the important relation types labeled by the encyclopedia community of contributors. On the other hand, the relatively small number of the existing triples and the moderate size of our annotated triples in DialogRE may suggest the low information density (Wang and Liu, 2011) in conversational speech in terms of relation extraction. For example, the average annotated triple per sentence in DialogRE is merely 0.21, compared to other exhaustively annotated datasets ACE (0.73) and KnowledgeNet (Mesquita et al., 2019) (1.44), in which corpora are formal written news reports and Wikipedia articles, respectively. 3.3 Discussions on Triggers As annotated triggers are rarely available in existing relation extraction datasets (Aguilar et al., 2014), the connections between different relation types and trigger existence are under-investigated. Relation Type: In DialogRE, 49"
2020.acl-main.444,W12-1642,1,0.787395,"aloguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods, and experimental results demonstrate that a speaker-aware extension on the best-performing model leads to substantial gains in both the standard and conversational settings."
2020.acl-main.444,D15-1206,0,0.0243389,"9) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom in"
2020.acl-main.444,W19-5923,0,0.0503467,"019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speaker"
2020.acl-main.444,P19-1074,0,0.534776,"guments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research"
2020.acl-main.444,W11-2008,0,0.0380999,"e seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods"
2020.acl-main.444,P16-1005,1,0.830453,"Missing"
2020.acl-main.444,D15-1203,0,0.0273618,"this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previ"
2020.acl-main.444,C14-1220,0,0.253942,"number of practical real-time dialoguebased applications such as chatbots, which would likely require recognition of a relation at its first mention in an interactive conversation. To encourage automated methods to identify the relationship between two arguments in a dialogue as early as possible, we further design a new performance evaluation metric for the conversational setting, which can be used as a supplement to the standard F1 measure (Section 4.1). In addition to dataset creation and metric design, we adapt a number of strong, representative learning-based relation extraction methods (Zeng et al., 2014; Cai et al., 2016; Yao et al., 2019; Devlin et al., 2019) and evaluate them on DialogRE to establish baseline results on the dataset going forward. We also extend the best-performing method (Devlin et al., 2019) among them by letting the model be aware of the existence of arguments that are dialogue participants (Section 4.2). Experiments on DialogRE demonstrate that this simple extension nevertheless yields substantial gains on both standard and conversational RE evaluation metrics, supporting our assumption regarding the critical role of tracking speakers in dialogue-based relation extracti"
2020.acl-main.444,D17-1004,0,0.252438,"rn, supporting our hypothesis that the positions of arguments and triggers may be good indicators for estimating the minimum turns for humans to make predictions. For convenience, we use BERT for the following discussions and comparisons. Ground Truth Argument Types: Methods in Table 5 are not provided with ground truth argument types considering the unavailability of this kind of annotation in practical use. To study the impacts of argument types on DialogRE, we report the performance of four methods, each of which additionally takes as input the ground truth argument types as previous work (Zhang et al., 2017; Yao et al., 2019). We adopt the same baseline for a direct comparison 4933 except that the input sequence is changed. 5.3 In Method 1, we simply extend the original input sequence of BERT (Section 4.2) with newly-introduced special tokens that represent argument types. The input sequence is [CLS]d[SEP]τ1 a1 [SEP]τ2 a2 [SEP], where τi is a special token representing the argument type of ai (i ∈ {1, 2}). For example, given a1 of type PER and a2 of type STRING, τ1 is [PER] and τ2 is [STRING]. In Method 2, we extend the input sequence of BERTS with τi defined in Method ˆ 1 (i.e., [CLS]d[SEP]τ ˆ1"
2020.acl-main.444,C18-1003,0,0.15861,"heebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid and Blanco, 2018; Yang and Choi, 2019). Altogether, we annotate 10,168 relational triples. For each (subject, relation type, object) triple, we also annotate the minimal contiguous text span that most clearly expresses the relation; this may enable researchers to explore relation extraction methods that provide fine-grained explanations along with evidence sentences. For example, the bolded text span “brother” in Table 1 indicates the PER : SIBLINGS relation (R1 and R2) between speaker 2 (S2) and “Frank”. Our analysis of DialogRE indicates that the supporting text for most (approximat"
2020.acl-main.444,D12-1062,0,\N,Missing
2020.acl-main.444,P15-2061,0,\N,Missing
2020.acl-main.444,doddington-etal-2004-automatic,0,\N,Missing
2020.acl-main.444,P16-1072,0,\N,Missing
2020.acl-main.444,catizone-etal-2010-using,0,\N,Missing
2020.acl-main.444,K17-1023,0,\N,Missing
2020.acl-main.444,S18-1007,0,\N,Missing
2020.acl-main.444,N18-1075,0,\N,Missing
2020.acl-main.444,N19-1423,0,\N,Missing
2020.acl-main.444,W14-2907,0,\N,Missing
2020.acl-main.482,D13-1135,0,0.318673,"Missing"
2020.acl-main.482,P16-1074,0,0.187763,"Missing"
2020.acl-main.482,K17-1023,0,0.0195992,"us stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,W16-3612,0,0.0368814,"Missing"
2020.acl-main.482,D10-1062,0,0.0193741,"o pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recov"
2020.acl-main.482,N19-1423,0,0.547208,"as a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their"
2020.acl-main.482,P11-1081,0,0.551431,"Missing"
2020.acl-main.482,D10-1086,0,0.525685,"Missing"
2020.acl-main.482,P17-1010,0,0.395405,"over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,P19-1441,0,0.0226198,"t al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the supervised data of several related tasks to achieve further improvements. In particular, Liu et al. (2019) utilize this framework to jointly solve GLUE tasks (Wang et al., 2019). But their experiments show that multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery,"
2020.acl-main.482,C96-2137,0,0.0827163,"Missing"
2020.acl-main.482,D16-1264,0,0.655517,"rom the supervised data of both tasks. As the result, we enjoy the benefit of more supervised training data. To improve the robustness of heterogeneous training and introduce more supervision, we introduce zero pronoun detection, a common sub-task for both ZP resolution and recovery. Zero pronoun detection is a binaryclassification task aiming to detect whether a word space has a dropped pronoun. We consider ZP recovery as a sequence labeling task, regarding whether each word space has a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both t"
2020.acl-main.482,N15-1052,0,0.211097,"Missing"
2020.acl-main.482,Q19-1016,0,0.0292498,"Missing"
2020.acl-main.482,C08-1097,0,0.354952,"Missing"
2020.acl-main.482,Q19-1014,1,0.87841,"Missing"
2020.acl-main.482,W16-4615,0,0.0407466,"Missing"
2020.acl-main.482,C18-1002,0,0.192729,"Missing"
2020.acl-main.482,N16-1113,0,0.219648,"t helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore any dropped pronouns f"
2020.acl-main.482,P13-1081,0,0.361222,"tion, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore an"
2020.acl-main.482,N19-1095,0,0.0207607,"Missing"
2020.acl-main.482,P15-2051,0,0.249019,"at multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al.,"
2020.acl-main.482,D17-1135,0,0.16016,"Missing"
2020.acl-main.603,P19-1424,0,0.0214896,"erest in MRC tasks that require the understanding of both questions and reference documents (Trischler et al., 2017; Rajpurkar et al., 2018; Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2018; Xu et al., 2019). Recent studies on pre-trained language models (Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo et al., 2018) or select a set of sentences as input based on the given questions (Hewlett et al., 2017; Min et al., 2018; Lin et al., 2018). However, they mainly focus on tasks in which most of the answers to given questions are formed by a single informative sentence. These previous approaches are less applicable to deal with those complicated questions that demand cross-sentences reasoning or have much lexical variability from their lengthy documents. 6 Con"
2020.acl-main.603,D18-1241,0,0.0697518,"MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer layers that only allow fixed-length (e.g., 512) inputs. However, it is often the case that input sequences exceed the length constraint, e.g., documents in the TriviaQA dataset (Joshi et al., 2017) contain 2,622 tokens on average. Some conversational MRC datasets such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) often go beyond the length limit as we may need to incorporate previous questions as well as relatively long documents into the input to answer the current question. To deal with long text inputs, a commonly used approach firstly chunks the input text into equallyspaced segments, secondly predicts the answer for each individual segment, and finally ensembles the answers from multiple segments (Devlin et al., 2019). However, there are two major limitations of this approach: first, a predetermined large stride size for chunking may result in incomplete answers, and we observe that models are mo"
2020.acl-main.603,P17-1147,0,0.0427691,"he document (in most cases) or instead generate an abstractive answer to answer the question. There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer layers that only allow fixed-length (e.g., 512) inputs. However, it is often the case that input sequences exceed the length constraint, e.g., documents in the TriviaQA dataset (Joshi et al., 2017) contain 2,622 tokens on average. Some conversational MRC datasets such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) often go beyond the length limit as we may need to incorporate previous questions as well as relatively long documents into the input to answer the current question. To deal with long text inputs, a commonly used approach firstly chunks the input text into equallyspaced segments, secondly predicts the answer for each individual segment, and finally ensembles the answers from multiple segments (Devlin et al., 2019). However, there are two major limitations of this ap"
2020.acl-main.603,Q18-1023,0,0.0634293,"Missing"
2020.acl-main.603,P18-1161,0,0.0376281,"2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo et al., 2018) or select a set of sentences as input based on the given questions (Hewlett et al., 2017; Min et al., 2018; Lin et al., 2018). However, they mainly focus on tasks in which most of the answers to given questions are formed by a single informative sentence. These previous approaches are less applicable to deal with those complicated questions that demand cross-sentences reasoning or have much lexical variability from their lengthy documents. 6 Conclusion In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning. We also add a recurrent mechanism to allow the information to flow across s"
2020.acl-main.603,N19-1423,0,0.526754,"nn et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Zhang et al., 2018; Koˇcisk`y et al., 2018). Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document (in most cases) or instead generate an abstractive answer to answer the question. There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer layers that only allow fixed-length (e.g., 512) inputs. However, it is often the case that input sequences exceed the length constraint, e.g., documents in the TriviaQA dataset (Joshi et al., 2017) contain 2,622 tokens on average. Some conversational MRC datasets such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) often go beyond the length limit as we may need to incorporate previous questions as well as relatively long documents into the input to answer the current question. To deal with long text inputs, a commonly used a"
2020.acl-main.603,P18-1160,0,0.02077,"(Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo et al., 2018) or select a set of sentences as input based on the given questions (Hewlett et al., 2017; Min et al., 2018; Lin et al., 2018). However, they mainly focus on tasks in which most of the answers to given questions are formed by a single informative sentence. These previous approaches are less applicable to deal with those complicated questions that demand cross-sentences reasoning or have much lexical variability from their lengthy documents. 6 Conclusion In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning. We also add a recurrent mechanism to allow the informati"
2020.acl-main.603,N19-1320,1,0.675176,"88 Train Avg tokens # 352 516 2,622 Max token # 1,323 2,310 5,839 Question # 7,983 7,354 7,993 Validation Avg tokens # 341 576 2,630 Max token # 1,037 2,146 6,690 Table 1: Statistics of the CoQA, QuAC and TriviaQA datasets. We report the number of sub-tokens generated by the BERT tokenizer. relies on the accumulated reward R(s0 , a0 ) of the next state when the answer is not available in the current segment. The chunking policy network can be trained by maximizing the expected accumulated reward (as shown in Eq. (13)) through the policy gradient algorithm (Williams, 1992; Sutton et al., 2000; Gong et al., 2019). J = Epact (a |s) [R(s, a)]. (13) To be consistent with the notations in answer extraction and chunking scorer modules, we denote the loss function of chunking policy as Lcp , which is the negative expected accumulated reward J in Eq. (13): Lcp = −J. Thus, the stochastic gradient of Lcp over a mini-batch of data B is given by: X ∇Lcp = − ∇ log pact (a |s)R(s, a), (14) (s,a)∈B where pact (a |s) is the chunking policy in Eq. (8). Training procedure. The overall training loss L is an sum of all three losses: L = Lans + Lcs + Lcp . In addition, we initialize the bottom representation layers with"
2020.acl-main.603,D17-1214,0,0.0194308,"rained language models (Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo et al., 2018) or select a set of sentences as input based on the given questions (Hewlett et al., 2017; Min et al., 2018; Lin et al., 2018). However, they mainly focus on tasks in which most of the answers to given questions are formed by a single informative sentence. These previous approaches are less applicable to deal with those complicated questions that demand cross-sentences reasoning or have much lexical variability from their lengthy documents. 6 Conclusion In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning. We also add a recurrent mechanism to a"
2020.acl-main.603,N18-4017,0,0.0169476,"n an answer. The selected sentences are then concatenated and fed to the BERTLarge model for answer extraction. For conversational datasets CoQA and QuAC, since a question is correlated with its previous questions within the same conversation, we apply the sentence selector to select sentences based on the current question alone or the concatenation of the previous questions and the current question. We only use the current question as the input to the sentence selector for TriviaQA, which does not involve any conversational history. The sentence selector we used in experiments is released by Htut et al. (2018). Evaluation Metric. The main evaluation metric is macro-average word-level F1 score. We compare each prediction with the reference answer. Precision is defined by the percentage of predicted answer tokens that appear in the reference answer, and recall is the percentage of reference answer tokens captured in the prediction. F1 score is the harmonic mean of the precision and recall. When multiple reference answers are provided, the maximum F1 score is used for evaluation. 3.3 Results on CoQA and QuAC We first perform experiments on two conversational MRC datasets, CoQA and QuAC. Setting. We pe"
2020.acl-main.603,P18-2124,0,0.0243774,"clude more right contexts and generates the second chunk. The stride size is a bit large since the answer is close to the left boundary of the second segment. The model then moves back to the left by 16 tokens and obtains its third segment. The chunking scorer assigns the three segments with the scores 0.24, 0.87, and 0.90, respectively. It suggests that the model considers the third segment as the most informative chunk in answer selection. 5 Related Work There is a growing interest in MRC tasks that require the understanding of both questions and reference documents (Trischler et al., 2017; Rajpurkar et al., 2018; Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2018; Xu et al., 2019). Recent studies on pre-trained language models (Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens ("
2020.acl-main.603,D16-1264,0,0.0656281,"he effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions. 1 Introduction Teaching machines to read, process, and comprehend natural language is a coveted goal of machine reading comprehension (MRC) problems ∗ The work was performed during an internship at Tencent AI Lab, Bellevue, WA, USA. † The work was performed when Yelong Shen was at Tencent AI Lab, Bellevue, WA, USA. (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Zhang et al., 2018; Koˇcisk`y et al., 2018). Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document (in most cases) or instead generate an abstractive answer to answer the question. There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a"
2020.acl-main.603,D18-1233,0,0.0145617,"s and generates the second chunk. The stride size is a bit large since the answer is close to the left boundary of the second segment. The model then moves back to the left by 16 tokens and obtains its third segment. The chunking scorer assigns the three segments with the scores 0.24, 0.87, and 0.90, respectively. It suggests that the model considers the third segment as the most informative chunk in answer selection. 5 Related Work There is a growing interest in MRC tasks that require the understanding of both questions and reference documents (Trischler et al., 2017; Rajpurkar et al., 2018; Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2018; Xu et al., 2019). Recent studies on pre-trained language models (Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo"
2020.acl-main.603,W17-2623,0,0.153064,"proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions. 1 Introduction Teaching machines to read, process, and comprehend natural language is a coveted goal of machine reading comprehension (MRC) problems ∗ The work was performed during an internship at Tencent AI Lab, Bellevue, WA, USA. † The work was performed when Yelong Shen was at Tencent AI Lab, Bellevue, WA, USA. (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Zhang et al., 2018; Koˇcisk`y et al., 2018). Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document (in most cases) or instead generate an abstractive answer to answer the question. There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer lay"
2020.acl-main.603,N19-4013,0,0.0812659,"k was performed during an internship at Tencent AI Lab, Bellevue, WA, USA. † The work was performed when Yelong Shen was at Tencent AI Lab, Bellevue, WA, USA. (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Zhang et al., 2018; Koˇcisk`y et al., 2018). Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document (in most cases) or instead generate an abstractive answer to answer the question. There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer layers that only allow fixed-length (e.g., 512) inputs. However, it is often the case that input sequences exceed the length constraint, e.g., documents in the TriviaQA dataset (Joshi et al., 2017) contain 2,622 tokens on average. Some conversational MRC datasets such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) often go beyond the length limit as we may nee"
2020.acl-main.603,N19-1241,0,0.0223641,"Missing"
2020.acl-main.603,P17-1172,0,0.0388222,"Missing"
2020.acl-main.603,P18-1078,0,\N,Missing
2020.acl-main.603,Q19-1016,0,\N,Missing
2020.acl-main.712,P17-2021,0,0.0210526,"lignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist tra"
2020.acl-main.712,P19-1080,0,0.0229762,"e girl wants the boy to go”, which conveys an opposite meaning to the AMR graph. In particular, this can be very likely if “the girl wants” appears much more frequent than “the boy wants” in the training corpus. This is a very important issue, because of its wide existence across many neural graph-to-text 7987 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may no"
2020.acl-main.712,W13-2322,0,0.0245618,"veness of our approach over a state-of-the-art baseline. Our code is available at http://github.com/ Soistesimmer/AMR-multiview. 1 boy ARG2 ARG1 eat-01 Above the Veil ARG0 followedBy lunch precededBy girl mod (a) beautiful Into Battle Aenir (b) Figure 1: (a) An AMR graph meaning “The boy wants the beautiful girl to eat lunch with him.”, and (b) A knowledge graph carrying the meaning “Above the Veil is an Australian novel and the sequel to Aenir. It was followed by Into the Battle.” Many text generation tasks take graph structures as their inputs, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Knowledge Graph (KG) and database tables. For example, as shown in Figure 1(a), AMR-to-text generation is to generate a sentence that preserves the meaning of an input AMR graph, which is composed by a set of concepts (such as “boy” and “want-01”) and their relations (such as “:ARG0” and “:ARG1”). Similarly, as shown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks main"
2020.acl-main.712,P18-1026,0,0.0217616,"hown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to prod"
2020.acl-main.712,N19-1223,0,0.0333895,"ormation of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) exte"
2020.acl-main.712,N19-1366,0,0.0480077,"le relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction l"
2020.acl-main.712,W14-3348,0,0.0263306,"One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pourdamghani et al., 2014). We use this da"
2020.acl-main.712,W19-8652,0,0.0315152,"Missing"
2020.acl-main.712,W18-6539,0,0.033398,"Missing"
2020.acl-main.712,S16-1186,0,0.0222153,"to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN"
2020.acl-main.712,W17-3518,0,0.413529,"y” in Figure 1(a)) contains a pair of entities and their relation. As the next step, the alignments between graph nodes and target words are generated to ground this view into the target sentence for reconstruction. Our second view is the linearization of each input graph produced by depth-first graph traversal, and this view is reconstructed token-by-token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effective"
2020.acl-main.712,Q19-1019,0,0.023917,"tion is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucia"
2020.acl-main.712,N19-1235,0,0.0208537,"iew focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (201"
2020.acl-main.712,D19-6310,0,0.299588,"res, only the most frequent 20K are kept, while the rest are mapped into a special UNK feature.1 X i2[1..N ] p(yi |si ; ✓), (5) where ✓ represents all model parameters. R2 | (3) where W Q , W K and W R2 are model parameters, and dh denotes the encoder-state dimension. The encoder adopts L self-attention layers and H L = L (hL 1 . . . h|V |) represents the concatenated top-layer hidden states of the encoder, which will be used in attention-based decoding. 4 Multi-View Autoencoding Losses Figure 2 visualizes the training framework using our multi-view autoencoding losses, where the 1 Zhu et al. (2019) also mentions other (such as CNN-based or self-attention-based) alternatives to calculate ij . While the GPU memory consumption of these alternatives is a few times more than our baseline, ours actually shows a comparable performance. 7989 View 1: triple relations ARG2 ARG1 want-01 ARG1 ARG0 boy ARG2 ARG1 lunch ARG0 mod ARG0 ARG1 The boy wants the beautiful girl to eat lunch with him eat-01 ARG0 girl Encoder Attention Language modeling loss Decoder mod View 2: linearized graph beautiful want :ARG0 boy :ARG1 eat ( :ARG0 (girl :mod beautiful) :ARG1 lunch :ARG2 boy) Figure 2: The training framew"
2020.acl-main.712,P17-1089,0,0.0189482,". . . sN ) denotes the concatenated states for the target sentence (Equation 4), and the loss for reconstructing this view is defined as the negative log-likelihood for the linearized graph: X lauto2 = log p(xi |ti ; ✓), (15) i2[1..M ] (12) where [x] in the subscript represents choosing the x-th item from the corresponding vector. As the final step, the loss for reconstructing this view is defined as the negative log-likelihood of all target arcs E 0 (the grounded triples from E): X lauto1 = log p(yj , l|yi ) (13) (yj ,l,yi )2E 0 4.2 infer the original graph structure. Besides, previous work (Iyer et al., 2017; Konstas et al., 2017) has shown the effectiveness of generating linearized graphs as sequences for graph parsing, which also confirms our observation. Given a linearized graph represented as a sequence of tokens x1 , . . . , xM , where each token xi can be a graph node, a edge label or a inserted bracket, we adopt another standard Transformer decoder (SADecoderg ) to produce the sequence: Loss 2: Reconstructing Linearized Graphs with a Transformer Decoder As a supplement to our first loss for reconstructing the local information of each grounded triple, we introduce the second loss for predi"
2020.acl-main.712,P19-1236,1,0.815011,"or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for traini"
2020.acl-main.712,N19-1238,0,0.067091,"Missing"
2020.acl-main.712,P17-1014,0,0.415658,"and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “t"
2020.acl-main.712,D18-1183,0,0.0602147,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,D18-1264,0,0.0957235,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,W18-6501,0,0.143887,"oder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (recon"
2020.acl-main.712,W03-3017,0,0.220221,"al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003) to convert tree parsing into the prediction of transition actions, while we study reconstructing graphs, where there is no common parsing algorithm for all graph types. 7988 (Liu et al., 2018b) and sentiment analysis (Rei and Søgaard, 2019). Since input reconstruction is not intuitively related to these tasks, the autoencoding loss only serves as more training signals. Different from these efforts, we leverage autoencoding loss as a means to preserve input knowledge. Besides, we study reconstructing complex graphs, proposing a general multi-view approach for this goal. 3 Base: Structure-Aware"
2020.acl-main.712,P02-1040,0,0.106933,"ighly sensitive to the input order. One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pou"
2020.acl-main.712,D14-1048,0,0.178545,"rds of the entity in order to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder"
2020.acl-main.712,P17-1194,0,0.169993,"nal Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may not even exist. To make our approach general across different types of graphs, we propose to reconstruct different views of each input graph (rather than the original graph), where each view highlights one aspect of the graph and is easy to produce. Then through multi-task learning, the autoencoding losses of all views are back-propagated to the whole model so that th"
2020.acl-main.712,D19-1314,0,0.0725088,"a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and re"
2020.acl-main.712,P16-1162,0,0.042332,"than LDC2015E86, we may conclude that the problem of dropping input information may not be effectively reduced by simply adding more supervised data, and as a result, our approach can still be effective on a larger dataset. This conclusion can also be confirmed by comparing the gains of our approach on both AMR datasets regarding BLEU score (2.3 vs 2.5). 5.9 Main Results on WebNLG Table 6 shows the comparison of our results with previous results on the WebNLG testset. ADAPT (Gardent et al., 2017) is based on the standard encoder-decoder architecture (Cho et al., 2014) with byte pair encoding (Sennrich et al., 2016), and it was the best system of the challenge. GCNEC (Marcheggiani and Perez-Beltrachini, 2018) is a recent model using a graph convolution network (Kipf and Welling, 2017) for encoding KGs. Our baseline shows a comparable performance with the previous state of the art. Based on this baseline, applying either loss leads to a significant improvement, and their combination brings a gain of more than 2 BLEU points. Although the baseline already achieves a very high BLEU score, yet the gains on this task are still comparable with those on AMR-to-text generation. This observation may imply that the"
2020.acl-main.712,Q19-1002,1,0.855603,"riments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple record"
2020.acl-main.712,P18-1150,1,0.905082,", KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentence"
2020.acl-main.712,N18-1106,0,0.0225874,"Missing"
2020.acl-main.712,P18-1151,0,0.0378542,"token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target"
2020.acl-main.712,D17-1129,0,0.0198313,"entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN , the representation"
2020.acl-main.712,2020.tacl-1.2,0,0.0576841,"orts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, which conveys an opposite meaning to the AM"
2020.acl-main.712,D18-1509,0,0.0228232,"struct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003"
2020.acl-main.712,D17-1239,0,0.0254359,"2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple records that fit into several fields.We study a more challenging topic on how to reconstruct a complex graph structure rather than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word."
2020.acl-main.712,P17-1065,0,0.0213542,"ur model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based alg"
2020.acl-main.712,D18-1112,1,0.768306,"hts the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wisem"
2020.acl-main.712,D19-1548,0,0.667496,"ns (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, whic"
2020.ccl-1.50,N18-2028,0,0.0603936,"Missing"
2020.ccl-1.68,D18-1289,0,0.0356268,"Missing"
2020.ccl-1.68,D14-1179,0,0.0127451,"Missing"
2020.ccl-1.68,W11-2308,0,0.0755084,"Missing"
2020.ccl-1.68,C10-1062,0,0.0895124,"Missing"
2020.ccl-1.68,D14-1181,0,0.00558459,"Missing"
2020.ccl-1.68,P16-2034,0,0.0917059,"Missing"
2020.ccl-1.68,P05-1065,0,0.243017,"Missing"
2020.ccl-1.68,2020.bea-1.1,0,0.0374294,"Missing"
2020.ccl-1.68,W12-2019,0,0.0602852,"Missing"
2020.ccl-1.68,W18-0535,0,0.0319479,"Missing"
2020.ccl-1.68,W16-0502,0,0.0364702,"Missing"
2020.ccl-1.68,Q15-1021,0,0.0271453,"Missing"
2020.ccl-1.68,N16-1174,0,0.129948,"Missing"
2020.emnlp-main.509,2020.acl-main.175,0,0.0198387,"he original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial importance to tasks such as producing informative snippets from search outputs (Kaisser et al., 2008), summarizing viewpoints in opinionated text (Paul et al., 2010; Amplayo and Lapata, 2020), and annotating website privacy policies to assist users in answering important questions (Sadeh et al., 2013). Determining the most appropriate textual unit for highlighting, however, has been an understudied problem. Extractive summarization selects whole sentences from documents; a sentence can contain 20 to 30 words on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance betwee"
2020.emnlp-main.509,P18-1063,0,0.0319353,"d as winter storms hit during one of the year’s busiest travel weeks. Self-Contained Segments • Some interstates are closed • hundreds of flights have been canceled as winter storms hit • flights have been canceled as winter storms hit • winter storms hit during one of the year’s busiest travel weeks Non-Self-Contained Segments • Some interstates are • closed and hundreds of flights have been • been canceled as winter storms hit during one of • hit during one of the year’s Table 2: Examples of self-contained and non-self-contained segments extracted from a document sentence. Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Laban et al., 2020). With greater flexibility comes increased risk. Failing to accurately convey the original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial"
2020.emnlp-main.509,P16-1046,0,0.0775281,"Missing"
2020.emnlp-main.509,P19-1098,1,0.890225,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,D19-5412,1,0.914146,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,P19-1102,0,0.197446,"onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (Over and Yen, 2004) and TAC-08/09/10/11 datasets (Dang and Owczarzak, 2008), which contain 60/5"
2020.emnlp-main.509,P19-1213,0,0.0364517,"Missing"
2020.emnlp-main.509,C10-1039,0,0.213358,"t summarization data by maximizing log-likelihood. At each iteration, we project the L-ensemble onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (O"
2020.emnlp-main.509,A83-1023,0,0.238369,"2019). We next discuss our method in greater detail. 3 Our Method We present a new method to identify self-contained segments, then select important and non-redundant segments to form a summary, as text fragments containing incomplete and disorganized information are hardly successful summary highlights. 3.1 Self-Contained Segments A self-contained segment is, in a sense, a miniature sentence. Any text segment containing incomplete or ungrammatical constructions is incomprehensible to humans. Table 2 presents examples of selfcontained and non-self-contained segments. Since its very inception (Vladutz, 1983), the concept of “semantically self-contained segment” has not been sufficiently examined in the literature and lacks an universal definition. We assume in this paper that a self-contained segment shall conform to certain syntactic validity constraints and there exists only weak dependencies between words that belong to the segment and those do not. The automatic identification of self-contained segments requires more than segmentation or parsing sentences into tree structures (Dozat and Manning, 2018). Self-contained segments do not necessarily correspond to constituents of the tree and furth"
2020.emnlp-main.509,P10-1058,0,0.0162456,"on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance between the amount and quality of emphasized content. It is best for highlighted segments to remain selfcontained. In fact, multiple partially-overlapping and self-contained segments can exist in a sentence, as illustrated in Table 2. Identifying self-contained segments has not been thoroughly investigated in previous studies. Woodsend and Lapata (2010) propose to generate story highlights by selecting and combining phrases. Li et al. (2016) explore elemen6283 tary discourse units generated using an RST parser as selection units. Spala et al. (2018) present a crowdsourcing method for workers to highlight sentences and compare systems. Arumae et al. (2019) propose to align human abstracts and source articles to create ground-truth highlight annotations. Importantly, and distinguishing our work from earlier literature, we make a first attempt to generate self-contained highlights, drawing on the successes of deep contextualized representations"
2020.emnlp-main.537,2021.ccl-1.108,0,0.186468,"Missing"
2020.emnlp-main.537,P14-5010,0,0.00441807,"Missing"
2020.emnlp-main.537,D18-1191,0,0.090127,"Missing"
2020.emnlp-main.537,D19-1605,0,0.0631344,"ng. One important factor that contributes to this difficulty is coreference and information omission, where mention is dropped or replaced by a pronoun for simplicity. These phenomena dramatically introduce the requirements for long-distance reasoning, as they frequently occurred in our daily conversations, especially in pro-drop languages like Chinese and Japanese. To tackle these problems, sentence rewriting was introduced to ease the burden of dialogue models by simplifying the multi-turn dialogue modeling into a single-turn problem. Several approaches (Su et al., 2019; Zhang et al., 2019; Elgohary et al., 2019) have been proposed to address the rewriting task. Conceptually, these models follow the conventional encoder-decoder architecture that first encodes the dialogue context into a distributional representation and then decodes it to the rewritten utterance. Their decoders mainly use global attention methods that attends to all words in the dialogue context without prior focus, which may result in inaccurate concentration on some dispensable words. We also observe that the accuracy of their models significantly decreases when working on long dialogue contexts. This observation is expected since i"
2020.emnlp-main.537,P19-1369,0,0.140627,"Missing"
2020.emnlp-main.537,P17-1061,0,0.0246771,"ho did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous stateof-the-art systems. Utterance 2 Utterance 3 Utterance 30 需要粤语 (I may need Cantonese.) 粤语ARG0 是普通话ARG1 吗 (Is Cantonese Mandarin ?) 不算predicate 吧 (Maybe Not.) 粤语不算普通话吧 (Cantonese may be not Mandarin.) Table 1: One example of multi-turn dialogue. The goal of dialogue rewriting is to rewrite utterance 3 into 30 . Introduction Recent research (Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2017; Zhao et al., 2017; Shao et al., 2017) on dialogue generation has been achieving impressive progress for making singleturn responses, while producing coherent multiturn replies still remains extremely challenging. One important factor that contributes to this difficulty is coreference and information omission, where mention is dropped or replaced by a pronoun for simplicity. These phenomena dramatically introduce the requirements for long-distance reasoning, as they frequently occurred in our daily conversations, especially in pro-drop languages like Chinese and Japanese. To tackle these problems, sentence rewr"
2020.emnlp-main.76,P17-2061,0,0.0657001,"Missing"
2020.emnlp-main.76,P05-1066,0,0.263519,"Missing"
2020.emnlp-main.76,N19-1312,1,0.860959,"nce. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the"
2020.emnlp-main.76,P16-1014,0,0.0417886,"Missing"
2020.emnlp-main.76,P15-1001,0,0.160488,"ent frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models hav"
2020.emnlp-main.76,W18-5712,0,0.0629753,"Missing"
2020.emnlp-main.76,D13-1176,0,0.243187,"Missing"
2020.emnlp-main.76,kocmi-bojar-2017-curriculum,0,0.0399389,"divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity."
2020.emnlp-main.76,D18-1149,0,0.0285784,"onsists of 1.25M sentence pairs from LDC corpora which has 27.9M Chinese words and 34.5M English words, respectively 2 . The data set MT02 was used as validation and MT03, MT04, MT05, MT06, MT08 were used for the test. We tokenized and lowercased English sentences using the Moses scripts3 , and segmented the Chinese sentences with the Stanford Segmentor4 . The two sides were further segmented into subword units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 30K merge operations separately. EN→RO. We used the preprocessed version of the WMT2016 English-Romanian dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and news-test 2016 for the test. The two languages shared the same vocabulary generated with 40K merge operations of BPE. EN→DE. The training data is from WMT2016 which consists of about 4.5M sentences pairs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our meth"
2020.emnlp-main.76,2015.iwslt-evaluation.11,0,0.0427153,"airs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our method is further trained based on the pre-trained model at a low learning rate, we also trained another baseline model following the same procedures as our methods have except that all the target tokens share equal weights in the objective, denoted as Baseline-FT. • Fine Tuning (Luong and Manning, 2015). This model was first trained with all the training sentence pairs and then further trained with sentences containing more low-frequency tokens. To filter out sentences containing more low-frequency tokens, the method in Platanios et al. (2019) was adopted as our judging metric with a small modification: Systems We used the open-source toolkit called Fairseqpy (Edunov et al., 2017) released by Facebook as our Transformer system. 2 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 3 http://www.statmt.org/moses/ 4 https://nlp.stan"
2020.emnlp-main.76,P16-1100,0,0.0343734,"Missing"
2020.emnlp-main.76,P15-1002,0,0.168608,"s appear with different frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word b"
2020.emnlp-main.76,J14-3004,0,0.0273182,"al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we f"
2020.emnlp-main.76,P02-1040,0,0.106664,"ve (Equation 1), where all the target tokens have the same training weights. Then the model was further trained with the adaptive objective at a low learning rate. The weights were produced by the Exponential form (Equation 8). k) For computing stability, we used Count(y Cmedian instead of Count(yk ) in the weighting function, where Cmedian is the median of the token frequency. • Our K2. This system was trained following the same procedure as system Our Exp except that the training weights were produced by the Chi-Square form (Equation 9). The translation quality was evaluated by 4-gram BLEU (Papineni et al., 2002) with the multi-bleu.pl script. Besides, we used beam search with a beam size of 4 and a length penalty of 0.6 during the decoding process. 4.3 Hyperparameters There are two hyperparameters in our weighting functions, A and T. In our experiments, we fixed A to narrow search space and the overall weight range is [1, e]. We tuned another hyperparameter T on the validation data sets under the criteria proposed in section 3.2. The results are shown in Table 3. According to the results, the best hyperparameters differed across different language pairs. It is affected by the proportion of low-freque"
2020.emnlp-main.76,C18-1265,0,0.0163854,"et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al.,"
2020.emnlp-main.76,W19-6622,0,0.043297,"slation of the rare words with the help of the memory network or the pointer network (Zhao et al., 2018; Pham et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work als"
2020.emnlp-main.76,W18-2712,0,0.0299546,"Missing"
2020.emnlp-main.76,I08-2084,0,0.00895525,"16; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the"
2020.emnlp-main.76,N19-1119,0,0.109348,"Missing"
2020.emnlp-main.76,P16-1162,0,0.864739,"rocessing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table 1 shows. Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies. It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the tra"
2020.emnlp-main.76,D18-1510,1,0.904744,"Missing"
2020.emnlp-main.76,D17-1155,0,0.019346,"er- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then pro"
2020.emnlp-main.76,2020.acl-main.278,0,0.035646,"based methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next,"
2020.emnlp-main.76,C18-1269,0,0.017616,"ng to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic cri"
2020.emnlp-main.76,P19-1426,1,0.880268,"Missing"
2020.emnlp-main.76,D18-1036,0,0.0674838,"ance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table"
2020.semeval-1.31,W13-3520,0,0.0951164,"Missing"
2020.semeval-1.31,D18-1269,0,0.0350646,"and binary prediction respectively. 1 Introduction Lexical entailment (LE) refers to the hyponymy-hypernymy relation, also known as TYPE-OF, or IS-A, which is a fundamental asymmetric lexical relation (Vuli´c et al., 2017). It is a basic requirement for tasks like Question Answering (QA) and Recognizing Textual Entailment (RTE). And more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts (Upadhyay et al., 2018). Cross-lingual LE recognition is crucial to tasks such as recognizing cross-lingual textual entailment (Conneau et al., 2018) and machine translation (Pad´o et al., 2009). Predicting binary and graded scores for multilingual and cross-lingual lexical entailment is the task of SemEval 2020 Task 2 (Glavaˇs et al., 2020). There are two subtasks. Subtask A is to predict binary or graded LE on a monolingual pair of words, e.g., (building, construction) and the subtask is in six multiple languages (i.e., English, German, Italian, Croatian, Turkish, Albanian). Subtask B, predicting cross-lingual LE, gives a pair of words in two different languages with prefix, e.g., (en dinosaur, de kreatur1 ). There are 15 cross-lingual p"
2020.semeval-1.31,P19-1476,0,0.128851,"Missing"
2020.semeval-1.31,2020.semeval-1.2,0,0.147018,"Missing"
2020.semeval-1.31,L18-1550,0,0.0200138,"we simply transform the graded score into the binary label, using a binarization threshold t. If ILE (x, y) < t, we predict that the LE relation holds between two given concepts. 3 Experiments 3.1 Experimental Setup Word Embedding. The English word embedding is the same with Vuli´c and Mrkˇsi´c (2018), which are Skip-Gram with Negative Sampling (SGNS - BOW2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou’ et al., 2013). And for the rest of the required languages, we use word embeddings trained on Common Crawl and Wikipedia using FASTTEXT (Grave et al., 2018). All the vectors are 300-dim. For all languages except English, we first shrink the input vector spaces according to word frequency lists that contains 50,000 words4 . This is to make sure our model works smoothly and fast. However, this raises a problem that some words may get word embeddings in the original larger vector space whereas not in the reduced space. Also, we notice that there are some multiword expressions in the datasets, e.g., macchina per scrivere (typewritter in English), and they may not get the corresponding word embeddings either. To address these problems, we conclude in"
2020.semeval-1.31,W19-4310,0,0.111211,"Missing"
2020.semeval-1.31,P14-2050,0,0.0411937,"+ (5) kxk + kyk x and y represent the vectors of any two words x and y in one subtask. We then normalise the results of the function to a range of (0,6) as a requirement. And for binary detection, we simply transform the graded score into the binary label, using a binarization threshold t. If ILE (x, y) < t, we predict that the LE relation holds between two given concepts. 3 Experiments 3.1 Experimental Setup Word Embedding. The English word embedding is the same with Vuli´c and Mrkˇsi´c (2018), which are Skip-Gram with Negative Sampling (SGNS - BOW2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou’ et al., 2013). And for the rest of the required languages, we use word embeddings trained on Common Crawl and Wikipedia using FASTTEXT (Grave et al., 2018). All the vectors are 300-dim. For all languages except English, we first shrink the input vector spaces according to word frequency lists that contains 50,000 words4 . This is to make sure our model works smoothly and fast. However, this raises a problem that some words may get word embeddings in the original larger vector space whereas not in the reduced space. Also, we notice that there are some multiw"
2020.semeval-1.31,N15-1100,0,0.0232177,"pairs of synonym and antonym relations are included as symmetric resources and concepts of IsA relation are regarded as 3 https://github.com/artetxem /vecmap https://github.com/hermitdave/FrequencyWords/tree/master/content/2016 5 https://github.com/commonsense/conceptnet5/wiki/Downloads 4 258 Figure 3: Different situations of loading input word embeddings asymmetric LE constraints. The number of constraints for each language is displayed in Table 1a. And for English, the other part is the same set as LEAR (Vuli´c and Mrkˇsi´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), and asymmetric LE constraints are also extracted from WordNet. We add these 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs into English external lexical constraints. Language Antonyms Synonyms EN DE IT TR HR SQ 10108 1247 675 451 106 55 48443 29699 4992 1048 703 298 LE pairs 46068 21807 1395 107 57 6 Total Language Antonyms Synonyms 104619 52753 7062 1606 866 359 EN DE IT HR TR SQ 187258 68801 63121 38665 51110 21542 593702 296700 252227 169151 180748 138143 LE pairs 1520948 46748"
2020.semeval-1.31,P09-1034,0,0.108931,"Missing"
2020.semeval-1.31,P18-2101,0,0.0424031,"Missing"
2020.semeval-1.31,N18-1056,0,0.188565,"Missing"
2020.semeval-1.31,N18-1103,0,0.0596949,"Missing"
2020.semeval-1.31,J17-4004,0,0.100335,"Missing"
2020.semeval-1.31,D14-1161,0,0.026898,"each language, word pairs of synonym and antonym relations are included as symmetric resources and concepts of IsA relation are regarded as 3 https://github.com/artetxem /vecmap https://github.com/hermitdave/FrequencyWords/tree/master/content/2016 5 https://github.com/commonsense/conceptnet5/wiki/Downloads 4 258 Figure 3: Different situations of loading input word embeddings asymmetric LE constraints. The number of constraints for each language is displayed in Table 1a. And for English, the other part is the same set as LEAR (Vuli´c and Mrkˇsi´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), and asymmetric LE constraints are also extracted from WordNet. We add these 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs into English external lexical constraints. Language Antonyms Synonyms EN DE IT TR HR SQ 10108 1247 675 451 106 55 48443 29699 4992 1048 703 298 LE pairs 46068 21807 1395 107 57 6 Total Language Antonyms Synonyms 104619 52753 7062 1606 866 359 EN DE IT HR TR SQ 187258 68801 63121 38665 51110 21542 593702 296700 252227 169151 180748 138143 LE"
2020.semeval-1.81,N19-1423,0,0.0650565,"Missing"
2020.semeval-1.81,P19-1356,0,0.0453103,"Missing"
2020.semeval-1.81,P17-2103,0,0.185357,"iveness of our approaches, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and our official submission achieves F1 0.802, which ranks us 16th in the competition. 1 Introduction Counterfactual statements describe events that did not actually happen or cannot happen, as well as the possible consequence if the events have had happened. Detecting counterfactual statements involves common sense, knowledge, and reasoning, it is also the basis for all down-stream counterfactual related causal inference analysis in natural language.(Son et al., 2017) The problem of counterfactual have been studied in many domains, like the perspective of literally logical relations between the antecedent and consequent of counterfactual forms and the outcomes(Goodman, 1947), and conducting counterfactual thought experiments for hypothetical tests on historical events, policies, or other aspects of a society and assess them by political scientists(Tetlock and Belkin, 1996). (Son et al., 2017) use a combination of a rule-based approach and a supervised classifier to capture counterfactual statements from Twitter, which is close to our problem. SemEval 2020"
2020.semeval-1.81,D15-1306,0,0.0271137,"ation while generate paraphrases vary from the original sentence. We use Chinese in down sampling scenario. French, Russian, German, Portuguese, Italian, Spanish and Greek are used in up sampling scenario. EDA Combined with WordNet and Feature Extraction EDA, or Easy Data Augmentation techniques, consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. EDA is presented by (Wei and Zou, 2019), which claims to have strong results for small datasets. In our system, we apply only synonym replacement, which shows good performance in (Wang and Yang, 2015)’s work, and random deletion combined with WordNet and a feature words set. WordNet(Miller, 1995) is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. We use WordNet to do synonym replacement. Through observing the data, we obtain a set of feature words which appear frequently in counterfactual examples with relatively fixed structures, but seldom in non-counterfactual examples. Part of the set shows in"
2020.semeval-1.81,D19-1670,0,0.106501,"tem. Experiments and analysis of the results are introduced in Section 3. We describe the conclusions in Section 4. 2 System description Our system consists of two components: data augmentation, which includes 2 different augmentation methods, and BERT model. Since the data provided by the task organizers are highly unbalanced in numbers of examples in two classes, we believe that the key to solve the task is to expand the number of counterfactual examples in some way. We apply 2 data augmentation methods to solve this problem: Back Translation and a simple EDA (Easy Data Augmentation) system(Wei and Zou, 2019) combine with WordNet and feature extraction. 2.1 Data augmentation This section briefly introduces the datasets of subtask 1, then we introduce our two data augmentation approaches. Data Overview The training data provided by organizers has 13000 examples in total, each example consists of 3 parts: sentence ID, gold label and sentence. Labels are either 0, denotes a non-counterfactual example, or 1, denotes a counterfactual example. While sentence length of the examples is between 6 to 3273 words, the mean and median length of the examples are 193 words and 177 words respectively. When doing"
2020.semeval-1.81,2020.semeval-1.40,0,0.0256795,"he perspective of literally logical relations between the antecedent and consequent of counterfactual forms and the outcomes(Goodman, 1947), and conducting counterfactual thought experiments for hypothetical tests on historical events, policies, or other aspects of a society and assess them by political scientists(Tetlock and Belkin, 1996). (Son et al., 2017) use a combination of a rule-based approach and a supervised classifier to capture counterfactual statements from Twitter, which is close to our problem. SemEval 2020 task 5 focus on the problem of detecting counterfactual statements, as (Yang et al., 2020) described in their work. Subtask 1 of task 5 is a two-classification problem, in which we need to find out whether a statement is counterfactual or not. We choose to apply feature extraction and data augmentation to tackle this problem, for the biggest challenge exists when doing this task is unbalanced data: although the training set has 13,000 examples in total, seems to be enough to fine-tune a BERT model, the number of both classes is highly unbalanced: there are only 1454 counterfactual examples, and 11546 non-counterfactual examples seriously dilute the proportion of counterfactual exam"
2020.tacl-1.10,D18-1241,0,0.0537264,"Missing"
2020.tacl-1.10,L18-1431,0,0.0720002,"2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely based on the information explicitly expressed in the provided text. With a goal of developing similarly challenging, but free-form multiple-choice datasets, and Abstract Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3 ), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-fo"
2020.tacl-1.10,N18-1144,0,0.04627,"Missing"
2020.tacl-1.10,N19-1423,0,0.156987,", 2016), in which no ground truth document supporting answers is provided with each question, making them relatively less suitable for isolating improvements to MRC. We will first discuss standard MRC datasets for English, followed by MRC/QA datasets for Chinese. English. Much of the early MRC work focuses on designing questions whose answers are spans from the given documents (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). As a question and its answer are usually in the same sentence, stateof-the-art methods (Devlin et al., 2019) have outperformed human performance on many such tasks. To increase task difficulty, researchers have explored a number of options including adding unanswerable (Trischler et al., 2017; Rajpurkar et al., 2018) or conversational (Choi et al., 2018; Reddy et al., 2019) questions that might require reasoning (Zhang et al., 2018a), and designing abstractive answers (Nguyen et al., 2016; Koˇcisk`y et al., 2018; Dalvi et al., 2018) or (question, answer) pairs that involve cross-sentence or crossdocument content (Welbl et al., 2018; Yang et al., 2018). In general, most questions concern the facts th"
2020.tacl-1.10,J02-2001,0,0.0602467,"analyze a subset of questions randomly sampled from the development and test sets of C3 and arrive at the following three kinds of prior knowledge required for answering questions. A question is labeled as matching if it exactly matches or nearly matches (without considering determiners, aspect particles, or conjunctive adverbs; Xia, 2000) a span in • Arithmetic† : This includes numerical computation and analysis (e.g., comparison and unit conversion). • Connotation: Answering questions requires knowledge about implicit and implied sentiment towards something or somebody, emotions, and tone (Edmonds and Hirst, 2002; 144 In 1928, recommended by Hsu Chih-Mo, Hu Shih, who was the president of the previous National University of China, employed Shen Ts’ung-wen as a lecturer of the university in charge of teaching the optional course of modern literature. At that time, Shen already made himself conspicuous in the literary world and was a little famous in society. For this sake, even before the beginning of class, the classroom was crowded with students. Upon the arrival of class, Shen went into the classroom. Seeing a dense crowd of students sitting beneath the platform, Shen was suddenly startled and his mi"
2020.tacl-1.10,N19-1300,0,0.0193072,"to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this"
2020.tacl-1.10,P13-1174,0,0.0112384,"m in mind. were greatly encouraged. expressed their understanding and encouraged him. ⋆ The passage above is mainly about the development of the Chinese educational system. how to make self-adjustment if one is nervous. the situation where Shen gave his lecture for the first time. ⋆ how Shen turned into a teacher from a writer. Table 2: A C3 -Mixed (C3M ) problem (left) and its English translation (right) (⋆: the correct option). itly in the text, which cannot be reached by paraphrasing sentences using linguistic knowledge. For example, Q4 in Table 2 and Q2 in Table 3 belong to this category. Feng et al., 2013; Van Hee et al., 2018). For example, the following conversation: ‘‘F: Ming Yu became a manager when he was so young! That’s impressive! M: It is indeed not easy!’’ is delivered in a tone for praise. • Part-whole: We require knowledge that object A is a part of object B. Relations such as member-of, stuff-of, and component-of between two objects also fall into this category (Winston et al., 1987; Miller, 1998). For example, we require implication mentioned above as well as part-whole knowledge (i.e., ‘‘teacher’’ is a kind of job) to summarize the main topic of the following • Cause-effect†: Th"
2020.tacl-1.10,A83-1007,0,0.625138,"nese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowledge including but not limited to: idioms, proverbs, negation, antonymy, synonymy, the possible meanings of the word, and syntactic transformations (Nassaji, 2006). DOMAIN-SPECIFIC: This kind of world knowledge consists of, but is not limited to, facts about domain-specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table"
2020.tacl-1.10,I17-4005,0,0.0308083,"k et al., 2016) ARC (Clark et al., 2016) ARC (Clark et al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table"
2020.tacl-1.10,E17-1011,0,0.190542,"k et al., 2016) ARC (Clark et al., 2016) ARC (Clark et al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table"
2020.tacl-1.10,C16-1167,0,0.565033,"the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely based on the information explicitly expressed in the provided text. With a goal of developing similarly challenging, but free-form multiple-choice datasets, and Abstract Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3 ), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associ"
2020.tacl-1.10,W93-0409,0,0.704013,"Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowledge including but not limited to: idioms, proverbs, negation, antonymy, synonymy, the possible meanings of the word, and syntactic transformations (Nassaji, 2006). DOMAIN-SPECIFIC: This kind of world knowledge consists of, but is not limited to, facts about domain-specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table 4. We observe n"
2020.tacl-1.10,Q18-1023,0,0.0605303,"Missing"
2020.tacl-1.10,D17-1082,0,0.187572,"rm question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Sha"
2020.tacl-1.10,D19-1597,0,0.158529,"al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowled"
2020.tacl-1.10,P18-2023,0,0.0254278,"testing on them separately, following the default setting on RACE that also contains two subsets (Lai et al., 2017). We run every experiment five times with different random seeds and report the best development set performance and its corresponding test set performance. Distance-Based Sliding Window. We simply treat each character as a token. We do not use Chinese word segmentation as it results in drops in performance based on our experiment. Co-Matching. We replace the English tokenizer with a Chinese word segmenter in HanLP.1 We use the 300-dimensional Chinese word embeddings released by Li et al. (2018). 4.3 Fine-Tuning Pre-Trained Language Models We also apply the framework of fine-tuning a pre-trained language model on machine reading comprehension tasks (Radford et al., 2018). We consider the following four pre-trained language models for Chinese: Chinese BERT-Base (denoted as BERT) (Devlin et al., 2019), Chinese ERNIE-Base (denoted as ERNIE) (Sun et al., 2019b), and Chinese BERT-Base with whole word masking during pre-training (denoted as BERT-wwm) (Cui et al., 2019) and its enhanced version pre-trained over larger corpora (denoted as BERT-wwm-ext). These models have the same number of l"
2020.tacl-1.10,W14-2903,0,0.0773902,"Missing"
2020.tacl-1.10,P17-1147,0,0.0646954,"Missing"
2020.tacl-1.10,P11-2057,0,0.0391109,"specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table 4. We observe notable differences exist between C3M and C3D . For example, C3M , in which most documents are formally written texts, has a larger vocabulary size compared to that of C3D with documents in spoken language. Similar observations have been made by Sun et al. (2019a) that the vocabulary size is relatively small in English dialogue-based machine reading comprehension tasks. In addition, the average document length (180.2) in C3M is longer than that in C3D (76.3). In general, C3 may not be suitable"
2020.tacl-1.10,N16-1098,0,0.159352,"2 Machine reading comprehension (MRC) tasks have attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answe"
2020.tacl-1.10,N18-1023,0,0.204962,"gle question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al."
2020.tacl-1.10,W16-5706,0,0.0588232,"Missing"
2020.tacl-1.10,S18-1119,0,0.0410237,"Missing"
2020.tacl-1.10,W10-0911,0,0.0503085,"Missing"
2020.tacl-1.10,P18-2124,0,0.0538498,"Missing"
2020.tacl-1.10,D13-1020,0,0.327171,"associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui"
2020.tacl-1.10,Q19-1014,1,0.555498,"as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely bas"
2020.tacl-1.10,W17-2623,0,0.0619458,"Missing"
2020.tacl-1.10,J18-4010,0,0.0631161,"Missing"
2020.tacl-1.10,P18-2118,0,0.0586708,"(Richardson et al., 2013), a rule-based method that chooses the answer option by taking into account (1) lexical similarity between a statement (i.e., a question and an answer option) and the given document with a fixed window size and (2) the minimum number of tokens between occurrences of the question and occurrences of an answer option in the document. This method assumes that a statement is more likely to be correct if there is a shorter distance between tokens within a statement, and more informative tokens in the statement appear in the document. 4.2 Co-Matching We utilize Co-Matching (Wang et al., 2018), a Bi-LSTM-based model for multiple-choice MRC tasks for English. It explicitly treats a question and one of its associated answer options as two sequences and jointly models whether or not the given document matches them. We modify the pre-processing step and adapt this model to MRC tasks for Chinese (Section 5.1). model, respectively. We add an embedding vector t1 to each token before the first [SEP] (inclusive) and an embedding vector t2 to every other token, where t1 and t2 are learned during language model pre-training for discriminating sequences. We denote the final hidden state for th"
2020.tacl-1.10,D16-1264,0,0.204036,"Missing"
2020.tacl-1.10,Q18-1021,0,0.0651545,"Missing"
2020.tacl-1.10,Q19-1016,0,0.0438365,"Missing"
2020.tacl-1.10,D18-1257,0,0.127078,"sion (MRC) tasks have attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions,"
2020.tacl-1.10,D18-1259,0,0.0722587,"Missing"
2020.tacl-1.10,C18-1038,0,0.0532576,"Missing"
2020.tacl-1.10,P19-1075,0,0.215414,"ave attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenge"
2021.acl-demo.1,C18-1139,0,0.0140491,"d the URL is available on the web page.4 The text matching API is used to calculate the similarity between a pair of sentences. Similar to the text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsin"
2021.acl-demo.1,P81-1022,0,0.278718,"lar expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level. For phraselevel segmentation, some phrases (especially noun phrases) may contained as a unit. An unsupervised algorithm is implemented in TexSmart for both English and Chinese word segmentation. We ch"
2021.acl-demo.1,U15-1010,0,0.0276516,"TTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the"
2021.acl-demo.1,W18-2501,0,0.0662142,"Missing"
2021.acl-demo.1,W09-3302,0,0.0687657,"Missing"
2021.acl-demo.1,C92-2082,0,0.277702,"de web search (e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster"
2021.acl-demo.1,C10-3004,0,0.154401,"Missing"
2021.acl-demo.1,P08-1067,0,0.029343,"ven by TexSmart for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en"
2021.acl-demo.1,D14-1082,0,0.0308477,"rt for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offl"
2021.acl-demo.1,D18-1536,0,0.0331353,"Missing"
2021.acl-demo.1,P18-1249,0,0.0167912,"he text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsing) tries to assign role labels to words or phrases in a sentence. TexSmart takes a sequence labeling model with BERT as the text encoder for sema"
2021.acl-demo.1,2021.findings-emnlp.18,1,0.731087,"Missing"
2021.acl-demo.1,P17-1152,0,0.0965523,"Missing"
2021.acl-demo.1,C16-1111,0,0.0255628,"HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms fo"
2021.acl-demo.1,2021.naacl-main.116,1,0.743751,"entation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0.3.0. Programming langu"
2021.acl-demo.1,W96-0213,0,0.664796,"ime was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will"
2021.acl-demo.1,C18-1166,0,0.0468114,"s a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK)"
2021.acl-demo.1,W03-0419,0,0.678216,"Missing"
2021.acl-demo.1,D15-1135,1,0.740722,"ient. In this sense, both methods are general in practice. ture. As a result, applications using these tools have to implement deep semantic representation by themselves. Some NLP toolkits make use of regular expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level."
2021.acl-demo.1,W02-0109,0,0.557609,"Missing"
2021.acl-demo.1,C10-1112,1,0.519815,"rocedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-grained NER, semantic"
2021.acl-demo.1,P14-5010,0,0.00644413,"Missing"
2021.acl-demo.1,N18-2028,1,0.809382,"uring the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-gra"
2021.acl-demo.1,J93-2004,0,0.0740651,"ng a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the RoBERTa-CRF and Flair models. As we found RoBERTa-CRF performs better on the Chinese dataset while Flair is better on the English dataset, we report results of RoBERTa-CRF for Chinese and Flair for English in our experiments. Constituency Parsing We conduct parsing experiments on both English and Chinese datasets. For English task, we use WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for th"
2021.acl-demo.1,W13-3516,0,0.0334885,"e WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for the training, development and test sets. The network parameters of our model are initialized using RoBERTa. The batch size is set to 32 and the learning rate is 5×10−5 . Text Matching Two text matching algorithms are evaluated: ESIM and Linkage. The datasets used in evaluating English text matching are MRPC6 and QUORA7 . For Chinese text matching, four datasets are involved: LCQMC (Liu et al., 2018b), AFQMC (Xu et al., 2020), BQ_CORPUS (Chen et al., 2018), and PAWSzh (Zhang et al., 2019). We evaluate the quality FGNER Base Hybrid 45.9 53.8 Table 1: Semantic"
2021.acl-demo.1,P11-1116,1,0.692266,"(e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally"
2021.acl-demo.1,P13-4009,0,0.0710732,"Missing"
2021.acl-demo.1,N19-1131,0,0.045871,"Missing"
2021.acl-long.445,N19-1388,0,0.0564376,"language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method. 1 Introduction Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all"
2021.acl-long.445,D19-1165,0,0.17933,"the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influence on the final performance, which requires specialized manual design. As a result, these problems often prevent the application of these methods in some scenarios. Based on the above, we aim to propose a method that can retain the general and language-specific knowledge, and keep a stable model size as the number of language-pair increases without introducing any spec"
2021.acl-long.445,C18-1263,0,0.142084,"of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influen"
2021.acl-long.445,P15-1166,0,0.0257854,"ee that when the neurons associated with the current language pair are erased, the performance of this language pair decreases greatly. However, the performance of other language pairs only declines slightly, because the specific knowledge captured by these specific neurons are not so important for other languages. 6 Related Work Our work closely relates to language-specific modeling for MNMT and model pruning which we will recap both here. Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al., 2016). Later, Ha et al. (2016) and Johnson et al. (2017) propose an universal MNMT model with a target language token to indicate the translation direction. While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific. Such approaches involve inserting conditional languagespecific"
2021.acl-long.445,2021.eacl-main.80,0,0.0588337,"Missing"
2021.acl-long.445,N16-1101,0,0.0801387,"eriority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for th"
2021.acl-long.445,N18-1032,0,0.0169507,"t years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sa"
2021.acl-long.445,Q17-1024,0,0.0614858,"Missing"
2021.acl-long.445,D13-1176,0,0.0428887,"e model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the languagespecific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method. 1 Introduction Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to"
2021.acl-long.445,N19-4009,0,0.026737,"Missing"
2021.acl-long.445,P02-1040,0,0.112281,"Missing"
2021.acl-long.445,W18-6319,0,0.0211124,"Missing"
2021.acl-long.445,W18-6300,0,0.139913,"Missing"
2021.acl-long.445,K16-1029,0,0.0618184,"Missing"
2021.acl-long.445,P16-1162,0,0.0158833,"ns that are responsible for capturing general knowledge. Many-to-Many For this translation scenario, we test our approach on IWSLT-171 translation datasets, including English, Italian, Romanian, Dutch (briefly, En, It, Ro, Nl). We experimented in eight directions, including It↔En, Ro↔En, Nl↔En, and It↔Ro, with 231.6k, 220.5k, 237.2k, and 217.5k data for each language pair. We choose test2016 and test2017 as our development and test set, respectively. Sentences of all languages were tokenized by the Moses scripts2 and further segmented into subword symbols using Byte-Pair Encoding (BPE) rules (Sennrich et al., 2016) with 40K merge operations for all languages jointly. Language-specific Neurons Next, we regard other neurons except for the general neurons as the language-specific neurons and determine which language pair to assign them to. To achieve this, we compute an importance threshold for each neuron: λ(i) = k × max(Θm (i)), m ∈ {1, . . . , M }, k ∈ [0, 1] (9) , where max(Θm (i)) denotes the maximum importance of this neuron in all language pairs and k is a hyper-parameter. The neuron will be assigned to the language-pairs whose importance is larger than the threshold. When the importance of neurons"
2021.acl-long.445,D16-1163,0,0.0234714,"th the current language pair are erased, the performance of this language pair decreases greatly. However, the performance of other language pairs only declines slightly, because the specific knowledge captured by these specific neurons are not so important for other languages. 6 Related Work Our work closely relates to language-specific modeling for MNMT and model pruning which we will recap both here. Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al., 2016). Later, Ha et al. (2016) and Johnson et al. (2017) propose an universal MNMT model with a target language token to indicate the translation direction. While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific. Such approaches involve inserting conditional languagespecific routing layer (Zhang et al., 2021), s"
2021.acl-long.445,D19-1089,0,0.0357038,"Missing"
2021.acl-long.445,W19-4305,0,0.033037,"Missing"
2021.acl-long.445,2020.emnlp-main.78,0,0.123316,"nson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influence on the final per"
2021.ccl-1.49,Q17-1010,0,0.0191136,"Missing"
2021.ccl-1.49,2020.emnlp-main.48,0,0.0403595,"Missing"
2021.ccl-1.49,P18-1067,0,0.0608411,"Missing"
2021.ccl-1.49,2020.nuse-1.15,0,0.0929631,"Missing"
2021.emnlp-main.311,W18-0701,0,0.0583601,"Missing"
2021.emnlp-main.402,2020.tacl-1.36,0,0.0661528,"Missing"
2021.emnlp-main.402,W14-3346,0,0.0961204,"ulti-task sequence tagging. In particular, for each input word, we decide whether to delete it or not, and at the same time, we choose what span from the dialogue context need to be inserted to the front of the current word. In this way, our solution enjoys a far smaller search space than the generation based approaches. Since our model does not directly take features from the word-to-word interactions of its output utterances, this may cause the lack of fluency. To encourage more fluent outputs, we propose to inject additional supervisions from two popular metrics, i.e., sentence-level BLEU (Chen and Cherry, 2014) and the perplexity of a pretrained GPT-2 (Radford et al., 2019) model, using the framework of “REINFORCE with a baseline” (Williams, 1992). Sentence-level BLEU is computationally efficient, but it requires references and thus may only provide domain-specific knowledge. Conversely, the perplexity by GPT-2 is reference-free, giving more guidance on open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art"
2021.emnlp-main.402,2020.emnlp-main.651,0,0.074226,"Missing"
2021.emnlp-main.402,N19-1423,0,0.0777142,"able has limited coverage. Though we also convert our original problem into a multi-task tagging problem, we predict what span to be inserted, avoiding the issues caused by using a phrase table. Besides, we study injecting richer supervision signals to improve the fluency of outputs, which is a common issue for tagging based approaches on text generation, as they do not directly model wordto-word dependencies. Finally, we are the first to apply sequence tagging on dialogue rewriting, showing much better performances than those of BERT-based strong baselines. 3 Our baseline consists of a BERT (Devlin et al., 2019) encoder and a Transformer (Vaswani et al., 2017) decoder with a copy mechanism. Given input tokens X = (x1 , . . . , xN ) that is the concatenation of the current dialogue context c = (u1 , . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) whe"
2021.emnlp-main.402,D19-1605,0,0.0392318,"Missing"
2021.emnlp-main.402,P16-1154,0,0.0773972,"Missing"
2021.emnlp-main.402,P16-1014,0,0.0473789,"Missing"
2021.emnlp-main.402,I17-1099,0,0.0609614,"Missing"
2021.emnlp-main.402,P19-1003,0,0.339307,"r the current state-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it"
2021.emnlp-main.402,Q19-1014,1,0.890552,"Missing"
2021.emnlp-main.402,2020.emnlp-main.227,0,0.0155517,"method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an accurate SRL model on dialogues, they manually annotate SRL information for more than 27,000 dialogue turns, which is timeconsuming and costly. Liu et al. (2020) casts this task into a semantic segmentation problem, a major task in computer vision. In particular, their model generates a word-level matrix, which contains the operations of substitution and insertion, for each original utterance. They adopt a heavy model that takes 10 convolution layers in addition to the BERT encoder. None of the existing efforts mention the robustness issue, a critical aspect for the usability of this task. Besides, they only compare performances under automatic metrics (e.g., BLEU). We take the first step to address this severe robustness issue, and we adopt multiple"
2021.emnlp-main.402,D19-1510,0,0.0209559,"to address this severe robustness issue, and we adopt multiple measures for comprehensive evaluation. Besides, we propose a novel model based on sequence tagging for solving this task, and our model takes a much smaller search space than previous models. Sequence tagging for text generation Given the intrinsic nature of typical text-generation problems (e.g., machine translation), i.e. (1) the number of predictions cannot be determined by inputs, and (2) the candidate space for each prediction is usually very large, sequence tagging is not commonly adopted on text-generation tasks. Recently, Malmi et al. (2019) proposed a model based on sequence tagging for sentence fusion and sentence splitting, and they show that their model outperforms a vanilla sequence-to-sequence baseline. In particular, their model can decide whether to keep or delete each input word and what phrase needs 2 Related Work to be inserted in front of it. As a result, they have Initial efforts (Su et al., 2019; Elgohary et al., to extract a large phrase table from the training 2019) treat dialogue utterance rewriting as a stan- data, causing inevitable computation for choosing 4914 phrases from the table. Their approach also faces"
2021.emnlp-main.402,D19-1191,0,0.228972,"te-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it integrates the rec"
2021.emnlp-main.402,P02-1040,0,0.1101,"2016; See et al., 2017). They have demonstrated recovery. But, the state-of-the-art performances almost ready-to-use performances on the test set ∗ Work done while J. Hao was interning and L. Wang was from the same data source as the training set. Howworking at Tencent AI Lab. † Corresponding author. ever, they are not robust, as our experiments show 4913 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913–4924 c November 7–11, 2021. 2021 Association for Computational Linguistics that their performances can drop dramatically (by roughly 33 BLEU4 (Papineni et al., 2002) points and 44 percent of exact match) on another test set created from a different data source (not necessarily from a totally different domain). We argue that it may not be the best practice to model utterance rewriting as standard text generation. One main reason is that text generation introduces an overly large search space, while a rewriting output (e.g., u03 in Table 1) always keeps the core semantic meaning of its input (e.g., u3 ). Besides, exposure bias (Wiseman and Rush, 2016) can further exacerbate the problem for test cases that are not similar to the training set, resulting in ou"
2021.emnlp-main.402,D16-1264,0,0.0452181,"of our model. For a fair comparison, it takes the same BERT-based encoder (Equation 1) as the baseline to represent each input. For simplicity, we directly apply classifiers to predict the corresponding tags for each input word. In particular, to determine whether each word xn in the current utterance ui should be kept or deleted, we use a binary classifier: p(dn |X, n) = Softmax(Wd en + bd ) (7) where Wd and bd are learnable parameters, dn is the binary classification result, and en is the BERT embedding for xn . 4916 Moreover, we cast span prediction as machine reading comprehension (MRC) (Rajpurkar et al., 2016), where a predicted span corresponds to an MRC target answer. For each input token xn ∈ ui , we follow the previous work on MRC to predict ed the start position sst n and end position sn for the target span sn , performing separate self-attention mechanisms for them: p(sst n |X, n) = Attnstart (E, en ) (8) p(sed n |X, n) = Attnend (E, en ) (9) where Attnstart and Attnend are the self-attention layers for predicting the start and end positions of a span. We use the standard additive attention mechanism (Bahdanau et al., 2014) to perform the attention function. The probability for the whole span"
2021.emnlp-main.402,Q19-1016,0,0.0404538,"Missing"
2021.emnlp-main.402,P17-1099,0,0.0428704,", . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) where TransDecoder is the Transformer decoder that returns the attention probability distribution pattn over the encoder states E and the latest det coder state st for each step t. Following See et al. (2017), the generation probability θt for timestep t is calculated from the weighted sum for the encoder-decoder cross attention distribution and the encoder hidden states. X θt = σ(w| (pattn [n] · en )) (5) t n∈[1..N ] where w represents the model parameter. In this way, the copy mechanism encourages copying words from the input tokens. The T RANS -PG baseline is trained with standard cross-entropy loss: X Lgen = − log p(yt |y<t , X; θ) (6) t∈[1..M ] where θ represents all model parameters. 5 06 06 27 Shanghai recently recently Reference <start> Input <start> Deletion Insertion often 5 Shanghai 9 w"
2021.emnlp-main.402,D16-1137,0,0.0229302,"Missing"
2021.emnlp-main.402,P19-1369,0,0.0548787,"Missing"
2021.emnlp-main.402,2020.emnlp-main.537,1,0.851999,"Missing"
2021.emnlp-main.402,2020.acl-main.444,1,0.888547,"Missing"
2021.emnlp-main.402,P18-1205,0,0.0608904,"Missing"
2021.emnlp-main.402,2020.acl-demos.30,0,0.0330435,".7 76.0 69.8 67.6 76.5 78.8 79.3 80.5 7.4 8.6 12.9 24.5 26.8 31.8 Table 3: Test results of all comparing models trained on the R EWRITE dataset. Model settings We implement the baseline and our model on top of a BERT-base model (Devlin et al., 2019), and we use Adam (Kingma and Ba, 2015) as the optimizer, setting the learning rate to 3e−5 as determined by a development experiment. For the reinforcement learning stage, we respectively use the sentence-level BLEU score with “Smoothing 3” (Chen and Cherry, 2014) or the perplexity score based on a Chinese GPT-2 model trained on massive dialogues (Zhang et al., 2020)4 as the reward function. It is worth noting that the GPT-2 model is not fine-tuned during the reinforcement learning stage. 6.2 Main Results Training on R EWRITE Table 3 shows the results when all comparing models are trained on the R EWRITE dataset, before evaluating on the indomain R EWRITE and the R ESTORATION test data for robustness examination. On the R EWRITE test set, our tagging-based models (Rows 4-6) are much better than the T RANS -PG+BERT baseline, and they can get comparable performances with RUN, the previous state-of-the-art model. RUN usually gets high numbers on BLEU1 withou"
2021.emnlp-main.402,2020.acl-main.635,0,0.0367031,"Missing"
2021.emnlp-main.402,D19-1192,0,0.0176334,"n open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art model for cross-dataset evaluation. More analysis shows that the outputs of our model keep more semantic information from the inputs. Our code is available at https://github.com/ freesunshine0316/RaST-plus. dard text generation problem, adopting sequenceto-sequence models with copy mechanism to tackle this problem. Later work (Pan et al., 2019; Zhou et al., 2019; Huang et al., 2021) explores taskspecific features for additional gains in performance. For instance, Pan et al. (2019) adopts a pipeline-based method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an"
2021.emnlp-main.457,P17-1147,0,0.0229705,"Missing"
2021.emnlp-main.457,D17-1126,0,0.0473201,"Missing"
2021.emnlp-main.457,D19-1362,0,0.0271465,"s show that: The wide availability of neural network models has allowed development of novel and complex natural language processing tasks, many of which are in low-resource settings. With new definitions of tasks comes challenges of constructing new datasets, which is still an expensive and timeintensive endeavor. Many researchers have resorted 1. Instance-adaptive noise-robust training proto constructing datasets by using completely autoposed in this work enhances the noisemated pipelines (e.g. Lan et al., 2017; Joshi et al., robustness of the losses on noisy and cor2017; Paul et al., 2019; Lange et al., 2019; Sousa rupted datasets, which results in large peret al., 2019; Wu et al., 2020). However, silver labels formance gains when instance-specific noisecollected this way are still quite noisy compared resistance hyperparameters are used. to expert annotation. Because such methods have been gaining popularity and practicality, it is impor2. Noise-robust losses are an effective way to 5647 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5647–5663 c November 7–11, 2021. 2021 Association for Computational Linguistics combat noise in silver-standard NLP d"
2021.emnlp-main.457,D14-1162,0,0.0861334,"Missing"
2021.emnlp-main.457,P17-1099,0,0.0412692,"Missing"
2021.emnlp-main.457,D13-1170,0,0.00892486,"Missing"
2021.emnlp-main.457,N19-1152,0,0.0224825,"Missing"
2021.emnlp-main.610,C14-1151,0,0.17872,"s Example sentence2 … w Transformer Gloss Sentence Gloss Alignment Transformer Example Sentence WSD Datasets … Oxford Figure 2: Overview of our approach. The left part illustrates the gloss alignment algorithm where each blue circle is a gloss containing one definition sentence and several example sentences. The right part is our model architecture to predict the semantic equivalence of a word in context and a gloss by comparing their representations obtained from a shared transformer encoder. Task-specific WSD datasets can be further used to fine-tune our model. of Machine Translation. Lesk (Basile et al., 2014) relies on a word-level similarity function to measure the semantic overlap between the context of a word and each sense definition. SENSEMBERT (Scarlini et al., 2020a) produces high-quality latent semantic representations of word meanings by incorporating knowledge contained in BabelNet into language models. Other approaches try to learn better gloss embeddings by considering the WordNet graph structure (e.g., hypernyms, hyponyms, synonyms, etc.) (Luo et al., 2018b; Loureiro and Jorge, 2019; Kumar et al., 2019; Bevilacqua and Navigli, 2020). For example, Kumar et al. (2019) proposed EWISE to"
2021.emnlp-main.610,2020.acl-main.255,0,0.0779927,"er used to fine-tune our model. of Machine Translation. Lesk (Basile et al., 2014) relies on a word-level similarity function to measure the semantic overlap between the context of a word and each sense definition. SENSEMBERT (Scarlini et al., 2020a) produces high-quality latent semantic representations of word meanings by incorporating knowledge contained in BabelNet into language models. Other approaches try to learn better gloss embeddings by considering the WordNet graph structure (e.g., hypernyms, hyponyms, synonyms, etc.) (Luo et al., 2018b; Loureiro and Jorge, 2019; Kumar et al., 2019; Bevilacqua and Navigli, 2020). For example, Kumar et al. (2019) proposed EWISE to improve model’s performance on rare or unseen senses by learning knowledge graph embeddings from WordNet. Building upon EWISE, Bevilacqua and Navigli (2020) developed a hybrid approach that incorporates more lexical knowledge (e.g., hypernymy, meronymy, similarity in WordNet) into the model through synset graph embeddings. 3 Overview of Our Approach Inventory Words Glosses ES Gls/W Oxford 52.5K 86.2K 96.8K 1.6 Webster 39.8K 72.5K 100.6K 1.8 Collins 34.4K 61.4K 89.5K 1.8 67.0K 64.9K 1.8 Cambridge 36.6K Longman 36.9K 63.8K 70.2K 1.7 WordNet 14"
2021.emnlp-main.610,2021.eacl-main.36,0,0.0448922,"espectively, with an overall accuracy of 0.87. In experiments, we apply a threshold of 0.6 to alignment results and only consider aligned gloss pairs with textual similarities above it, which can further improve gloss alignment accuracy to 0.98 based on human annotations. In this way, we can significantly improve the quality of training data that are generated from the automatically aligned dictionaries. 6.2 Experiments on WSD We evaluate our model on two WSD datasets, i.e., WSD tasks standardized by Raganato et al. (2017b) that focuses on all-words WSD evaluation and FEWS dataset proposed by Blevins et al. (2021) that emphasizes low-shot WSD evaluation. Since both datasets are annotated using word senses in WordNet 3.0 (Miller, 1995), we pair the context sentence with the annotated gloss in WordNet 3.0 7 Our implementation was based on https://github. com/huggingface/transformers. 7745 Models 1 Most Frequent Sense 2 Leskemb (2014) 3 BiLSTM (2017a) 4 HCAN (2018a) 5 EWISE (2019) 6 LMMSBERT (2019) 7 GlossBERT (2019) 8 BEM (2020) 9 AdaptBERTLarge (2020) 10 EWISER (2020) 11 SemEq-Base Ours: Data Augmentation 12 SemEq-Base Ours: Transfer Learning 13 SemEq-Base-General 14 SemEq-Base-Expert 15 SemEq-Large-Gen"
2021.emnlp-main.610,2020.acl-main.95,0,0.161154,"BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense corresponds to the correct sense or not. Bi-Encoder Model (BEM) (Blevins and Zettlemoyer, 2020) represents the target words and senses in the same embedding space using a context encoder and a gloss encoder but optimizes on each word individually. Yap et al. (2020) formulated WSD as a relevance ranking task and fine-tuned BERT to select the most probable sense definition from candidate senses. The neural architecture of our semantic equivalence recognizer realizes the benefits of GlossBERT and BEM. Knowledge-Based WSD Approaches. Closely Overall, the major contributions of our work are related to our work, many knowledge-based aptwo-fold. 1) We propose a gloss alignment algo- proaches r"
2021.emnlp-main.610,2020.lrec-1.732,0,0.0404123,"Missing"
2021.emnlp-main.610,N19-1423,0,0.0425486,"Missing"
2021.emnlp-main.610,S01-1001,0,0.424906,"Missing"
2021.emnlp-main.610,D19-1355,0,0.0139036,"e-tuning. 2 Related Work Supervised WSD Approaches. Most existing WSD models are learned in a supervised manner and depend on human-annotated data. For example, Raganato et al. (2017a) regarded WSD as a sequence labeling task and trained a BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense corresponds to the correct sense or not. Bi-Encoder Model (BEM) (Blevins and Zettlemoyer, 2020) represents the target words and senses in the same embedding space using a context encoder and a gloss encoder but optimizes on each word individually. Yap et al. (2020) formulated WSD as a relevance ranking task and fine-tuned BERT to select the most probable sense definition from candidate senses. The neural architecture of our semantic equivale"
2021.emnlp-main.610,P19-1568,0,0.0165909,"atasets can be further used to fine-tune our model. of Machine Translation. Lesk (Basile et al., 2014) relies on a word-level similarity function to measure the semantic overlap between the context of a word and each sense definition. SENSEMBERT (Scarlini et al., 2020a) produces high-quality latent semantic representations of word meanings by incorporating knowledge contained in BabelNet into language models. Other approaches try to learn better gloss embeddings by considering the WordNet graph structure (e.g., hypernyms, hyponyms, synonyms, etc.) (Luo et al., 2018b; Loureiro and Jorge, 2019; Kumar et al., 2019; Bevilacqua and Navigli, 2020). For example, Kumar et al. (2019) proposed EWISE to improve model’s performance on rare or unseen senses by learning knowledge graph embeddings from WordNet. Building upon EWISE, Bevilacqua and Navigli (2020) developed a hybrid approach that incorporates more lexical knowledge (e.g., hypernymy, meronymy, similarity in WordNet) into the model through synset graph embeddings. 3 Overview of Our Approach Inventory Words Glosses ES Gls/W Oxford 52.5K 86.2K 96.8K 1.6 Webster 39.8K 72.5K 100.6K 1.8 Collins 34.4K 61.4K 89.5K 1.8 67.0K 64.9K 1.8 Cambridge 36.6K Longman 3"
2021.emnlp-main.610,2021.ccl-1.108,0,0.0472503,"Missing"
2021.emnlp-main.610,P19-1569,0,0.0113839,"coder. Task-specific WSD datasets can be further used to fine-tune our model. of Machine Translation. Lesk (Basile et al., 2014) relies on a word-level similarity function to measure the semantic overlap between the context of a word and each sense definition. SENSEMBERT (Scarlini et al., 2020a) produces high-quality latent semantic representations of word meanings by incorporating knowledge contained in BabelNet into language models. Other approaches try to learn better gloss embeddings by considering the WordNet graph structure (e.g., hypernyms, hyponyms, synonyms, etc.) (Luo et al., 2018b; Loureiro and Jorge, 2019; Kumar et al., 2019; Bevilacqua and Navigli, 2020). For example, Kumar et al. (2019) proposed EWISE to improve model’s performance on rare or unseen senses by learning knowledge graph embeddings from WordNet. Building upon EWISE, Bevilacqua and Navigli (2020) developed a hybrid approach that incorporates more lexical knowledge (e.g., hypernymy, meronymy, similarity in WordNet) into the model through synset graph embeddings. 3 Overview of Our Approach Inventory Words Glosses ES Gls/W Oxford 52.5K 86.2K 96.8K 1.6 Webster 39.8K 72.5K 100.6K 1.8 Collins 34.4K 61.4K 89.5K 1.8 67.0K 64.9K 1.8 Cambr"
2021.emnlp-main.610,D18-1170,0,0.0663033,"dictionaries as a whole for all word senses, especially for rare senses that are less frequently seen in human-annotated data. but demonstrates strong applicability to low-shot senses. The general model can turn into an expert model to achieve new state-of-the-art performance after further fine-tuning. 2 Related Work Supervised WSD Approaches. Most existing WSD models are learned in a supervised manner and depend on human-annotated data. For example, Raganato et al. (2017a) regarded WSD as a sequence labeling task and trained a BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense corresponds to the correct sense or not. Bi-Encoder Model (BEM) (Blevins and Zettlemoyer, 2020) represents the target words and senses in the same e"
2021.emnlp-main.610,P18-1230,0,0.0856623,"dictionaries as a whole for all word senses, especially for rare senses that are less frequently seen in human-annotated data. but demonstrates strong applicability to low-shot senses. The general model can turn into an expert model to achieve new state-of-the-art performance after further fine-tuning. 2 Related Work Supervised WSD Approaches. Most existing WSD models are learned in a supervised manner and depend on human-annotated data. For example, Raganato et al. (2017a) regarded WSD as a sequence labeling task and trained a BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense corresponds to the correct sense or not. Bi-Encoder Model (BEM) (Blevins and Zettlemoyer, 2020) represents the target words and senses in the same e"
2021.emnlp-main.610,W04-0807,0,0.274302,"Missing"
2021.emnlp-main.610,H93-1061,0,0.853162,"entories and then train the semantic equivalence recognizer (SemEq) to do WSD. 6.2.1 All-Words WSD Tasks We evaluate our model on the all-words WSD framework established by Raganato et al. (2017b). The testing dataset contains 5 benchmark datasets from previous Senseval and SemEval competitions, including Senseval-2 (SE2) (Edmonds and Cotton, 2001), Senseval-3 (SE3) (Mihalcea et al., 2004), SemEval-07 (SE07) (Pradhan et al., 2007), SemEval-13 (SE13) (Navigli et al., 2013), and SemEval-15 (SE15) (Moro and Navigli, 2015). Following Raganato et al. (2017b) and other previous work, we use SemCor (Miller et al., 1993) that contains 226,036 annotated instances as the build-in training set and choose SemEval-07 as the development set for hyper-parameter tuning. Since all datasets are mapped to word senses in WordNet 3.0 (Miller, 1995), we retrieve all definition sentences of the target word from WordNet 3.0 to form gloss-context pairs for both training and testing. Transfer Learning. We first train our semantic equivalence recognizer ONLY using gloss-context pairs generated from our aligned word sense inventories. The trained classifier is a general model (SemEq-General) capable of deciding whether a Table 3"
2021.emnlp-main.610,S15-2049,0,0.065499,"Missing"
2021.emnlp-main.610,S13-2040,0,0.0501708,"Missing"
2021.emnlp-main.610,P10-1023,0,0.0786678,"Missing"
2021.emnlp-main.610,D19-1005,0,0.0341157,"Missing"
2021.emnlp-main.610,N19-1128,0,0.0574969,"Missing"
2021.emnlp-main.610,S07-1016,0,0.0514706,"Missing"
2021.emnlp-main.610,D17-1120,0,0.285361,"the best matching that maximizes the overall textual similarity. In this way, we can gather general semantic equivalence knowledge from various dictionaries as a whole for all word senses, especially for rare senses that are less frequently seen in human-annotated data. but demonstrates strong applicability to low-shot senses. The general model can turn into an expert model to achieve new state-of-the-art performance after further fine-tuning. 2 Related Work Supervised WSD Approaches. Most existing WSD models are learned in a supervised manner and depend on human-annotated data. For example, Raganato et al. (2017a) regarded WSD as a sequence labeling task and trained a BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense correspo"
2021.emnlp-main.610,E17-1010,0,0.315449,"the best matching that maximizes the overall textual similarity. In this way, we can gather general semantic equivalence knowledge from various dictionaries as a whole for all word senses, especially for rare senses that are less frequently seen in human-annotated data. but demonstrates strong applicability to low-shot senses. The general model can turn into an expert model to achieve new state-of-the-art performance after further fine-tuning. 2 Related Work Supervised WSD Approaches. Most existing WSD models are learned in a supervised manner and depend on human-annotated data. For example, Raganato et al. (2017a) regarded WSD as a sequence labeling task and trained a BiLSTM model with self-attention using multiple auxiliary losses. Luo et al. (2018a) introduced a hierarchical coattention mechanism to generate gloss and context representations that can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense correspo"
2021.emnlp-main.610,D19-1410,0,0.016107,"ved two word sense sets S1 and S2 from two inventories, where each set consists of a list of definition sentences (glosses). Given a reward function r: S1 ×S2 → R, we want to find a P matching2 f : S1 → S2 such that the total rewards a∈S1 ,f (a)∈S2 r(a, f (a)) is maximized. By finding the matching f , we will know the best alignment between two word sense sets S1 and S2 . In this paper, we use the sentence-level textual similarity as the reward function to find the best word sense alignment. To measure the textual similarity between two definition sentences, we apply a pretrained model SBERT (Reimers and Gurevych, 2019) that has achieved state-of-the-art performance on many Semantic Textual Similarity (STS) tasks and Paraphrase Detection tasks. Specifically, we apply SBERT to S1 and S2 to get sentence embeddings and then calculate cosine similarity as the reward function. xij ∈ {0, 1}, i ∈ S1 , j ∈ S2 In our implementation, we consider all possible inventory combinations (select two from six) and apply the gloss alignment solver3 to all common words shared by two inventories. For each word, the gloss alignment solver is only applied to glosses under the same POS category. Overall, we obtain 704K gloss alignm"
2021.emnlp-main.610,2020.emnlp-main.285,0,0.0309007,"rt illustrates the gloss alignment algorithm where each blue circle is a gloss containing one definition sentence and several example sentences. The right part is our model architecture to predict the semantic equivalence of a word in context and a gloss by comparing their representations obtained from a shared transformer encoder. Task-specific WSD datasets can be further used to fine-tune our model. of Machine Translation. Lesk (Basile et al., 2014) relies on a word-level similarity function to measure the semantic overlap between the context of a word and each sense definition. SENSEMBERT (Scarlini et al., 2020a) produces high-quality latent semantic representations of word meanings by incorporating knowledge contained in BabelNet into language models. Other approaches try to learn better gloss embeddings by considering the WordNet graph structure (e.g., hypernyms, hyponyms, synonyms, etc.) (Luo et al., 2018b; Loureiro and Jorge, 2019; Kumar et al., 2019; Bevilacqua and Navigli, 2020). For example, Kumar et al. (2019) proposed EWISE to improve model’s performance on rare or unseen senses by learning knowledge graph embeddings from WordNet. Building upon EWISE, Bevilacqua and Navigli (2020) developed"
2021.emnlp-main.610,2020.findings-emnlp.4,0,0.0203759,"t can attend to each other. More recently, several BERT-based models have achieved new state-of-the-art performance on WSD by fine-tuning a pretrained language model. GlossBERT (Huang et al., 2019) appends each gloss to a given context sentence to create pseudo sentences and predicts them as either positive or negative depending on whether the sense corresponds to the correct sense or not. Bi-Encoder Model (BEM) (Blevins and Zettlemoyer, 2020) represents the target words and senses in the same embedding space using a context encoder and a gloss encoder but optimizes on each word individually. Yap et al. (2020) formulated WSD as a relevance ranking task and fine-tuned BERT to select the most probable sense definition from candidate senses. The neural architecture of our semantic equivalence recognizer realizes the benefits of GlossBERT and BEM. Knowledge-Based WSD Approaches. Closely Overall, the major contributions of our work are related to our work, many knowledge-based aptwo-fold. 1) We propose a gloss alignment algo- proaches rely on Lexical Knowledge Bases (LKB), rithm that can integrate lexical knowledge from such as Wikipedia and WordNet, to enhance repredifferent word sense inventories to t"
2021.findings-emnlp.6,D19-1253,0,0.0238646,"several methods to use the generated QA-based weakly-labeled MRC data. We further propose a simple yet effective self-teaching paradigm to better utilize large-scale weakly-labeled data. • We show that our QA-based weakly-labeled MRC data can be easily used along with other types of weakly-labeled data for further gains. 2 2.1 Related Work From Question Answering to Machine Reading Comprehension This work is related to data augmentation in semi-supervised MRC studies, which partially or fully rely on the document-question-answer triples (Yang et al., 2017; Yuan et al., 2017; Yu et al., 2018; Zhang and Bansal, 2019; Zhu et al., 2019; Dong et al., 2019; Sun et al., 2019b; Alberti et al., 2019; Asai and Hajishirzi, 2020; Rennie et al., 2020) of target MRC tasks or at least similar domain corpora (Dhingra et al., 2018). We focus on leveraging multi-domain QA data to improve different types of general-domain MRC tasks. We study the effect of our large-scale weaklylabeled MRC data on representative MRC datasets for Chinese: a multiple-choice dataset, C3 (Sun et al., 2020b), in which most questions cannot be solved solely by matching or paraphrasing, and an extractive dataset, CMRC 2018 (Cui et al., 2019), in"
2021.findings-emnlp.6,2020.emnlp-main.142,0,0.18817,"eved context instead of noisy answers, (ii) we generate weakly-labeled data based on existing large-scale QA data covering a wide range of domains, instead of the same domain (He et al., 2020; Xie et al., 2020a; Zhao et al., 2020; Chen et al., 2020) or at least approximately in-domain (Du et al., 2020) as the target MRC task, and (iii) ground-truth labels of weaklylabeled data are used directly or indirectly to train teacher models. Note that we use teacher models to generate new soft labels for fixed weakly-labeled data instead of new pseudo data with noisy labels from unlabeled data (e.g., (Wang et al., 2020a)). Compared with previous multi-teacher student paradigms (You et al., 2019; Wang et al., 2020b; Yang et al., 2020), to train models to be strong teachers, we conduct iterative training and leverage large-scale weakly-labeled data rather than using clean, human-labeled data of similar tasks. 3 3.1 3.2 Comparisons with Existing Subject-Area Question-Answering Datasets Subject-area QA is an increasingly popular direction focusing on closing the performance gap between humans and machines in answering questions collected from real-world exams that are carefully designed by subject-matter expert"
2021.findings-emnlp.6,2020.coling-main.248,0,0.0406771,"e widely used for knowledge distillation (Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015). We aim to let a student model outperform its teacher model for performance improvements and thus use the same architecture for all teacher and student models. Our work is related to self-training (Yarowsky, 1995; Riloff, 1996). The main differences are (i) noise is introduced by retrieved context instead of noisy answers, (ii) we generate weakly-labeled data based on existing large-scale QA data covering a wide range of domains, instead of the same domain (He et al., 2020; Xie et al., 2020a; Zhao et al., 2020; Chen et al., 2020) or at least approximately in-domain (Du et al., 2020) as the target MRC task, and (iii) ground-truth labels of weaklylabeled data are used directly or indirectly to train teacher models. Note that we use teacher models to generate new soft labels for fixed weakly-labeled data instead of new pseudo data with noisy labels from unlabeled data (e.g., (Wang et al., 2020a)). Compared with previous multi-teacher student paradigms (You et al., 2019; Wang et al., 2020b; Yang et al., 2020), to train models to be strong teachers, we conduct iterative training and leverage large-scale"
2021.findings-emnlp.6,P19-1415,0,0.0251983,"the generated QA-based weakly-labeled MRC data. We further propose a simple yet effective self-teaching paradigm to better utilize large-scale weakly-labeled data. • We show that our QA-based weakly-labeled MRC data can be easily used along with other types of weakly-labeled data for further gains. 2 2.1 Related Work From Question Answering to Machine Reading Comprehension This work is related to data augmentation in semi-supervised MRC studies, which partially or fully rely on the document-question-answer triples (Yang et al., 2017; Yuan et al., 2017; Yu et al., 2018; Zhang and Bansal, 2019; Zhu et al., 2019; Dong et al., 2019; Sun et al., 2019b; Alberti et al., 2019; Asai and Hajishirzi, 2020; Rennie et al., 2020) of target MRC tasks or at least similar domain corpora (Dhingra et al., 2018). We focus on leveraging multi-domain QA data to improve different types of general-domain MRC tasks. We study the effect of our large-scale weaklylabeled MRC data on representative MRC datasets for Chinese: a multiple-choice dataset, C3 (Sun et al., 2020b), in which most questions cannot be solved solely by matching or paraphrasing, and an extractive dataset, CMRC 2018 (Cui et al., 2019), in which all answers"
2021.naacl-main.119,N19-1116,0,0.38966,"odality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conventional ap- important knowledge required for language learnproaches learn constituency parser from human"
2021.naacl-main.119,P19-1031,0,0.0143096,"rom different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conve"
2021.naacl-main.119,Q18-1016,1,0.473556,"each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conventional ap- important knowledge required for language learnproaches learn constitu"
2021.naacl-main.119,P19-1234,1,0.359842,"run of SENet154, I3D and MMC-PCFG. We can observe that SENet identifies all NPs but fails at the VP. I3D correctly predicts the VP but fails at recognizing a NP, “the man”. Our MMCPCFG can take advantages of all experts and produce the correct prediction. 5 Related Work Grammar Induction Grammar induction and unsupervised parsing has been a long-standing problem in computational linguistics (Carroll and Charniak, 1992). Recent work utilized neural networks in predicting constituency structures with no supervision (Shen et al., 2018a; Drozdov et al., 2019; Shen et al., 2018b; Kim et al., 2019; Jin et al., 2019a) and showed promising results. In addition to learning purely from text, there is a growing interest to use image information to improve accuracy of induced constituency trees (Shi et al., 2019; Kojima et al., 2020; Zhao and Titov, 2020; Jin and Schuler, 2020). Different from previous work, our work improves the constituency parser by using videos containing richer information than images. Video-Text Matching Video-text matching has been widely studied in various tasks, such as video 4 retrieval (Liu et al., 2019; Gabeur et al., 2020), moDifferent runs represent models trained with different"
2021.naacl-main.119,2020.aacl-main.42,1,0.595113,"knowledge required for language learnproaches learn constituency parser from human- ing that is ignored by text (Gleitman, 1990; Pinker annotated datasets such as Penn Treebank (Marcus and MacWhinney, 1987; Tomasello, 2003). This et al., 1993). However, annotating syntactic trees task aims to learn a constituency parser from raw by human language experts is expensive and time- unlabeled text aided by its visual context. consuming, while the supervised approaches are Previous methods (Shi et al., 2019; Kojima et al., limited to several major languages. In addition, 2020; Zhao and Titov, 2020; Jin and Schuler, 2020) the treebanks for training these supervised parsers learn to parse sentences by exploiting object inforare small in size and restricted to the newswire mation from images. However, images are static domain, thus their performances tend to be worse and cannot present the dynamic interactions among ∗ visual objects, which usually correspond to verb This work was done when Songyang Zhang was an intern at Tencent AI Lab. phrases that carry important information. There1513 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Langu"
2021.naacl-main.119,Q18-1019,0,0.0606338,"Missing"
D18-1038,P16-1168,0,0.0671343,"Missing"
D18-1038,P15-2130,0,0.0572375,"Missing"
D18-1038,P17-1163,0,0.0507404,"Missing"
D18-1038,W14-4340,0,0.0784981,"Missing"
D18-1038,Q17-1022,0,0.282844,"Missing"
D18-1038,kamholz-etal-2014-panlex,0,0.0230065,"for testing. We use the German and Italian as the target language to transfer our knowledge from English DST system. In the experiments, we do not have access to any training or validation dataset for German and Italian, and we only have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We split the raw DST corpus into turn-level examples. During training, we use the ground truth previous state Vt−1 as inputs. At test time, we use the model searched states as the previous state to continue tracking intention until"
D18-1038,D17-1302,0,0.0540255,"s, showing that our methods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset English training dat"
D18-1038,P17-1135,0,0.09568,"Missing"
D18-1038,P17-4012,0,0.183119,"get language word by word using the bilingual dictionary, which is used to train the NBT in target side. We demonstrate the results for our proposed algorithms and other competing algorithms in Table 2, from which we can easily conclude that that (i) our Decoupled NBT does not affect the performance, and (ii) our cross-lingual NBT framework is able to achieve significantly better accuracy for both languages in both parallel-resource scenarios. Compare with Translator/WBW. With bilingual corpus, XL-NBT-C with pre-trained bilingual embedding can significantly outperform our Translator baseline (Klein et al., 2017). This is intuitive because the translation model requires 3 https://github.com/nmrksic/ neural-belief-tracker/tree/master/data 4 https://wit3.fbk.eu/mt.php?release= 2014-01 5 https://github.com/nmrksic/ neural-belief-tracker 420 Error Type Examples Modify Failure Machine: I have two options that fit that description, golden wok Chinese restaurant and the Nirala which serves Indian food, do you have a preference? User: How about Nirala, whats the address and phone of that? Previous State: food=Chinese; Prediction: food=none; Groundtruth: food=Indian Maintain Failure History Failure Machine: th"
D18-1038,I17-1074,0,0.0897675,"Missing"
D18-1038,P17-1178,0,0.0378339,"ods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset English training dataset User: I’m looking"
D18-1038,2012.eamt-1.60,0,0.0331517,"es. The train, valid and test datasets for three different languages (English, German, Italian) are available online3 . We use the English as source language where 600 dialogs are used for training, 200 for validation and 400 for testing. We use the German and Italian as the target language to transfer our knowledge from English DST system. In the experiments, we do not have access to any training or validation dataset for German and Italian, and we only have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We sp"
D18-1038,D14-1162,0,0.0888557,"ly have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We split the raw DST corpus into turn-level examples. During training, we use the ground truth previous state Vt−1 as inputs. At test time, we use the model searched states as the previous state to continue tracking intention until the end of the 6.2 Results Here we highlight the baselines we use to compare with our cross-lingual algorithm as follows: (1) Supervised: this baseline algorithm assumes the existence of annotated dialog belief tracking datasets, and"
D18-1038,P13-1046,0,0.0281024,"ing no target-side dialog data, our method relies on other easy-to-access parallel resources to understand the connection between languages. Depending on the popularity and availability of target language resources, we study two kinds of parallel data: bilingual corpus and bilingual dictionary, and we respectively design two transfer learning strategies. languages with zero annotated data. 2 2.1 Related Work Dialog State Tracking Broadly speaking, the dialog belief tracking algorithms can be divided into three families: 1) hand-crafted rules 2) generative models, and 3) maximum-entropy model (Metallinou et al., 2013). Later on, many deep learning based discriminative models have surged to replace the traditional strategies (Henderson et al., 2014a; Mrksic et al., 2017; Williams et al., 2016) and achieved state-of-the-art results on various datasets. Though the discriminative models are reported to achieve fairly high accuracy, their applications are heavily restricted by the domain, ontology, and language. Recently, a pointer network based algorithm (Xu and Hu, 2018) and another multi-domain algorithm (Rastogi et al., 2017) have been proposed to break the ontology and domain boundary. Besides, (Mrkˇsi´c e"
D18-1038,E17-1042,0,0.0553359,"Missing"
D18-1038,D15-1199,0,0.0815689,"Missing"
D18-1038,P17-1062,0,0.036233,"Missing"
D18-1038,P18-1134,0,0.0115166,"ef tracking algorithms can be divided into three families: 1) hand-crafted rules 2) generative models, and 3) maximum-entropy model (Metallinou et al., 2013). Later on, many deep learning based discriminative models have surged to replace the traditional strategies (Henderson et al., 2014a; Mrksic et al., 2017; Williams et al., 2016) and achieved state-of-the-art results on various datasets. Though the discriminative models are reported to achieve fairly high accuracy, their applications are heavily restricted by the domain, ontology, and language. Recently, a pointer network based algorithm (Xu and Hu, 2018) and another multi-domain algorithm (Rastogi et al., 2017) have been proposed to break the ontology and domain boundary. Besides, (Mrkˇsi´c et al., 2017) has proposed an algorithm to train a unified framework to deal with multiple languages with annotated datasets. In contrast, our paper focuses on breaking the language boundary and transfer DST knowledge from one language into other zeroannotation languages. We use the popular Wizard-of-Oz (RojasBarahona et al., 2017) dataset as our DST benchmark to evaluate the effectiveness of our crosslingual transfer learning. We specify English as the so"
D18-1038,N16-1156,0,0.0393479,"the proposed methods, showing that our methods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset E"
D18-1038,D16-1137,0,\N,Missing
D19-1528,J10-4007,0,0.091241,"Missing"
D19-1528,D14-1082,0,0.354488,"and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975)"
D19-1528,D14-1004,0,0.446568,"Missing"
D19-1528,N19-1423,0,0.0292815,"counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the state-of-the-art performance on the pseudo-disambiguation task. For word2Vec and GloVe, we use their released code. For D-embedding, we follow their origin"
D19-1528,J15-4004,0,0.0271936,"cing their overall semantics. We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities. 3.4 Word Similarity Measurement In addition to SP acquisition, we also evaluate our embeddings on word similarity (WS) measurement to test whether the learned embedding can effectively capture the overall semantics. We use SimLex-999 (Hill et al., 2015) as the evaluation dataset for this task because it contains different word types, i.e., 666 noun pairs, 222 verb pairs, and 111 adjective pairs. We follow the conventional setting that uses the Spearman’s correlation to assess the correspondence between the similarity scores and human annotations on all word pairs. Evaluations are conducted on the final embeddings v for each relation and the center ones. Results are reported in Table 3 with several observations. First, our model achieves the best overall performance and significantly better on nouns, which can be explained by that nouns appea"
D19-1528,C16-1266,0,0.0324516,"Missing"
D19-1528,J03-3005,0,0.150689,"n different parameters in a sequential manner and applied in various areas. natively update c and u upon the convergence of c. As a result, we set λ to 1 in the first half of the training process and 0 afterwards. 3 Experiments Experiments are conducted to evaluate how our embeddings are performed on SP acquisition and word similarity measurement. 3.1 Implementation Details We use the English Wikipedia4 as the training corpus. The Stanford parser5 is used to obtain dependency relations among words. For the fair comparison, we follow existing work and set d = 300, s = 10, and a = 1. Following (Keller and Lapata, 2003) and (de Cruys, 2014), we select three dependency relations (nsubj, dobj, and amod) as follows: • nsubj: The preference of subject for a given verb. For example, it is plausible to say ‘dog barks’ rather than ‘stone barks’. The verb is viewed as the predicate (head) while the subject as the argument (tail). • dobj: The preference of object for a given verb. For example, it is plausible for ‘eat food’ rather than ‘eat house’. The verb is viewed as the predicate (head) while the object as the argument (tail). • amod: The preference of modifier for a given noun. For example, it is plausible to sa"
D19-1528,N18-2108,0,0.0513578,"o proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975) is a typical scenario. In general, SP refers to that, given a word (predicate) and a dependency relation"
D19-1528,P14-2050,0,0.500537,"ociation for Computational Linguistics Figure 1: Illustration of the multiplex embeddings for ‘sing’ and ‘song’. Black arrows present center embeddings for words’ overall semantics; blue and green arrows refer to words’ relational embeddings for relationdependent semantics. All relational embeddings for each word are designed to near its center embedding. nsubj and dobj relations are used as examples. treat ‘food’ and ‘eat’ as highly relevant words but never distinguish the function of ‘food’ to be a subject or an object to ‘eat’. To address this problem, the dependency-based embedding model (Levy and Goldberg, 2014) is proposed to treat a word through separate ones, e.g., ‘food@dobj’ and ‘food@nsubj’, under different syntactic relations, with the skip-gram (Mikolov et al., 2013) model being used to train the final embeddings. However, this method is limited in two aspects. First, sparseness is introduced because each word is treated as two irrelevant ones (e.g., ‘food@dobj’ and ‘food@nsubj’), so that the overall quality of learned embeddings is affected. Second, the resulting embedding size is too large1 , which is not appropriate either for storage or usage. Therefore, in this paper, we propose a multip"
D19-1528,D14-1162,0,0.123887,"small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are n"
D19-1528,N18-1202,0,0.0549045,"lear how to leverage these methods in downstream tasks, we label these methods as downstream unfriendly. • Posterior Probability (PP) (Resnik, 1997), a counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the stat"
D19-1528,P99-1014,0,0.0505662,"Missing"
D19-1528,D17-1068,0,0.0394857,"Missing"
D19-1528,P19-1071,1,0.908357,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,N19-1093,1,0.889896,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,P19-1083,1,0.86796,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,D13-1141,0,0.0335748,"s unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of the resulting embeddings. Alternatively, our MWE model uses a set of (constrained and small)"
D19-1528,K17-1016,1,0.831006,"ional embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scen"
D19-1528,N18-2028,1,0.859782,"t is easily computed for D-embeddings that 200,000 words will result in a 10GB model, which is unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of"
D19-1528,C16-1203,0,0.0273576,"Missing"
D19-5412,C10-1039,0,0.699654,"are incorporated in this work to capture the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35"
D19-5412,W09-1802,0,0.288394,"resentations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the qualit"
D19-5412,hong-etal-2014-repository,0,0.313108,"e modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and select"
D19-5412,P19-1098,1,0.830669,"ters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an emphasis on better sentence selection for extractive multi-document summarization. The major research contributions of this work include the following: (i) we make a first attempt to combine DPP with BERT representations to measure sentence quality and diversity and report encouraging results on benchmark summarization datasets; (ii) our findings suggest that it is best to model sentence quality, i.e., how important a sentence is to the summary, by combining semantic representations and surface indicators of the sentence,"
D19-5412,P19-1285,0,0.0200765,"high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an em"
D19-5412,P19-1209,1,0.829815,"s a feature vector for sentence i and θ are feature weights to be learned during DPP training. We optimize θ by maximizing log-likelihood with gradient descent, illustrated as follows: L(θ)= M X logP(Yˆ (m);L(m)(θ)), 2.1 We introduce two models that fine-tune the BERTbase architecture (Devlin et al., 2018) to calculate the similarity between a pair of sentences (BERTsim) and learn representations that characterize the importance of a single sentence (BERT-imp). Importantly, training instances for both BERT models are derived from single-document summarization dataset (Hermann et al., 2015) by Lebanoff et al. (2019), containing a collection of single sentences (or sentence pairs) and their associated labels. During testing, the trained BERT models are applied to single sentences and sentence pairs derived from multi-document input to obtain quality and similarity measures. BERT-sim takes as input a pair of sentences and transforms each token in the sentence into an embedding using an embedding layer. They are then passed through the BERT-base architecture to pro(3) m=1 ∇θ = M X X m=1 i∈Yˆ (m) f (i)− X (m) f (j)Kjj , BERT Architecture (4) j 99 duce a vector representing the input sentence pair. The vector"
D19-5412,D15-1228,1,0.848755,"dings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proceedings of the 2nd Worksh"
D19-5412,W04-1013,0,0.028359,"ce pairs and the instances are balanced. 1 The sentence features include the length and position of a sentence, the cosine similarity between sentence and document TF-IDF vectors (Kulesza and Taskar, 2011). We abstain from using sophisticated features to avoid model overfitting. 2 100 The coefficient is set to be 0.9 for both datasets. DUC/TAC We evaluate our DPP approach (§2) on multi-document summarization datasets including DUC and TAC (Over and Yen, 2004; Dang and Owczarzak, 2008). The task is to generate a summary of 100 words from a collection of news articles. We report ROUGE F-scores (Lin, 2004)3 on DUC-04 (trained on DUC-03) and TAC-11 (trained on TAC-08/09/10) following standard settings (Hong et al., 2014). Ground-truth extractive summaries used in DPP training are obtained from Cho et al. (2019). 3.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm (Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) Experiment Settings We"
D19-5412,N10-1134,0,0.0438514,"DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proce"
D19-5412,2021.ccl-1.108,0,0.129635,"Missing"
D19-5412,N18-1202,0,0.0532157,"ty measure compares a pair of sentences for redundancy. If a sentence is of high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019)"
D19-5412,P17-1099,0,0.52447,"the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 dataset evaluated by ROUGE. † indica"
D19-5412,C10-1111,0,0.0374697,"ther, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar"
D19-5412,C18-1146,1,0.940578,"e the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 datas"
D19-5412,N09-1041,0,\N,Missing
D19-5605,P98-1013,0,0.218741,"han one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for generating continuations. Frame Semantics. A story is composed of a sequence of meaningful events (Chatman, 1980), often following particular patterns described in various terms such as scripts (Schank and Abelson, 1977) and frames. FrameNet (Baker et al., 1998) is an inventory of semantic frames, which are semantic abstractions describing universal categories of events, concepts, and relationships. We consider frame semantics as another control attribute in our framework. In order to get a frame semantic representation for a continuation, we use SEMAFOR (Das et al., 2014). SEMAFOR automatically produces a frame-semantic parse for a sentence, which consists of spans that evoke particular frames in FrameNet as well as annotations Table 2: Generated continuations from our framework with different control attribute values. Boldface indicates attribute v"
D19-5605,D17-1168,0,0.0183918,"nteresting story endings, albeit without control variables. In stronger relevance to our work, Clark et al. (2018b) explore a creative writing setting with a machine in the loop, albeit with mixed results in terms of the quality of system suggestions. Predicting and controlling with frame values suggests a new way of interacting with collaborative writing systems, as long as frames can be communicated to users in ways they can easily understand. Recently, Clark et al. (2018a) proposed a neural text generation method that explicitly represents and tracks entities. In addition, event sequences (Chaturvedi et al., 2017; Liu et al., 2018) are important elements in narrative texts but under-explored for story generation. 5 The BS and TS baselines do not use control variables. We remove results from 10-question sets where more than half of the questions were answered with the “neither” option, as we were concerned that these annotators did not fully understand the task or did not spend enough time studying the continuations. This occurred in roughly one third of question sets. 6 51 References These and related characteristics of creative writing could be incorporated into our framework as control attributes in"
D19-5605,N18-1204,0,0.160133,"pare the diversity of story continuations controlled by different sentence attributes and find 44 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 44–58 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d where vi is the vector representation of word xi , si ∈ Rd is the hidden state at time i, and fe1 and fe2 are the forward and backward RNN functions. frames to generate continuations. One potential use case of controllable, diverse story generation is collaborative writing applications (Clark et al., 2018b). We conduct a human evaluation to assess the utility of providing multiple suggestions from our models in this setting, demonstrating promising results for the potential of controllable generation for collaborative writing. 2 Decoder. Our decoder uses an RNN with the general global attention scheme from Luong et al. (2015). An additional input zdec is fed to the decoder at each time step to reflect the characteristics of the control variable: Task Description and Definitions hj = fd ([yj−1 ; zdec ], hj−1 ) Given a story context and a control attribute value, our goal is to generate a story"
D19-5605,W18-2501,0,0.0134655,"ted summaries with a desired length (Kikuchi et al., 2016; Fan et al., 2018a). We similarly use length of the continuation as a control attribute. Instead of using an embedding for each integer length value, we group the lengths into a small number of bins (details are provided below). zenc and zdec are fixed one-hot vectors for each bin. Verbal Predicates. Semantic role labeling (SRL) is a form of shallow semantic parsing that annotates predicates and their arguments in sentences. We consider predicates from a semantic role labeling as control attributes. We use the SRL system from AllenNLP (Gardner et al., 2018) to automatically obtain predicates for the continuations in our training set. Then, a predicate vector is obtained by first summing up 100-dimensional GloVe embeddings (Pennington et al., 2014) of the predicted predicates (if there is more than one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for g"
D19-5605,W04-1013,0,0.0194067,"frames. For example, in the sentence “Roa’s advice made Emma a lot happier in her life!”, “a lot” evokes the Quantity frame while “Emma a lot happier” evokes the Effect frame. The frame set variable z is computed by summing embeddings for the frames in the set: X z = R(l) = Rj (2) “continuation”) given the previous four sentences. We use the 10k most frequent words in the training set as our vocabulary. A special token hunk i is introduced for unknown words. 5.2 Previous work evaluates generation tasks with automatic metrics, such as perplexity (PPL), BLEU (Papineni et al., 2002),3 and ROUGE (Lin, 2004). We adopt these in our evaluation and add three more metrics using the pretrained story scorer from Sagarkar et al. (2018). The scorer rates a generated continuation given its context along three dimensions: relevance (R), interestingness (I), and overall quality (O). The story scorer does not use a gold standard continuation. In addition, to evaluate the diversity of the generation, we use Max-BLEU4 and Max-ROUGE. First, we compute BLEU and ROUGE scores over a set of outputs (y1 , y2 , ..., yn ) with different attribute values given the same story context, then we compute the max scores: j∈l"
D19-5605,P02-1040,0,0.107184,"Missing"
D19-5605,D14-1162,0,0.0823943,"er length value, we group the lengths into a small number of bins (details are provided below). zenc and zdec are fixed one-hot vectors for each bin. Verbal Predicates. Semantic role labeling (SRL) is a form of shallow semantic parsing that annotates predicates and their arguments in sentences. We consider predicates from a semantic role labeling as control attributes. We use the SRL system from AllenNLP (Gardner et al., 2018) to automatically obtain predicates for the continuations in our training set. Then, a predicate vector is obtained by first summing up 100-dimensional GloVe embeddings (Pennington et al., 2014) of the predicted predicates (if there is more than one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for generating continuations. Frame Semantics. A story is composed of a sequence of meaningful events (Chatman, 1980), often following particular patterns described in various terms such as scripts ("
D19-5605,D17-1228,0,0.159581,"Missing"
D19-5605,N12-1059,0,\N,Missing
D19-5605,D15-1166,0,\N,Missing
D19-5605,P10-1158,0,\N,Missing
D19-5605,C98-1013,0,\N,Missing
D19-5605,J14-1002,0,\N,Missing
D19-5605,D13-1170,0,\N,Missing
D19-5605,D13-1111,1,\N,Missing
D19-5605,N16-1098,0,\N,Missing
D19-5605,S18-2024,1,\N,Missing
D19-5605,P19-1254,0,\N,Missing
D19-5605,W18-2706,0,\N,Missing
D19-5605,W16-0202,0,\N,Missing
D19-5605,N16-1014,0,\N,Missing
D19-5804,P04-3031,0,0.221819,"l language exams, respectively. Experiments and Discussions Datasets in Section 2.1). We first fine-tune BERTLARGE for five epochs on RACE to get the pre-fine-tuned model and then further fine-tune the model for eight epochs on the target QA datasets in scientific domains. We show the accuracy of the prefine-tuned model on RACE in Table 4. We use the noun phrase chunker in spaCy2 to extract concept mentions. For information retrieval, we use the version 7.4.0 of Lucene (McCandless et al., 2010) and set the maximum number of the retrieved sentences K to 50. We use the stop word list from NLTK (Bird and Loper, 2004). In addition, we design two slightly different settings for information retrieval. In setting 1, the original reference corpus of each dataset is independent. Formally, for each dataset x ∈ D, we perform information retrieval based on the corresponding original reference corpus of x and/or the external corpus generated based on problems in x, where D = {ARC-Easy, ARC-Challenge, OpenBookQA}. In setting 2, all original reference corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference"
D19-5804,N19-1423,0,0.365628,"through magnetism” and “iron is always magnetic”}, as well as general world knowledge extracted from an external source such as {“a belt buckle is often made of iron” and “iron is metal”} are required. Thus, these QA tasks provide suitable testbeds for evaluating external knowledge exploitation and intergration. Previous subject-area QA methods (e.g., (Khot et al., 2017; Zhang et al., 2018; Zhong et al., 2018)) explore many ways of exploiting structured knowledge. Recently, we have seen that the framework of fine-tuning a pre-trained language model (e.g., GPT (Radford et al., 2018) and BERT (Devlin et al., 2019)) outperforms previous state-of* Equal contribution. This work was conducted when the two authors were at Tencent AI Lab, Bellevue, WA. 1 Ground truth facts are usually not provided in this kind of question answering tasks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploitin"
D19-5804,P17-1147,0,0.0411808,"activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang"
D19-5804,K17-1010,0,0.0528847,"et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et al., 2017; Musa et al., 2018; Ni et al., 2019; Yadav et al., 2019) perform information retrieval on the reference corpus to retrieve relevant sentences to form reference documents. In contrast, we retrieve relevant sentences from the combination of an opendomain resource and the original reference corpus to generate a reference document for each (question, answer option) pair. We still keep up to top K sentences for each reference document (Section 2.1). See the framework overview in Figure 1. 29 2.3 Utilization of External Knowledge from In-Domain Data Since there are a relatively small number of trai"
D19-5804,D18-1260,0,0.475044,"nell University, Ithaca, NY, USA 3 Tencent AI Lab, Bellevue, WA, USA obtained from sources outside of the text (McNamara et al., 2004; Salmer´on et al., 2006). It is perhaps not surprising then, that machine readers also require knowledge external to the text itself to perform well on question answering (QA) tasks. We focus on multiple-choice QA tasks in subject areas such as science, in which facts from the given reference corpus (e.g., a textbook) need to be combined with broadly applicable external knowledge to select the correct answer from the available options (Clark et al., 2016, 2018; Mihaylov et al., 2018). For convenience, we call these subject-area QA tasks. Abstract We focus on multiple-choice question answering (QA) tasks in subject areas such as science, where we require both broad background knowledge and the facts from the given subject-area reference corpus. In this work, we explore simple yet effective methods for exploiting two sources of external knowledge for subject-area QA. The first enriches the original subject-area reference corpus with relevant text snippets extracted from an open-domain resource (i.e., Wikipedia) that cover potentially ambiguous concepts in the question and a"
D19-5804,I17-1097,0,0.0238765,"leading to large forest fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a"
D19-5804,D18-1055,0,0.0207706,"Missing"
D19-5804,Q19-1026,0,0.0212927,"change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang et al., 2019). As shown in Table 5, the basel"
D19-5804,N19-1030,0,0.587879,"sks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploiting more sources of external knowledge, and we hope the present empirical study can serve as a new starting point for researchers to identify the remaining challenges in this area. the-art methods (Mihaylov et al., 2018; Ni et al., 2019). However, it is still not clear how to incorporate different sources of external knowledge, especially unstructured knowledge, into this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background informatio"
D19-5804,N15-1119,1,0.804061,"tion retrieval; MRC: machine reading comprehension). Q, O, q, oi , di , and n denote the set of all questions, the set of all answer options, a question, one of the answer options associated with question q, the document (formed by retrieved sentences) associated with the (q, oi ) pair, and the number of answer options of q, respectively. Wikipedia. For example, the ambiguous concept mention “Mercury” in Table 2 should be linked to the concept Mercury (planet) rather than Mercury (element) in Wikipedia. For concept disambiguation and linking, we simply adopt an existing unsupervised approach (Pan et al., 2015) that first selects high quality sets of concept collaborators to feed a simple similarity measure (i.e., Jaccard) to link concept mentions. Question: Mercury, the planet nearest to the Sun, has extreme surface temperatures, ranging from 465◦ C in sunlight to −180◦ C in darkness. Why is there such a large range of temperatures on Mercury? A. The planet is too small to hold heat. B. The planet is heated on only one side. C. The planet reflects heat from its dark side. D. The planet lacks an atmosphere to hold heat. X Table 2: A sample problem from the ARC-Challenge dataset (Clark et al., 2018)"
D19-5804,D17-1082,0,0.346799,"sentences using the non-stop words in q and oi as the query and then concatenate the retrieved sentences to form di (Sun et al., 2019). The final prediction for each question is obtained by a linear plus softmax layer over the output of the final hidden state of the first token in each input sequence. By default, we employ the following two-step fine-tuning approach unless explicitly specified. Following previous work (Sun et al., 2019) based on GPT (Radford et al., 2018), we first finetune BERT (Devlin et al., 2019) on a large-scale multiple-choice machine reading comprehension dataset RACE (Lai et al., 2017) collected from English-as-a-foreign-language exams, which provides a ground truth reference document instead of a reference corpus for each question. Then, we further fine-tune the model on the target multiplechoice science QA datasets. For convenience, we call the model obtained after the first fine-tuning phase as a pre-fine-tuned model. We conduct experiments on three challenging multiple-choice science QA tasks where existing methods stubbornly continue to exhibit performance gaps in comparison with humans: ARC-Easy, ARC-Challenge (Clark et al., 2016, 2018), and OpenBookQA (Mihaylov et al"
D19-5804,D18-1053,0,0.0549991,"Missing"
D19-5804,D16-1264,0,0.190821,"Missing"
D19-5804,D15-1236,0,0.106165,"d science QA datasets. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a"
D19-5804,P16-2076,0,0.0294061,"s. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a clear distinction bet"
D19-5804,P18-1161,0,0.0703199,"Missing"
D19-5804,P14-5010,0,0.00309586,"can serve as a reliable piece of evidence to infer the correct answer option D for the question in Table 2. Just as human readers activate their background knowledge related to the text materials (Kendeou and Van Den Broek, 2007), we link concepts identified in questions and answer options to an opendomain resource (i.e., Wikipedia) and provide machine readers with unstructured background information relevant to these concepts, used to enrich the original reference corpus. Concept Identification and Linking: We first extract concept mentions from texts. Most mention extraction systems (e.g., Manning et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et a"
D19-5804,S18-1120,0,0.138342,"ence corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference corpus of D and/or the external corpus generated based on problems in D instead of x.3 . In our experiment, we use RACE (Lai et al., 2017) — the largest existing multiple-choice machine reading comprehension dataset collected from real and practical language exams — in the pre-finetuning stage. Questions in RACE focus on evaluating linguistic knowledge acquisition of participants and are commonly used in previous methods (Wang et al., 2018a; Sun et al., 2019). We evaluate the performance of our methods on three multiple-choice science QA datasets: ARC-Easy, ARC-Challenge, and OpenBookQA. ARC-Challenge and ARC-easy originate from the same set of exam problems collected from multiple sources. ARC-Challenge contains questions answered incorrectly by both a retrieval-based method and a word co-occurrence method, and the remaining questions form ARC-Easy. Questions in OpenBookQA are crowdsourced by turkers and then carefully filtered and modified by experts. See the statistics of these datasets in Table 3. Note that for OpenBookQA,"
D19-5804,W17-4413,0,0.028917,"fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained languag"
D19-5804,N19-1274,0,0.223526,"to this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background information relevant to the concepts and used to enrich the original reference corpus (Section 2.2). In comparison to previous work (e.g., (Yadav et al., 2019)), we perform information retrieval based on the enriched corpus instead of the original one to form a document for answering a question. Second, we increase the amount of training data by appending additional in-domain subject-area QA datasets (Section 2.3). 2 Method In this section, we first introduce our BERT-based QA baseline (Section 2.1). Then, we present how we incorporate external open-domain (Section 2.2) and in-domain (Section 2.3) sources of knowledge into the baseline. 2.1 Baseline Framework Given a question q, an answer option oi , and a reference document di , we concatenate them"
D19-5804,N19-4013,0,0.0363013,"Missing"
D19-5804,P06-4018,0,\N,Missing
D19-5804,P16-1223,0,\N,Missing
D19-5804,P17-2049,0,\N,Missing
D19-5804,N18-1202,0,\N,Missing
D19-5804,S18-1119,0,\N,Missing
D19-5804,D18-1453,0,\N,Missing
D19-5804,N19-1270,1,\N,Missing
D19-6012,D17-1082,0,0.100538,"Missing"
D19-6012,N16-1098,0,0.0691709,"Missing"
D19-6012,L18-1564,0,0.0337804,"ReCoRD RACE+ReCoRD RACE+ReCoRD All Datasets MCScript2.0 MCScript2.0 + MCScript MCScript2.0 + MCScript-w/o-who-how MCScript2.0 + SWAG MCScript2.0 + MCScript-w/o-who-how 85.0 86.0 86.6 85.9 84.7 84.2 - Table 2: Main results. when fine-tuned. edge. 3.2 4.2 Classifier and Encoder Fine-tuning Experiment Setup The fist stage fine-tuning endows the encoder with more new knowledge, while the second stage finetuning focuses on adjusting weights in the encoder to adapt to the target task. This phase is carried on the fine-tuned encoder with the support of additional commonsense datasets, like MCScript (Ostermann et al., 2018), SWAG (Zellers et al., 2018). The most benefits come from the MCScript, which is the dataset used for evaluation of SemEval 2018 Task 11. When doing experiments, an interesting discovery is found that using the entire MCScript is not the best choice. Filtering some types of questions leads to better results on the development set of MCScript2.0. During fine-tuning, a classification layer is also added on the top of BERT and the training is guided by minimizing the cross-entropy loss. We use the Pytorch version of pre-trained BERT implemented by huggingface1 . Adam Optimizer (Kingma and Ba, 20"
D19-6012,S19-1012,0,0.107405,"Missing"
D19-6012,N18-1202,0,0.0788165,"Missing"
D19-6012,W19-3410,0,0.0506682,"Missing"
D19-6012,D18-1009,0,0.0315421,"All Datasets MCScript2.0 MCScript2.0 + MCScript MCScript2.0 + MCScript-w/o-who-how MCScript2.0 + SWAG MCScript2.0 + MCScript-w/o-who-how 85.0 86.0 86.6 85.9 84.7 84.2 - Table 2: Main results. when fine-tuned. edge. 3.2 4.2 Classifier and Encoder Fine-tuning Experiment Setup The fist stage fine-tuning endows the encoder with more new knowledge, while the second stage finetuning focuses on adjusting weights in the encoder to adapt to the target task. This phase is carried on the fine-tuned encoder with the support of additional commonsense datasets, like MCScript (Ostermann et al., 2018), SWAG (Zellers et al., 2018). The most benefits come from the MCScript, which is the dataset used for evaluation of SemEval 2018 Task 11. When doing experiments, an interesting discovery is found that using the entire MCScript is not the best choice. Filtering some types of questions leads to better results on the development set of MCScript2.0. During fine-tuning, a classification layer is also added on the top of BERT and the training is guided by minimizing the cross-entropy loss. We use the Pytorch version of pre-trained BERT implemented by huggingface1 . Adam Optimizer (Kingma and Ba, 2014) is used to optimize the m"
K19-1030,W18-3219,0,0.0399561,"Missing"
K19-1030,D17-1078,0,0.0203783,"al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some extent. In contrast, we focus on dealing with the OOV issue at subword level in the context of pre-trained BERT model. OOV poses challenges for many tasks (Pinter et al., 2017) such as machine translation (Razmara et al., 2013; Sennrich et al., 2016) and sentiment analysis (Kaewpitakkun et al., 2014). Even for tasks such as machine reading comprehension that are less sensitive to the meanings of each word, OOV still hurts the performance (Chu et al., 2017; Zhang"
K19-1030,D18-1214,0,0.0496296,"Missing"
K19-1030,P17-1168,0,0.0296616,"for tasks such as machine reading comprehension that are less sensitive to the meanings of each word, OOV still hurts the performance (Chu et al., 2017; Zhang et al., 2018). We now discuss previous methods in two settings. 4.1 Multilingual Setting Monolingual Setting Most previous work address the OOV problems in monolingual settings. Before more fine-grained encoding schema such as BPE (Sennrich et al., 2016) is proposed, prior work mainly focused on OOV for token-level representations (Taylor et al., 2011; Kolachina et al., 2017). Besides simply assigning random embeddings to unseen words (Dhingra et al., 2017) or using an unique symbol to replace all these words with a shared embedding (Hermann et al., 2015), a thread of research focuses on refining the OOV representations based on word-level information, such as using similar in-vocabulary words (Luong et al., 2015; Cho et al., 2015; Tafforeau et al., 2015; Li et al., 2016), mapping initial embedding to task-specific embedding (Rothe et al., 2016; Madhyastha et al., 2016), using definitions of OOV words from auxiliary data (Long et al., 2016; Bahdanau et al., 2017), and tracking contexts to build/update representations (Henaff et al., 2016; 5 Conc"
K19-1030,P18-2049,0,0.0175286,"8) pre-train the multilingual BERT on Wikipedia in 102 languages, with a shared vocabulary that contains 110k subwords calculated from the WordPiece model (Wu et al., 2016). If we ignore the shared subwords between languages, on average, each language has a 1.1k vocabulary, which is significantly smaller than that of a monolingual pre-trained model such as GPT (40k). The OOV problem tends to be less serious for languages (e.g., French and Spanish) that belong to the same language family of English. However, this is not always true, especially for morphologically rich languages such as German (Ataman and Federico, 2018; Lample et al., 2018). OOV problem is much more severe in lowresource scenarios, especially when a language (e.g., Japanese and Urdu) uses an entirely different character set from high-resource languages. We focus on addressing the OOV issue at subword level in multilingual settings. Formally, suppose we have an embedding Ebert extracted from the (non-contextualized) embedding layer in the multilingual BERT (i.e., the first layer of BERT). And suppose we have another set of (non-contextualized) sub-word embeddings {El1 , El2 , . . . , Eln } ∪ {Een }, which are pre-trained on large corpora usi"
K19-1030,E14-1049,0,0.0398898,"Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some extent. In contrast, we focus on dealing with the"
K19-1030,N16-1155,0,0.0640413,"Missing"
K19-1030,Q17-1010,0,0.292076,"for details. Figure 1 (right) illustrates the joint and mixture mapping. El (w0 )Bl − Een (fl (w0 )) 2 F w0 ∈Vl ∩{w:fl (w)∈Ven } where k·kF denotes the Frobenius norm. Otherwise, for language pair (e.g., English-Urdu) that meets neither of the above two conditions, we obtain Bl by an unsupervised word alignment method from MUSE (Conneau et al., 2018). We then map El0 to Ebert by an orthogonal mapping matrix A0l , which is obtained by minimizing X 0 E (w)A0 − Ebert (w) 2 l l F 3 3.1 Experiment Experiment Settings We obtain the pre-trained embeddings of a specific language by training fastText (Bojanowski et al., 2017) on Wikipedia articles in that language, with context window 5 and negative sampling 5. Before training, we first apply BPE (Sennrich et al., 2016) to tokenize the corpus with subword vocabulary size 50k. For joint mapping method MJ , w∈fl (Vl )∩Vbert We denote this method by MJ in our discussion below, where the subscript J stands for “joint”. Mixture Mapping Following the work of Gu et al. (2018) where they use English as “universal tokens” and map all other languages to English 318 we find that the accuracy of the mapping from our pre-trained English embedding to multilingual BERT embedding"
K19-1030,D16-1157,0,0.0134311,"eneck. Task: We see more significant performance gains on NER, POS and MT Quality Estimation, possibly because token-level understanding is more critical for these tasks, therefore alleviating OOV helps more. In comparison, for sequence level classification tasks such as machine reading comprehension (Section 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addres"
K19-1030,N18-1032,0,0.0416946,"Missing"
K19-1030,N18-1116,0,0.0232828,"Ithaca, NY, USA haiwang@ttic.edu, ks985@cornell.edu, {yudian,jianshuchen,dyu}@tencent.com i.e., out-of-vocabulary (OOV) words (Søgaard and Johannsen, 2012; Madhyastha et al., 2016). A higher OOV rate (i.e., the percentage of the unseen words in the held-out data) may lead to a more severe performance drop (Kaljahi et al., 2015). OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar invocabulary words (Madhyastha et al., 2016; Kolachina et al., 2017) or using character/word information (Kim et al., 2016, 2018; Chen et al., 2018) or subword information like byte pair encoding (BPE) (Sennrich et al., 2016; Stratos, 2017). Recently, fine-tuning a pre-trained deep language model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese BERT, a more natural choice is to develop a powerful multilingual model such as the multilingual BERT. H"
K19-1030,W18-1205,0,0.0212234,"resenting words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size f"
K19-1030,W17-4750,0,0.0266484,"representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual sett"
K19-1030,D15-1176,0,0.0303337,"align their embeddings is the main bottleneck. Task: We see more significant performance gains on NER, POS and MT Quality Estimation, possibly because token-level understanding is more critical for these tasks, therefore alleviating OOV helps more. In comparison, for sequence level classification tasks such as machine reading comprehension (Section 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which i"
K19-1030,D17-1195,0,0.0310624,"t is necessary to add additional subwords. However, as the grammar of such a language is very different from that of English, how to accurately align their embeddings is the main bottleneck. Task: We see more significant performance gains on NER, POS and MT Quality Estimation, possibly because token-level understanding is more critical for these tasks, therefore alleviating OOV helps more. In comparison, for sequence level classification tasks such as machine reading comprehension (Section 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pint"
K19-1030,Y14-1026,0,0.0227312,", and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some extent. In contrast, we focus on dealing with the OOV issue at subword level in the context of pre-trained BERT model. OOV poses challenges for many tasks (Pinter et al., 2017) such as machine translation (Razmara et al., 2013; Sennrich et al., 2016) and sentiment analysis (Kaewpitakkun et al., 2014). Even for tasks such as machine reading comprehension that are less sensitive to the meanings of each word, OOV still hurts the performance (Chu et al., 2017; Zhang et al., 2018). We now discuss previous methods in two settings. 4.1 Multilingual Setting Monolingual Setting Most previous work address the OOV problems in monolingual settings. Before more fine-grained encoding schema such as BPE (Sennrich et al., 2016) is proposed, prior work mainly focused on OOV for token-level representations (Taylor et al., 2011; Kolachina et al., 2017). Besides simply assigning random embeddings to unseen w"
K19-1030,D15-1157,0,0.0234168,"Missing"
K19-1030,P16-2019,0,0.0209443,"ylor et al., 2011; Kolachina et al., 2017). Besides simply assigning random embeddings to unseen words (Dhingra et al., 2017) or using an unique symbol to replace all these words with a shared embedding (Hermann et al., 2015), a thread of research focuses on refining the OOV representations based on word-level information, such as using similar in-vocabulary words (Luong et al., 2015; Cho et al., 2015; Tafforeau et al., 2015; Li et al., 2016), mapping initial embedding to task-specific embedding (Rothe et al., 2016; Madhyastha et al., 2016), using definitions of OOV words from auxiliary data (Long et al., 2016; Bahdanau et al., 2017), and tracking contexts to build/update representations (Henaff et al., 2016; 5 Conclusion We investigated two methods (i.e., joint mapping and mixture mapping) inspired by monolingual solutions to alleviate the OOV issue in multilingual settings. Experimental results on several benchmarks demonstrate the effectiveness of mixture mapping and the usefulness of bilingual information. To the best of our knowledge, this is 323 the first work to address and discuss OOV issues at the subword level in multilingual settings. Future work includes: investigating other embedding a"
K19-1030,C18-1216,0,0.0384685,"Missing"
K19-1030,N15-1028,0,0.0293376,"g and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some extent. In contrast, we focus on dealing with the OOV issue at subw"
K19-1030,I17-1048,0,0.0194738,"ferent character sets, it is necessary to add additional subwords. However, as the grammar of such a language is very different from that of English, how to accurately align their embeddings is the main bottleneck. Task: We see more significant performance gains on NER, POS and MT Quality Estimation, possibly because token-level understanding is more critical for these tasks, therefore alleviating OOV helps more. In comparison, for sequence level classification tasks such as machine reading comprehension (Section 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Ma"
K19-1030,P16-1100,0,0.0459568,"Missing"
K19-1030,W17-0202,0,0.241507,"nstitute at Chicago, Chicago, IL, USA 2 Tencent AI Lab, Bellevue, WA, USA 3 Cornell, Ithaca, NY, USA haiwang@ttic.edu, ks985@cornell.edu, {yudian,jianshuchen,dyu}@tencent.com i.e., out-of-vocabulary (OOV) words (Søgaard and Johannsen, 2012; Madhyastha et al., 2016). A higher OOV rate (i.e., the percentage of the unseen words in the held-out data) may lead to a more severe performance drop (Kaljahi et al., 2015). OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar invocabulary words (Madhyastha et al., 2016; Kolachina et al., 2017) or using character/word information (Kim et al., 2016, 2018; Chen et al., 2018) or subword information like byte pair encoding (BPE) (Sennrich et al., 2016; Stratos, 2017). Recently, fine-tuning a pre-trained deep language model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese BERT, a more natural cho"
K19-1030,P15-1002,0,0.0354793,"ting Most previous work address the OOV problems in monolingual settings. Before more fine-grained encoding schema such as BPE (Sennrich et al., 2016) is proposed, prior work mainly focused on OOV for token-level representations (Taylor et al., 2011; Kolachina et al., 2017). Besides simply assigning random embeddings to unseen words (Dhingra et al., 2017) or using an unique symbol to replace all these words with a shared embedding (Hermann et al., 2015), a thread of research focuses on refining the OOV representations based on word-level information, such as using similar in-vocabulary words (Luong et al., 2015; Cho et al., 2015; Tafforeau et al., 2015; Li et al., 2016), mapping initial embedding to task-specific embedding (Rothe et al., 2016; Madhyastha et al., 2016), using definitions of OOV words from auxiliary data (Long et al., 2016; Bahdanau et al., 2017), and tracking contexts to build/update representations (Henaff et al., 2016; 5 Conclusion We investigated two methods (i.e., joint mapping and mixture mapping) inspired by monolingual solutions to alleviate the OOV issue in multilingual settings. Experimental results on several benchmarks demonstrate the effectiveness of mixture mapping and t"
K19-1030,W16-1612,0,0.305878,"1 Toyota Technological Institute at Chicago, Chicago, IL, USA 2 Tencent AI Lab, Bellevue, WA, USA 3 Cornell, Ithaca, NY, USA haiwang@ttic.edu, ks985@cornell.edu, {yudian,jianshuchen,dyu}@tencent.com i.e., out-of-vocabulary (OOV) words (Søgaard and Johannsen, 2012; Madhyastha et al., 2016). A higher OOV rate (i.e., the percentage of the unseen words in the held-out data) may lead to a more severe performance drop (Kaljahi et al., 2015). OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar invocabulary words (Madhyastha et al., 2016; Kolachina et al., 2017) or using character/word information (Kim et al., 2016, 2018; Chen et al., 2018) or subword information like byte pair encoding (BPE) (Sennrich et al., 2016; Stratos, 2017). Recently, fine-tuning a pre-trained deep language model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese"
K19-1030,J82-2005,0,0.647438,"Missing"
K19-1030,P16-2067,0,0.0290186,"Missing"
K19-1030,W17-2617,0,0.0530135,"Missing"
K19-1030,N18-1130,0,0.0198374,"ave been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the"
K19-1030,D18-1039,0,0.0256706,"n, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell a"
K19-1030,P13-2017,0,0.0348336,"Missing"
K19-1030,E17-2025,0,0.0327378,"uage model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese BERT, a more natural choice is to develop a powerful multilingual model such as the multilingual BERT. However, all those pre-trained models rely on language modeling, where a common trick is to tie the weights of softmax and word embeddings (Press and Wolf, 2017). Due to the expensive computation of softmax (Yang et al., 2017) and data imbalance across different languages, the vocabulary size for each language in a multilingual model is relatively small compared to the monolingual BERT/GPT models, especially for lowresource languages. Even for a high-resource language like Chinese, its vocabulary size 10k in the multilingual BERT is only half the size of that in the Chinese BERT. Just as in monolingual settings, the OOV problem also hinders the performance of a multilingual model on tasks that are sensitive to token-level or sentence-level information"
K19-1030,P13-1109,0,0.0220908,"OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some extent. In contrast, we focus on dealing with the OOV issue at subword level in the context of pre-trained BERT model. OOV poses challenges for many tasks (Pinter et al., 2017) such as machine translation (Razmara et al., 2013; Sennrich et al., 2016) and sentiment analysis (Kaewpitakkun et al., 2014). Even for tasks such as machine reading comprehension that are less sensitive to the meanings of each word, OOV still hurts the performance (Chu et al., 2017; Zhang et al., 2018). We now discuss previous methods in two settings. 4.1 Multilingual Setting Monolingual Setting Most previous work address the OOV problems in monolingual settings. Before more fine-grained encoding schema such as BPE (Sennrich et al., 2016) is proposed, prior work mainly focused on OOV for token-level representations (Taylor et al., 2011; Kola"
K19-1030,D15-1064,0,0.0781259,"Missing"
K19-1030,N16-1091,0,0.050588,"Missing"
K19-1030,W17-2612,0,0.060617,"Missing"
K19-1030,W15-3904,0,0.0347115,"Missing"
K19-1030,P17-1161,0,0.0329873,"e pre-training procedure of this model (Section 2.1) and then introduce two methods we investigate to alleviate the OOV issue by expanding the vocabulary (Section 2.2). Note that these approaches are not restricted to BERT but also applicable to other similar models. 2.1 Vocabulary Expansion Pre-Trained BERT Compared to GPT (Radford et al., 2018) and ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) uses a bidirectional transformer, whereas GPT pre-trains a left-to-right transformer (Liu et al., 2018); ELMo (Peters et al., 2018) independently trains left-to-right and right-to-left LSTMs (Peters et al., 2017) to generate representations as additional features for end tasks. In the pre-training stage, Devlin et al. (2018) use two objectives: masked language model (LM) and next sentence prediction (NSP). In masked LM, they randomly mask some input tokens and then predict these masked tokens. Compared to unidirectional LM, masked LM enables representations to fuse the context from both directions. In the 1 Improved models will be available at https:// github.com/sohuren/multilingul-bert. 317 cer Joint Space Joint Mapping как BERT Space er or ch er: 0.7 cer or: 0.2 as ch: 0.1 so as: 0.5 как so: 0.3 ho"
K19-1030,N18-1202,0,0.0303248,"rd w in Vbert , we use Ebert (w) to denote the pretrained embedding of word w in Ebert . Eli (·) and Een (·) are defined in a similar way as Ebert (·). For Approach We use the multilingual BERT as the pre-trained model. We first introduce the pre-training procedure of this model (Section 2.1) and then introduce two methods we investigate to alleviate the OOV issue by expanding the vocabulary (Section 2.2). Note that these approaches are not restricted to BERT but also applicable to other similar models. 2.1 Vocabulary Expansion Pre-Trained BERT Compared to GPT (Radford et al., 2018) and ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) uses a bidirectional transformer, whereas GPT pre-trains a left-to-right transformer (Liu et al., 2018); ELMo (Peters et al., 2018) independently trains left-to-right and right-to-left LSTMs (Peters et al., 2017) to generate representations as additional features for end tasks. In the pre-training stage, Devlin et al. (2018) use two objectives: masked language model (LM) and next sentence prediction (NSP). In masked LM, they randomly mask some input tokens and then predict these masked tokens. Compared to unidirectional LM, masked LM enables representations to fuse"
K19-1030,P16-1162,0,0.67266,"dyu}@tencent.com i.e., out-of-vocabulary (OOV) words (Søgaard and Johannsen, 2012; Madhyastha et al., 2016). A higher OOV rate (i.e., the percentage of the unseen words in the held-out data) may lead to a more severe performance drop (Kaljahi et al., 2015). OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar invocabulary words (Madhyastha et al., 2016; Kolachina et al., 2017) or using character/word information (Kim et al., 2016, 2018; Chen et al., 2018) or subword information like byte pair encoding (BPE) (Sennrich et al., 2016; Stratos, 2017). Recently, fine-tuning a pre-trained deep language model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese BERT, a more natural choice is to develop a powerful multilingual model such as the multilingual BERT. However, all those pre-trained models rely on language modeling, where a comm"
K19-1030,C12-2114,0,0.066115,"Missing"
K19-1030,D17-1010,0,0.13286,"2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Ma"
K19-1030,P18-1072,0,0.0383072,"Missing"
K19-1030,D18-1036,0,0.0154409,"add additional subwords. However, as the grammar of such a language is very different from that of English, how to accurately align their embeddings is the main bottleneck. Task: We see more significant performance gains on NER, POS and MT Quality Estimation, possibly because token-level understanding is more critical for these tasks, therefore alleviating OOV helps more. In comparison, for sequence level classification tasks such as machine reading comprehension (Section 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bah"
K19-1030,W18-6451,0,0.0417454,"Missing"
K19-1030,D17-1075,0,0.0217664,"out-of-vocabulary (OOV) words (Søgaard and Johannsen, 2012; Madhyastha et al., 2016). A higher OOV rate (i.e., the percentage of the unseen words in the held-out data) may lead to a more severe performance drop (Kaljahi et al., 2015). OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar invocabulary words (Madhyastha et al., 2016; Kolachina et al., 2017) or using character/word information (Kim et al., 2016, 2018; Chen et al., 2018) or subword information like byte pair encoding (BPE) (Sennrich et al., 2016; Stratos, 2017). Recently, fine-tuning a pre-trained deep language model, such as Generative Pre-Training (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), has achieved remarkable success on various downstream natural language processing tasks. Instead of pre-training many monolingual models like the existing English GPT, English BERT, and Chinese BERT, a more natural choice is to develop a powerful multilingual model such as the multilingual BERT. However, all those pre-trained models rely on language modeling, where a common trick is to t"
K19-1030,D13-1141,0,0.0433792,"word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual word embeddings (Zou et al., 2013; Andrew et al., 2013; Faruqui and Dyer, 2014; Lu et al., 2015; Ruder et al., 2017). Madhyastha and Espa˜na-Bonet (2017) increase the vocabulary size for statistical machine translation (SMT). Given an OOV source word, they generate a translation list in target language, and integrate this list into SMT system. Although they also generate translation list (similar with us), their approach is still in monolingual setting with SMT. Cotterell and Heigold (2017) train charlevel taggers to predict morphological taggings for high/low resource languages jointly, alleviating OOV problems to some exten"
K19-1030,W18-3221,0,0.0446635,"Missing"
K19-1030,1983.tc-1.13,0,0.368549,"Missing"
K19-1030,N15-1104,0,0.0719634,"Missing"
K19-1030,D17-1027,0,0.0287176,"n 3.5), OOV issue is less severe since the result is based on the entire sentence. Kobayashi et al., 2017; Ji et al., 2017; Zhao et al., 2018). Meanwhile, there have been efforts in representing words by utilizing character-level (Zhang et al., 2015; Ling et al., 2015a,b; Kim et al., 2016; Gimpel and Livescu, 2016) or subwordlevel representations (Sennrich et al., 2016; Bojanowski et al., 2017). To leverage the advantages in character and (sub)word level representation, some previous work combine (sub)wordand character-level representations (Santos and Zadrozny, 2014; dos Santos et al., 2015; Yu et al., 2017) or develop hybrid word/subword-character architectures (Chung et al., 2016; Luong and Manning, 2016; Pinter et al., 2017; Bahdanau et al., 2017; Matthews et al., 2018; Li et al., 2018). However, all those approaches assume monolingual setting, which is different from ours. 4 4.2 Related Work Addressing OOV problems in a multilingual setting is relatively under-explored, probably because most multilingual models use separate vocabularies (Jaffe, 2017; Platanios et al., 2018). While there is no direct precedent, previous work show that incorporating multilingual contexts can improve monolingual"
K19-1030,C18-1153,0,0.0415208,"Missing"
K19-1030,P15-1001,0,\N,Missing
K19-1030,P16-1160,0,\N,Missing
K19-1030,D18-1549,0,\N,Missing
K19-1030,W18-6401,0,\N,Missing
K19-1030,E17-2009,1,\N,Missing
K19-1065,P11-1049,0,0.0386787,"last token in Xi , which is further fed into a linear layer followed by a softmax layer to generate the probability: exp(Wy hM i ) M 1≤i≤N exp(Wy hi ) Pi = P make predictions for a new instance during testing. We provide more details in Appendix A and refer readers to Wang and Poon (2018) for how to apply DPL as a tool in a downstream task such as relation extraction. 2.2 Silver Standard Evidence Generation Given correct answer options, we use a distant supervision method to generate the silver standard evidence sentences. Inspired by Integer Linear Programming models (ILP) for summarization (Berg-Kirkpatrick et al., 2011; Boudin et al., 2015), we model evidence sentence extraction as a maximum coverage problem and define the value of a selected sentence set as the sum of the weights for the unique words it contains. Formally, let vi denote the weight of word i, vi = 1 if word i appears in the correct answer option, vi = 0.1 if it appears in the question but not in the correct answer option, and vi = 0 otherwise.1 We use binary variables ci and sj to indicate the presence of word i and sentence j in the selected sentence set, respectively. Occi,j is a binary variable indicating the occurrence of word i in sent"
K19-1065,D15-1060,0,0.0471132,"Missing"
K19-1065,D15-1220,0,0.0230168,"ther fed into a linear layer followed by a softmax layer to generate the probability: exp(Wy hM i ) M 1≤i≤N exp(Wy hi ) Pi = P make predictions for a new instance during testing. We provide more details in Appendix A and refer readers to Wang and Poon (2018) for how to apply DPL as a tool in a downstream task such as relation extraction. 2.2 Silver Standard Evidence Generation Given correct answer options, we use a distant supervision method to generate the silver standard evidence sentences. Inspired by Integer Linear Programming models (ILP) for summarization (Berg-Kirkpatrick et al., 2011; Boudin et al., 2015), we model evidence sentence extraction as a maximum coverage problem and define the value of a selected sentence set as the sum of the weights for the unique words it contains. Formally, let vi denote the weight of word i, vi = 1 if word i appears in the correct answer option, vi = 0.1 if it appears in the question but not in the correct answer option, and vi = 0 otherwise.1 We use binary variables ci and sj to indicate the presence of word i and sentence j in the selected sentence set, respectively. Occi,j is a binary variable indicating the occurrence of word i in sentence j, lj denotes the"
K19-1065,N18-4017,0,0.0360985,"Missing"
K19-1065,P17-1171,0,0.0660781,"Missing"
K19-1065,P17-1020,0,0.128428,"Missing"
K19-1065,D17-1070,0,0.0766524,"Missing"
K19-1065,C16-1278,0,0.0478116,"Missing"
K19-1065,D18-1546,0,0.0467381,"Missing"
K19-1065,N18-2007,0,0.0657117,"Missing"
K19-1065,N18-1023,1,0.907106,"ion. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve comparable or better performance than the same reader that considers the full context. The comparison between ground truth evidence sentences and automatically selected sentences indicates that there is still room for improvement. 2.1 Evidence Sentence Extractor We use a multi-layer multi-head transformer (Vaswani et al., 2017) to extract evidence sentences. Let Ww and Wp be the word (subword) and position embeddings, respectively. Let M denote the total number of layers in the transformer. T"
K19-1065,Q18-1023,0,0.0579117,"Missing"
K19-1065,W18-5516,0,0.0784622,"Missing"
K19-1065,D17-1082,0,0.0686899,"ican history. ??? : As knowledge increased, Harvard and other colleges began to teach many new subjects. Question: Which of the following statements is true according to the passage? Options: A. in the early years, everyone can go to colleges. B. in 1782, Harvard began to teach German. C. in the early years, different colleges majored in different fields. D. more and more courses were taught in college with the improvement of knowledge. Evidence Sentence Extractor ?? , ?? , ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidenc"
K19-1065,D17-1214,0,0.0459874,"Missing"
K19-1065,P18-1161,0,0.0650009,", ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve"
K19-1065,P17-1015,0,0.061652,"Missing"
K19-1065,D15-1162,0,0.0199433,"Dataset MultiRC DREAM RACE # of documents Train Dev Test 456 3,869 25,137 83 1,288 1,389 332 1,287 1,407 # of questions Train Dev Test 5,131 6,116 87,866 953 2,040 4,887 3,788 2,041 4,934 Average # of sentences per document Train + Dev + Test 14.5 (Train + Dev) 8.5 17.6 Table 1: Statistics of multiple-choice machine reading comprehension and question answering datasets. 3.2 we set L, the maximum number of silver standard evidence sentences of a question, to 3. For MultiRC, we set L to 5 since many questions have more than 5 ground truth evidence sentences. Implementation Details We use spaCy (Honnibal and Johnson, 2015) for tokenization and named entity tagging. We use the pre-trained transformer (i.e., GPT) released by Radford et al. (2018) with the same preprocessing procedure. When GPT is used as the neural reader, we set training epochs to 4, use eight P40 GPUs for experiments on RACE, and use one GPU for experiments on other datasets. When GPT is used as the evidence sentence extractor, we set batch size 1 per GPU and dropout rate 0.3. We keep other parameters default. Depending on the dataset, training the evidence sentence extractor generally takes several hours. 3.3 Evaluation on MultiRC Since its te"
K19-1065,P18-1160,0,0.133031,"Extractor ?? , ?? , ??? Questions Options Passage Reader D Output Figure 1: An overview of our pipeline. The input instance comes from RACE (Lai et al., 2017). We will present our evidence sentence extractor (Section 2.1) trained on the noisy training data generated by distant supervision (Section 2.2) and denoised by an existing deep probabilistic logic framework that incorporates different kinds of linguistic indicators (Section 2.3). The extractor is followed by an independent neural reader for evaluation. See an overview in Figure 1. Previous extractive MRC and question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show t"
K19-1065,K17-1009,0,0.0612813,"Missing"
K19-1065,N16-1098,0,0.0195499,"rect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for multiple-choice MRC; 2) we s"
K19-1065,P15-1121,0,0.0279158,"o share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate the quality of the extracted e"
K19-1065,speer-havasi-2012-representing,0,0.0301794,"Missing"
K19-1065,S18-1119,0,0.0224214,"he extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for multiple-choice MRC; 2) we show that it may be a prom"
K19-1065,Q19-1014,1,0.892017,"d question answering studies (Min et al., 2018; Lin et al., 2018) indicate that a model should be able to achieve comparable end-to-end performance if it can accurately predict the evidence sentence(s). Inspired by the observation, to indirectly evaluate the quality of the extracted evidence sentences, we only keep the selected sentences as the new reference document for each instance and evaluate the performance of a machine reader (Wang et al., 2018b; Radford et al., 2018) on three challenging multiple-choice MRC datasets: MultiRC (Khashabi et al., 2018), RACE (Lai et al., 2017), and DREAM (Sun et al., 2019). Experimental results show that we can achieve comparable or better performance than the same reader that considers the full context. The comparison between ground truth evidence sentences and automatically selected sentences indicates that there is still room for improvement. 2.1 Evidence Sentence Extractor We use a multi-layer multi-head transformer (Vaswani et al., 2017) to extract evidence sentences. Let Ww and Wp be the word (subword) and position embeddings, respectively. Let M denote the total number of layers in the transformer. Then, the m-th layer hidden state hm of a token is given"
K19-1065,C18-1069,0,0.0562068,"nappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate the quality of the extracted evidence sentences,"
K19-1065,P18-2124,0,0.0571023,"Missing"
K19-1065,N18-1074,0,0.0433229,"Missing"
K19-1065,D16-1264,0,0.102451,"Missing"
K19-1065,Q19-1016,0,0.0576815,"Missing"
K19-1065,W18-5446,0,0.0505641,"Missing"
K19-1065,D13-1020,0,0.3834,"istic indicators for indirect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiplechoice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC. 1 Introduction Recently, there have been increased interests in machine reading comprehension (MRC). In this work, we mainly focus on multiple-choice MRC (Richardson et al., 2013; Mostafazadeh et al., 2016; Ostermann et al., 2018): given a document and a question, the task aims to select the correct answer option(s) from a small number of answer options associated with this ques* This work was done when H. W. and K. S. were at Tencent AI Lab, Bellevue, WA. 696 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 696–707 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Our primary contributions are as follows: 1) to the best of our knowledge, this is the first work to extract evidence sentences for m"
K19-1065,P15-2115,1,0.856476,"excited, like I have some news I have to share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. T"
K19-1065,P15-1024,0,0.0280347,"ve some news I have to share!”) by our methods are inappropriate. A possible solution is to predict whether a question is answerable following previous work (e.g., (Hu et al., 2019)) on addressing unanswerable questions in extractive machine reading comprehension tasks such as SQuAD (Rajpurkar et al., 2018) before to extract the evidence sentences for this question. 4.2 Machine Reading Comprehension with External Linguistic Knowledge Linguistic knowledge such as coreference resolution, frame semantics, and discourse relations is widely used to improve machine comprehension (Wang et al., 2015; Sachan et al., 2015; Narasimhan and Barzilay, 2015; Sun et al., 2018) especially when there are only hundreds of documents available in a dataset such as MCTest (Richardson et al., 2013). Along with the creation of large-scale reading comprehension datasets, recent machine reading comprehension models rely on end-to-end neural models, and it primarily uses word embeddings as input. However, Wang et al. (2016); Dhingra et al. (2017, 703 pervision to noisy labels and apply a deep probabilistic logic framework that incorporates linguistic indicators for denoising noisy labels during training. To indirectly evaluate"
K19-1065,D18-1215,1,0.938366,"ruth evidence sentences in most multiplechoice MRC tasks, inspired by distant supervision, we first extract silver standard evidence sentences based on the lexical features of a question and its correct answer option (Section 2.2), then we use these noisy labels to train an evidence sentence extractor (Section 2.1). To denoise imperfect labels, we also manually design sentence-level and cross-sentence linguistic indicators such as “adjacent sentences tend to have the same label” and accommodate all the linguistic indicators with a recently proposed deep probabilistic logic learning framework (Wang and Poon, 2018) for indirect supervision (Section 2.3). Method Reference Document ?? : Started in 1636, Harvard University is the oldest of all the colleges and universities in the United States, followed by Yale, Princeton, Columbia... ?? : In the early years, these schools were nearly the same. ?? : Only young men went to college. ?? : All the students studied the same subjects, and everyone learned Latin and Greek………. ?? : In 1782, Harvard started a medical school for young men who wanted to become doctors………. ??? : In 1825, besides Latin and Greek, Harvard began to teach modern languages, such as French"
K19-1065,P18-2118,0,0.339408,"ical Institute at Chicago, Chicago, IL, USA 2 Tencent AI Lab, Bellevue, WA, USA 3 Cornell, Ithaca, NY, USA 4 University of Pennsylvania, Philadelphia, PA, USA {haiwang,mcallester}@ttic.edu, ks985@cornell.edu, {yudian,jianshuchen,dyu}@tencent.com, danroth@seas.upenn.edu tion. Compared to extractive and abstractive MRC tasks (e.g., (Rajpurkar et al., 2016; Koˇcisk`y et al., 2018; Reddy et al., 2019)) where most questions can be answered using spans from the reference documents, the majority of answer options cannot be directly extracted from the given texts. Existing multiple-choice MRC models (Wang et al., 2018b; Radford et al., 2018) take as input the entire reference document and seldom offer any explanation, making interpreting their predictions extremely difficult. It is a natural choice for human readers to use sentences from a given text to explain why they select a certain answer option in reading tests (Bax, 2013). In this paper, as a preliminary attempt, we focus on exacting evidence sentences that entail or support a question-answer pair from the given reference document. For extractive MRC tasks, information retrieval techniques can be very strong baselines to extract sentences that conta"
K19-1065,P18-1042,0,0.0468066,"Missing"
K19-1065,N18-1101,0,0.0799555,"Missing"
K19-1065,D18-1010,1,0.879016,"Missing"
K19-1065,P17-1172,0,0.0699146,"Missing"
K19-1065,C18-1171,0,0.0909856,"Missing"
K19-1065,D15-1075,0,\N,Missing
K19-1065,D17-2011,0,\N,Missing
K19-1065,W17-2604,1,\N,Missing
K19-1065,N19-1423,0,\N,Missing
K19-1065,P18-1157,0,\N,Missing
N16-1044,H90-1021,0,0.155082,"tochastic gradient descent algorithm (SGD) (Panagiotakopoulos and Tsampouka, 2013) for model training. The loss function is critical to the sequence level max-margin training criterion, which defines the margin. In this paper, we apply the sequence level hard loss function rather than traditional Hamming loss function (Nguyen and Guo, 2007). In sequence level hard loss function, the wrong sequence is assigned loss one without considering the number of wrong slot labels in the sequence. In the experiments on two bench mark datasets, namely the ATIS (Airline Travel Information Systems) dataset (Hemphill et al., 1990b; Yao et al., 2014b) and the CoNLL 2000 Chunking dataset 1 , and private Cortana live log dataset, RSVMs outperformed previous results. 2 Recurrent Support Vector Machines In this section, we propose RSVM that uses the structured SVM algorithm (Tsochantaridis et al., 2005) to estimate the weights for RNN and label transition probabilities based on the entire training sequence. The training objective in RSVM is the following constrained optimization. K 1 1 min ||W ||22 + ||A||22 +C ∑ ζk 2 W,A 2 k=1 ∗ s.t. f (Y (k) ) + ζk ≥ f (Y (k) ) + L(Y (k) ) L(Y (k) )≥0 ∀Y (k) ζk ≥ 0 ∀Y (k) (1) (2) (3) whe"
N16-1044,W00-0730,0,0.277869,"Missing"
N16-1044,N01-1025,0,0.178725,"Missing"
N16-1044,H05-1124,0,0.0267602,"Missing"
N16-1044,C08-1106,0,0.0557362,"Missing"
N16-2001,P98-1012,0,0.0642621,"ach verb. Training is done by stochastic gradient descent with shuffled minibatches and we keep the word vectors static only update other parameters. In our experiments we keep all the same hyperparameters for each verb. we set learning rate to 0.1, lw and rw to 5, minibatch size to 5, L2 regularization parameter β to 0.0001, the number of hidden unit to 30 and λ to 0. Because of limited training data, we do not use early stopping. Training will stop when the zero-one loss is zero over training data for each verb. The official evaluation method used B-cubed definition of Precision and Recall (Bagga and Baldwin, 1998) for CPA clustering. The final score is the average of B-cubed F-scores over all verbs. Since our task can be regarded as a supervised classification, we also use the micro-average F-score to evaluate our results. 3.3 Experimental Results Table 1 shows the results on MTDSEM with supervised and unsupervised approaches. SemEval-2015 Task 15 baseline (SEB) clusters all sentences together for each verb. That is to say, SEB assigns the Verb-specific Sentences Mary resisted the temptation to answer her back and after a moment’s silence Pamala Klein would seem to have a lot to answer for. and I will"
N16-2001,S15-2053,0,0.508712,"Missing"
N16-2001,P98-1013,0,0.589149,"n 407 verbs. 1 Introduction Lexical items usually have particular requirements for their semantic roles. Semantic frames are the structures of the linked semantic roles near the lexical items. A semantic frame specifies its characteristic interactions with things necessarily or typically associated with it (Alan, 2001). It is valuable to build such resources. These resources can be effectively used in many natural language processing (NLP) tasks, such as question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002). Current semantic frame resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005), have been manually created. These resources have promising applications, but they are time-consuming and expensive. El Maarouf and Baisa (2013) used a ∗ bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end."
N16-2001,boas-2002-bilingual,0,0.0380131,"ility. Finally we get 0.82 F-score on 63 verbs and 0.73 F-score on 407 verbs. 1 Introduction Lexical items usually have particular requirements for their semantic roles. Semantic frames are the structures of the linked semantic roles near the lexical items. A semantic frame specifies its characteristic interactions with things necessarily or typically associated with it (Alan, 2001). It is valuable to build such resources. These resources can be effectively used in many natural language processing (NLP) tasks, such as question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002). Current semantic frame resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005), have been manually created. These resources have promising applications, but they are time-consuming and expensive. El Maarouf and Baisa (2013) used a ∗ bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But"
N16-2001,de-marneffe-etal-2006-generating,0,0.0219789,"Missing"
N16-2001,W13-3826,0,0.135204,"Missing"
N16-2001,el-maarouf-etal-2014-disambiguating,0,0.182445,"Missing"
N16-2001,S15-2054,1,0.431327,"o obtain semantic frames. Most current supervised and unsupervised approaches are under similar pipeline procedure. The procedure can be summarized as follows with an example sentence ”The old music deeply moved 1 The corresponding author. http://pdev.org.uk/ 1 Proceedings of NAACL-HLT 2016, pages 1–7, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics the old man”: Entity move Human step 1 Identify the arguments near ”moved”, which can be expressed as (subject:music, object:man) However, step 1 and 2 are proved to be difficult in SemEval-2015 task 15 2 (Feng et al., 2015; Mills and Levow, 2015). This paper presents an end-to-end approach by directly learning semantic frames from verb-specific sentences. One key component of our model is well pre-trained word vectors. These vectors capture fine-grained semantic and syntactic regularities (Mikolov et al., 2013) and make our model have a good generalization ability. Another key component is FNN model. A supervised signal allows FNN to learn the semantic frames directly. As a result, this simple model achieves good results. On the instances resources of PDEV, we got 0.82 F-score on 63 verbs and 0.73 on 407 verbs."
N16-2001,P14-1097,0,0.0129619,"English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end. Popescu used Finite State Automata (FSA) to learn the pattern of semantic frames (Popescu, 2013). But the generalization ability of this rule-based method may be weak. Recently, some unsupervised studies have focused on acquiring semantic frames from raw corpora (Materna, 2012; Materna, 2013; Kawahara et al., 2014b; Kawahara et al., 2014a). Materna used LDA-Frame for identifying semantic frames based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. Kawahara et al. used Chinese Restaurant Process to induce semantic frames from a syntactically annotated corpus. These unsupervised approaches have a different goal compared with supervised approaches. They aim at identifying the semantic frames by clustering the parsed sentences but they do not learn from semantic frames that have been built. These unsupervised approaches are also under a pipeline framework and not end-to-end. One related res"
N16-2001,E14-1007,0,0.0153833,"English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end. Popescu used Finite State Automata (FSA) to learn the pattern of semantic frames (Popescu, 2013). But the generalization ability of this rule-based method may be weak. Recently, some unsupervised studies have focused on acquiring semantic frames from raw corpora (Materna, 2012; Materna, 2013; Kawahara et al., 2014b; Kawahara et al., 2014a). Materna used LDA-Frame for identifying semantic frames based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. Kawahara et al. used Chinese Restaurant Process to induce semantic frames from a syntactically annotated corpus. These unsupervised approaches have a different goal compared with supervised approaches. They aim at identifying the semantic frames by clustering the parsed sentences but they do not learn from semantic frames that have been built. These unsupervised approaches are also under a pipeline framework and not end-to-end. One related res"
N16-2001,N13-1051,0,0.0195458,"n Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end. Popescu used Finite State Automata (FSA) to learn the pattern of semantic frames (Popescu, 2013). But the generalization ability of this rule-based method may be weak. Recently, some unsupervised studies have focused on acquiring semantic frames from raw corpora (Materna, 2012; Materna, 2013; Kawahara et al., 2014b; Kawahara et al., 2014a). Materna used LDA-Frame for identifying semantic frames based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. Kawahara et al. used Chinese Restaurant Process to induce semantic frames from a syntactically annotated corpus. These unsupervised approaches have a different goal compared with supervised approaches. They aim at identifying the semantic frames by clustering the parsed sentences but they do not learn from semantic frames that have been built. These unsupervised approaches are also under a pipeline framework and not end-"
N16-2001,N13-1090,0,0.00875607,"6, pages 1–7, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics the old man”: Entity move Human step 1 Identify the arguments near ”moved”, which can be expressed as (subject:music, object:man) However, step 1 and 2 are proved to be difficult in SemEval-2015 task 15 2 (Feng et al., 2015; Mills and Levow, 2015). This paper presents an end-to-end approach by directly learning semantic frames from verb-specific sentences. One key component of our model is well pre-trained word vectors. These vectors capture fine-grained semantic and syntactic regularities (Mikolov et al., 2013) and make our model have a good generalization ability. Another key component is FNN model. A supervised signal allows FNN to learn the semantic frames directly. As a result, this simple model achieves good results. On the instances resources of PDEV, we got 0.82 F-score on 63 verbs and 0.73 on 407 verbs. The contributions of this paper are summarized as follows: • Semantic frames can be learned with neural network in an end-to-end map and we also analysed our method in detail. • We showed the power of pre-trained vectors and simple neural network for the learning of semantic frames. It is hel"
N16-2001,S15-2075,0,0.189182,"rames. Most current supervised and unsupervised approaches are under similar pipeline procedure. The procedure can be summarized as follows with an example sentence ”The old music deeply moved 1 The corresponding author. http://pdev.org.uk/ 1 Proceedings of NAACL-HLT 2016, pages 1–7, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics the old man”: Entity move Human step 1 Identify the arguments near ”moved”, which can be expressed as (subject:music, object:man) However, step 1 and 2 are proved to be difficult in SemEval-2015 task 15 2 (Feng et al., 2015; Mills and Levow, 2015). This paper presents an end-to-end approach by directly learning semantic frames from verb-specific sentences. One key component of our model is well pre-trained word vectors. These vectors capture fine-grained semantic and syntactic regularities (Mikolov et al., 2013) and make our model have a good generalization ability. Another key component is FNN model. A supervised signal allows FNN to learn the semantic frames directly. As a result, this simple model achieves good results. On the instances resources of PDEV, we got 0.82 F-score on 63 verbs and 0.73 on 407 verbs. The contributions of th"
N16-2001,C04-1100,0,0.0379343,"sults on annotated data and has a good generalization ability. Finally we get 0.82 F-score on 63 verbs and 0.73 F-score on 407 verbs. 1 Introduction Lexical items usually have particular requirements for their semantic roles. Semantic frames are the structures of the linked semantic roles near the lexical items. A semantic frame specifies its characteristic interactions with things necessarily or typically associated with it (Alan, 2001). It is valuable to build such resources. These resources can be effectively used in many natural language processing (NLP) tasks, such as question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002). Current semantic frame resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005), have been manually created. These resources have promising applications, but they are time-consuming and expensive. El Maarouf and Baisa (2013) used a ∗ bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV"
N16-2001,J05-1004,0,0.122634,"ical items usually have particular requirements for their semantic roles. Semantic frames are the structures of the linked semantic roles near the lexical items. A semantic frame specifies its characteristic interactions with things necessarily or typically associated with it (Alan, 2001). It is valuable to build such resources. These resources can be effectively used in many natural language processing (NLP) tasks, such as question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002). Current semantic frame resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005), have been manually created. These resources have promising applications, but they are time-consuming and expensive. El Maarouf and Baisa (2013) used a ∗ bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end. Popescu used Finite State Autom"
N16-2001,S15-2076,0,0.0229159,"ck [[Human 1]] [[Human]] have a lot to answer for [NO OBJ] [[Human]] answer [NO OBJ] for [[Eventuality]] [[Human]] answer [NO OBJ] for [[Eventuality]] [[Human]] act [[Event or Human Role or Emotion]] out [[Human]] build ([[Entity]]) up same cluster to all the sentences and is evaluated by B-cubed F-score for clustering. So its score depends on the distribution of semantic frames. The higher the score is, the more concentrated the distribution of semantic frames is. SEB to get higher score usually indicates other methods are more likely to get high scores, so we use it as a base score. DULUTH (Pedersen, 2015) treated this task as an unsupervised word sense discrimination or induction problem. The number of semantic frames was predicted on the basis of the best value for the clustering criterion function. BOB90 4 used a supervised approach to tackle the clustering problem (Baisa et al., 2015) and get the best score on MTDSEM. An example result of FNN model on PDEV is shown in Table 2 4 Discussions 4.1 Large vs. Small Training Data MTDSEM is divided into two parts to report on the left part of Table 1. One part has larger training data while the other part has little. Our FNN model gets a relatively"
N16-2001,D14-1162,0,0.0761948,"Missing"
N16-2001,W13-0117,0,0.0255897,"eated. These resources have promising applications, but they are time-consuming and expensive. El Maarouf and Baisa (2013) used a ∗ bootstrapping model to classify the patterns of verbs from Pattern Dictionary of English1 (PDEV). El Maarouf et al. (2014) used a Support Vector Machine (SVM) model to classify the patterns of PDEV . The above supervised approaches are most closely related to ours since PDEV is also used in our experiment. But the models above are tested only on 25 verbs and they are not end-to-end. Popescu used Finite State Automata (FSA) to learn the pattern of semantic frames (Popescu, 2013). But the generalization ability of this rule-based method may be weak. Recently, some unsupervised studies have focused on acquiring semantic frames from raw corpora (Materna, 2012; Materna, 2013; Kawahara et al., 2014b; Kawahara et al., 2014a). Materna used LDA-Frame for identifying semantic frames based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. Kawahara et al. used Chinese Restaurant Process to induce semantic frames from a syntactically annotated corpus. These unsupervised approaches have a different goal compared with supervised approaches. They aim at identifying th"
N16-2001,C98-1012,0,\N,Missing
N16-2001,C98-1013,0,\N,Missing
N19-1270,D15-1220,0,0.0304878,"Missing"
N19-1270,P16-1223,0,0.0939856,"Missing"
N19-1270,W16-3612,0,0.0118964,", 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of a"
N19-1270,D18-1241,0,0.0296487,"ave been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (K"
N19-1270,N18-1143,0,0.095059,"Missing"
N19-1270,P17-1168,0,0.0607419,"Missing"
N19-1270,S18-1189,0,0.0133255,"answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strate"
N19-1270,D17-1087,0,0.0148323,"ldom take the rich external knowledge (other than pretrained word embeddings) into considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reorde"
N19-1270,P17-1147,0,0.105329,"tion 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extract"
N19-1270,N18-1023,0,0.522404,"oaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of ∗ This work was done when K. S. was an intern at the Tencent AI Lab, Bellevue, WA. Introduction Recent years have seen a growing interest in machine reading comprehension (MRC) (Rajpurkar et al., 2016; Choi et al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-"
N19-1270,Q18-1023,0,0.0617034,"Missing"
N19-1270,D17-1082,0,0.740196,"al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained generalpurpose language model (Radford et al., 2018; Devlin et al., 2018). However, similar to the process of knowledge accumulation for human readers, it is time-consuming and resource-demanding to impart massive amounts of general domain knowledge from external corpora into a deep language model via pre-training. For example, it takes a month to pre-train a 12-layer transformer on eight P100 GPUs over the BooksCorpus (Zhu et al., 2015; Radford et al., 2018);"
N19-1270,S18-1180,0,0.0280285,", 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strategies (i.e., back and"
N19-1270,N18-1185,0,0.0109608,"ults indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this se"
N19-1270,D18-1260,0,0.250953,"ectly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (Khashabi et al., 2018), and OpenBookQA (Mihaylov et al., 2018)) by crowdsourcing. Besides crowdsourcing, datasets such as RACE (Lai et al., 2017) and ARC (Clark et al., 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014; Shibuki et al., 2014; Tseng et al., 2016) to evaluate the comprehension level of human participants. Compared to questions in extractive MRC tasks, besides surface matching, there are various types of complicated questions such as math word problems, summarization, logical reasoning, and sentiment analysis, requiring advanced read2634 RACE ARC OpenBookQA MCTest SemEval-2018 Task 11 ROCS"
N19-1270,D16-1241,0,0.0274386,"Task 11 (Yang et al., 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answe"
N19-1270,S18-1119,0,0.402795,"ative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of ∗ This work was done when K. S. was an intern at the Tencent AI Lab, Bellevue, WA. Introduction Recent years have seen a growing interest in machine reading comprehension (MRC) (Rajpurkar et al., 2016; Choi et al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained gen"
N19-1270,P17-2081,0,0.0230051,"d embeddings) into considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and"
N19-1270,R13-1061,0,0.0293908,"ack and forth reading (BF) (Section 3.2), highlighting (HL) (Section 3.3), and self-assessment (SA) (Section 3.4), respectively. 3.3 Highlighting (HL) In the original implementation (Radford et al., 2018), during the fine-tuning stage of GPT, the text embedding of a document is independent of its associated questions and answer options. Inspired by highlights used in human reading, we aim to make the document encoding aware of the associated question-answer option pair (q, oi ). We focus on the content words in questions and answer options since they appear to provide more useful information (Mirza and Bernardi, 2013), and we identify them via their part of speech (POS) tags, one of: noun, verb, adjective, adverb, numeral, or foreign word. Formally, we let T be the set of POS tags of the content words. We let d denote the sequence of the text embedding of document d. We use dj to represent the j th token in d and dj to denote the text embedding of dj . Given d and a (q, oi ) pair, we define a highlight embedding hji for the j th token in d as: hji =  +  `  `− if the POS tag of dj belongs to T , and dj appears in either q or oi otherwise (2) where `+ and `− are two trainable vectors of the same dimensi"
N19-1270,D16-1264,0,0.0838319,"f our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoi"
N19-1270,D13-1020,0,0.158061,"ed on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (Khashabi et al., 2018), and OpenBookQA (Mihaylov et al., 2018)) by crowdsourcing. Besides crowdsourcing, datasets such as RACE (Lai et al., 2017) and ARC (Clark et al., 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014; Shibuki et al., 2014; Tseng et al., 2016) to evaluate the comprehension level of human participants. Compared to questions in extractive MRC tasks, besides surface matching, there are various types of complicated questions such as math word problems, summarization, logi"
N19-1270,N16-1098,0,0.0733986,"te improvement in accuracy over the previous best result achieved by the same pretrained transformer fine-tuned on RACE without the use of strategies (Section 4.2). We further fine-tune the resulting model on a target MRC task. Experiments show that our method achieves new state-of-the-art results on six representative non-extractive MRC datasets that require a range of reading skills such as commonsense and multi-sentence reasoning (i.e., ARC (Clark et al., 2016, 2018), OpenBookQA (Mihaylov et al., 2018), MCTest (Richardson et al., 2013), SemEval-2018 Task 11 (Yang et al., 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016;"
N19-1270,P17-1075,0,0.0336289,"Missing"
N19-1270,W17-2623,0,0.0527475,"rsatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer opt"
N19-1270,S18-1120,0,0.0564461,"ing the joint exact match accuracy (i.e., EM0 reported by the official evaluation (Khashabi et al., 2018))). 4.4 Adaptation to Other Non-Extractive Machine Reading Comprehension Tasks We follow the philosophy of transferring the knowledge from a high-performing model pretrained on a large-scale supervised data of a source task to a target task, in which only a small amount of training data is available (Chung et al., 2018). RACE has been used to pre-train a model for other MRC tasks as it contains the largest number of general domain non-extractive questions (Table 1) (Ostermann et al., 2018; Wang et al., 2018a). In our experiment, we also treat RACE as the source task and regard six representative non-extractive multiple-choice MRC datasets from multiple domains as the target tasks. We require some task-specific modifications considering the different structures of these datasets. In ARC and OpenBookQA, there is no reference document associated with each question. Instead, a reference corpus is provided, which consists of unordered science-related sentences relevant to questions. We therefore first use Lucene (McCandless et al., 2010) to retrieve the top 50 sentences by using the non-stop words in"
N19-1270,P18-2118,0,0.0369884,"ing the joint exact match accuracy (i.e., EM0 reported by the official evaluation (Khashabi et al., 2018))). 4.4 Adaptation to Other Non-Extractive Machine Reading Comprehension Tasks We follow the philosophy of transferring the knowledge from a high-performing model pretrained on a large-scale supervised data of a source task to a target task, in which only a small amount of training data is available (Chung et al., 2018). RACE has been used to pre-train a model for other MRC tasks as it contains the largest number of general domain non-extractive questions (Table 1) (Ostermann et al., 2018; Wang et al., 2018a). In our experiment, we also treat RACE as the source task and regard six representative non-extractive multiple-choice MRC datasets from multiple domains as the target tasks. We require some task-specific modifications considering the different structures of these datasets. In ARC and OpenBookQA, there is no reference document associated with each question. Instead, a reference corpus is provided, which consists of unordered science-related sentences relevant to questions. We therefore first use Lucene (McCandless et al., 2010) to retrieve the top 50 sentences by using the non-stop words in"
N19-1270,K17-1029,0,0.022419,"considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or gener"
N19-1270,P17-1096,0,0.0246747,"fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strategies (i.e., back and forth reading, highlighting, and self-assessment), aiming at im"
P19-1016,C18-1139,0,0.0697448,"essfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional"
P19-1016,K18-1028,0,0.026665,"o recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute r"
P19-1016,D14-1082,0,0.0601337,"Missing"
P19-1016,N15-1075,0,0.0303189,"tion Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation"
P19-1016,N16-1030,0,0.0148324,"and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the p"
P19-1016,Q16-1026,0,0.519455,"el. The basis of this dynamic composition mechanism is the reliability signals that inform the model of the quality of each word embedding. Specifically, we assume that if a word occurs more frequently, its word embedding will be more fully trained as it has richer contexts and its embedding is updated more often during training. Thus, we design a set of reliability signals based on word frequency in the embedding training corpus and name tagging training corpus. 2.1 Baseline Model We adopt a state-of-the-art name tagging model LSTM-CNN (Long-short Term Memory - Convolutional Neural Network) (Chiu and Nichols, 2016) as our base model. In this architecture, the input sentence is represented as a sequence of vectors X = {x1 , ..., xL }, where xi is the vector representation of the i-th word, and L is the length of the sequence. Generally, xi is a concatenation of word embedding and character-level representation generated with a group of convolutional neural networks (CNNs) with various filter sizes from compositional character embeddings of the word. Next, the sequence X is fed into a bi-directional Recurrent Neural Network (RNN) with Longshort Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997)."
P19-1016,P18-1074,1,0.833856,"onstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-genre settings demonstrate the effectiveness of our model and verify our intui"
P19-1016,P15-1033,0,0.026624,"Missing"
P19-1016,D18-1153,1,0.841269,"F) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve th"
P19-1016,P05-1045,0,0.00980259,"omposes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling met"
P19-1016,P16-1101,0,0.0223755,"l random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new d"
P19-1016,P14-1146,0,0.117179,"Missing"
P19-1016,mota-grishman-2008-ne,0,0.0329385,"-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark dat"
P19-1016,W17-2612,0,0.0123742,"ods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose feat"
P19-1016,N16-1174,0,0.0614333,"Missing"
P19-1016,D14-1162,0,0.0917685,"l knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, eith"
P19-1016,N18-1202,0,0.29,"though our model successfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeli"
P19-1016,W13-3516,0,0.0320936,"Missing"
P19-1016,C16-1030,0,0.0599161,"Missing"
P19-1016,W04-1221,0,0.0386504,"ns in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven"
P19-1016,P16-2038,0,0.0178532,"6). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-g"
P19-1083,E09-1018,0,0.0431558,"1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, esp"
P19-1083,P15-1136,0,0.0462386,"plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word u"
P19-1083,D16-1245,0,0.0444309,"male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To c"
P19-1083,P81-1019,0,0.771448,"r, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, especially for pronouns, which are hard to be interpreted owing to their weak semantic meanings (Ehrlich, 1981). As one challenging yet vital subtask of the general coreference resolution, pronoun coreference resolution (Hobbs, 1978) is to find the correct reference for a given pronominal anaphor in the context and has showed its importance in many natural language processing (NLP) ∗ This work was partially done during the internship of the first author in Tencent AI Lab. 867 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate"
P19-1083,D10-1048,0,0.109095,"the noun phrases, so as to automatically generate linguistic knowledge (in the form of triplets) for our data. Specifically, the plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selec"
P19-1083,P11-1082,0,0.0351462,", for natural language understanding. Mention detection and coreference prediction are the two major focuses of the task as listed in Lee et al. (2017). Compared to general coreference problem, pronoun coreference resolution has its unique challenge since pronouns themselves have weak semantics meanings, which make it the most challenging sub-task in general coreference resolution. To address the unique difficulty brought by pronouns, we thus focus on resolving pronoun coreferences in this paper. Resolving pronoun coreference relations often requires the support of manually crafted knowledge (Rahman and Ng, 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu"
P19-1083,N18-2108,0,0.35483,"electional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 1"
P19-1083,P11-1117,0,0.0529445,"Missing"
P19-1083,N18-2028,1,0.77866,"ing embeddings of all words in its tail. For example, if s is ‘the apple’ and the knowledge triplet (‘the apple’, IsA, ‘healthy food’) is found by searching the KG, we represent this relation from the averaged embeddings of ‘healthy’ and ‘food’. Consequently, for s and p, we denote their retrieved knowledge set as Ks and Kp respectively, where Ks contains ms related knowledge embeddings k1,s , k2,s , ..., kms ,s and Kp contains mp of them k1,p , k2,p , ..., kmp ,p . Contextual information is crucial to distinguish the semantics of a word or phrase, especially for text representation learning (Song et al., 2018; Song and Shi, 2018). In this work, a standard bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) model is used to encode each span with attentions (Bahdanau et al., 2014), which is similar to the one used in Lee et al. (2017). The structure is shown in Figure 2. Let initial word embeddings in a span si be denoted as x1 , ..., xT and their encoded representation be x∗1 , ..., x∗T . The weighted embeddings of each span xˆi is obtained by T X (3) Thus the span representation of s and p are marked as es and ep , respectively. Span Representation xˆi = , where αt is a standard feed-fo"
P19-1083,W18-2315,1,0.841852,"for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate contextual information from large-scale external unlabeled data into the model, they are insufficient to incorporate existing complex knowledge into the representation for covering all the knowledge one needs to build a successful pronoun coreference system. In addition, overfitting is always observed on deep models, whose performance is thus limited in cross-domain scenarios and restricts their usage in real applications (Liu et al., 2018, 2019). Recently, a joint model (Zhang et al., 2019b) was proposed to connect the contextual information and human-designed features together for pronoun coreference resolution task (with gold mention support) and achieved the state-of-the-art performance. However, their model still requires the complex features designed by experts, which is expensive and difficult to acquire, and requires the support of the gold mentions. To address the limitations of the aforementioned models, in this paper, we propose a novel end-toend model that learns to resolve pronoun coreferences with general knowledg"
P19-1083,P19-1189,1,0.884331,"Missing"
P19-1083,P03-1022,0,0.665428,"Missing"
P19-1083,W18-2324,0,0.0308416,", 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu et al., 2018) ones, and world knowledge (Rahman and Ng, 2011), such as selectional preference (Wilks, 1975). Later, end-to-end solutions (Lee et al., 2017, 2018) were proposed to learn contextual information and solve coreferences synchronously with neural networks, e.g., LSTM. Their results proved that such knowledge is helpful when appropriately used for coreference resolution. However, external knowledge is often omitted in their models. Consider that context and external knowledge have their own advantages: the contextual information covering diverse text expressions that are difficult to be predefined"
P19-1083,P98-2143,0,0.505205,"respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for hu"
P19-1083,1995.tmi-1.6,0,0.49878,"Missing"
P19-1083,C94-2189,0,0.612771,"ine blue fonts, respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings sim"
P19-1083,D14-1162,0,0.0891682,"efer to complex things and occur with low frequency. Moreover, there are significant gaps in the performance of different models, with the following observations. First, models with manually defined rules or features, which cannot cover rich contextual information, perform poorly. In contrast, deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on the recall. SecImplementation Following the previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embeddings (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embeddings as the initial word representations for computing span representations. For knowledge triplets, we use the GloVe embeddings to encode tail words in them. Outof-vocabulary words are initialized with zero vectors. The hidden state of the LSTM module is set to 200, and all the feed-forward networks have two 150-dimension hidden layers. The selection thresholds are set to 10−2 and 10−8 for the CoNLL and i2b2 dataset, respectively. For model training, we use cross-entropy as the loss function and Adam (Kingma and Ba, 2014) as the optimizer. All the afo"
P19-1083,N18-1202,0,0.357819,"nerally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 10 https://github.com/kentonl/e2e-coref https://stanfordnlp.github.io/CoreNLP/ https://dumps.wikimedia.org/enwiki/ 8 In the Stanford parser, an ‘nsubj’ edge is created between i"
P19-1083,N19-1093,1,0.547847,"tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while"
P19-1083,W12-4501,0,0.114752,"in certain contexts. To solve it, a knowledge attention module is proposed to select the appropriate knowledge. For each pair of (s, p), as shown in Figure 3, we first concatenate es and ep to get the overall (span, pronoun) representation es,p , which is used to select knowledge for both s and p. Taking that for s as example, we compute the weight of each ki ∈ Ks by wi = P βki e kj ∈Ks e βkj , Dataset X wi · k i . 7,749 1,007 1,037 2,229 222 321 31,806 3,747 4,078 i2b2 train test 2,024 1,244 685 367 270 166 2,979 1,777 30,334 10,845 3,208 44,387 Datasets • CoNLL: The CoNLL-2012 shared task (Pradhan et al., 2012) corpus, which is a widely used dataset selected from the Ontonotes 5.05 . • i2b2: The i2b2 shared task dataset (Uzuner et al., 2012), consisting of electronic medical records from two different organizations, namely, Partners HealthCare (Part) and Beth Israel Deaconess medical center (Beth). All records have been fully de-identified and manually annotated with coreferences. (6) We split the datasets into different proportions based on their original settings. Three types of pronouns are considered in this paper following Ng (2005), i.e., third personal pronoun (e.g., she, her, he, him, them,"
P19-1083,C98-2138,0,\N,Missing
P19-1304,D18-1246,1,0.897995,"ic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index. This aggregator 1 Lebron James is transl"
P19-1304,D18-1032,0,0.0812748,"ultilingual knowledge graphs (KGs), such as DBpedia (Auer et al., 2007) and Yago (Suchanek et al., 2007), represent human knowledge in the structured format and have been successfully used in many natural language processing applications. These KGs encode rich monolingual knowledge but lack the cross-lingual links to bridge the language gap. Therefore, the cross-lingual KG alignment task, which automatically matches entities in a multilingual KG, is proposed to address this problem. Most recently, several entity matching based approaches (Hao et al., 2016; Chen et al., 2016; Sun et al., 2017; Wang et al., 2018) have been proposed for this task. Generally, these approaches first project entities of each KG into lowdimensional vector spaces by encoding monolingual KG facts, and then learn a similarity score function to match entities based on their vector representations. However, since some entities in different languages may have different KG To address these drawbacks, we propose a topic entity graph to represent the KG context information of an entity. Unlike previous methods that utilize entity embeddings to match entities, we formulate this task as a graph matching problem between the topic enti"
P19-1304,Q17-1010,0,0.0352098,"8 37.29 74.49 83.45 91.56 84.71 92.35 EN-FR @1 @10 14.61 37.25 21.26 50.60 32.97 65.91 36.77 73.06 81.03 90.79 84.15 91.76 66.91 67.93 67.92 64.01 65.28 65.21 72.63 73.97 73.52 69.76 71.29 70.18 87.62 89.38 88.96 87.65 88.18 88.01 77.52 78.48 78.36 78.12 79.64 79.48 85.09 87.15 86.87 83.48 84.63 84.29 94.19 95.24 94.28 93.66 94.75 94.37 Table 1: Evaluation results on the datasets. non-linearity function σ is ReLU (Glorot et al., 2011) and the parameters of aggregators are randomly initialized. Since KGs are represented in different languages, we first retrieve monolingual fastText embeddings (Bojanowski et al., 2017) for each language, and apply the method proposed in Conneau et al. (2017) to align these word embeddings into a same vector space, namely, crosslingual word embeddings. We use these embeddings to initialize word representations in the first layer of GCN1 . Results and Discussion. Following previous works, we used Hits@1 and Hits@10 to evaluate our model, where Hits@k measures the proportion of correctly aligned entities ranked in the top k. We implemented a baseline (referred as BASELINE in Table 1) that selects k closest G2 entities to a given G1 entity in the cross-lingual embedding space,"
P19-1304,D18-1223,1,0.789993,"Missing"
P19-1304,D18-1110,1,0.821566,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,D18-1112,1,0.597172,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,P18-1030,0,0.0133111,"raph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index."
Q19-1014,P17-1055,0,0.0523942,"Missing"
Q19-1014,P17-1168,0,0.0194867,"ion to improve the original sliding window baseline, formulated in Expression 8 (Section 4.2). • Stanford Attentive Reader This neural baseline compares each candidate answer (i.e., entity) representation to the question-aware document representation built with attention mechanism (Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford e"
Q19-1014,P04-3031,0,0.108587,"The formal definition of δis is as follows. s (7) To make the final answer option selection, our rule-based method combines Expressions (5) and (7): s IQ δis = (6) s swi Q + swi∗ d Q + d∗i − i 2 2 (5) • Pointwise mutual information (PMI): sQ sQ ∗ ∗ pmimax ,1..3 , pmimax,1..3 , pmimin,1..3, pmimin,1..3, sQ pmiavg,1..3 , and pmi∗avg,1..3 , where pmisf,i is defined as Since a large percentage of questions cannot be solved by word-level matching, we also attempt to incorporate general world knowledge into our rule-based method. We calculate cssi , the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |"
Q19-1014,P08-1090,0,0.0431185,", the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |. R = {r1 , r2 , . . .} is the set of ConceptNet relation types (e.g., ‘‘CapableOf’’ and ‘‘PartOf’’). cri,j is the number of relation triples (w1 , rj , w2 ) that appear in the ConceptNet (Speer et al., 2017), where w1 represents a word in answer option Oi , w2 represents a word in D, and the relation type rj ∈ R. Similar to the motivation for using PMI, we use CR to capture the association between an answer option and the source dialogue based on raw co-occurrence counts in the commonsense knowledge base. • ConceptNet embeddings (CE): Besides the l"
Q19-1014,P17-1025,1,0.884975,"Missing"
Q19-1014,P16-1223,0,0.0604832,"Missing"
Q19-1014,W16-3612,0,0.28737,"ractive since candidate answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the co"
Q19-1014,C18-1018,0,0.0606449,"Missing"
Q19-1014,D18-1241,1,0.873778,"Missing"
Q19-1014,P17-1147,0,0.0815762,"Missing"
Q19-1014,N18-1023,0,0.133945,"Missing"
Q19-1014,Q18-1023,0,0.115052,"Missing"
Q19-1014,D16-1241,0,0.0576388,"Missing"
Q19-1014,S18-1119,0,0.10702,"Missing"
Q19-1014,P11-2057,0,0.205028,"Missing"
Q19-1014,N18-1185,0,0.441948,"te answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the correct answer optio"
Q19-1014,N18-1202,0,0.0524594,"Missing"
Q19-1014,D18-1260,0,0.067851,"Missing"
Q19-1014,D16-1264,0,0.392475,"Missing"
Q19-1014,D13-1020,0,0.409742,"‘‘really hot,’’ ‘‘really beautiful,’’ ‘‘very bad,’’ and ‘‘very important’’ rather than more appropriate yet more advanced adjectives that might hinder reading comprehension of language learners with smaller vocabularies. According to the explanations provided by the tool, the readability scores for both data sets fall into the same category ‘‘Your text is very simple and easy to read, likely to be understood by an average 5th-grader (age 10).’’ 4 4.2 Rule-Based Approaches We first attempt to incorporate dialogue structure information into sliding window (SW), a rulebased approach developed by Richardson et al. (2013). This approach matches a bag of words constructed from a question Q and one of its answer option Oi with a given document, and calculates the TF-IDF style matching score for each answer option. ˆ , and O ˆ i be the unordered set of ˆ s, Q Let D distinct words (excluding punctuation marks) in Ds , Q, and Oi , respectively. Instead of only regarding dialogue D as a non-conversational text snippet, we also pay special attention to the context that is relevant to the target speaker mentioned in the question. Therefore, given a target speaker sQ , we propose to compute a speaker-focused sliding wi"
Q19-1014,D18-1132,0,0.0646353,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P18-2118,0,0.297659,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P13-1171,0,0.114272,"Missing"
S15-2054,W13-3821,0,0.490977,"Missing"
S15-2054,W06-2920,0,0.0523001,"Missing"
S15-2054,W04-2412,0,0.118835,"Missing"
S15-2054,W13-3826,0,0.236499,"Missing"
S15-2054,popescu-2012-buildind,0,0.0638283,"Missing"
S17-1010,C08-1002,0,0.0608562,"Missing"
S17-1010,U05-1028,0,0.11819,"Missing"
S17-1010,S15-2053,0,0.0272133,"Missing"
S17-1010,P16-1085,0,0.0222402,"Missing"
S17-1010,P98-1013,0,0.108214,"k 15. Finally, we extend the task to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work. 1 Introduction and Related Work Semantic frame labeling is the task of selecting the correct frame for a given target based on its semantic scene. A target is often called lexical unit which evokes the corresponding semantic frame. The lexical unit can be a verb, adjective or noun. Generally, a semantic frame describes how the lexical unit is used and specifies its characteristic interactions. There are many semantic frame resources, such as FrameNet (Baker et al., 1998), VerbNet (Schuler, 2006), PropBank (Palmer et al., 2005) and Corpus Pattern Analysis (CPA) frames (Hanks, 2012). However, most existing frame resources are manually created, which is time-consuming and expensive. Automatic semantic frame labeling can lead to the development of a broader range of resources. ∗ The corresponding author 91 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 91–96, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics noise caused by irrelevant words in long sentences may hinder learning"
S17-1010,W13-3826,0,0.0508987,"Missing"
S17-1010,S15-2075,0,0.0183333,"erns in which a word is used in text and is currently being used to build the PDEV resource as we mentioned above. It is also a shared task in SemEval-2015 task 15 (Baisa et al., 2015). The task is divided into three subtasks: CPA parsing, CPA clustering and CPA lexicography. We only introduce the first two related subtasks. CPA parsing aims at identifying the arguments of the target and tagging predefined semantic meaning on them; CPA clustering clusters the instances to obtain CPA frames based on the result of CPA parsing. However, the first step results seem unpromising (Feng et al., 2015; Mills and Levow, 2015; Elia, 2016) which will influence the process of obtaining CPA frames. Since our model can be applied on sentence-level input without feature extraction we can directly evaluate https://github.com/lzhang10/maxent 94 ID 1 2 3 4 Sentences One of the farmer’s cows had died of BSE raising fears of cross-infection... One of the farmer’s ducks|chickens|geese had died of BSE raising fears of cross-infection... Elegans also in central America die of damping off as a function of distance Indeed, the MEC does not advise the use of any insecticidal shampoo for... Frame Prediction True Frame Same with tr"
S17-1010,el-maarouf-etal-2014-disambiguating,0,0.0415511,"Missing"
S17-1010,J05-1004,0,0.113179,"uation task and we also achieve a strong result in comparison to state-of-the-art work. 1 Introduction and Related Work Semantic frame labeling is the task of selecting the correct frame for a given target based on its semantic scene. A target is often called lexical unit which evokes the corresponding semantic frame. The lexical unit can be a verb, adjective or noun. Generally, a semantic frame describes how the lexical unit is used and specifies its characteristic interactions. There are many semantic frame resources, such as FrameNet (Baker et al., 1998), VerbNet (Schuler, 2006), PropBank (Palmer et al., 2005) and Corpus Pattern Analysis (CPA) frames (Hanks, 2012). However, most existing frame resources are manually created, which is time-consuming and expensive. Automatic semantic frame labeling can lead to the development of a broader range of resources. ∗ The corresponding author 91 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 91–96, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics noise caused by irrelevant words in long sentences may hinder learning. In fact, the arguments related to the target are usuall"
S17-1010,D14-1162,0,0.0803697,"aining data. However, in many cases, the unseen words can be captured by well trained word embeddings as the sentence 2 shows where ’ducks’, ’chickens’ and ’geese’ are all unseen words. The number of the iterations for MaxEnt is decided by the validation set. For simplicity, we set the learning rate to 1.0 for TRNN and LSTM. The number of hidden units is tested on validation data with the values {35, 45, 55} for per-target resource and {80, 100, 120} for non per-target resource. We use the publicly available word2vec vectors, a dimensionality of 300, that were trained through the GloVe model (Pennington et al., 2014) on Wikipedia and Gigaword. For words not appeared in the vector model, their word vectors are all set to zero vectors. We train these models by stochastic gradient descent with minibatches. The minibatch is set to 10 for per-target resource and 50 for non per-target resource. We keep the word vectors static since no obvious improvement has been observed. Training will stop when the zeroone loss is zero over training data. 3.3 Models MF Target-Only MaxEnt LSTM TRNN Results The results of the above datasets are in Table 3. Target-Only gets very high scores on FrameNet dataset. FrameNet dataset"
S17-1010,S15-2054,1,0.828233,"fying the main patterns in which a word is used in text and is currently being used to build the PDEV resource as we mentioned above. It is also a shared task in SemEval-2015 task 15 (Baisa et al., 2015). The task is divided into three subtasks: CPA parsing, CPA clustering and CPA lexicography. We only introduce the first two related subtasks. CPA parsing aims at identifying the arguments of the target and tagging predefined semantic meaning on them; CPA clustering clusters the instances to obtain CPA frames based on the result of CPA parsing. However, the first step results seem unpromising (Feng et al., 2015; Mills and Levow, 2015; Elia, 2016) which will influence the process of obtaining CPA frames. Since our model can be applied on sentence-level input without feature extraction we can directly evaluate https://github.com/lzhang10/maxent 94 ID 1 2 3 4 Sentences One of the farmer’s cows had died of BSE raising fears of cross-infection... One of the farmer’s ducks|chickens|geese had died of BSE raising fears of cross-infection... Elegans also in central America die of damping off as a function of distance Indeed, the MEC does not advise the use of any insecticidal shampoo for... Frame Prediction"
S17-1010,S07-1016,0,0.0301125,"Missing"
S17-1010,N16-2001,1,0.867323,"amed Entities (NE) and dependency relations related to the target. Currently, some deep learning models have been applied with dependency features. Hermann et al. (2014) used the direct dependents and dependency path to extract the context representation based on distributed word embeddings on English FrameNet. Inspired by the work, Zhao et al. (2016) used a deep feed forward neural network on Chinese FrameNet with similar features. This is different from our goal where we want to explore an appropriate deep learning architecture without complex rules to construct the context representations. Feng et al. (2016) used a multilayer perceptrons (MLP) model on CPA frames without extra feature extraction, but the model is quite simple and has an input window which is not convenient. In this paper, we present a target-based neural model which takes the whole target-specific sentence as input and gives the semantic frame label as output. Our goal is to make the model light without explicit rules to construct context representations and applicable to a range of resources. To cope with variable-length sentences under our constraint, a simple idea is to use recurrent neural networks (RNN) to process the senten"
S17-1010,P10-4014,0,0.0356515,"n Table 5. All the models are supervised except for baseline and DULUTH. Feng et al. (2016) used the MLP to classify fixedlength local text of the target based on distributed word embeddings. But the representation of the target’s context is simply constructed with concatenated word embeddings and the length of local context has to be chosen manually. Besides, MLP may fail to train or predict well when some key words are out of its input window. System BOB90(Best in SemEval 2015) SemEval 2015 baseline DULUTH Feng et al. (2016) This paper grate word embeddings into IMS (It Makes Sense) system (Zhong and Ng, 2010) which uses support vector machine as its classifier based on some standard WSD features and they get the best result; they use an exponential decay function, also designed to give more importance to close context, to compute the word representation, but their method need manually choose the window size of the target word and one parameter of their exponential decay function. Both with word vectors only, our model is comparable with the sixth row. System Rank 1 system in SemEval 2007 Rank 2 system in SemEval 2007 IMS (2010) IMS + word vectors (2016) IMS + word vectors only (2016) This paper Ta"
S17-1010,P14-1136,0,0.0227647,"5; Abend et al. 2008). Recently, there are some works on learning CPA frames based on a new semantic frame resource, the Pattern Dictionary of English Verbs (PDEV) (El Maarouf and Baisa, 2013; El Maarouf et al., 2014). The above two works also rely on features and both are only tested on 25 verbs. Most works aim at constructing the context representations of the target with explicit rules based on some basic features, e.g., Parts Of Speech (POS), Named Entities (NE) and dependency relations related to the target. Currently, some deep learning models have been applied with dependency features. Hermann et al. (2014) used the direct dependents and dependency path to extract the context representation based on distributed word embeddings on English FrameNet. Inspired by the work, Zhao et al. (2016) used a deep feed forward neural network on Chinese FrameNet with similar features. This is different from our goal where we want to explore an appropriate deep learning architecture without complex rules to construct the context representations. Feng et al. (2016) used a multilayer perceptrons (MLP) model on CPA frames without extra feature extraction, but the model is quite simple and has an input window which"
S18-1186,P17-1152,0,0.0730886,"Missing"
S18-1186,N18-1175,0,0.0735373,"structure of a debate (Hastings, 1963; Walton et al., 2008; Walton, 1990). Argument reasoning comprehension, a new task in SemEval 2018, sheds some light on the core of reasoning in natural language argumentation: implicit warrants, which are seen as a bridge between claims and reasons. Given a reason R, a claim C and two alternative warrants W0 and W1, the goal of this task is to identify the right warrant which can justify the use of R as support for C. The difficulty of the task is the warrants are plausible and lexically close ∗ *The corresponding author but lead to contradicting claims (Habernal et al., 2018). To be more specific, the reason R and the claim C are propositions extracted from a natural language argument. And warrant W is the relation between R and C which is characterized by a rule of inference (Newman and Marshall, 1992). Walton proposed that an argument refers to a claim based on reasons given in the premises (Walton, 1990). The most central part of this task is how to find the warrant for the given R and C. In the argument reasoning comprehension task, the organizer extracts the instances from Room for Debate section of the New York Times. After a complex crowd-sourcing process,"
S19-2191,S17-2080,0,0.140808,"each conversation context to 70 tokens and each target tweet to 28 tokens. For those that exceed the limit in the training set, we truncate each piece to the edge to create multiple instances and thus enlarge the training set. For the development set and test set, the length restrictions are also set, but only the first truncated piece for each instance is taken as input. Although data slicing may hurt long-distance dependency, the experiment result shows that this method performs better than the original. 2.4 Feature Extraction Inspired by the features extracted by Kochkina et al. (2017) and Bahuleyan and Vechtomova (2017), we collect 56 word-level features and 16 tweet-level features. For word-level features, we calculate their distribution percentage on the four categories in subtask A and the three categories in subtask B in the training and development set, and only apply those that mark a clear distinction between the classes for each subtask. For the numerical tweet-level features, we cluster each one into several groups according to their values and determine a common value for the whole group. Where to add the features is illustrated on the right side in Figure 2. After selection, the word-level feature"
S19-2191,S17-2082,0,0.466291,"Missing"
S19-2191,N16-1138,0,0.0453085,"ata Expansion We find that the data distribution in the training set is skewed toward comment in subtask A, which explains why it is hard for the model to reach high precision and recall scores in the other three classes. Thus we expand the training data with datasets on similar tasks with labels corresponding to the three minority classes in seeking for better class balance and more sufficient training. For support and deny, we take each claim as both the target tweet and its conversation context in stance classification datasets SemEval 2016 task 6 dataset (Mohammad et al., 2016), Emergent (Ferreira and Vlachos, 2016), and twitter sentiment analysis dataset sentiment140 (Go et al., 2009). For query, we use passages as the conversation context, unanswerable questions as the target tweet in reading comprehension datasets SQuAD 2.0 (Rajpurkar et al., 2018) and CoQA (Reddy et al., 2018). The acquirement of extended data takes two steps. We first calculate the sentence similarity of each data piece in external datasets with all sentences in the original dataset in terms of Levenshtein Distance, and only keep instances whose minimal distances with the original dataset are below 0.7. Then we test them in a model"
S19-2191,D11-1147,0,0.0548966,"racteristics of data coming from social media: conversation structure, rich intrinsic features, skewed distribution toward the comment class in rumour stance data and scarcity of available data for rumour veracity classification. While most pioneering works treated rumour evaluation as a single-tweet task, attempts to utilize the conversation structure included pairing source and replies together to make up input (Singh et al., 2017), and adopting the full conversation thread as input in the time sequence (Kochkina et al., 2017). With the realization of rich features hidden in tweet contexts, Qazvinian et al. (2011) was one of the first who extracted them and combined them with model input. The feature sets were augmented during the following work. In trying to acquire more comprehensive information, not only features of the tweet for prediction were taken into consideration, but also features from its conversation context (Enayet and El-Beltagy, 2017). To address the class imbalance problem, Wang et al. (2017) transformed subtask A into a two-step classification task: they first classified comments and noncomments, and then categorized non-comments into the other three classes. Finally, in order to make"
S19-2191,P18-2124,0,0.0814692,"Missing"
S19-2191,S17-2087,0,0.159508,"city of a rumour as true or false given the rumourous post and a set of additional resources. Apart from variations in models, research in this area mainly focuses on the special characteristics of data coming from social media: conversation structure, rich intrinsic features, skewed distribution toward the comment class in rumour stance data and scarcity of available data for rumour veracity classification. While most pioneering works treated rumour evaluation as a single-tweet task, attempts to utilize the conversation structure included pairing source and replies together to make up input (Singh et al., 2017), and adopting the full conversation thread as input in the time sequence (Kochkina et al., 2017). With the realization of rich features hidden in tweet contexts, Qazvinian et al. (2011) was one of the first who extracted them and combined them with model input. The feature sets were augmented during the following work. In trying to acquire more comprehensive information, not only features of the tweet for prediction were taken into consideration, but also features from its conversation context (Enayet and El-Beltagy, 2017). To address the class imbalance problem, Wang et al. (2017) transforme"
S19-2191,S17-2086,0,0.0789413,"Missing"
S19-2191,S17-2083,0,0.487846,"Apart from variations in models, research in this area mainly focuses on the special characteristics of data coming from social media: conversation structure, rich intrinsic features, skewed distribution toward the comment class in rumour stance data and scarcity of available data for rumour veracity classification. While most pioneering works treated rumour evaluation as a single-tweet task, attempts to utilize the conversation structure included pairing source and replies together to make up input (Singh et al., 2017), and adopting the full conversation thread as input in the time sequence (Kochkina et al., 2017). With the realization of rich features hidden in tweet contexts, Qazvinian et al. (2011) was one of the first who extracted them and combined them with model input. The feature sets were augmented during the following work. In trying to acquire more comprehensive information, not only features of the tweet for prediction were taken into consideration, but also features from its conversation context (Enayet and El-Beltagy, 2017). To address the class imbalance problem, Wang et al. (2017) transformed subtask A into a two-step classification task: they first classified comments and noncomments,"
S19-2191,S16-1003,0,0.182903,"Missing"
S19-2198,S17-2050,0,0.0150175,"sion, feature extraction, and input transformation. The task includes two subtasks and they are both three classification problems. In subtask A, we need to find out whether a question seeks a We apply three points to solve these problems. We extend the training set of subtask A from two other datasets: DailyDialog (Li et al., 2017) and SQuAD2.0 (Rajpurkar et al., 2018). We use two methods to guarantee the quality of expanded datasets. Firstly we use the Levenshtein Distance to screen similar data, and then we use the prediction of the model to further screen the results of the previous step. Goyal (2017) and Xie et al. (2017) used various features. Le et al. (2017) used keywords to solve the previous similar problem. We follow their work in feature extraction. Working on subtask A, we also use characteristic words as features to improve the system. Input transformation for classification task is Start + T ext + Extract, including randomly initialized start and end tokens. We concatenate the text and features token sequences with a delimiter token. The remainder of this paper is organized as follows. Section 2 contains a description of our system. The experiments and analysis of the results ar"
S19-2198,P82-1020,0,0.800305,"Missing"
S19-2198,S17-2049,0,0.0143621,"sk includes two subtasks and they are both three classification problems. In subtask A, we need to find out whether a question seeks a We apply three points to solve these problems. We extend the training set of subtask A from two other datasets: DailyDialog (Li et al., 2017) and SQuAD2.0 (Rajpurkar et al., 2018). We use two methods to guarantee the quality of expanded datasets. Firstly we use the Levenshtein Distance to screen similar data, and then we use the prediction of the model to further screen the results of the previous step. Goyal (2017) and Xie et al. (2017) used various features. Le et al. (2017) used keywords to solve the previous similar problem. We follow their work in feature extraction. Working on subtask A, we also use characteristic words as features to improve the system. Input transformation for classification task is Start + T ext + Extract, including randomly initialized start and end tokens. We concatenate the text and features token sequences with a delimiter token. The remainder of this paper is organized as follows. Section 2 contains a description of our system. The experiments and analysis of the results are introduced in section 3. We describe the conclusions in sect"
S19-2198,S17-2047,0,0.0165578,"raction, and input transformation. The task includes two subtasks and they are both three classification problems. In subtask A, we need to find out whether a question seeks a We apply three points to solve these problems. We extend the training set of subtask A from two other datasets: DailyDialog (Li et al., 2017) and SQuAD2.0 (Rajpurkar et al., 2018). We use two methods to guarantee the quality of expanded datasets. Firstly we use the Levenshtein Distance to screen similar data, and then we use the prediction of the model to further screen the results of the previous step. Goyal (2017) and Xie et al. (2017) used various features. Le et al. (2017) used keywords to solve the previous similar problem. We follow their work in feature extraction. Working on subtask A, we also use characteristic words as features to improve the system. Input transformation for classification task is Start + T ext + Extract, including randomly initialized start and end tokens. We concatenate the text and features token sequences with a delimiter token. The remainder of this paper is organized as follows. Section 2 contains a description of our system. The experiments and analysis of the results are introduced in sectio"
S19-2198,I17-1099,0,0.0257471,"fication problem. We study these issues in SemEval2019 Task 8 (Mihaylova et al., 2019) by using the contextual Knowledge-enhanced GPT (Radford et al., 2018), which use Transformer (Vaswani et al., 2017) as model architecture. The contextual knowledge enhancement includes data extension, feature extraction, and input transformation. The task includes two subtasks and they are both three classification problems. In subtask A, we need to find out whether a question seeks a We apply three points to solve these problems. We extend the training set of subtask A from two other datasets: DailyDialog (Li et al., 2017) and SQuAD2.0 (Rajpurkar et al., 2018). We use two methods to guarantee the quality of expanded datasets. Firstly we use the Levenshtein Distance to screen similar data, and then we use the prediction of the model to further screen the results of the previous step. Goyal (2017) and Xie et al. (2017) used various features. Le et al. (2017) used keywords to solve the previous similar problem. We follow their work in feature extraction. Working on subtask A, we also use characteristic words as features to improve the system. Input transformation for classification task is Start + T ext + Extract,"
S19-2198,S19-2149,0,0.0267809,"quires some modifications to fit specific tasks. Introduction With the development of Community Question Answering (cQA) forums, massive information is being shared. However, not all information is factual, which makes finding an appropriate answer to satisfy the information needs of questioners more difficult. Previous work which concentrated on these problems (Nakov et al., 2017) reranked the questions based on their relevance with ˇ the original question. Saina et al. (2017) treated the similarity ranking task as a binary classification problem. We study these issues in SemEval2019 Task 8 (Mihaylova et al., 2019) by using the contextual Knowledge-enhanced GPT (Radford et al., 2018), which use Transformer (Vaswani et al., 2017) as model architecture. The contextual knowledge enhancement includes data extension, feature extraction, and input transformation. The task includes two subtasks and they are both three classification problems. In subtask A, we need to find out whether a question seeks a We apply three points to solve these problems. We extend the training set of subtask A from two other datasets: DailyDialog (Li et al., 2017) and SQuAD2.0 (Rajpurkar et al., 2018). We use two methods to guarante"
S19-2198,S17-2003,0,0.147153,"Missing"
S19-2198,N18-1202,0,0.0190705,"Missing"
S19-2198,P18-2124,0,0.0541499,"Missing"
S19-2198,S17-2055,0,0.0405124,"Missing"
W14-6819,C10-3004,0,0.0766735,"Missing"
W14-6819,P11-1115,0,0.111675,"vent tracking, entity disambiguation and other related research areas. In the task, the incomplete attributes of a target person are defined as Slots, i.e. the extracted attribute value need to be filled into these slots. There are 3 kinds of slots, name slots, value slots and string slots, in which only entity name, number/time and string can be filled in. Single-value slots have only one correct answer while listvalue slots have a set of answers. There are totally 25 attributes need to be extracted, as shown in Table 1. Slot filling task has been one of shared tasks in the TAC KBP workshop [Ji and Grishman, 2011] science 2009. In this area, earlier systems generally use one main pipeline that contains 3 stages: document retrieval, answer extraction, and answer combination. Supervised learning normally leads to a reasonably good performance. Both bootstrapping and rule based pattern matching with trigger words are used in [Li, et al., 2013]. Active learning techniques are also used in the task [Chen, et al, 2010]. UNED system introduces a graph structure to solve the problem [ Garrido, et al., 2013]. CMUML uses distant supervision and CRF-based structured prediction for producing the final answers [Ki"
Y18-1045,P17-2097,0,0.0245566,"Missing"
Y18-1045,D17-1168,0,0.0468368,"Missing"
Y18-1045,P17-1171,0,0.0516803,"Missing"
Y18-1045,P17-1152,0,0.0577445,"Missing"
Y18-1045,D17-1216,0,0.0428742,"Missing"
Y18-1045,P18-2045,0,0.020199,"Missing"
Y18-1045,N16-1098,0,0.0456265,"Missing"
Y18-1045,P16-2022,0,0.0705971,"Missing"
Y18-1045,D14-1162,0,0.0796306,"Missing"
Y18-1045,K17-1004,0,0.0363891,"Missing"
Y18-1045,N18-2015,0,0.0232671,"Missing"
Y18-1045,P18-2118,0,0.0614446,"Missing"
Y18-1045,N16-1174,0,0.125419,"Missing"
