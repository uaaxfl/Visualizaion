2011.mtsummit-papers.46,2008.iwslt-papers.1,0,0.0141744,"presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions of translation systems between these language pairs suffers greatly on the scarce resource, such as parallel data. We introduced the idea of compatibility, where all languages can be mapped to the same semantic mea"
2011.mtsummit-papers.46,J93-2003,0,0.0305137,"f each sentence pair and choose a certain percentage of the best scored sentences for training. In order to include information from various resources, the quality of a sentence pair is measured using a log-linear model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and e"
2011.mtsummit-papers.46,2002.tmi-tutorials.2,0,0.0711211,"n the idea of compatibility. Generally speaking, the quality of compatible predictions provided by multiple systems is more reliable. For simple classiﬁcation problems, it is reasonable to take a prediction as good which the multiple systems agree on. This idea is widely used in ensemble learning and semi-supervised learning. Take Bootstrap aggregating, a meta-algorithm for ensemble learning as an example, multiple models are separately trained on randomly generated sub-samples, and then vote to achieve ﬁnal predictions. Another example closely related to our method is co-training such as in (Callison-Burch, 2002). One way to select automatic predictions for re-training in co-training is to choose the agreed ones. Different from simple classiﬁcation problems, even complex structured prediction problems such as parsing, the output of MT is in human languages, which may be the most complicated way to represent the meaning of another human language. It is too strict to ask multiple systems to provide exactly the same translated sentence for an input. We extend the agreement idea to the compatibility idea. Informally, two sentences are called compatible if they express the same meaning to some extent. We c"
2011.mtsummit-papers.46,P07-1092,0,0.0172329,"language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited."
2011.mtsummit-papers.46,P08-1010,1,0.89531,"Missing"
2011.mtsummit-papers.46,2008.eamt-1.6,0,0.0157302,"d Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by"
2011.mtsummit-papers.46,W09-0431,0,0.0128027,"e, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical b"
2011.mtsummit-papers.46,P07-2045,0,0.0071307,"d using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). E"
2011.mtsummit-papers.46,2005.mtsummit-papers.11,0,0.0723615,"ll scarce resourced language pairs. The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on l"
2011.mtsummit-papers.46,D07-1005,0,0.0142377,"Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation sys"
2011.mtsummit-papers.46,2010.iwslt-papers.12,0,0.0137576,"les obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions o"
2011.mtsummit-papers.46,N01-1020,0,0.0459602,"rget languages. Callison-Burch (2002) presented a co-training method for SMT, the agreement of multiple translation systems is explored to ﬁnd the best translation for re-training. We applied compatibility instead of agreement based approach, detailed description on the difference between compatibility and agreement is referred to Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 20"
2011.mtsummit-papers.46,N04-1034,0,0.0630649,"Missing"
2011.mtsummit-papers.46,J03-1002,0,0.00365166,"model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a s"
2011.mtsummit-papers.46,P02-1040,0,0.0891495,"Missing"
2011.mtsummit-papers.46,N10-1063,0,0.0275757,"Missing"
2011.mtsummit-papers.46,W11-2100,0,0.0953753,"The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on less studied language pairs, we co"
2011.mtsummit-papers.46,steinberger-etal-2006-jrc,0,0.253155,"Missing"
2011.mtsummit-papers.46,C96-2141,0,0.262133,"ature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). Each language model is trained with the target side of the parallel data. We do not apply any zmert tuning in EMS because it does not improve our translation results on the evaluation set. Imp"
2011.mtsummit-papers.46,P07-1108,0,0.0233794,"from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including"
2020.acl-main.377,P18-1038,1,0.902718,"Missing"
2020.acl-main.377,P13-1091,0,0.430837,"he framework of graph-based semantic representations (Koller et al., 2019) and Hyperedge Replacement Grammar (Drewes et al., 1997). The ability to enumerate all possible analyses of a graph facilitates surface realization, grammar induction, recursive graph embedding, etc. The advance in efficiency is from exploiting locality of HRG rules from the rarely discussed perspective of language production, a reversed direction to language understanding. We discuss locality in a sense of terminal edge-adjacency and develop a locality-centric complexity analysis of the de facto algorithm introduced by Chiang et al. (2013). Our analysis motivates (1) a terminal edge-first parsing strategy, (2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and (3) a computational support in the constructivist hypothesis in theoretical linguistics. Altogether, our analysis leads to a substantial improvement in practical graph parsing. An MR with the number of conceptual nodes ranging from 5 to 50 corresponding to a Wall Street Journal sentence can receive a fullforest analysis in 0.089 second on average with a large-scale comprehensive grammar; Even semantic graphs with c.a. 80 conceptual"
2020.acl-main.377,P96-1027,0,0.0626681,"V, E, ℓ, Vx ⟩). A tree decomposition TR is nice, if every node of TR must be one of: (1) a leaf node associated to empty graph; (2) a unary node which introduces exactly one edge; (3) a binary node which introduces no edges. Throughout, for convenience, let η denote a node from TR and R⊵η denote the subgraph of R whose edges are induced by nodes in the subtree rooted by η. If η is binary, its children are denoted by η1 and η2 . If η is unary, the edge introduced by it and its only child are denoted by e and η1 respectively. Oriented by the fundamental architecture of chart parsing/generation (Kay, 1996), TR are used to define active/passive items and inference rules that process such items. A passive item is of 4102 [A → R, η1 , I, φ1 ][A → R, η2 , J, φ2 ] [A → R, ηr , J, ψ] (R1) (R3) [A, J, ψ(XR )] [A → R, η, ∅, ∅] [A → R, η, I ∪ J, φ1 ∪ φ2 ] [A → R, η1 , I, φ1 ] [A → R, η1 , I, φ1 ][ℓ(e), J, X] (R2.T) (R2.NT) [A → R, η, I ∪ e∗ , φ1 ∪ {e 7→ e∗ }] [A → R, η, I ∪ J, φ1 ∪ {e 7→ X}] (R0)   P2 : X,    P1 : Y,  X arg1 E   P3 : Z,  A3 : γ2 , η2 ,  Y cjt-l D cjt-r cjt-l F  , ( E ) B A C  ,(A C F   )  A   A2 : γ2 , η1 ,  A arg1 E G cjt-r arg1  C cjt-l C D cjt-l arg1 cjt-r E"
2020.acl-main.377,P19-4002,1,0.812596,"in modeling language production is parsing meaning representations, i.e. computing all possible analyses of a given meaning representation (MR) according to a (competence) grammar. In theory, the worst-case complexities of existing algorithms are exponential or high-degree polynomial w.r.t. grammar size and input length. In practice, there are few systems that can parse large but frequent MRs with a realistic, wide-coverage grammar in a reasonable time. The major contribution of this paper is an exact yet efficient method to parse MRs in the framework of graph-based semantic representations (Koller et al., 2019) and Hyperedge Replacement Grammar (Drewes et al., 1997). The ability to enumerate all possible analyses of a graph facilitates surface realization, grammar induction, recursive graph embedding, etc. The advance in efficiency is from exploiting locality of HRG rules from the rarely discussed perspective of language production, a reversed direction to language understanding. We discuss locality in a sense of terminal edge-adjacency and develop a locality-centric complexity analysis of the de facto algorithm introduced by Chiang et al. (2013). Our analysis motivates (1) a terminal edge-first par"
2020.acl-main.377,oepen-lonning-2006-discriminant,0,0.0261535,"Missing"
2020.acl-main.377,S17-1024,0,0.0168585,"with an RGG can be finished in linear time by applying Chiang et al.’s algorithm. 4106 η0 η1 η2 e1 e2 ... ηl ηl+1 ηl+2 el el+1 el+2 VP D really arg1 seem C arg1 care B γ12 arg1 2 1 ADV VRP Figure 10: A terminal edge-first tree decomposition of a binary and weakly regular rule. For every node ηi (1 ≤ i ≤ l + 2), Vηi = bn(R⊵ηi ) ∪ V (ei ) and Eηi = {ei }. e1 , . . . , el are terminal edges ordered by visiting time of a depth-first traversal. el+1 , el+2 are nonterminal edges arranged in order such that R⊵ηl+1 is also a connected graph. This result is comparable to another algorithm proposed by Gilroy et al. (2017). However, the strong restrictions of RGG make it too weak to model linguistic structures. WRGG is much more linguistically adequate. 4.4 Distributed Argument-Structure We value the trigger role played by terminal edges in an HRG rule. Now let us revisit the derivation governed by a lexicalized grammar. It is obvious that lexical rules try to use up all terminal edges at the initial stage of syntactico-semantic composition. If we can distribute terminal edges to all rules, both lexical and phrasal, we are able to get a reduced number of free nodes on average and in exactly this way improve gra"
2020.acl-main.377,P15-1143,0,0.0139288,"adjacency. What does terminal edge-adjacency actually mean? From a semiotic perspective of a language system, being either natural or artificial, a key property is form-meaning connection. A particular form triggers a particular meaning. What can be observed can be directly recognized, and then makes other things recognizable. Considering language production, the input is an MR, and in the graph-based framework, it is terminal edges that are directly observable. In this way a terminal edge makes nodes connected to it co-recognizable. The existing algorithms, including Chiang et al. (2013) and Groschwitz et al. (2015), do not consider terminal edge-adjacency. We will show that capturing locality in this sense is beneficial, just like what successful string parsing algorithms do. 4.2 Locality-centric Complexity Analysis Some active nodes are not independent with each other if we take terminal edge-adjacency into consideration. We call a graph consisting of only terminal edges a terminal graph. For a graph fragment H, we use term(H) to denote the subgraph of H that is induced from all and only terminal edges. We informally illustrate the idea of dependency between nodes in a rule, and then present a precise"
2020.acl-main.605,W13-2322,0,0.050452,"Missing"
2020.acl-main.605,P17-1112,0,0.0182554,"X tm = (um,k · wm,k ) 0≤k&lt;n We use the similarity between tm and si,j as the score of this graph fragment. For training, we use the cross-entropy function as loss. SCORE(Hm , i, j) 5 5.1 = (tm )> W2 si,j Experiments Data Setup DeepBank (Flickinger et al., 2012) is a deep linguistic resource that covers the Wall Street Journal section of Penn TreeBank (PTB; Marcus et al., 1993). All annotations are governed by English Resource Grammar (ERG; Flickinger, 2000). We use the DeepBank v1.1 data, and split it into training, development and test sets along with previous work (Oepen et al., 2014, 2015; Buys and Blunsom, 2017; Chen et al., 2018) to make sure that the numeric performance can be directly compared to the results in the literature. 5.2 Evaluation Metrics Token-wise Evaluation for Accuracy The semantic annotations in DeepBank are presented as variable-in-situ MRS style originally. It is a non-trivial problem to measure the similarity between different logical forms accordingly. Copestake (2009) provides a method to reversibly translate them into variable-reduced semantic graphs, namely dubbed Dependency MRS (DMRS), in an information-equivalent fashion, which is widely used by previous studies. We conve"
2020.acl-main.605,P13-2131,0,0.0193971,"antic annotations in DeepBank are presented as variable-in-situ MRS style originally. It is a non-trivial problem to measure the similarity between different logical forms accordingly. Copestake (2009) provides a method to reversibly translate them into variable-reduced semantic graphs, namely dubbed Dependency MRS (DMRS), in an information-equivalent fashion, which is widely used by previous studies. We convert our outputs to DMRS, and re-use the evaluation metrics for variable-reduced graph representations, including Elementary Dependency Match (EDM; 6778 Dridan and Oepen, 2011) and SMATCH (Cai and Knight, 2013) to perform evaluation. TH AO Span Search-Based Evaluation for Coherence Another dimension for parser evaluation—the coherence of the output structures—is as essential as accuracy, since we also emphasize on the logical nature. Under the framework of underspecification, the coherence of a semantic structure entails that there must be at least one fully specified, i.e. scope-resolved logical form, which satisfies all the constraints encoded by that structure. The following shows a by-design incoherent semantic graph: N Y Y dog every RSTR BODY ? EDM A EDM SMATCH 80.52 84.91 90.91 80.79 85.42 91."
2020.acl-main.605,P18-1038,1,0.913274,"RG1 c x 0 x 1 VP chase L x 0 ∅ xx 2 N h x ⇐= Rule ® NP VP BODY h chase L ARG0 1 x L cat c ⇐= Rule  N ARG2 chase h 0 x 1 h 0 RSTR c ARG0 c ARG1 h D V 2 x of nodes, and E ⊆ V + is a finite set of hyperedges. A hyperedge is an extension of a normal edge which can connect to more than two nodes or only one node. l : E → L assigns a label from a finite set L to each hyperedge. Since nodes receive no informative labels, we use single-node edges with terminal labels to represent predicates. This strategy is widely used by HRG-based NLP systems, including Chiang et al. (2013), Peng et al. (2015) and Chen et al. (2018). X ∈ V ∗ defines an ordered list of nodes called external nodes, which specify the docking points during graph rewriting. t : V → T assigns a type from a finite set T to each node. Different from the hypergraphs used by Chiang et al. (2013) and Chen et al. (2018), we highlight the usage of node types which has a significant impact on making parsing results logically coherent. Three node types are utilized: h, x and c, which indicate handle, variable and predicate respectively. During node gluing, we must make sure that the types of nodes are identical. If the type of any node is still unspeci"
2020.acl-main.605,P13-1091,0,0.0323202,"RG0 cat L NP 1 x ARG2 L c some h 0 x ARG0 c ARG1 c x 0 x 1 VP chase L x 0 ∅ xx 2 N h x ⇐= Rule ® NP VP BODY h chase L ARG0 1 x L cat c ⇐= Rule  N ARG2 chase h 0 x 1 h 0 RSTR c ARG0 c ARG1 h D V 2 x of nodes, and E ⊆ V + is a finite set of hyperedges. A hyperedge is an extension of a normal edge which can connect to more than two nodes or only one node. l : E → L assigns a label from a finite set L to each hyperedge. Since nodes receive no informative labels, we use single-node edges with terminal labels to represent predicates. This strategy is widely used by HRG-based NLP systems, including Chiang et al. (2013), Peng et al. (2015) and Chen et al. (2018). X ∈ V ∗ defines an ordered list of nodes called external nodes, which specify the docking points during graph rewriting. t : V → T assigns a type from a finite set T to each node. Different from the hypergraphs used by Chiang et al. (2013) and Chen et al. (2018), we highlight the usage of node types which has a significant impact on making parsing results logically coherent. Three node types are utilized: h, x and c, which indicate handle, variable and predicate respectively. During node gluing, we must make sure that the types of nodes are identica"
2020.acl-main.605,E09-1001,0,0.223952,")) 2.2 Natural language utterances are often ambiguous, i.e., they have more than one reading. Take scope ambiguity, an important type of ambiguity that has been receiving heightened attention by semanticists, for example. Considering the following sentence: (2) a. Every dog happily chases some cat. b. some(y, cat(y), every(x, dog(x), chase dog happily Representing Underspecification happy(e2 , e1 ) ∧ chase(e1 , x, y))) cat c. every(x, dog(x), some(y, cat(y), every some happy(e2 , e1 ) ∧ chase(e1 , x, y))) Figure 2: Dependency-based syntactic analysis. Previous study (Oepen and Lønning, 2006; Copestake, 2009) shows that there are some good engineering reasons for producing a dependency style representation (see Fig. 1c) with links between predicates: It improves readability for consumers of the representation and eases integration with distributional semantics. Exploiting this direction further, we augment such a semantic deThe sentence is ambiguous: it can either mean that for every dog it is the case that it chases some— potentially different— cats; or else it can mean that there is a particular group of cats which are chased by every dog. The two readings are all made up of the same set of pred"
2020.acl-main.605,W11-2927,0,0.0303094,"wise Evaluation for Accuracy The semantic annotations in DeepBank are presented as variable-in-situ MRS style originally. It is a non-trivial problem to measure the similarity between different logical forms accordingly. Copestake (2009) provides a method to reversibly translate them into variable-reduced semantic graphs, namely dubbed Dependency MRS (DMRS), in an information-equivalent fashion, which is widely used by previous studies. We convert our outputs to DMRS, and re-use the evaluation metrics for variable-reduced graph representations, including Elementary Dependency Match (EDM; 6778 Dridan and Oepen, 2011) and SMATCH (Cai and Knight, 2013) to perform evaluation. TH AO Span Search-Based Evaluation for Coherence Another dimension for parser evaluation—the coherence of the output structures—is as essential as accuracy, since we also emphasize on the logical nature. Under the framework of underspecification, the coherence of a semantic structure entails that there must be at least one fully specified, i.e. scope-resolved logical form, which satisfies all the constraints encoded by that structure. The following shows a by-design incoherent semantic graph: N Y Y dog every RSTR BODY ? EDM A EDM SMATCH"
2020.acl-main.605,N16-1024,0,0.0686474,"Missing"
2020.acl-main.605,W17-6810,0,0.036992,"Missing"
2020.acl-main.605,P18-1170,0,0.0388886,"Missing"
2020.acl-main.605,P18-1249,0,0.0334001,"Missing"
2020.acl-main.605,W15-0127,0,0.0490124,"Missing"
2020.acl-main.605,P05-3003,0,0.0402253,"ments. There are some other natural language constructions that also involve scope ambiguity, e.g. negation and modality. Underspecification is by now the standard technique to deal with semantic ambiguities in many modern semantic theories, e.g. Underspecified Discourse Representation Theory (Kamp et al., 2011) and Hole Semantics (Bos, 1996). The basic idea behind it is to derive a single compact representation that describes the set of readings for a sentence that exhibits a scope ambiguity. The individual readings can be enumerated from such an underspecified description if it is required (Koller and Thater, 2005), but it is also possible to process underspecified representations directly without enumerating the readings (Koller and Thater, 2010). In this paper, we make our logico-semantic graph representations expressive to exhibit the complexities of human language semantics to some extent, by adopting a specific formalism for underspecification, i.e. Minimal Recursion Semantics (MRS; Copestake et al., 2005), a widelyused computational semantic framework in NLP. In addition to variables to represent individuals or events, an MRS structure use another kind of element, called handle, to represent out-o"
2020.acl-main.605,P10-1004,0,0.0310323,"ation is by now the standard technique to deal with semantic ambiguities in many modern semantic theories, e.g. Underspecified Discourse Representation Theory (Kamp et al., 2011) and Hole Semantics (Bos, 1996). The basic idea behind it is to derive a single compact representation that describes the set of readings for a sentence that exhibits a scope ambiguity. The individual readings can be enumerated from such an underspecified description if it is required (Koller and Thater, 2005), but it is also possible to process underspecified representations directly without enumerating the readings (Koller and Thater, 2010). In this paper, we make our logico-semantic graph representations expressive to exhibit the complexities of human language semantics to some extent, by adopting a specific formalism for underspecification, i.e. Minimal Recursion Semantics (MRS; Copestake et al., 2005), a widelyused computational semantic framework in NLP. In addition to variables to represent individuals or events, an MRS structure use another kind of element, called handle, to represent out-of-scope relationships between predicates. Each node is assigned with a label handle, and some arguments of a concept are specified as h"
2020.acl-main.605,P19-1450,0,0.0133613,"d Work It has been a long time since researchers manipulated semantic construction following the principle of compositionality. Different formalisms have been developed to express the syntactic-semantic interface in natural language utterances. To manipulate compositional construction, HRG is a popular framework to define a graph-structured syntax-semantics interface (Peng et al., 2015; Chen et al., 2018). AM algebra (Koller, 2015; Groschwitz et al., 2017) is another formalism to handle graph construction which has been successfully explored to build semantic parsers (Groschwitz et al., 2018; Lindemann et al., 2019). Compositional vector representation is also widely studied in recent years. Kiperwasser and Goldberg (2016) encodes syntactic dependency trees with a recursive recurrent neural network, which acts as the core of a bottom-up dependency parser. Dyer et al. (2016) introduced Recurrent Neural Network Grammar, a probabilistic model of sentences with explicit phrase structure. A recursive syntactic composition function is used to compute an embedding of a completed phrasestructure subtree. Modeling discrete structures with principled neural networks has received an increasing interest. Kipf and We"
2020.acl-main.605,J93-2004,0,0.0696557,"Missing"
2020.acl-main.605,P03-1047,0,0.167488,"coding naming convention and tagged by “L.” h1 h7 L h4 BODY h9 L every some RSTR ARG0 h2 x L ARG0 dog BODY chase ARG1 ARG0 ARG2 RSTR y h5 ARG0 L e1 ARG0 h3 ARG1 L happy ARG0 L cat e2 Figure 3: Underspecified logico-semantic graph. Handles associated to predicates are labeled with “L,” while handles play as arguments are labeled with semantic roles, like “RSTR.” 2.3 Structural Validity Considering any type of logical form-equivalent representation, we need to be careful that our structures are well-formed. MRS provides a principled way to enumerate readings from an underspecified logical form (Niehren and Thater, 2003), showing us a way to validate the output logic structure. We thus define a valid semantic structure as an MRS in which a scope-resolved logical form is allowable. To be more precise, a variablein-situ logico-semantic graph is valid if and only if there exists at least one fully specified logical form that satisfies all the constraints encoded by the graph. 3 Neural Graph Rewriting Automatically constructing a semantic representation can be achieved by exploring the compositionality principle: The meaning of a complex expression is a function of the meanings of its parts and of the syntactic r"
2020.acl-main.605,S15-2153,0,0.0921317,"Missing"
2020.acl-main.605,S14-2008,0,0.0154816,": wm,k = (um,k )> W si,j X tm = (um,k · wm,k ) 0≤k&lt;n We use the similarity between tm and si,j as the score of this graph fragment. For training, we use the cross-entropy function as loss. SCORE(Hm , i, j) 5 5.1 = (tm )> W2 si,j Experiments Data Setup DeepBank (Flickinger et al., 2012) is a deep linguistic resource that covers the Wall Street Journal section of Penn TreeBank (PTB; Marcus et al., 1993). All annotations are governed by English Resource Grammar (ERG; Flickinger, 2000). We use the DeepBank v1.1 data, and split it into training, development and test sets along with previous work (Oepen et al., 2014, 2015; Buys and Blunsom, 2017; Chen et al., 2018) to make sure that the numeric performance can be directly compared to the results in the literature. 5.2 Evaluation Metrics Token-wise Evaluation for Accuracy The semantic annotations in DeepBank are presented as variable-in-situ MRS style originally. It is a non-trivial problem to measure the similarity between different logical forms accordingly. Copestake (2009) provides a method to reversibly translate them into variable-reduced semantic graphs, namely dubbed Dependency MRS (DMRS), in an information-equivalent fashion, which is widely used"
2020.acl-main.605,K15-1004,0,0.021115,"L c some h 0 x ARG0 c ARG1 c x 0 x 1 VP chase L x 0 ∅ xx 2 N h x ⇐= Rule ® NP VP BODY h chase L ARG0 1 x L cat c ⇐= Rule  N ARG2 chase h 0 x 1 h 0 RSTR c ARG0 c ARG1 h D V 2 x of nodes, and E ⊆ V + is a finite set of hyperedges. A hyperedge is an extension of a normal edge which can connect to more than two nodes or only one node. l : E → L assigns a label from a finite set L to each hyperedge. Since nodes receive no informative labels, we use single-node edges with terminal labels to represent predicates. This strategy is widely used by HRG-based NLP systems, including Chiang et al. (2013), Peng et al. (2015) and Chen et al. (2018). X ∈ V ∗ defines an ordered list of nodes called external nodes, which specify the docking points during graph rewriting. t : V → T assigns a type from a finite set T to each node. Different from the hypergraphs used by Chiang et al. (2013) and Chen et al. (2018), we highlight the usage of node types which has a significant impact on making parsing results logically coherent. Three node types are utilized: h, x and c, which indicate handle, variable and predicate respectively. During node gluing, we must make sure that the types of nodes are identical. If the type of an"
2020.acl-main.605,N18-1202,0,0.0795906,"Missing"
2020.acl-main.605,P18-1150,0,0.0220791,"r. Dyer et al. (2016) introduced Recurrent Neural Network Grammar, a probabilistic model of sentences with explicit phrase structure. A recursive syntactic composition function is used to compute an embedding of a completed phrasestructure subtree. Modeling discrete structures with principled neural networks has received an increasing interest. Kipf and Welling (2017) proposed Graph Convolution Network to classify nodes in graphs. DAG-structured LSTM is a natural extension to tree LSTM which treats nodes as basic states (Zhu et al., 2016). Graph-state LSTM can be used in both generation task (Song et al., 2018a) and relation extraction (Song et al., 2018b). 7 Conclusion Graph-structured meaning representations provide an effective way to encode rich semantic information of natural language sentences and have been extensively studied recently. We enriched the discussion by studying an alternative graph-based representation for underspecified logical forms. In particular, we introduced a novel neural graph rewriting system and developed a new state-ofthe-art semantic parser for variable-in-situ graphs. Acknowledgments This work is supported in part by the National Hi-Tech R&D Program of China (No. 20"
2020.acl-main.605,D18-1246,0,0.0208326,"r. Dyer et al. (2016) introduced Recurrent Neural Network Grammar, a probabilistic model of sentences with explicit phrase structure. A recursive syntactic composition function is used to compute an embedding of a completed phrasestructure subtree. Modeling discrete structures with principled neural networks has received an increasing interest. Kipf and Welling (2017) proposed Graph Convolution Network to classify nodes in graphs. DAG-structured LSTM is a natural extension to tree LSTM which treats nodes as basic states (Zhu et al., 2016). Graph-state LSTM can be used in both generation task (Song et al., 2018a) and relation extraction (Song et al., 2018b). 7 Conclusion Graph-structured meaning representations provide an effective way to encode rich semantic information of natural language sentences and have been extensively studied recently. We enriched the discussion by studying an alternative graph-based representation for underspecified logical forms. In particular, we introduced a novel neural graph rewriting system and developed a new state-ofthe-art semantic parser for variable-in-situ graphs. Acknowledgments This work is supported in part by the National Hi-Tech R&D Program of China (No. 20"
2020.acl-main.605,P15-1150,0,0.0844654,"Missing"
2020.acl-main.605,N16-1106,0,0.0233786,"nt neural network, which acts as the core of a bottom-up dependency parser. Dyer et al. (2016) introduced Recurrent Neural Network Grammar, a probabilistic model of sentences with explicit phrase structure. A recursive syntactic composition function is used to compute an embedding of a completed phrasestructure subtree. Modeling discrete structures with principled neural networks has received an increasing interest. Kipf and Welling (2017) proposed Graph Convolution Network to classify nodes in graphs. DAG-structured LSTM is a natural extension to tree LSTM which treats nodes as basic states (Zhu et al., 2016). Graph-state LSTM can be used in both generation task (Song et al., 2018a) and relation extraction (Song et al., 2018b). 7 Conclusion Graph-structured meaning representations provide an effective way to encode rich semantic information of natural language sentences and have been extensively studied recently. We enriched the discussion by studying an alternative graph-based representation for underspecified logical forms. In particular, we introduced a novel neural graph rewriting system and developed a new state-ofthe-art semantic parser for variable-in-situ graphs. Acknowledgments This work"
2020.acl-main.605,oepen-lonning-2006-discriminant,0,0.140253,"Missing"
2020.acl-main.606,P13-1023,0,0.0240186,"meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that expli"
2020.acl-main.606,W13-2322,0,0.0372891,"plorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation proc"
2020.acl-main.606,W15-0128,0,0.0592677,"king with ERG As there is no gold semantics-annotated corpus for learner English and building such a corpus from scratch is tedious and time-consuming, we exploit ERG to establish a large-scale sembanking with informative semantic representations. To be specific, for each input sentence S, we generate K-best semantic graphs G1 , G2 , ..., GK with an ERG-based processor, i.e. ACE2 . The created grammar-licensed analyses contain both a derivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees he"
2020.acl-main.606,P16-1070,0,0.550433,"y to annotate such large-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper,"
2020.acl-main.606,P13-2131,0,0.15408,"raphs, we can pick out graph Gp with the highest score SCORE(Gp , T ). Our goal is to ensure SCORE(Gg , T ) > SCORE(Gp , T ), which can be achieved with the help of the averaged structured perceptron learning algorithm. 3.3.4 Effectiveness of the Reranking Model To evaluate the capability of our proposed reranking model, we randomly extract 10,000 and 2,476 sentences from DeepBank (Flickinger et al., 2012) as the training and validation data respectively. The gold UD analyses are derived from the original PTB (Marcus et al., 1993) annotations. With regard to evaluation metrics, we use SMATCH (Cai and Knight, 2013) and Elementary Dependency Matching (EDM; Dridan and Oepen, 2011). Results are shown in Table 1. The first three rows EDM Node Edge All All Top-1 92.8 90.0 91.4 87.8 Rerank (50) Rerank (500) 94.7 95.1 93.4 93.9 94.1 94.5 92.0 92.7 Oracle (50) Oracle (500) 97.6 98.7 96.9 98.5 97.2 98.6 95.6 97.6 – – – 94-95 Inter-Annotator Agreement Table 1: Results of reranking. “Top-1” means the most preferable graph generated by the ACE parser. “Rerank (50)” and “Rerank (500)” means that K is set to 50 and 500 during reranking respectively. “Oracle” means directly selecting the best-performing graph for each"
2020.acl-main.606,K18-1054,1,0.929555,", 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement Grammar (SHRG; Chen et al., 2018b). The parser constructs DMRS graphs by explicitly modeling such derivations. It utilizes a constituent parser to build a syntactic derivation, and then selects semantic HRG rules associated to syntactic CFG rules to generate a graph. When multiple rules are applicable for a single phrase, a neural network is used to rank them. We use the parser in Chen et al. (2019) based on both the lexicalized grammar and the constructional grammar (refer to Chen et al. (2018b) for the distinction). Henceforth, they are called lexicalized and constructional compositionbased parsers respectively. Figure 3 s"
2020.acl-main.606,P18-1038,1,0.92732,", 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement Grammar (SHRG; Chen et al., 2018b). The parser constructs DMRS graphs by explicitly modeling such derivations. It utilizes a constituent parser to build a syntactic derivation, and then selects semantic HRG rules associated to syntactic CFG rules to generate a graph. When multiple rules are applicable for a single phrase, a neural network is used to rank them. We use the parser in Chen et al. (2019) based on both the lexicalized grammar and the constructional grammar (refer to Chen et al. (2018b) for the distinction). Henceforth, they are called lexicalized and constructional compositionbased parsers respectively. Figure 3 s"
2020.acl-main.606,P81-1022,0,0.679333,"Missing"
2020.acl-main.606,P18-2077,0,0.0151891,"ed nodes) in a sub-graph are marked as communication channels to other meaning parts. According to the construction rule (shown in double-framed box), we glue the two sub-parts via the filled nodes, forming a larger graph with the syntactic label “NP”. More details are illustrated in Chen et al. (2019) possible candidates. The parser works with a twostage pipeline structure, for concept identification and relation detection, as illustrated in Figure 4. In the first phase, sequence labeling models are used to predict nodes, and in the second phase, we utilize the dependency model introduced by Dozat and Manning (2018) to link nodes. The two models in both stages use a multi-layer BiLSTM to encode tokens. In the first stage, another softmax layer is utilized to predict concept-related labels, while in the second stage, the dependency model is utilized to calculate a score for selecting token pairs. 5 5.1 Parsing to Literal Meanings Robustness of Parsing Models We experiment with three different parsers introduced in last section, i.e., lexicalized and constructional composition-based parsers and the factorization-based parser. We train these parsers on DeepBank version 1.1, corresponding to ERG 1214, and us"
2020.acl-main.606,W11-2927,0,0.0317885,", T ). Our goal is to ensure SCORE(Gg , T ) > SCORE(Gp , T ), which can be achieved with the help of the averaged structured perceptron learning algorithm. 3.3.4 Effectiveness of the Reranking Model To evaluate the capability of our proposed reranking model, we randomly extract 10,000 and 2,476 sentences from DeepBank (Flickinger et al., 2012) as the training and validation data respectively. The gold UD analyses are derived from the original PTB (Marcus et al., 1993) annotations. With regard to evaluation metrics, we use SMATCH (Cai and Knight, 2013) and Elementary Dependency Matching (EDM; Dridan and Oepen, 2011). Results are shown in Table 1. The first three rows EDM Node Edge All All Top-1 92.8 90.0 91.4 87.8 Rerank (50) Rerank (500) 94.7 95.1 93.4 93.9 94.1 94.5 92.0 92.7 Oracle (50) Oracle (500) 97.6 98.7 96.9 98.5 97.2 98.6 95.6 97.6 – – – 94-95 Inter-Annotator Agreement Table 1: Results of reranking. “Top-1” means the most preferable graph generated by the ACE parser. “Rerank (50)” and “Rerank (500)” means that K is set to 50 and 500 during reranking respectively. “Oracle” means directly selecting the best-performing graph for each sentence from the K-best list. The inter-annotator agreement of"
2020.acl-main.606,D19-1222,0,0.0165987,"identify concepts and also to determine dependency relations together with the resulted conceptual embeddings (in yellow). Comparing different models, we can see that the factorization-based approach performs better on all setups, which is consistent with previous studies (Koller et al., 2019). The gap between results on DeepBank and the other two datasets demonstrates the existence of cross-domain effect, which has been observed in plenty of NLP tasks, including but not limited to semantic parsing (Chen et al., 2018a; Lindemann et al., 2019; Blitzer and Pereira, 2007; Ben-David et al., 2010; Elsahar and Gallé, 2019). Furthermore, it is clear that there is a drop from L1 to L2 data. The gap is marked in the last row, the average of which is about 4 points, indicating the insufficiency of using standard models to parse learner texts. Still, the factorization-based model yields a little bit more robust results on non-native data. We hold that the poor performance of compositionbased model is caused by the explicit syntacticosemantic derivation process. Since the interface between syntax and semantics of learner languages is somewhat unclear, directly applying rewriting rules extracted from L1 data may be pa"
2020.acl-main.606,flickinger-etal-2014-towards,0,0.0602175,"Missing"
2020.acl-main.606,N16-4001,0,0.028172,"parsing on learner English. 3.3.1 2014). ERS helps to reveal much deeper semantic analysis than other shallow target structures such as the predicate-argument relations in the semantic role labeling (SRL) task. Moreover, it can be derived into several different forms, like the logicalform-based representation Minimal Recursion Semantics (MRS) and the graph-shaped structure Elementary Dependency Structures (EDS). We resort to this resource to build an informative analysis for learner English and choose EDS as the target structure. Target Meaning Representation English Resource Semantics (ERS; Flickinger et al., 2016) is an important resource of semantic representations produced by the English Resource Grammar (ERG; Flickinger, 1999), a broad-coverage, linguistically motivated precision Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) of English (Flickinger, 2000, 2011). It provides rich semantic representations including the semantic roles and other detailed information such as the scope of quantifiers and scopal operators including negation, as well as semantic representations of linguistically complex phenomena such as time and date expressions, conditionals, and comparatives (Flicking"
2020.acl-main.606,P17-1044,0,0.0396301,"ivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after reranking, provin"
2020.acl-main.606,P13-1088,0,0.0163775,"analyses contain both a derivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after"
2020.acl-main.606,P19-4002,1,0.830458,"tences are parallel, we can compare the graph structures directly. We use SMATCH (Cai and Knight, 2013) as the evaluation metric which provides the token-wise evaluation along with effective explorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorizatio"
2020.acl-main.606,P98-1116,0,0.46553,"tion along with effective explorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the re"
2020.acl-main.606,D18-1414,1,0.674505,"ge-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper, we present, to the b"
2020.acl-main.606,P19-1450,0,0.0684563,"y divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement G"
2020.acl-main.606,J93-2004,0,0.0732856,"structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper, we present, to the best of our knowledge, the first study on Semantic Parsing for English as a Second Language. Motivated by the Interface Hypothesis (Sorace, 2011) in SLA, we emphasize on the divergence between literal and inten"
2020.acl-main.606,P16-1173,0,0.52004,"e need an automatic machinery to annotate such large-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on"
2020.acl-main.606,P11-1121,0,0.0754947,"Missing"
2020.acl-main.606,W14-1701,0,0.0176324,"icit syntacticosemantic derivation process. Since the interface between syntax and semantics of learner languages is somewhat unclear, directly applying rewriting rules extracted from L1 data may be partly misleading. 5.2 Relatedness to Grammatical Errors It is crucial to understand whether and to what extent parsers are indeed robust to learner errors. We re-analyse the results from two aspects. First, we modify the original SMATCH evaluation metric and enable it to be sensitive to distances from errors. Then we make a distinction among typical error types proposed in CoNLL-2014 Shared Task (Ng et al., 2014). Results show that standard parsers can not handle learner errors well enough and their behaviors vary among different 6788 Node LEX Edge All Node CXG Edge All Node FAC Edge All DeepBank 94.05 92.96 93.50 95.83 92.87 94.34 96.85 95.19 96.01 L1 L2 ∆ 88.41 84.38 4.03 86.44 82.23 4.21 87.41 83.29 4.12 90.32 86.47 3.85 86.04 81.70 4.34 88.14 84.04 4.10 92.28 88.68 3.60 89.12 84.45 4.67 90.91 86.91 4.00 Data Table 3: SMATCH scores of semantic parsing on different test data. Henceforth, LEX, CXG and FAC refer to lexicalized and constructional composition-based parsers and the factorization-based pa"
2020.acl-main.606,W17-4305,0,0.0126699,"rding the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after reranking, proving the power of the p"
2020.acl-main.606,C12-2094,0,0.0257966,"directly annotate the linguistic properties in learner sentences (Dickinson and Ragheb, 2009; DıazNegrillo et al., 2010; Rastelli, 2013). The lack of precisely annotated data has limited the systematic analysis of interlanguages. There are several attempts to set up annotation schemes for different linguistic layers of learner languages, such as POS tags and syntactic information (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014; Nagata and Sakaguchi, 2016; Berzak et al., 2016b). But it is challenging to elucidate the exact definition of “syntax” for learner languages. Ragheb and Dickinson (2012) defines multiple layers (morphological dependencies, distributional dependencies, and subcategorization) based on different evidence to capture non-canonical properties. Similarly, motivated by the Interface Hypothesis (Sorace, 2011), we employ a principled method to create parallel semantic representations for learner English by discriminating between the literal and intended meanings. With regard to the semantic analysis for learner languages, Lin et al. (2018) takes the first step in this direction. Based on a parallel semantic role labeling (SRL) corpus, they prove the importance of synta"
2020.acl-main.606,W10-1004,0,0.0433926,"eaker meaning or interpretation). The former puts an emphasis on the linguistic code features appearing in the sentence, while the latter is derived from the author’s intention. When we consider an interlanguage, the divergence between literal and intended meanings is much larger due to various cross-lingual influences. It is reasonable to consider both aspects to develop a principled method to process outputs from L2 learners. 3.1 SLA at the Syntax-Semantics Interface VP 2 Related Work PP VP Early work regarding the collection of learner corpora mainly concentrates on tagging alleged errors (Rozovskaya and Roth, 2010; Nagata et al., 2011). The past decade has seen a tendency to directly annotate the linguistic properties in learner sentences (Dickinson and Ragheb, 2009; DıazNegrillo et al., 2010; Rastelli, 2013). The lack of precisely annotated data has limited the systematic analysis of interlanguages. There are several attempts to set up annotation schemes for different linguistic layers of learner languages, such as POS tags and syntactic information (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014; Nagata and Sakaguchi, 2016; Berzak et al., 2016b). But it is challenging to eluc"
2020.acl-main.606,P19-1009,0,0.0217843,"Missing"
2020.acl-main.606,N19-1014,0,0.0549157,"n resort to Grammatical Error Correction (GEC), the task of correcting different kinds of errors in text. It has attracted a lot of attention and considerable effort has been made to promote the performance on specific benchmark data. We utilize two off-the-shelf GEC models. One is a multilayer convolutional encoder-decoder neural network proposed in Chollampatt and Ng (2018). We choose the basic model introduced in the paper. The other model copies the unchanged words from the source sentence to the target sentence using a pretrained copy-augmented architecture with a denoising auto-encoder (Zhao et al., 2019). It achieves the state-of-the-art performance without extra pseudo data. Performances of the two GEC models on CoNLL-2014 test set are shown in Table 5. We train the factorization-based model on DeepBank and examine the performance on L2 and L1 sentences as well as the revised sentences by two GEC models. The produced graphs are compared with I-silver which represents the intended meaning. We notice that during the computation of SMATCH, some disagreements of nodes result from the discrepancy of morphological variation or different collocations between the input and the standard sentence. Hen"
2020.emnlp-main.104,Q17-1010,0,0.146614,"Missing"
2020.emnlp-main.104,D15-1249,0,0.274362,"Missing"
2020.emnlp-main.104,P16-1160,0,0.02076,"Missing"
2020.emnlp-main.104,N15-1034,0,0.0431913,"Missing"
2020.emnlp-main.104,N19-4009,0,0.0265402,"states c˜ = c + cγ. (5) Single-head Attention The attention model looks at both word-based and code-based encoders simultaneously. A context vector from each source encoder ct and cγt is created instead of the just ct in the single-source attention model. Hidden states from the top decoder layer looks back at previous ˜ and the context vectors of the hidden states ht−1 encoders: ˜ ; ct ; cγt ]) h˜t = tanh(Wc [ht−1 (6) Multi-head Attention Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. We apply the Fairseq (Ott et al., 2019) implementation of Multilingual Translation in Transformer (Vaswani et al., 2017) treating text and codewords as two language inputs. The multilingual transformer trains on two encoders in turn iteratively. For example, in the first epoch it trains the textual encoder then trains the codeword encoder; in the second epoch, it trains again the textual then the codeword encoder, and so on. 4 4.1 LM For Neural Language Modeling, we treat the text sentence as one language and the coded sentence as another language and combine them with the cross-lingual Language model (XLM; Lample and Conneau, 2019"
2020.emnlp-main.104,W16-2209,0,0.0602335,"Missing"
2020.emnlp-main.104,N16-1004,0,0.0128725,"n each of the encoder layer j ∈ [1, 2, · · · , J]. J is the number of nodes at each decoder layer. Recall that each hidden state is a (3) As shown in Figure (3b), the combined last hidden state in each layer is fed into the baseline decoder. The black blocks contains only the operator of +, as shown in the gray ellipse. α is the encoder weight of the coded sentence, and here, α = 0.5. 3.3 Multi-Source Encoding. In the linear combination method, the weight α is shared among all states in one encoder. To allow different weights for each state, we implement variations of multi-source encoding by Zoph and Knight (2016) for the POS tagging model (Joshi, 2018) (see Figure 3b). The combined hidden state ˜ j in a layer j is a non-linear transformation of the h 1354 concatenation of word-based and code-based hidden states of the last position I in layer j multiplied by the weight Wc ˜ j = tanh(Wc [hj ; hγ j ]). h I I (4) Bi-LSTM In Bi-LSTM decoder, the cell state c of an encoder is a concatenation of the forward and backward cell states. The combined cell state c˜ is the sum of the word-based c and code-based cγ encoder’s cell states c˜ = c + cγ. (5) Single-head Attention The attention model looks at both word-b"
2021.bea-1.1,W10-3110,0,0.0219555,"beling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daelemans, 2012a), automatic question and answer (Councill et al., 2010), etc. In the NLP community, the morpheme or word that expresses negation is called the negation cue, which can be a single word or multiple words. A negation is related to an event, i.e. so-called negation event, which is identified by some keys words, like need in (1). If we want to know the complete information conveyed by a negation event such as what is not needed in (1), we need to identify a scope in the sentence, which is a set of words. To be more precise, the negation scope is the maximum part(s) of the sentence that are influenced or negated by negation cue. In (1), the negation sco"
2021.bea-1.1,P16-1047,0,0.0132265,"tomatic NSR can be categorized into word-based, syntax-based and semantics-based approaches. Similar to many word-based solutions (Tan et al., 2018; He et al., 2017) to Semantic Role Labeling (Carreras and M`arquez, 2004), NSR can be cast as a sequence labeling problem over word sequence. Each word is assigned a position label, e.g. BEGIN - SCOPE, which indicates if the word s in the scope of a particular cue. Representative sequence labeling models, e.g. linear-chain CRF, semi-Markov CRF and latent variable CRF, have been evaluated (Li and Lu, 2018). Neural models have also been explored in (Fancellu et al., 2016, 2017): for each token the corresponding lemma PoS and cue features are fed into BiLSTM to do binary classification to indicate whether the token is in the negation scopes. Syntactic parsing provides valuable structural information for semantic analysis. It is a usual case that a negation scope is correlated to a single constituent. Following this property, NSR can be treated as a discriminative ranking problem over constituents returned by a phrase-structure parser. Read et al. (2012) used some hand-written heuristic features derived from constituent syntactic trees to guide a statistical ra"
2021.bea-1.1,S12-1040,0,0.0504194,"Missing"
2021.bea-1.1,P16-1070,0,0.179162,"ntary to hands-on experiences in interpreting second language phenomena (Gass, 2013). It is insufficient to directly apply models designed for first languages to handle the second language data because a second language is a standalone linguistic system (Selinker, 1972) that distinguishes itself from both the source and target languages in linguistic features. We need second language-specific methodologies by taking its characteristics into account. This direction has been recently explored, including syntactic and semantic parsing for English as a Second Language (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daeleman"
2021.bea-1.1,P17-1044,0,0.0332722,"le of L2-CHI, L1-JPN (2a) as well as its correction (2b). (2) 2 2.1 Related Work Negation Scope Resolution There are three corpora annotated with negation cues and negation scopes for English (Morante and Daelemans, 2012b; Konstantinova et al.; Vincze et al., 2008) and one for Chinese (Zou et al., 2015). All of them focus on first languages only. Tab. 1 summarizes the sizes of existing corpora as well as our new creation. Previous approaches to automatic NSR can be categorized into word-based, syntax-based and semantics-based approaches. Similar to many word-based solutions (Tan et al., 2018; He et al., 2017) to Semantic Role Labeling (Carreras and M`arquez, 2004), NSR can be cast as a sequence labeling problem over word sequence. Each word is assigned a position label, e.g. BEGIN - SCOPE, which indicates if the word s in the scope of a particular cue. Representative sequence labeling models, e.g. linear-chain CRF, semi-Markov CRF and latent variable CRF, have been evaluated (Li and Lu, 2018). Neural models have also been explored in (Fancellu et al., 2016, 2017): for each token the corresponding lemma PoS and cue features are fed into BiLSTM to do binary classification to indicate whether the tok"
2021.bea-1.1,2020.lrec-1.704,0,0.0349374,"Missing"
2021.bea-1.1,A00-1031,0,0.413654,"Missing"
2021.bea-1.1,W04-2412,0,0.0337127,"Missing"
2021.bea-1.1,konstantinova-etal-2012-review,0,0.019689,"Missing"
2021.bea-1.1,S12-1042,0,0.0168246,"tion to indicate whether the token is in the negation scopes. Syntactic parsing provides valuable structural information for semantic analysis. It is a usual case that a negation scope is correlated to a single constituent. Following this property, NSR can be treated as a discriminative ranking problem over constituents returned by a phrase-structure parser. Read et al. (2012) used some hand-written heuristic features derived from constituent syntactic trees to guide a statistical ranker (Read et al., 2012). Dependency-based analysis can also provide effective syntactic analysis. For example, Lapponi et al. (2012) augmented a word-based sequence model with dependency tree based features (Lapponi et al., a. 换 [宗教 言 说 ， 没有 Change text speak , have-not religion 生活 与 日常 生活 差距]。 life and ordinary life difference. ‘In other words, there is no difference between religious life and ordinary life’ b. 换 言 说 ， [宗教 生活 与 Change text speak , religion life and 日常 生活 之间] 没有 ordinary life between have-not [距离]。 difference. ‘In other words, there is no difference between religious life and ordinary life’ With the availability of high-quality annotations, we study neural NSR models which have achieved significant advance"
2021.bea-1.1,N18-1202,0,0.00856824,"the state-of-the-art solution that leverages on neural word-based tagging to solve the problem. The overview of our neural model is shown in Fig. 1. Formally, our task is to predict a sequence y given a word-PoS-cue pair hw, p, ci as input. Each yi ∈ y is a binary label to show whether the word is inside the negation scope. A sentence may include more than one negation cues. To make sure the neural model is aware of which negation cue is in question, the value of ci ∈ c is set to 1 if the current cue is in question, and 0 otherwise. As Fig. 1 shows, we use BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018) or randomly initialized feedforward layers for word embedding of wi , randomly initialized feedforward layers for PoS embedding of pi , randomly initialized feedforward layers for cue embedding of ci . Then we use feed forward layers to compress word embedding vectors of BERT and ELMo to a lower dimension. The concatenation of feature embeddings is fed to an encoder (e.g., BiLSTM). The encoder’s outputs then go through feedforward lay的 美好 的 事情， Summer-holidays De wonderful De things, [有的 已经 想] 不 [起来]。 some already remember not QILAI. (6) 暑假 ‘I have not remembered some of the wonderful things"
2021.bea-1.1,S12-1041,0,0.014774,"and latent variable CRF, have been evaluated (Li and Lu, 2018). Neural models have also been explored in (Fancellu et al., 2016, 2017): for each token the corresponding lemma PoS and cue features are fed into BiLSTM to do binary classification to indicate whether the token is in the negation scopes. Syntactic parsing provides valuable structural information for semantic analysis. It is a usual case that a negation scope is correlated to a single constituent. Following this property, NSR can be treated as a discriminative ranking problem over constituents returned by a phrase-structure parser. Read et al. (2012) used some hand-written heuristic features derived from constituent syntactic trees to guide a statistical ranker (Read et al., 2012). Dependency-based analysis can also provide effective syntactic analysis. For example, Lapponi et al. (2012) augmented a word-based sequence model with dependency tree based features (Lapponi et al., a. 换 [宗教 言 说 ， 没有 Change text speak , have-not religion 生活 与 日常 生活 差距]。 life and ordinary life difference. ‘In other words, there is no difference between religious life and ordinary life’ b. 换 言 说 ， [宗教 生活 与 Change text speak , religion life and 日常 生活 之间] 没有 ordina"
2021.bea-1.1,P18-2085,0,0.0418039,"rpora as well as our new creation. Previous approaches to automatic NSR can be categorized into word-based, syntax-based and semantics-based approaches. Similar to many word-based solutions (Tan et al., 2018; He et al., 2017) to Semantic Role Labeling (Carreras and M`arquez, 2004), NSR can be cast as a sequence labeling problem over word sequence. Each word is assigned a position label, e.g. BEGIN - SCOPE, which indicates if the word s in the scope of a particular cue. Representative sequence labeling models, e.g. linear-chain CRF, semi-Markov CRF and latent variable CRF, have been evaluated (Li and Lu, 2018). Neural models have also been explored in (Fancellu et al., 2016, 2017): for each token the corresponding lemma PoS and cue features are fed into BiLSTM to do binary classification to indicate whether the token is in the negation scopes. Syntactic parsing provides valuable structural information for semantic analysis. It is a usual case that a negation scope is correlated to a single constituent. Following this property, NSR can be treated as a discriminative ranking problem over constituents returned by a phrase-structure parser. Read et al. (2012) used some hand-written heuristic features d"
2021.bea-1.1,D18-1414,1,0.88544,"rst languages to handle the second language data because a second language is a standalone linguistic system (Selinker, 1972) that distinguishes itself from both the source and target languages in linguistic features. We need second language-specific methodologies by taking its characteristics into account. This direction has been recently explored, including syntactic and semantic parsing for English as a Second Language (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daelemans, 2012a), automatic question and answer (Councill et al., 2010), etc. In the NLP community, the morpheme or word that expresses negation is ca"
2021.bea-1.1,P14-5010,0,0.00251948,"notations and discussed incompatible phenomena to produce the initial annotation guideline. Then the two annotators proceeded to annotate an 800-sentence set independently based on this guideline. It is on this set that we calculate the inter-annotator agreement, as shown in Tab. 2. Based on the disagreement of the second annotation round, we made some minor adjustments to the guideline. Since the agreement is relatively satisfactory, the rest sentences only include single blind annotations. Noted that the annotation was based on the segmentation results produced by the Stanford CoreNLP tool (Manning et al., 2014), and during the annotation process wrong segmentation was corrected. 3.3 ple, “没有不必要的东西/no unnecessary things”, the scope of the second cue is “必要/necessary”. If the negated verb is the main verb of the sentence, the entire sentence is under the scope. In addition, the scope can be discontinuous, which means that if the subject or the object are elliptical but explicit in another conjunct clause, then we will mark it over clause, as shown in (3). If the sentence contains different clauses and only one of them contains a cue, then we will mark the most relevant part, as shown in (4). But when"
2021.bea-1.1,I11-1017,0,0.0448772,"erations. Nevertheless, the output structures provided by a semantic parser have been shown very powerful to assist the discovery of negation scopes (Packard et al., 2014; Basile et al.). 2.2 3.1 The New Corpus The Raw Data Lang-8 (https://lang-8.com/) is a languageexchange social networking platform, where second language learners of different languages can put their essays and receive modifications from native speakers. There are about 68,500 Chinese learners registering on this platform, most of whose mother tongues are English (ca. 15,500 users) and Japanese (ca. 25,700 users). Following (Mizumoto et al., 2011; Lin et al., 2018), we build a large-scale parallel Chinese learners’ corpus by extracting and cleaning the original sentences (L2) written by Mandarin learners and the corresponding revised version (L1) from native speakers. It consists of 717,241 sentence pairs from writers of 61 different native languages, of which English and Japanese constitute the majority. We carefully select 4,000 sentence pairs and manually annotate them with negative cues and scopes based on parallel linguistic analysis. We notice that learners’ native languages can have a great impact on grammatical errors and henc"
2021.bea-1.1,W08-0606,0,0.181755,"Missing"
2021.bea-1.1,morante-daelemans-2012-conandoyle,0,0.0369301,"Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daelemans, 2012a), automatic question and answer (Councill et al., 2010), etc. In the NLP community, the morpheme or word that expresses negation is called the negation cue, which can be a single word or multiple words. A negation is related to an event, i.e. so-called negation event, which is identified by some keys words, like need in (1). If we want to know the complete information conveyed by a negation event such as what is not needed in (1), we need to identify a scope in the sentence, which is a set of words. To be more precise, the negation scope is the maximum part(s) of the sentence that are influe"
2021.bea-1.1,W10-3111,0,0.0282065,"lish as a Second Language (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daelemans, 2012a), automatic question and answer (Councill et al., 2010), etc. In the NLP community, the morpheme or word that expresses negation is called the negation cue, which can be a single word or multiple words. A negation is related to an event, i.e. so-called negation event, which is identified by some keys words, like need in (1). If we want to know the complete information conveyed by a negation event such as what is not needed in (1), we need to identify a scope in the sentence, which is a set of words. To be more precise, the negation"
2021.bea-1.1,2020.acl-main.606,1,0.901618,"riences in interpreting second language phenomena (Gass, 2013). It is insufficient to directly apply models designed for first languages to handle the second language data because a second language is a standalone linguistic system (Selinker, 1972) that distinguishes itself from both the source and target languages in linguistic features. We need second language-specific methodologies by taking its characteristics into account. This direction has been recently explored, including syntactic and semantic parsing for English as a Second Language (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension (Morante and Daelemans, 2012a), automatic"
2021.bea-1.1,P16-1173,0,0.0313774,"@cam.ac.uk Abstract complementary to hands-on experiences in interpreting second language phenomena (Gass, 2013). It is insufficient to directly apply models designed for first languages to handle the second language data because a second language is a standalone linguistic system (Selinker, 1972) that distinguishes itself from both the source and target languages in linguistic features. We need second language-specific methodologies by taking its characteristics into account. This direction has been recently explored, including syntactic and semantic parsing for English as a Second Language (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Zhao et al., 2020) and semantic role labeling for Chinese as a Second Language (hereafter L2-Chinese or L2-CHI for short) (Lin et al., 2018). Negation is a ubiquitous phenomenon in spoken text (27.6 per 1000 words) and in written text (12.8 per 1000 words) (Tottie, 1991). As a type of extra-propositional semantics, negation is relevant to analyze factuality of propositions and thus relevant to accurately understand natural languages which has been proven useful for several NLP applications such as sentiment analysis (Wiegand et al., 2010), machine reading comprehension"
2021.bea-1.1,P15-1064,0,0.366691,"ckground native languages of language learners. We use L1English (L1-ENG for short) and L1-Japanese (L1JPN for short) to denote them respectively2 . We also consider the corrected sentences by Chinese native speakers and use “CHIL2⇒L1 3 , L1-ENG” and “CHIL2⇒L1 , L1-JPN” to denote them. The following are an example of L2-CHI, L1-JPN (2a) as well as its correction (2b). (2) 2 2.1 Related Work Negation Scope Resolution There are three corpora annotated with negation cues and negation scopes for English (Morante and Daelemans, 2012b; Konstantinova et al.; Vincze et al., 2008) and one for Chinese (Zou et al., 2015). All of them focus on first languages only. Tab. 1 summarizes the sizes of existing corpora as well as our new creation. Previous approaches to automatic NSR can be categorized into word-based, syntax-based and semantics-based approaches. Similar to many word-based solutions (Tan et al., 2018; He et al., 2017) to Semantic Role Labeling (Carreras and M`arquez, 2004), NSR can be cast as a sequence labeling problem over word sequence. Each word is assigned a position label, e.g. BEGIN - SCOPE, which indicates if the word s in the scope of a particular cue. Representative sequence labeling models"
2021.bea-1.1,P14-1007,0,0.0145277,"ntence” shows the total numbers of sentences; ”#Negation” shows the total numbers of negation expressions (=cues). 3 2012) . Though the scope of negation is a type of important semantics, it is not included in almost all sentence-level SemBanks, including English Resource Semantics (Flickinger et al.), Groningen Meaning Bank (Bos et al., 2017) and Abstract Meaning Representations (Burns et al., 2016), due to either theoretical or practical considerations. Nevertheless, the output structures provided by a semantic parser have been shown very powerful to assist the discovery of negation scopes (Packard et al., 2014; Basile et al.). 2.2 3.1 The New Corpus The Raw Data Lang-8 (https://lang-8.com/) is a languageexchange social networking platform, where second language learners of different languages can put their essays and receive modifications from native speakers. There are about 68,500 Chinese learners registering on this platform, most of whose mother tongues are English (ca. 15,500 users) and Japanese (ca. 25,700 users). Following (Mizumoto et al., 2011; Lin et al., 2018), we build a large-scale parallel Chinese learners’ corpus by extracting and cleaning the original sentences (L2) written by Manda"
2021.naacl-main.440,E17-2039,0,0.0387144,"Missing"
2021.naacl-main.440,W17-6901,0,0.0576181,"projects, e.g. DeepBank (Flickinger et al., 2012), PropBank (Palmer et al., 2005) and OntoNotes (Weischedel et al., 2013). We then obtain Chinese counterparts of original English sentences by employing English–Chinese bilinguals to do literal translation. In addition, we also select 1000 sentences from Chinese TreeBank (CTB; Xue et al., 2005), where manual syntactic analyses are available. 3.2 Annotation One doctoral student and one undergraduate student, majoring in linguistics, annotate the pair sentences. The guideline for English annotation is derived from the universal semantic tag set (Abzianidze and Bos, 2017) with reference to data in PMB and Chinese is annotated based on the modified tag set in the appendix. The annotation process consists of three steps: firstly, annotators independently annotate 100 Chinese WSJ sentences, and later compare and discuss disagreements between the annotations. The conflicting cases are then analyzed to modify the specification. After some iterations, the consistency between annotators is significantly improved. Additionally, we find partof-speech (POS) tags are quite useful to accelerate manual annotation. Therefore, we apply the Stanford CoreNLP tool (Manning et a"
2021.naacl-main.440,C18-1139,0,0.0116183,"fi and bi to a multilayer perceptron (MLP) to calculate the scores vector si over semantic tags. si = MLP(fi ⊕ bi ) Finally, we feed si into a softmax layer to choose a tag with highest probability for each word inde4 Tagging Models pendently, or a CRF layer which can select the tag sequence with highest probability for the whole Long Short-Term Memory (Hochreiter and sentence. Schmidhuber, 1997, LSTM) models have been 5558 Subword/Character-level Models In order to solve the out-of-vocabulary (OOV) issues in sequence tagging tasks, many subword-level and character-level models are proposed (Akbik et al., 2018; Ling et al., 2015; Bohnet et al., 2018). We do not use these models for experiments, instead we leverage pretrained language models to handle OOV issues, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). These pretrained language models are trained on large corpus and use a subword/character-level vocabulary, which provide better contextual word representations. 128 for each direction and the number of layers is set to 1; 2) the embeddings of POS tags are randomly initialized and has a dimension of 32 while the embeddings of words have a dimension of 300 and are initialized"
2021.naacl-main.440,W17-2406,0,0.0206619,"Existing approaches can be roughly divided into three categories: the crosslingual1 approach focuses on lending semantic annotation of a resource-rich language, such as English, to an under-resourced language (Wang et al., 2019; Blloshmi et al., 2020; Mohiuddin and Joty, 2020); the interlingual approach attempts to provide a unified semantic framework for all languages (Abend and Rappoport, 2013; White et al., 2016; Ranta et al., 2020); the multilingual approach aims at developing comparable but not necessarily identical annotation schemes shared by different languages (Bond and Foster, 2013; Baker and Ellsworth, 2017; Pires et al., 2019). In line with the interlingual approach, Universal Semantic Tagging (UST; Bjerva et al., 2016) develops a set of language-neutral tags (hereafter referred to as sem-tag) to annotate individual words, providing shallow yet effective semantic information. Semantic analyses of different languages utilise a same core tag set, but may also employ a few language-specific tags. Figure 1 presents an example. English I/PRO had/PST repaired/EXT my/HAS watch/CON ./NIL German Ich/PRO hatte/PST meine/HAS Armbanduhr/CON repariert/EXT ./NIL Italian Ho/NOW riparito/EXT il/DEF mio/HAS oro"
2021.naacl-main.440,P98-1013,0,0.479764,"Missing"
2021.naacl-main.440,C16-1333,0,0.165204,"annotation of a resource-rich language, such as English, to an under-resourced language (Wang et al., 2019; Blloshmi et al., 2020; Mohiuddin and Joty, 2020); the interlingual approach attempts to provide a unified semantic framework for all languages (Abend and Rappoport, 2013; White et al., 2016; Ranta et al., 2020); the multilingual approach aims at developing comparable but not necessarily identical annotation schemes shared by different languages (Bond and Foster, 2013; Baker and Ellsworth, 2017; Pires et al., 2019). In line with the interlingual approach, Universal Semantic Tagging (UST; Bjerva et al., 2016) develops a set of language-neutral tags (hereafter referred to as sem-tag) to annotate individual words, providing shallow yet effective semantic information. Semantic analyses of different languages utilise a same core tag set, but may also employ a few language-specific tags. Figure 1 presents an example. English I/PRO had/PST repaired/EXT my/HAS watch/CON ./NIL German Ich/PRO hatte/PST meine/HAS Armbanduhr/CON repariert/EXT ./NIL Italian Ho/NOW riparito/EXT il/DEF mio/HAS orologio/CON ./NIL Chinese 我/PRO 把/OBJ 我/PRO 的/MOD 手 表/CON 修/EXT 好/EXT 了/PFT 。/NIL Figure 1: An example of parallel sen"
2021.naacl-main.440,2020.emnlp-main.195,0,0.0553444,"Missing"
2021.naacl-main.440,P18-1246,0,0.0611957,"Missing"
2021.naacl-main.440,P13-1133,0,0.0460594,"Missing"
2021.naacl-main.440,A00-1031,0,0.224416,"our view, these strategies are employed by researchers to study the major challenge i.e., the divergence of languages, encountered in representing multilingual data. And UST, which is in line with interlingual method, attempts to address it by a relatively shallow scheme. Despite the high inter-annotator agreements and tagging accuracies, there are still some divergences, which requires more in-depth study of multilingual annotation. 7 Related Work modification of deriving the tagset in a data-driven manner to disambiguate categories. In this work, a tri-gram based tagging model, TnT tagger (Brants, 2000), is also initially explored for bootstrapping utilization. In a recent study built on Bjerva et al. (2016), employing sem-tag in multi-task learning is found to be beneficial to both sem-tag task and other NLP tasks including Universal Dependency POS tagging, Universal Dependency parsing, and Natural Language Inference (Abdou et al., 2018). Overall, these studies indicate that sem-tags are effective in conducting various NLP tasks. 8 Conclusion In this paper, we take Chinese into account to provide a more comprehensive tag set based on which we establish a reliable manually-annotated corpus,"
2021.naacl-main.440,C18-1251,0,0.0286842,"Missing"
2021.naacl-main.440,2020.cl-2.1,0,0.0378534,"Missing"
2021.naacl-main.440,N18-1104,0,0.0225896,"rds contribute to annolikely to make annotators choose sem-tags related tations needs further research. to POS features. For instance, unable may be 5561 6.2 Challenges of Multilingual Annotations Building comparative semantic representations across languages has been an important topic in recent years as a strategy to both contribute to semantic parsing and syntactic analysis. Existing approaches towards it can be roughly divided into three categories. First, crosslingual approach is proposed, which lends semantic annotation of a resource-rich language to an underresourced language; see e.g. Damonte and Cohen (2018). However, crosslingual divergence between the lender and the borrower is likely to be retained to a considerable extent, especially for the languages which are phylogenetically distant. Another widely-discussed multilingual approach aims to achieve the goal by developing a comparable scheme of annotations for different languages, such as multilingual FrameNet (Baker et al., 1998) and multilingual WordNet (Miller, 1995), whose main limitation is that the semantic information represented is at the risk of oversimplifying since many in-depth properties are language-specific. The third one, the i"
2021.naacl-main.440,N19-1423,0,0.0131243,"robability for each word inde4 Tagging Models pendently, or a CRF layer which can select the tag sequence with highest probability for the whole Long Short-Term Memory (Hochreiter and sentence. Schmidhuber, 1997, LSTM) models have been 5558 Subword/Character-level Models In order to solve the out-of-vocabulary (OOV) issues in sequence tagging tasks, many subword-level and character-level models are proposed (Akbik et al., 2018; Ling et al., 2015; Bohnet et al., 2018). We do not use these models for experiments, instead we leverage pretrained language models to handle OOV issues, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). These pretrained language models are trained on large corpus and use a subword/character-level vocabulary, which provide better contextual word representations. 128 for each direction and the number of layers is set to 1; 2) the embeddings of POS tags are randomly initialized and has a dimension of 32 while the embeddings of words have a dimension of 300 and are initialized by the GloVe vectors3 (Pennington et al., 2014) and pre-trained word vectors4 (Li et al., 2018) for English and Chinese respectively5 ; 3) the parameters of BERT/ELMo are fixed during the tr"
2021.naacl-main.440,P14-5010,0,0.0136098,"a significant influence on semantic sorts. In this regard, annotations are undoubtedly impacted by POS tags. Nonetheless, some researchers rebate it, believing that the notional definitions of POS are not applicable because of its unclearness. According to them, distribution, morphological features, grammatical functions are all useful criteria for the identification of POS. In our view, contradiction between notion-based and distribution-based approach leads to some difficulties in annotation. To avoid this, we applied POS tags which are automatically-generated by the Stanford CoreNLP tool (Manning et al., 2014b) to assist manual annotation. However, though POS tags actually improve the inter-annotator agreement by regulating manual annotations of sem-tags in two ways, it is not clear whether they improve the quality of annotations— After a detailed investigation, we summarize the the first one increases the possibility of one opinfluences of POS tags on inter-annotator agreetion while the second one directly makes choices ments as two points: (i) Some tokens have multifor annotators. To what extent more coarsedimensional semantic features and POS tags are grained annotating standards contribute to"
2021.naacl-main.440,2020.lrec-1.382,0,0.0654179,"Missing"
2021.naacl-main.440,J93-2004,0,0.0745031,"最 ORD ordinal: 第一、首次 Table 4: COM tags of English and Chinese. 3 The Corpus We introduce a new moderate-sized corpus containing high-quality manual annotations for EnClassifier Classifier is a Chinese-specific word class which is inserted between numerals and glish and Chinese, which is now available at nouns to denote quantity. This category does not https://github.com/pkucoli/UST. 5556 3.1 Data Source To support fine-grained cross-lingual comparisons, the corpus includes 1100 parallel sentence pairs. We select 1100 sentences from the Wall Street Journal (WSJ) section of Penn TreeBank (PTB; Marcus et al., 1993). We choose it because it contains detailed semantic annotations and the sentences are relatively long, thus potentially carrying more complex information. It is noteworthy that various syntactic and semantic analyses of these English sentences have been built by multiple projects, e.g. DeepBank (Flickinger et al., 2012), PropBank (Palmer et al., 2005) and OntoNotes (Weischedel et al., 2013). We then obtain Chinese counterparts of original English sentences by employing English–Chinese bilinguals to do literal translation. In addition, we also select 1000 sentences from Chinese TreeBank (CTB;"
2021.naacl-main.440,P18-2023,0,0.0173834,"iments, instead we leverage pretrained language models to handle OOV issues, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). These pretrained language models are trained on large corpus and use a subword/character-level vocabulary, which provide better contextual word representations. 128 for each direction and the number of layers is set to 1; 2) the embeddings of POS tags are randomly initialized and has a dimension of 32 while the embeddings of words have a dimension of 300 and are initialized by the GloVe vectors3 (Pennington et al., 2014) and pre-trained word vectors4 (Li et al., 2018) for English and Chinese respectively5 ; 3) the parameters of BERT/ELMo are fixed during the training of our sequence tagging models; 4) for models with MTL, we directly optimize the sum of the losses for both POS tagging and universal semantic tagging. POS features POS categories can provide lowlevel syntax infomation which is beneficial for sem-tagging. In our experiments, we try to use POS tags as additional inputs for our baseline systems. 5.2 Multi-task Learning (MTL) Multi-task learning (MTL) is a widely discussed technique in the literature. Previous work (Changpinyo et al., 2018) shows"
2021.naacl-main.440,P16-1101,0,0.0395273,"Missing"
2021.naacl-main.440,2020.cl-2.2,0,0.0313559,"Missing"
2021.naacl-main.440,J05-1004,0,0.422088,"https://github.com/pkucoli/UST. 5556 3.1 Data Source To support fine-grained cross-lingual comparisons, the corpus includes 1100 parallel sentence pairs. We select 1100 sentences from the Wall Street Journal (WSJ) section of Penn TreeBank (PTB; Marcus et al., 1993). We choose it because it contains detailed semantic annotations and the sentences are relatively long, thus potentially carrying more complex information. It is noteworthy that various syntactic and semantic analyses of these English sentences have been built by multiple projects, e.g. DeepBank (Flickinger et al., 2012), PropBank (Palmer et al., 2005) and OntoNotes (Weischedel et al., 2013). We then obtain Chinese counterparts of original English sentences by employing English–Chinese bilinguals to do literal translation. In addition, we also select 1000 sentences from Chinese TreeBank (CTB; Xue et al., 2005), where manual syntactic analyses are available. 3.2 Annotation One doctoral student and one undergraduate student, majoring in linguistics, annotate the pair sentences. The guideline for English annotation is derived from the universal semantic tag set (Abzianidze and Bos, 2017) with reference to data in PMB and Chinese is annotated b"
2021.naacl-main.440,D14-1162,0,0.0850722,"hnet et al., 2018). We do not use these models for experiments, instead we leverage pretrained language models to handle OOV issues, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). These pretrained language models are trained on large corpus and use a subword/character-level vocabulary, which provide better contextual word representations. 128 for each direction and the number of layers is set to 1; 2) the embeddings of POS tags are randomly initialized and has a dimension of 32 while the embeddings of words have a dimension of 300 and are initialized by the GloVe vectors3 (Pennington et al., 2014) and pre-trained word vectors4 (Li et al., 2018) for English and Chinese respectively5 ; 3) the parameters of BERT/ELMo are fixed during the training of our sequence tagging models; 4) for models with MTL, we directly optimize the sum of the losses for both POS tagging and universal semantic tagging. POS features POS categories can provide lowlevel syntax infomation which is beneficial for sem-tagging. In our experiments, we try to use POS tags as additional inputs for our baseline systems. 5.2 Multi-task Learning (MTL) Multi-task learning (MTL) is a widely discussed technique in the literatur"
2021.naacl-main.440,N18-1202,0,0.0254188,"Tagging Models pendently, or a CRF layer which can select the tag sequence with highest probability for the whole Long Short-Term Memory (Hochreiter and sentence. Schmidhuber, 1997, LSTM) models have been 5558 Subword/Character-level Models In order to solve the out-of-vocabulary (OOV) issues in sequence tagging tasks, many subword-level and character-level models are proposed (Akbik et al., 2018; Ling et al., 2015; Bohnet et al., 2018). We do not use these models for experiments, instead we leverage pretrained language models to handle OOV issues, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). These pretrained language models are trained on large corpus and use a subword/character-level vocabulary, which provide better contextual word representations. 128 for each direction and the number of layers is set to 1; 2) the embeddings of POS tags are randomly initialized and has a dimension of 32 while the embeddings of words have a dimension of 300 and are initialized by the GloVe vectors3 (Pennington et al., 2014) and pre-trained word vectors4 (Li et al., 2018) for English and Chinese respectively5 ; 3) the parameters of BERT/ELMo are fixed during the training of our sequence tagging"
2021.naacl-main.440,P19-1493,0,0.0211711,"roughly divided into three categories: the crosslingual1 approach focuses on lending semantic annotation of a resource-rich language, such as English, to an under-resourced language (Wang et al., 2019; Blloshmi et al., 2020; Mohiuddin and Joty, 2020); the interlingual approach attempts to provide a unified semantic framework for all languages (Abend and Rappoport, 2013; White et al., 2016; Ranta et al., 2020); the multilingual approach aims at developing comparable but not necessarily identical annotation schemes shared by different languages (Bond and Foster, 2013; Baker and Ellsworth, 2017; Pires et al., 2019). In line with the interlingual approach, Universal Semantic Tagging (UST; Bjerva et al., 2016) develops a set of language-neutral tags (hereafter referred to as sem-tag) to annotate individual words, providing shallow yet effective semantic information. Semantic analyses of different languages utilise a same core tag set, but may also employ a few language-specific tags. Figure 1 presents an example. English I/PRO had/PST repaired/EXT my/HAS watch/CON ./NIL German Ich/PRO hatte/PST meine/HAS Armbanduhr/CON repariert/EXT ./NIL Italian Ho/NOW riparito/EXT il/DEF mio/HAS orologio/CON ./NIL Chine"
2021.naacl-main.440,2020.cl-2.6,0,0.0177554,"ferent languages plays a fundamental and essential role in multilingual natural language processing, and is attracting more and more research interests (Costa-jussà et al., 2020). Existing approaches can be roughly divided into three categories: the crosslingual1 approach focuses on lending semantic annotation of a resource-rich language, such as English, to an under-resourced language (Wang et al., 2019; Blloshmi et al., 2020; Mohiuddin and Joty, 2020); the interlingual approach attempts to provide a unified semantic framework for all languages (Abend and Rappoport, 2013; White et al., 2016; Ranta et al., 2020); the multilingual approach aims at developing comparable but not necessarily identical annotation schemes shared by different languages (Bond and Foster, 2013; Baker and Ellsworth, 2017; Pires et al., 2019). In line with the interlingual approach, Universal Semantic Tagging (UST; Bjerva et al., 2016) develops a set of language-neutral tags (hereafter referred to as sem-tag) to annotate individual words, providing shallow yet effective semantic information. Semantic analyses of different languages utilise a same core tag set, but may also employ a few language-specific tags. Figure 1 presents"
2021.naacl-main.440,D19-1575,0,0.0234513,"Missing"
2021.naacl-main.440,D16-1177,0,0.0579472,"Missing"
C08-1105,W05-0620,0,0.0428654,"Missing"
C08-1105,J02-3001,0,0.25207,"may cause labeling errors such as constituents outside active region of arguments may be falsely recognized as roles. Introduction Semantic Role Labeling (SRL) has gained the interest of many researchers in the last few years. SRL consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. As a well defined task of shallow semantic parsing, SRL has a variety of applications in many kinds of NLP tasks. A variety of approaches has been proposed for the different characteristics of SRL. More recent approaches have involved calibrating features (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; ∗ This work was partial completed while this author was at Toshiba (China) R&D Center. ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper uses insights from generative linguistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates1 active region of arguments according to the projection principle of principle and parameters. Two methods, the anchor group approach and the single anch"
C08-1105,P07-1025,0,0.0332107,"Missing"
C08-1105,W05-0625,0,0.0288869,"Missing"
C08-1105,P04-1043,0,0.0708338,"Missing"
C08-1105,P05-1073,0,0.0397734,"Missing"
C08-1105,W04-3212,0,0.0123727,"D836 structure plus movement. NP-movement principle in principle and parameters indicates that noun phrases only move from A-positions (argument position) which have been assigned roles to A-positions which have not, leaving an NPtrace. On account of θ-theory and government, Apositions are nodes m-commanded 3 by predicates in D-structure. In NP-movement, arguments move to positions which are C-commanded 4 by target predicate and m-commanded by other predicates. Broadly speaking, A-positions are C-commanded by predicates after NP-movement. The key of the well-known pruning algorithm raised in (Xue and Palmer, 2004) is extracting sisters of ancestors as role candidates. Those candidate nodes are all Ccommanders of a predicate. NP-movement can give an explanation why the algorithm works. 4.1.2 Definition of Argument Anchor To capture the characteristics of A-positions, we make definition of A-anchor as following. For every predicate p in the syntax tree T , denote A the set of C-commanders of p: • a left-A-anchor satisfies: 1. left-A-anchor belongs to A; 2. left-A-anchor is a noun phrase (including NNS, NNP, etc.) or simple declarative clause (S); 3. left-A-anchor is on the left hand of p. • a right-A-anc"
C08-1105,D07-1062,0,\N,Missing
C10-2139,W06-1655,0,0.659996,"sets from the second SIGHAN bakeoff. 1 Introduction To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. There are two dominant models for Chinese word segmentation. The first one is what we call “word-based” approach, where the basic predicting units are words themselves. This kind of segmenters sequentially decides whether the local sequence of characters make up a word. This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). The second is “character-based” approach, where basic processing units are characters which compose words. Segmentation is This paper is concerned with the behavior of different segmentation models in general. We present a theoretical and empirical comparative analysis of the two dominant approaches. Theoretically, these approaches are different. The word-based models do prediction on a dynamic sequence of possible words, while characterbased models on a static character sequence. The former models have a stronger ability to represent word token features for disambiguation, while the latter"
C10-2139,C92-1019,0,0.0561507,"iple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff. 1 Introduction To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. There are two dominant models for Chinese word segmentation. The first one is what we call “word-based” approach, where the basic predicting units are words themselves. This kind of segmenters sequentially decides whether the local sequence of characters make up a word. This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). The second is “character-based” approach, where basic processing units are characters which compose words. Segmentation is This paper is concerned with the behavior of different segmentation models in general. We present a theoretical and empirical comparative analysis of the two dominant approaches. Theoretically, these approaches are different. The word-based models do prediction on a dynamic sequence of possible words, while characterbased models on a static character sequence. The former models have a"
C10-2139,I05-3017,0,0.129028,"01 10 00 00 1- -1 10 10 11 5 6- V O 3- 2 1 O - 10 01 word occurances in training data 10 00 1- -1 10 10 11 5 6- 3- 2 V Tab. 1 shows the performance of our two segmenters. Numbers of iterations are respectively set to 15 and 20 for our word-based segmenter and character-based segmenter. The word-based segmenter performs slightly worse than the characterbased segmenter. This is different from the experiments reported in (Zhang and Clark, 2007). We 60 O Baseline Performance character-based word-based 65 70 1 4.2 character-based word-based O We used the data provided by the second SIGHAN Bakeoff (Emerson, 2005) to test the two segmentation models. The data contains four corpora from different sources: Academia Sinica Corpus (AS), City University of Hong Kong (CU), Microsoft Research Asia (MSR), and Peking University (PKU). There is no fixed standard for Chinese word segmentation. The four data sets above are annotated with different standards. To catch general properties, we do experiments on all the four data sets. Three metrics were used for evaluation: precision (P), recall (R) and balanced F-score (F) defined by 2PR/(P+R). 75 word occurances in training data Figure 1: Segmentation recall relativ"
C10-2139,P09-1059,0,0.0547598,"biguity that most Chinese characters can occur in different positions within different words. Linear models are also popular for character disambiguation (i.e. segmentation disambiguation). Denote a sequence of character labels y ∈ Y n , a linear model is defined as: ˆ = arg max θ  Ψ(c, y) y y∈Y |c| = arg max θ y∈Y |c|  |c|  ψ(c, y[1:i] ) (3) (4) i=1 Note that local feature map ψ is defined only on the sequence of characters and their labels. 1212 Several discriminative models have been exploited for parameter estimation, including perceptron, CRFs, and discriminative latent variable CRFs (Jiang et al., 2009; Tseng, 2005; Sun et al., 2009b). 2.3 Theoretical Comparison Theoretically, the two types of models are different. We compare them from four aspects. 2.3.1 Internal Structure of Words Chinese words have internal structures. In most cases, Chinese character is a morpheme which is the smallest meaningful unit of the language. Though we cannot exactly infer the meaning of a word from its character components, the character structure is still meaningful. Partially characterizing the internal structures of words, one advantage of character-based models is the ability to induce new words. E.g., cha"
C10-2139,W95-0107,0,0.0304414,"Bound of System Combination To get an upper bound of the improvement that can be obtained by combining the strengths of each model, we have performed an oracle experiment. We think the optimal combination system should choose the right prediction when the two segmenters do not agree with each other. There is a gold segmenter that generates gold-standard segmentation results. In the oracle experiment, we let the three segmenters, i.e. baseline segmenters and the gold segmenter, vote. The three segmenters output three segmentation results, which are further transformed into IOB2 representation (Ramshaw and Marcus, 1995). Namely, each character has three B or I labels. We assign each character an oracle label which is chosn by at least two segmenters. When the baseline segmenters are agree with each other, the gold segmenter cannot change the segmentation whether it is right or wrong. In the situation that the two baseline segmenters disagree, the vote given by the gold segmenter will decide the right prediction. This kind of optimal performance is presented in Tab. 4. Compared these results with Tab. 1, we see a significant increase in accuracy for the four data sets. The upper bound of error reduction with"
C10-2139,D09-1153,1,0.864898,"ers can occur in different positions within different words. Linear models are also popular for character disambiguation (i.e. segmentation disambiguation). Denote a sequence of character labels y ∈ Y n , a linear model is defined as: ˆ = arg max θ  Ψ(c, y) y y∈Y |c| = arg max θ y∈Y |c|  |c|  ψ(c, y[1:i] ) (3) (4) i=1 Note that local feature map ψ is defined only on the sequence of characters and their labels. 1212 Several discriminative models have been exploited for parameter estimation, including perceptron, CRFs, and discriminative latent variable CRFs (Jiang et al., 2009; Tseng, 2005; Sun et al., 2009b). 2.3 Theoretical Comparison Theoretically, the two types of models are different. We compare them from four aspects. 2.3.1 Internal Structure of Words Chinese words have internal structures. In most cases, Chinese character is a morpheme which is the smallest meaningful unit of the language. Though we cannot exactly infer the meaning of a word from its character components, the character structure is still meaningful. Partially characterizing the internal structures of words, one advantage of character-based models is the ability to induce new words. E.g., character “/person” is usually us"
C10-2139,N09-1007,0,0.843168,"ers can occur in different positions within different words. Linear models are also popular for character disambiguation (i.e. segmentation disambiguation). Denote a sequence of character labels y ∈ Y n , a linear model is defined as: ˆ = arg max θ  Ψ(c, y) y y∈Y |c| = arg max θ y∈Y |c|  |c|  ψ(c, y[1:i] ) (3) (4) i=1 Note that local feature map ψ is defined only on the sequence of characters and their labels. 1212 Several discriminative models have been exploited for parameter estimation, including perceptron, CRFs, and discriminative latent variable CRFs (Jiang et al., 2009; Tseng, 2005; Sun et al., 2009b). 2.3 Theoretical Comparison Theoretically, the two types of models are different. We compare them from four aspects. 2.3.1 Internal Structure of Words Chinese words have internal structures. In most cases, Chinese character is a morpheme which is the smallest meaningful unit of the language. Though we cannot exactly infer the meaning of a word from its character components, the character structure is still meaningful. Partially characterizing the internal structures of words, one advantage of character-based models is the ability to induce new words. E.g., character “/person” is usually us"
C10-2139,O03-4002,0,0.884571,"Missing"
C10-2139,N06-2049,0,0.0681673,"egmenter. Wordbagging is named in the same way. Fig. 3 shows the precision, recall, F-score of the two baseline systems and our final system for which we generate m = 15 new data sets for bagging. We can see significant improvements on the four datasets in terms of the balanced Fscore. The improvement of precision and recall are not consistent. The improvement of AS and CU datasets is from the recall improvement; the improvement of PKU datasets is from the precision improvement. We think the different performance is mainly because the four datasets are annotated by using different standards. (Zhang et al., 2006) (Zhang and Clark, 2007) (Sun et al., 2009b) This paper AS 95.1 94.6 N/A 95.2 CU 95.1 95.1 94.6 95.6 MSR 97.1 97.2 97.3 96.9 PKU 95.1 94.5 95.2 95.2 Table 5: Segmentation performance presented in previous work and of our combination model. Tab. 5 summarizes the performance of our final system and other systems reported in a majority of previous work. The left most column indicates the reference of previous systems that represent stateof-the-art results. The comparison of the accuracy between our integrating system and the state-ofthe-art segmentation systems in the literature indicates that ou"
C10-2139,P07-1106,0,0.867835,"m (m is the number of words, i.e. m varies with w), max w∈GEN(c) max w∈GEN(c) θ  Φ(c, w) θ  |w|  (1) φ(c, w[1:i] ) (2) i=1 where Φ and φ are global and local feature maps and θ is the parameter vector to learn. The inner product θ  φ(c, w[1:i] ) can been seen as the confidence score of whether wi is a word. The disambiguation takes into account confidence score of each word, by using the sum of local scores as its criteria. Markov assumption is necessary for computation, so φ is usually defined on a limited history. Perceptron and semi-Markov CRFs were used to estimate θ in previous work (Zhang and Clark, 2007; Andrew, 2006). 2.2 Character-Based Approach Most previous data-driven segmentation solutions took an alternative, character-based view. This approach observes that by classifying characters as different positions in words, segmentation can be treated as a sequence labeling problem, assigning labels to the characters in a sentence indicating whether a character ci is a single character word (S) or the begin (B), middle (I) or end (E) of a multi-character word. For word prediction, word tokens are inferred based on the character classes. The main difficulty of this model is character ambiguity"
C10-2139,I05-3027,0,\N,Missing
C12-1053,J12-2006,0,0.0807906,"that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument structures and can capture essential meaning of sentences. Since the seminal work of (Gildea and Jurafsky, 2002), quite a few researchers concentrate on resolving SRL with different machine learning methods. Nowadays, good SRL systems can be built based on accurate syntactic parsers for English, as well as many other languages. In this paper, we explore predicate-argument analysis of source sentences to improve phrasebased SMT systems. On one hand, the predicate-argume"
C12-1053,P08-1009,0,0.520845,"ity, Beijing, China feng@cs.rwth-aachen.de, ws@pku.edu.cn, ney@cs.rwth-aachen.de ABSTRACT In this paper, we propose a novel semantic cohesion model. Our model utilizes the predicateargument structures as soft constraints and plays the role as a reordering model in the phrasebased statistical machine translation system. We build a translation system with GALE data. Experimental results on the NIST02, NIST03, NIST04, NIST05 and NIST08 Chinese-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument stru"
C12-1053,W02-1001,0,0.059381,"ition of semantic chunks is described below. • Constituent outside an argument receive the tag O. • For a sequence of constituents forming a semantic role of Ax, the first constituent receives the semantic chunk label B(egin)-Ax, • and the remaining ones receive the label I(nside)-Ax. Developing features has been shown crucial to advancing the state-of-the-art in SRL. To achieve good Chinese SRL results, we utilize rich syntactic features introduced in (Sun, 2010). For sequential tagging, we use a first order linear-chain global linear model and estimate parameters with structured preceptron (Collins, 2002). Our semantic chunking method can tolerate two types of parsing errors that are shown in Figure 3. Assume tree structures (1 and 4) on the left hand side are the correct syntactic analysis, while tree structures (2, 3, 5 and 6) on the right hand side are some wrong analysis. Though a constituent classification system, the arguments Ax and A y can not be recovered since there is no node to express them. In our constituent chunking system, however, when these errors occur, the arguments can still be found, if XP1 is assigned a label B-Ax or B-A y and XP2 is assigned a label I-Ax or I-A y. 1 C-c"
C12-1053,W11-1012,0,0.104881,"se-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument structures and can capture essential meaning of sentences. Since the seminal work of (Gildea and Jurafsky, 2002), quite a few researchers concentrate on resolving SRL with different machine learning methods. Nowadays, good SRL systems can be built based on accurate syntactic parsers for English, as well as many other languages. In this paper, we explore predicate-argument analysis of source sentences to improve phrasebased SMT systems. On one hand,"
C12-1053,J02-3001,0,0.0536937,"ation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument structures and can capture essential meaning of sentences. Since the seminal work of (Gildea and Jurafsky, 2002), quite a few researchers concentrate on resolving SRL with different machine learning methods. Nowadays, good SRL systems can be built based on accurate syntactic parsers for English, as well as many other languages. In this paper, we explore predicate-argument analysis of source sentences to improve phrasebased SMT systems. On one hand, the predicate-argument event layer of SRL captures global dependencies which is crucial for the MT output quality. On the other hand, the semantic role information contained in SRL also provide a good clue to the appropriateness of a phrase segment chosen by"
C12-1053,N03-1017,0,0.053277,"irectly using a log-linear combination of several models (Och and Ney, 2002): P  M exp λm hm (e1I , f1J ) P r(e1I |f1J ) = m=1 P 0 I ,e exp P M m=1 0 0I 1  0 0I λm hm (e 1 , f1J ) (2) The denominator is to make the P r(e1I |f1J ) to be a probability distribution and it depends only on the source sentence f1J . For search, the decision rule is simply: ˆeiI = arg max ˆ M nX m=1 o λm hm (e1I , f1J ) (3) The model scaling factors λ1M are trained with Minimum Error Rate Training (MERT). In this paper, the phrase-based machine translation system is utilized (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003). The translation process consists in segmentation of the source sentence according to the phrase table which is built from the word alignment. The translation of each of these segments is then just extracting the target side from the phrase pair. With the corresponding target side, the final translation is the composition of these translated segments. In this last step, reordering is allowed. 2.2 Semantic Role Labeling In the last decade, there has been an increasing interest in SRL on several languages, which consists of recognizing arguments involved by predicates in a given sentence and la"
C12-1053,P03-1056,0,0.019061,"model does not have hard limit. We list the important information regarding the experimental setup below. All those conditions have been kept same in this work. • lowercased training data (Table 1) from GALE task alignment trained with GIZA++ • tuning corpus: NIST06 test corpora: NIST02 03 04 05 and 08 • 5-gram LM (1 694 412 027 running words) trained by SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing LM training data: target side of bilingual data. • BLEU (Papineni et al., 2001) and TER (Snover et al., 2005) reported all scores calculated in lowercase way. • Stanford Parser (Levy and Manning, 2003) used to get the Chinese constituent tree for the SRL and the dependency tree for the syntactic cohesion model Chinese Sentences Running Words Vocabulary English 5 384 856 115 172 748 129 820 318 1 125 437 739 251 Table 1: training data statistics 4.2 A Full Parsing Based Chinese SRL System 4.2.1 Background SRL methods that are successful on English are adopted to resolve Chinese SRL (Xue, 2008; Sun, 2010). Previous work indicates that syntactic information is very important for SRL and full parsing based approaches are considerably better than shallow parsing based ones. Based on a phrase-str"
C12-1053,C10-1081,0,0.133761,"IST05 and NIST08 Chinese-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument structures and can capture essential meaning of sentences. Since the seminal work of (Gildea and Jurafsky, 2002), quite a few researchers concentrate on resolving SRL with different machine learning methods. Nowadays, good SRL systems can be built based on accurate syntactic parsers for English, as well as many other languages. In this paper, we explore predicate-argument analysis of source sentences to improve phrasebased SMT"
C12-1053,P02-1038,1,0.614626,"Secondly, the SRL framework will be given. Thirdly, we demonstrate how the semantic role information can be used for translation. 2.1 Principle In statistical machine translation, we are given a source language sentence f1J = f1 . . . f j . . . f J . The objective is to translate the source into a target language sentence e1I = e1 . . . ei . . . e I . The strategy is among all possible target language sentences, we will choose the one with the highest probability: ˆeiI = arg max{P r(e1I |f1J )} ˆ I,e1I 868 (1) We model P r(e1I |f1J ) directly using a log-linear combination of several models (Och and Ney, 2002): P  M exp λm hm (e1I , f1J ) P r(e1I |f1J ) = m=1 P 0 I ,e exp P M m=1 0 0I 1  0 0I λm hm (e 1 , f1J ) (2) The denominator is to make the P r(e1I |f1J ) to be a probability distribution and it depends only on the source sentence f1J . For search, the decision rule is simply: ˆeiI = arg max ˆ M nX m=1 o λm hm (e1I , f1J ) (3) The model scaling factors λ1M are trained with Minimum Error Rate Training (MERT). In this paper, the phrase-based machine translation system is utilized (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003). The translation process consists in segmentation of the"
C12-1053,W99-0604,1,0.565703,"e1I 868 (1) We model P r(e1I |f1J ) directly using a log-linear combination of several models (Och and Ney, 2002): P  M exp λm hm (e1I , f1J ) P r(e1I |f1J ) = m=1 P 0 I ,e exp P M m=1 0 0I 1  0 0I λm hm (e 1 , f1J ) (2) The denominator is to make the P r(e1I |f1J ) to be a probability distribution and it depends only on the source sentence f1J . For search, the decision rule is simply: ˆeiI = arg max ˆ M nX m=1 o λm hm (e1I , f1J ) (3) The model scaling factors λ1M are trained with Minimum Error Rate Training (MERT). In this paper, the phrase-based machine translation system is utilized (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003). The translation process consists in segmentation of the source sentence according to the phrase table which is built from the word alignment. The translation of each of these segments is then just extracting the target side from the phrase pair. With the corresponding target side, the final translation is the composition of these translated segments. In this last step, reordering is allowed. 2.2 Semantic Role Labeling In the last decade, there has been an increasing interest in SRL on several languages, which consists of recognizing arguments involved"
C12-1053,2001.mtsummit-papers.68,0,0.0299556,"penalty. The reordering model for the baseline system is the distance-based jump model which uses linear distance. This model does not have hard limit. We list the important information regarding the experimental setup below. All those conditions have been kept same in this work. • lowercased training data (Table 1) from GALE task alignment trained with GIZA++ • tuning corpus: NIST06 test corpora: NIST02 03 04 05 and 08 • 5-gram LM (1 694 412 027 running words) trained by SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing LM training data: target side of bilingual data. • BLEU (Papineni et al., 2001) and TER (Snover et al., 2005) reported all scores calculated in lowercase way. • Stanford Parser (Levy and Manning, 2003) used to get the Chinese constituent tree for the SRL and the dependency tree for the syntactic cohesion model Chinese Sentences Running Words Vocabulary English 5 384 856 115 172 748 129 820 318 1 125 437 739 251 Table 1: training data statistics 4.2 A Full Parsing Based Chinese SRL System 4.2.1 Background SRL methods that are successful on English are adopted to resolve Chinese SRL (Xue, 2008; Sun, 2010). Previous work indicates that syntactic information is very importan"
C12-1053,W95-0107,0,0.00786778,"can tolerate some syntactic parsing errors. First, our system collects all c-commanders1 and puts them in order. Because c-commanders of a predicate are not overlapped with each other and compose the whole sentence, we can take this step as a sequentialization procedure. Sun et al. (2008) present a theoretical analysis about argument positions and suggest that an argument should c-command a predicate. Therefore, our sequentialization precedure keeps most semantic roles. On basis of sequentialized constituents, we define semantic chunks which do not overlap nor embed using IOB2 representation (Ramshaw and Marcus, 1995) and transfer the SRL problem as a constituent tagging problem. Our definition of semantic chunks is described below. • Constituent outside an argument receive the tag O. • For a sequence of constituents forming a semantic role of Ax, the first constituent receives the semantic chunk label B(egin)-Ax, • and the remaining ones receive the label I(nside)-Ax. Developing features has been shown crucial to advancing the state-of-the-art in SRL. To achieve good Chinese SRL results, we utilize rich syntactic features introduced in (Sun, 2010). For sequential tagging, we use a first order linear-chain"
C12-1053,P10-2031,1,0.934056,"LM training data: target side of bilingual data. • BLEU (Papineni et al., 2001) and TER (Snover et al., 2005) reported all scores calculated in lowercase way. • Stanford Parser (Levy and Manning, 2003) used to get the Chinese constituent tree for the SRL and the dependency tree for the syntactic cohesion model Chinese Sentences Running Words Vocabulary English 5 384 856 115 172 748 129 820 318 1 125 437 739 251 Table 1: training data statistics 4.2 A Full Parsing Based Chinese SRL System 4.2.1 Background SRL methods that are successful on English are adopted to resolve Chinese SRL (Xue, 2008; Sun, 2010). Previous work indicates that syntactic information is very important for SRL and full parsing based approaches are considerably better than shallow parsing based ones. Based on a phrase-structure parsing, SRL is usually formulated as a constituent classification problem. In particular, SRL is divided into three sub-tasks: 1) pruning with a heuristic rule, 2) argument identification (AI) to recognize arguments, and 3) semantic role classification (SRC) to predict semantic types. To efficiently excluded non-arguments, a pruning procedure is executed to filter out constituents that are highly u"
C12-1053,C08-1105,1,0.856819,"ll parsing based constituent chunking. span with an argument in the manual annotation, the system cannot possibly get a correct prediction. In other words, the best the system can do is to correctly label all arguments that have a counterpart node in the parse tree. In this paper, we implement a semantic chunking method which can tolerate some syntactic parsing errors. First, our system collects all c-commanders1 and puts them in order. Because c-commanders of a predicate are not overlapped with each other and compose the whole sentence, we can take this step as a sequentialization procedure. Sun et al. (2008) present a theoretical analysis about argument positions and suggest that an argument should c-command a predicate. Therefore, our sequentialization precedure keeps most semantic roles. On basis of sequentialized constituents, we define semantic chunks which do not overlap nor embed using IOB2 representation (Ramshaw and Marcus, 1995) and transfer the SRL problem as a constituent tagging problem. Our definition of semantic chunks is described below. • Constituent outside an argument receive the tag O. • For a sequence of constituents forming a semantic role of Ax, the first constituent receive"
C12-1053,N09-2004,0,0.154278,", NIST03, NIST04, NIST05 and NIST08 Chinese-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867 1 Introduction In recent years, there are growing interests in incorporating semantics into statistical machine translation (SMT) (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011; Baker et al., 2012). Among all existing semantic representations, semantic role labeling (SRL) aims at automatically analyzing predicate-argument structures and can capture essential meaning of sentences. Since the seminal work of (Gildea and Jurafsky, 2002), quite a few researchers concentrate on resolving SRL with different machine learning methods. Nowadays, good SRL systems can be built based on accurate syntactic parsers for English, as well as many other languages. In this paper, we explore predicate-argument analysis of source sentences to im"
C12-1053,J08-2004,0,0.0386738,"smoothing LM training data: target side of bilingual data. • BLEU (Papineni et al., 2001) and TER (Snover et al., 2005) reported all scores calculated in lowercase way. • Stanford Parser (Levy and Manning, 2003) used to get the Chinese constituent tree for the SRL and the dependency tree for the syntactic cohesion model Chinese Sentences Running Words Vocabulary English 5 384 856 115 172 748 129 820 318 1 125 437 739 251 Table 1: training data statistics 4.2 A Full Parsing Based Chinese SRL System 4.2.1 Background SRL methods that are successful on English are adopted to resolve Chinese SRL (Xue, 2008; Sun, 2010). Previous work indicates that syntactic information is very important for SRL and full parsing based approaches are considerably better than shallow parsing based ones. Based on a phrase-structure parsing, SRL is usually formulated as a constituent classification problem. In particular, SRL is divided into three sub-tasks: 1) pruning with a heuristic rule, 2) argument identification (AI) to recognize arguments, and 3) semantic role classification (SRC) to predict semantic types. To efficiently excluded non-arguments, a pruning procedure is executed to filter out constituents that"
C12-1053,2002.tmi-tutorials.2,0,0.0769249,"el P r(e1I |f1J ) directly using a log-linear combination of several models (Och and Ney, 2002): P  M exp λm hm (e1I , f1J ) P r(e1I |f1J ) = m=1 P 0 I ,e exp P M m=1 0 0I 1  0 0I λm hm (e 1 , f1J ) (2) The denominator is to make the P r(e1I |f1J ) to be a probability distribution and it depends only on the source sentence f1J . For search, the decision rule is simply: ˆeiI = arg max ˆ M nX m=1 o λm hm (e1I , f1J ) (3) The model scaling factors λ1M are trained with Minimum Error Rate Training (MERT). In this paper, the phrase-based machine translation system is utilized (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003). The translation process consists in segmentation of the source sentence according to the phrase table which is built from the word alignment. The translation of each of these segments is then just extracting the target side from the phrase pair. With the corresponding target side, the final translation is the composition of these translated segments. In this last step, reordering is allowed. 2.2 Semantic Role Labeling In the last decade, there has been an increasing interest in SRL on several languages, which consists of recognizing arguments involved by predicates in a"
C12-1053,P02-1040,0,\N,Missing
D09-1153,W04-2412,0,0.0160239,"Missing"
D09-1153,P06-2013,0,0.18143,"ce reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This performance drops to 71.9 when a real parser is used instead1 (Xue, 2008). Comparatively, the best English SRL results reported drops from 91.2 (Pradhan et al., 2008) to 80.56 (Surdeanu et al., 2007). These results suggest that as still in its infancy stage, Chinese full parsing acts as a central bottleneck that severely limits our ability to solve Chinese SRL. On the contrary, Chinese shallow parsing has gained a promising result (Chen et al., 2006); hence it is an alternative choice for Chinese SRL. This paper addresses the Chinese SRL problem on the basis of shallow syntactic information at the level of phrase chunks. We first extend the study on Chinese chunking presented in (Chen et al., 2006) by raising a set of additional features. The new set of features yield improvement over the strong chunking system described in (Chen et al., 2006). On the basis of our shallow parser, we implement lightweight systems which solve SRL as a sequence labeling problem. This is accomplished by casting SRL as the classification of syntactic chunks (e"
D09-1153,I08-2132,0,0.0184496,"antically annotated corpus of Chinese. Their experiments were evaluated only on ten specified verbs with a small collection of Chinese sentences. This work made the first attempt on Chinese SRL and produced promising results. After the CPB was built, (Xue and Palmer, 2005) and (Xue, 2008) have produced more complete and systematic research on Chinese SRL. Ding and Chang (2008) divided SRC into two sub-tasks in sequence. Under the hierarchical architecture, each argument should first be determined whether it is a core argument or an adjunct, and then be classified into fine-grained categories. Chen et al. (2008) introduced an application of transductive SVM in Chinese SRL. Because their experiments took hand-crafted syntactic trees as input, how transductive SVMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxin"
D09-1153,D08-1034,0,0.292702,"over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This perform"
D09-1153,N03-2009,0,0.0190685,"ok hand-crafted syntactic trees as input, how transductive SVMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For example, Hacioglu and Ward (2003) considered SRL as a chunking task; Pradhan et al. (2005) introduced a new procedure to incorporate SRL results predicted respectively on full and shallow syntactic parses. Previous work on English suggests that even good labeling performance has been achieved by full parse based SRL systems, partial parse based SRL systems can still enhance their performance. Though better understanding of SRL with shallow parsing on English is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004), little is known about how these SRL methods perform on Chinese. 3 Chinese Shallow Parsing There have"
D09-1153,W00-0730,0,0.0392688,"Missing"
D09-1153,N01-1025,0,0.127166,"Missing"
D09-1153,W05-0634,0,0.0259716,"VMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For example, Hacioglu and Ward (2003) considered SRL as a chunking task; Pradhan et al. (2005) introduced a new procedure to incorporate SRL results predicted respectively on full and shallow syntactic parses. Previous work on English suggests that even good labeling performance has been achieved by full parse based SRL systems, partial parse based SRL systems can still enhance their performance. Though better understanding of SRL with shallow parsing on English is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004), little is known about how these SRL methods perform on Chinese. 3 Chinese Shallow Parsing There have been some research on Chinese shallow parsing, and a vari"
D09-1153,J08-2006,0,0.0246748,"ing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This performance drops to 71.9 when a real parser is used instead1 (Xue, 2008). Comparatively, the best English SRL results reported drops from 91.2 (Pradhan et al., 2008) to 80.56 (Surdeanu et al., 2007). These results suggest that as still in its infancy stage, Chinese full parsing acts as a central bottleneck that severely limits our ability to solve Chinese SRL. On the contrary, Chinese shallow parsing has gained a promising result (Chen et al., 2006); hence it is an alternative choice for Chinese SRL. This paper addresses the Chinese SRL problem on the basis of shallow syntactic information at the level of phrase chunks. We first extend the study on Chinese chunking presented in (Chen et al., 2006) by raising a set of additional features. The new set of fe"
D09-1153,W95-0107,0,0.0640529,"chunk definitions have been proposed. However, most of these studies did not provide sufficient detail. In our system, we use chunk definition presented in (Chen et al., 2006), which provided a chunk extraction tool. The tool to extract chunks from CTB was developed by modifying the English tool used in CoNLL-2000 shared task, Chunklink3 , and is publicly available at http://www.nlplab.cn/chenwl/chunking.html. The definition of syntactic chunks is illustrated in Line CH in Figure 1. For example, ”保险公司/the insurance company”, consisting of two nouns, is a noun phrase. With IOB2 representation (Ramshaw and Marcus, 1995), the problem of Chinese chunking can be regarded as a sequence labeling task. In this paper, we first implement the chunking method described in (Chen et al., 2006) as a strong baseline. To conveniently illustrate, we denote a word in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is current token. The baseline features includes: • Uni-gram word/POS tag feature: w−2 , w−1 , w, w+1 , w+2 ; • Bi-gram word/POS tag feature: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; 3 http://ilk.uvt.nl/team/sabine/chunklink/chunklink 2-22000 for conll.pl 1476 WORD: POS: CH: M1: M2-AI: M2-SRC: 截止 目前 保险 公司 已 为 三峡 工程"
D09-1153,N04-1032,0,0.124339,"achieving significant improvements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese"
D09-1153,J08-2004,0,0.367894,"mprovements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treeb"
D09-1153,N07-1070,0,\N,Missing
D11-1090,N04-1043,0,0.254346,"engineering” approach for learning segmentation models from both labeled and unlabeled data. Our method is a simple two-stage process. First, we use unannotated corpus to extract string and document information, and then we use these information to construct new statisticsbased and document-based feature mapping for a discriminative word segmenter. We are relying on the ability of discriminative learning method to identify and explore informative features, which play central role to boost the segmentation performance. This simple solution has been shown effective for named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008). In their implementations, word clusters derived from unlabeled data are imported as features to discriminative learning approaches. To demonstrate the effectiveness of our approach, we conduct experiments on the Penn Chinese Treebank (CTB) data. CTB is a collection of documents which are separately annotated. This annotation style allows us to evaluate our transductive segmentation method. Our experiments show that both statistics-based and document-based features are effective in the word segmentation application. In general, the use of unlabeled da"
D11-1090,P10-1130,0,0.0145323,"data, and calculate their punctuation variety values. The length of each string is also restricted between 2 and 4. For each character ci , we import the following information into our model, • Punctuation variety of strings with length 4: 4 (c L4pv (c[i:i+3] ), Rpv [i−3:i] ); • Punctuation variety of strings with length 3: 3 (c L3pv (c[i:i+2] ), Rpv [i−2:i] ); • Punctuation variety of strings with length 2: 2 (c L2pv (c[i:i+1] ), Rpv [i−1:i] ). Punctuations can be viewed as mark-up’s of Chinese text. Our motivation to use the punctuation information to assist a word segmenter is similiar to (Spitkovsky et al., 2010) in a way to explore “artificial” word (or phrase) break symbols. In their work, four common HTML tags are successfully used as raw phrase bracketings to improve unsupervised dependency parsing. 2.3.4 Binary or Numeric Features The derived information introduced above is all expressed as real values. The natural way to incorporate these statistics into a discriminative learning model is to directly use them as numeric features. However, our experiments show that this simple choice does not work well. The reason is that these statistics actually behave non-linearly to predict character labels."
D11-1090,J04-1004,0,0.825001,".de/˜wsun/ idiom.txt. Chinese character string c[i−2:i+1] , the mutual information between substrings c[i−2:i−1] and c[i:i+1] is computed as: M I(c[i−2:i−1] , c[i:i+1] ) = log p(c[i−2:i+1] ) p(c[i−2:i−1] )p(c[i:i+1] ) For each character ci , we incorporate the MI of the character bi-grams into our model. They include, • M I(c[i−2:i−1] , c[i:i+1] ), • M I(c[i−1:i] , c[i+1:i+2] ). 2.3.2 Accessor Variety When a string appears under different linguistic environments, it may carry a meaning. This principle is introduced as the accessor variety criterion for identifying meaningful Chinese words in (Feng et al., 2004). This criterion evaluates how independently a string is used, and thus how likely it is that the string can be a word. Given a string s, which consists of l (l ≥ 2) characters, we define the left accessor variety of Llav (s) as the number of distinct characters that precede s in a corpus. Similarly, the l (s) is defined as the numright accessor variety Rav ber of distinct characters that succeed s. We first extract all strings whose length are between 2 and 4 from the unlabeled data, and calculate their accessor variety values. For each character ci , we then incorporate the following informa"
D11-1090,P08-1068,0,0.162893,"on models from both labeled and unlabeled data. Our method is a simple two-stage process. First, we use unannotated corpus to extract string and document information, and then we use these information to construct new statisticsbased and document-based feature mapping for a discriminative word segmenter. We are relying on the ability of discriminative learning method to identify and explore informative features, which play central role to boost the segmentation performance. This simple solution has been shown effective for named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008). In their implementations, word clusters derived from unlabeled data are imported as features to discriminative learning approaches. To demonstrate the effectiveness of our approach, we conduct experiments on the Penn Chinese Treebank (CTB) data. CTB is a collection of documents which are separately annotated. This annotation style allows us to evaluate our transductive segmentation method. Our experiments show that both statistics-based and document-based features are effective in the word segmentation application. In general, the use of unlabeled data can be motivated by two concerns: First"
D11-1090,J09-4006,0,0.439733,"res in discriminative learning. Moreover, Turian et al. (2010) compared different word clustering algorithms and evaluated their effect on both named entity recognition and text chunking. As mentioned earlier, the feature design is inspired by some previous research on word segmentation. The accessor variety criterion is proposed to extract word types, i.e. the list of possible words, in (Feng et al., 2004). Different from their work, our method resolves the segmentation problem of running texts, in which this criterion is used to define features correlated with the character position labels. Li and Sun (2009) observed that punctuations are perfect delimiters which provide useful information for segmentation. Their method can be viewed as a self-training procedure, in which extra punctuation information is incorporated to filter out automatically predicted samples. We use the punctuation information in a different way. In our method, the counts of the preceding and succeeding strings of punctuations are incorporated directly as features into a supervised model. In machine learning, transductive learning is a learning framework that typically makes use of unlabeled data. The goal of transductive lea"
D11-1090,C10-2139,1,0.689111,"2 2.1 Method Discriminative Character-based Word Segmentation The Character-based approach is a dominant word segmentation solution for Chinese text processing. This approach treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside or at the end of a word. This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al., 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al., 2005), and latent variable CRFs (Sun et al., 2009). In this work, we use the Start/End representation to express the position information of every character. Table 2.1 shows the meaning of each character label. For example, the target label representation of the book title “赵紫阳总理的秘密日记/The Secret Journal of Premier Zhao Ziyang” is as follows. 赵 紫 阳 总 理 的 秘 密 日 记 B I E B E S B E B E Key to our approach is to allow informative features derived from unlabeled data to assist the segmenter. In our experiments, we employed three different feature sets"
D11-1090,P10-1040,0,0.0529103,"ian semisupervised approach to derive task-oriented word 977 segmentation for machine translation (MT). This method learns new word types and word distributions on unlabeled data by considering segmentation as a hidden variable in MT. Different from their concern, our focus is general word segmentation. The “feature-engineering” semi-supervised approach has been successfully applied to named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008). These two papers demonstrated the effectiveness of using word clusters as features in discriminative learning. Moreover, Turian et al. (2010) compared different word clustering algorithms and evaluated their effect on both named entity recognition and text chunking. As mentioned earlier, the feature design is inspired by some previous research on word segmentation. The accessor variety criterion is proposed to extract word types, i.e. the list of possible words, in (Feng et al., 2004). Different from their work, our method resolves the segmentation problem of running texts, in which this criterion is used to define features correlated with the character position labels. Li and Sun (2009) observed that punctuations are perfect delim"
D11-1090,C08-1128,1,0.843734,"Missing"
D11-1090,O03-4002,0,0.868251,"ne character. Table 1: The start/end representation. Section 3 presents experimental results and empirical analysis. Section 4 reviews the related work. Section 5 concludes the paper. 2 2.1 Method Discriminative Character-based Word Segmentation The Character-based approach is a dominant word segmentation solution for Chinese text processing. This approach treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside or at the end of a word. This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al., 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al., 2005), and latent variable CRFs (Sun et al., 2009). In this work, we use the Start/End representation to express the position information of every character. Table 2.1 shows the meaning of each character label. For example, the target label representation of the book title “赵紫阳总理的秘密日记/The Secret Journal of Premier Zhao Ziyang” is as follows. 赵 紫 阳 总 理 的 秘 密 日 记 B I"
D11-1090,N09-1007,0,\N,Missing
D11-1090,P09-1059,0,\N,Missing
D11-1090,P11-1139,1,\N,Missing
D11-1090,I05-3027,0,\N,Missing
D17-1003,C10-1011,0,0.0293179,"mposition for IntC [i, j, x]. 5 5.1 Data and Preprocessing • The DeepBank, Enju HPSGBank and Prague Dependency TreeBank are from SemEval 2014 Task 8 (Oepen et al., 2014), and the data splitting policy follows the shared task. Practical Parsing Derivation-Sensitive Training Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. All experimental results consider directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. To estimate"
D17-1003,Q15-1040,0,0.192719,"ruction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing. 1 Introduction Previous work showed that treating semantic dependency parsing as the search for Maximum Subgraphs is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017). In particular, our previous work showed that 1-endpoint-crossing, pagenumber-2 (1 EC / P 2) graphs are an appropriate graph class for modelling semantic dependency structures (Cao et al., 2017). On the one hand, it is highly expressive to cover a majority of semantic analysis. On the other hand, the corresponding Maximum Subgraph problem with an arc-factored disambiguation model can be solved in low-degree polynomial time. Defining disambiguation models on wider contexts than individual bi-lexical dependencies improves various syntactic parsers in different architectures."
D17-1003,P17-1193,1,0.858836,"s and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing. 1 Introduction Previous work showed that treating semantic dependency parsing as the search for Maximum Subgraphs is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017). In particular, our previous work showed that 1-endpoint-crossing, pagenumber-2 (1 EC / P 2) graphs are an appropriate graph class for modelling semantic dependency structures (Cao et al., 2017). On the one hand, it is highly expressive to cover a majority of semantic analysis. On the other hand, the corresponding Maximum Subgraph problem with an arc-factored disambiguation model can be solved in low-degree polynomial time. Defining disambiguation models on wider contexts than individual bi-lexical dependencies improves various syntactic parsers in different architectures. This paper studies"
D17-1003,E06-1011,0,0.719571,"(GCHSW, hereafter), has two properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order f"
D17-1003,D07-1101,0,0.167503,"properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it"
D17-1003,W02-1001,0,0.091698,"er directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). Our training proceudre is sensitive to derivation rather then derived graphs. For each sentence, we first apply our algorithm to find the optimal prediction derivation. The we collect all first- and second-order factors from this derivation to update parameters. To train a first-order model, because our algorithm includes all factors, viz. depencies, there is no difference between our derivationbased method and a traditional derived structurebased method. For the second-order model, our method increases the second-order scores somehow. 5.3 Accuracy Table 1 lists the accuracy of our system. T"
D17-1003,S14-2008,0,0.280148,"acteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it separates the construction for noncrossing edges and possible crossing edges; (2) whether an edge is created is deterministic in each construction rule. We then introduce a new algorithm to perform secondorder parsing. When all second-order scores are greater than or equal to 0, it exactly solves the corresponding optimization problem. We implement a practical parser with a statistical disambiguation model and evaluate it on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our second-order parsing models are more acWe propose a new Maximum Subgraph algorithm for first-order parsing to 1endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features."
D17-1003,C96-1058,0,0.609042,"Maximum Subgraph algorithm, viz. GCHSW, for 1 EC / P 2 graphs by exploring the following property: Every subgraph of a 1 EC / P 2 graph is also a 1 EC / P 2 graph. GCHSW defines a number of prototype backbones for decomposing a 1 EC / P 2 graph in a principled way. In each decomposition step, GCHSW focuses on the edges that can be created without violating either the 1 EC nor P 2 restriction. Sometimes, multiple edges can be created simultaneously in one single step. Figure 4 is an example. There is an important difference between GCHSW and Eisner-style Maximum Spanning Tree algorithms (MST; Eisner, 1996; McDonald and Pereira, 2006; Koo and Collins, 2010). In each construction step, GCHSW allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. For the higher-order MST algorithms, in a single construction step, it is clear whether adding a new arc, and which one. There is no local search. This deterministic strategy is also followed by Kuhlmann and Jonsson’s Maximum Subgraph algorithm for n"
D17-1003,Q14-1004,0,0.220407,"ce and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {junjie.cao,huangsheng,ws,wanxiaojun}@pku.edu.cn Abstract (GCHSW, hereafter), has two properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping t"
D17-1003,Q13-1002,0,0.550744,"rds, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Figure 1 is an example. Definition 3. A pagenumber-k graph means it consists at most k half-planes, and arcs on each half-plane are noncrossing. These half-planes may be thought of as the pages of a book, with the vertex line corresponding to the books spine, and the embedding of a graph into such a structure is known as a book embedding. Figure 2 is an example. (Pitler et al., 2013) proved that 1-endpointcrossing trees are a subclass of graphs whose pagenumber is at most 2. In Cao et al. (2017), we studied graphs that are constrained to be both 1-endpoint-crossing and pagenumber-2. In this paper, we ignored a complex and linguistic-rare And the objective function turns to be: X d Definition 1. Edges e1 and e2 cross if e1 and e2 have distinct endpoints and exactly one of the endpoints of e1 lies between the endpoints of e2 . d∈A RC(G∗ ) sarc (d) + c The formal description of the 1-endpoint-crossing property is adopted from (Pitler et al., 2013). p in G∗ If G is the set of"
D17-1003,hajic-etal-2012-announcing,0,0.0570488,"Missing"
D17-1003,N12-1054,0,0.0445511,"Missing"
D17-1003,J07-3004,0,0.142743,"crossingsensitive, second-order features: (1) it separates the construction for noncrossing edges and possible crossing edges; (2) whether an edge is created is deterministic in each construction rule. We then introduce a new algorithm to perform secondorder parsing. When all second-order scores are greater than or equal to 0, it exactly solves the corresponding optimization problem. We implement a practical parser with a statistical disambiguation model and evaluate it on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our second-order parsing models are more acWe propose a new Maximum Subgraph algorithm for first-order parsing to 1endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demo"
D17-1003,P17-1077,1,0.748186,"raph. The upper and the lower figures represent two half-planes respectively. G(s, G) denotes the set of all graphs that belong to G and are compatible with s and G. G is usually a complete digraph. spart (s, p) evaluates the event that part p (from a candidate graph G∗ ) is good. We define the order of p according to the number of arcs it contains, in analogy with tree parsing in terminology. Previous work only discussed the first-order case: X arg max sarc (d) X d Page 1 Preliminaries G∗ ∈G(G) c b Figure 1: e(a,c) ’s crossing edges e(b,d) and e(b,e) share an endpoint b. ssib (s) s∈S IB(G∗ ) Sun et al. (2017) introduced a dynamic programming algorithm for second-order planar parsing. Their empirical evaluation showed that secondorder features are effective to improve parsing accuracy. It is still unknown how to incorporate such features for 1 EC / P 2 parsing. x k i b Figure 3: C structure has two crossing chains. 25 k i l j x Figure 4: A prototype backbone of 1 EC / P 2 graphs. To decompose this structure, GCHSW focuses on e(i,j) and e(l,j) , because these two edges can be optionally created without violation of both 1 EC and P 2 restrictions. Our algorithm focuses on the existence of e(i,k) , an"
D17-1003,D10-1002,0,0.0137562,"• Following previous experimental setup for English CCG parsing, we use section 02-21 as training data, section 00 as the development data, and section 23 for testing. Figure 14: Decomposition for IntC [i, j, x]. 5 5.1 Data and Preprocessing • The DeepBank, Enju HPSGBank and Prague Dependency TreeBank are from SemEval 2014 Task 8 (Oepen et al., 2014), and the data splitting policy follows the shared task. Practical Parsing Derivation-Sensitive Training Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. All experimental results consider directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with man"
D17-1003,D12-1030,0,0.0412381,"Missing"
D17-1003,P10-1001,0,0.444093,"make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it separates the constructi"
D18-1414,J08-2001,0,\N,Missing
D18-1414,P00-1065,0,\N,Missing
D18-1414,J08-2005,0,\N,Missing
D18-1414,J08-2004,0,\N,Missing
D18-1414,P06-1055,0,\N,Missing
D18-1414,I11-1017,0,\N,Missing
D18-1414,C12-1053,1,\N,Missing
D18-1414,P15-1109,0,\N,Missing
D18-1414,D15-1186,0,\N,Missing
D18-1414,P16-1070,0,\N,Missing
D18-1414,P16-1173,0,\N,Missing
D18-1414,K17-1041,0,\N,Missing
D18-1414,P17-1044,0,\N,Missing
D18-1414,W17-6306,0,\N,Missing
D18-1414,Q15-1003,0,\N,Missing
D18-1414,N06-1014,0,\N,Missing
I13-1021,A00-2018,0,0.154822,"elman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue"
I13-1021,C04-1041,0,0.0359093,"undamental NLP tasks, including named entity recognition, POS tagging, text chunking, supertagging, etc., employ sequential classifiers for lexical and syntactic disambiguation. In addition to learning linear chain structures, sequence models can even be applied to acquire hierarchical syntactic structures (Tsuruoka et al., 2009). However, long-distance dependencies widely exist in linguistic structures, and many NLP systems suffer from the incapability of capturing these dependencies. For example, previous work has shown that sequence models alone cannot deal with syntactic ambiguities well (Clark and Curran, 2004; Tsuruoka et al., 2009). On the contrary, stateof-the-art systems usually utilize high complexity models, such as lexicalized PCFG models for syntactic parsing, to achieve high accuracy. Unfortunately, they are not suitable for many real world applications due to the sacrifice of efficiency. In this paper, we are concerned with capturing long-distance dependencies in sequence models. Our goal is to develop efficient models with linear time complexity that are also capable to capture non-local dependencies. Two techniques are studied to achieve this goal. First, stacked learning (Breiman, 1996"
I13-1021,I11-1136,0,0.0696737,"parser. Our linguistic analysis can also well explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran, 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertag• Character n-gram prefixes and suffixes for n up to 3. To train LLMs, we use the open source linear classifier – LIBLINEAR1 . To train LGLMs, we choose structured perceptron (SP) (Collins, 2002) and passive aggressive (PA) (Crammer et al., 2006) learning algorithms. For the LAHMM and DEP models, we use the systems discribed in (Huang et al., 2009; Hatori et al., 2011); for the PCFGLA models, we use the Berkeley parser2 . 2.3.2 Results Table 1 summarizes the performance in terms of per word classification of different supervised models on the development data. We present the results of both first order (on the left) and second order (on the right) LGLMs. We can see that the perceptron algorithm performs a little better than the PA algorithm for Chinese POS tagging. There is only a slight gap between the local classification model and various structured models. This is very different from English POS tagging. Although the local classifier achieves comparable"
I13-1021,P12-1046,0,0.0598969,"iscriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output spaces, which significantly boost the accuracy and increase the robustness of simple generative models. Hidden Markov model with latent variables (HMMLA) Generative models with latent annotations (LA) obtain state-of-the-art performance for a number of NLP tasks. For example, both PCFG and TSG with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki et al., 2005; Shindo et al., 2012). For Chinese POS tagging, Huang, Eidelman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learni"
I13-1021,N09-2054,0,0.558188,"many errors to the parser. Our linguistic analysis can also well explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran, 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertag• Character n-gram prefixes and suffixes for n up to 3. To train LLMs, we use the open source linear classifier – LIBLINEAR1 . To train LGLMs, we choose structured perceptron (SP) (Collins, 2002) and passive aggressive (PA) (Crammer et al., 2006) learning algorithms. For the LAHMM and DEP models, we use the systems discribed in (Huang et al., 2009; Hatori et al., 2011); for the PCFGLA models, we use the Berkeley parser2 . 2.3.2 Results Table 1 summarizes the performance in terms of per word classification of different supervised models on the development data. We present the results of both first order (on the left) and second order (on the right) LGLMs. We can see that the perceptron algorithm performs a little better than the PA algorithm for Chinese POS tagging. There is only a slight gap between the local classification model and various structured models. This is very different from English POS tagging. Although the local classifi"
I13-1021,P12-1026,1,0.841523,"al linear model (LGLM) Sequence labeling models can capture output structures by exploiting local dependencies among words. A global linear model is flexible to in181 provements over the pipeline systems in both POS tagging and dependency parsing tasks. clude linguistic knowledge from multiple information sources, and thus suitable to recognize more new words. A majority of state-of-the-art English POS taggers are based on LGLMs, e.g. structured perceptron (Collins, 2002) and conditional random fields (Lafferty et al., 2001). Such models are also very popular for building Chinese POS taggers (Sun and Uszkoreit, 2012). 2.2 Comparison We can distinguish the five representative tagging models from two views (see Table 2). From a linguistic view, we can distinguish syntax-free and syntax-based models. In a syntex-based model, POS tagging is integrated into parsing, and thus (to some extent) is capable of capturing long range syntactic information. From a machine learning view, we can distinguish generative and discriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output"
I13-1021,P11-1139,1,0.863776,"3: Tagging accuracies of different stacking models on the development data. fine features for the LLM/LGLM. ging. Devel. Berkeley 1or LGLM 2or LGLM HMMLA 1or LGLM(HMMLA) 1or LGLM(PCFGLA) 1or LGLM(DEP) LP 80.44 80.38 80.98 80.65 81.55 82.84 82.69 LR 80.31 79.48 79.93 79.62 80.80 81.75 81.68 3.1 F1 81.36 79.93↓ 80.45↓ 80.13↓ 81.17↓ 82.29↑ 82.18↑ Stacked generalization is a meta-learning algorithm that has been first proposed in (Wolpert, 1992) and (Breiman, 1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun, 2011), and dependency parsing (Nivre and McDonald, 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Table 4: Parsing accuracies on the development data. 1or and 2or respectively denote first order and second order. L"
I13-1021,N12-1030,0,0.238547,"Missing"
I13-1021,D11-1109,0,0.122737,"Missing"
I13-1021,E09-1090,0,0.0199536,"al Linguistics {ws,wanxiaojun}@pku.edu.cn; pxc.pku@gmail.com Abstract perform well for many applications, they are inadequate for tasks where many long-distance dependencies are involved. Sequential classification models play an important role in natural language processing (NLP). Several fundamental NLP tasks, including named entity recognition, POS tagging, text chunking, supertagging, etc., employ sequential classifiers for lexical and syntactic disambiguation. In addition to learning linear chain structures, sequence models can even be applied to acquire hierarchical syntactic structures (Tsuruoka et al., 2009). However, long-distance dependencies widely exist in linguistic structures, and many NLP systems suffer from the incapability of capturing these dependencies. For example, previous work has shown that sequence models alone cannot deal with syntactic ambiguities well (Clark and Curran, 2004; Tsuruoka et al., 2009). On the contrary, stateof-the-art systems usually utilize high complexity models, such as lexicalized PCFG models for syntactic parsing, to achieve high accuracy. Unfortunately, they are not suitable for many real world applications due to the sacrifice of efficiency. In this paper,"
I13-1021,P05-1010,0,0.0537042,"inguish generative and discriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output spaces, which significantly boost the accuracy and increase the robustness of simple generative models. Hidden Markov model with latent variables (HMMLA) Generative models with latent annotations (LA) obtain state-of-the-art performance for a number of NLP tasks. For example, both PCFG and TSG with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki et al., 2005; Shindo et al., 2012). For Chinese POS tagging, Huang, Eidelman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on"
I13-1021,P08-1108,0,0.0639317,"tacking models on the development data. fine features for the LLM/LGLM. ging. Devel. Berkeley 1or LGLM 2or LGLM HMMLA 1or LGLM(HMMLA) 1or LGLM(PCFGLA) 1or LGLM(DEP) LP 80.44 80.38 80.98 80.65 81.55 82.84 82.69 LR 80.31 79.48 79.93 79.62 80.80 81.75 81.68 3.1 F1 81.36 79.93↓ 80.45↓ 80.13↓ 81.17↓ 82.29↑ 82.18↑ Stacked generalization is a meta-learning algorithm that has been first proposed in (Wolpert, 1992) and (Breiman, 1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun, 2011), and dependency parsing (Nivre and McDonald, 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Table 4: Parsing accuracies on the development data. 1or and 2or respectively denote first order and second order. LGLM(X) denotes a stacking model with X as the level"
I13-1021,W09-3825,0,0.0829788,"constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation, POS tagging, syntactic parsing in both constituency and dependency formalisms. In this paper, we use CTB 6.0 as the labeled training data for the study. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task (Hajiˇc et al., 2009), which i"
I13-1021,P06-1055,0,0.0399384,"tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chines"
I13-1021,D10-1001,0,0.0618766,"ve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of the HMMLA, PCFGLA and DEP to de184 added: w−1 , w, w+1 , w−1 w, w w+1 . The clusters are acquired based on the Chinese giga-word data with the MKCLS tool. The number of total clusters is set to 500, which is tuned by (Sun and Uszkoreit, 2012). 3.3 that it uses parser multiple times. We also implement their method and compare the results with our stacking model. We find the accuracy performance produced by the two different methods are comparable. (Rush et al., 2010) introduced dual decomposition as a framework for deriving inference algorithms for serious combinatorial problems in NLP. They successfully applied dual decomposition to the combination of a lexicalized parsing model and a trigram POS tagger. Despite the effectiveness, their method iteratively parses a sentence many times to achieve convergence, and thus is not as efficient as stacking. Evaluation Table 3 summarizes the tagging accuracy of different stacking models. From this table, we can clearly see that the new features derived from the outputs of other models lead to substantial improveme"
I13-1021,W02-1001,0,\N,Missing
I13-1021,J03-4003,0,\N,Missing
I13-1021,W09-1201,0,\N,Missing
J16-3001,P11-1048,0,0.0161427,"gnificantly improved by system combination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, strictly less informati"
J16-3001,D11-1031,0,0.059111,"Missing"
J16-3001,C10-1011,0,0.104622,"Missing"
J16-3001,P11-2121,0,0.013648,". Accordingly, their algorithm is specially designed to handle projective trees and two-planar trees, but not all graphs. Because many more crossing arcs exist in deep dependency structures and more sentences are assigned with neither planar nor two-planar graphs, their strategy of utilizing two stacks is not suitable for the deep dependency parsing problem. Different from their system, our new system maximizes the utility of two memory modules and is able to handle any directed graphs. The list-based systems, such as the basic one introduced by Nivre (2008) and the extended one introduced by Choi and Palmer (2011), also use two memory modules. The function of the secondary memory module of their systems and ours is very different. In our design, only nodes involved in a subgraph that contains crossing arcs may be put into the second stack. In the existing list-based systems, both lists are heavily used, and nodes may be transferred between them many times. The function of the two lists is to simulate one memory module that allows accessing any unit in it. 2.5 Extension 2.5.1 Graphs with Loops. It is easy to extend our system to generate arbitrary directed graphs by adding a new transition: r S ELF -A R"
J16-3001,J07-4004,0,0.188421,"Missing"
J16-3001,P02-1042,0,0.321502,"Missing"
J16-3001,W02-1001,0,0.800236,"k is about the model diversity obtained by the heterogeneous design of transition systems for general graph spanning. Empirical evaluation indicates that statistical parsers built upon our new transition systems as well as the existing best transition system—namely, Titov et al. (2009)’s system ( THMM, hereafter)—exhibit complementary parsing strengths, which benefit system combination. In order to take advantage of this model diversity, we propose a simple yet effective ensemble model to build a better hybrid system. We implement statistical parsers using the structured perceptron algorithm (Collins 2002) for transition classification and use a beam decoder for global inference. Concerning the disambiguation problem, we introduce two new techniques, namely, transition combination and tree approximation, to improve parsing quality. To increase system coverage, the A RC transitions designed by the THMM as well as our systems do not change the nodes in the stack nor buffer in a configuration: Only the nodes linked to the top of the stack or buffer are modified. Therefore, features derived from the configurations before and after an A RC transition are not distinct enough to train a good classifie"
J16-3001,P04-1015,0,0.0712844,"on of the maximization is extremely hard without any assumption of φ. Even with a proper φ for real-word parsing, exact decoding is still impractical for most practical feature designs. In this article, we follow the recent success of using beam search for approximate decoding. During parsing, the parser keeps track of multiple yet a fixed number of partial outputs to avoid making decisions too early. Training a parser in the discriminative setting corresponds to estimating θ associated with rich features. Previous research on dependency parsing shows that structured perceptron (Collins 2002; Collins and Roark 2004) is one of the strongest learning algorithms. In all experiments, we use the averaged perceptron algorithm with early update to estimate parameters. The whole parser is very similar to the transition-based system introduced in Zhang and Clark (2008, 2011b). 3.2 Transition Combination In either THMM, SS , or S2S , the L EFT /R IGHT-A RC transition does not modify either the stack or the buffer. Only new edges are added to the target graph. When automatic classifiers are utilized to approximate an oracle, a majority of features for predicting an A RC transition will be overlapped with the featur"
J16-3001,P15-1149,1,0.838524,"Missing"
J16-3001,S14-2080,1,0.843481,"ly less information is used. This result demonstrates the effectiveness of data-driven approaches to the deep linguistic processing problem. Compared to other types of data-driven parsers, our individual parser achieves equivalent performance to and our hybrid parser obtains slightly better results than factorization parsers based on dual decomposition (Martins and Almeida 2014; Du, Sun, and Wan 2015). This result highlights the effectiveness of the lightweight, transitionbased approach. Parsers based on the two new transition systems have been utilized as base components for parser ensemble (Du et al. 2014) for SemEval 2014 Task 8 (Oepen et al. 2014). Our hybrid system obtained the best overall performance of the closed track of this shared task. In this article, we re-implement all models, calibrate features more carefully, and thus obtain improved accuracy. The idea to extract tree-shaped backbone from a deep dependency graph has also been used to design other types of parsing models in our early work (Du et al. 2014, 2015; Du, Sun, and Wan 2015). Nevertheless, the idea to train a pseudo tree parser to serve a transition-based graph parser is new. The implementation of our parser is available"
J16-3001,S15-2154,1,0.797819,"Missing"
J16-3001,P10-1151,0,0.667255,"ows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been show"
J16-3001,J13-4002,0,0.0360302,"Missing"
J16-3001,W09-1201,0,0.0990053,"Missing"
J16-3001,J13-4006,0,0.155892,"ge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study on applying transition-based methods to generate HPSG-style predicate–argument structures, and have obtained competitive results. Furthermore, Titov et al. (2009) and Henderson et al. (2013) have shown that more general graphs rather than planars can be produced by augmenting existing transition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency a"
J16-3001,J07-3004,0,0.0745068,"y structures, for example, CCG-grounded functor–argument analysis and HPSG-grounded reduced minimal recursion semantics (MRS; Copestake et al. 2005) analysis, we find that syntactic trees can provide very helpful features. In case the syntactic information is not available, we introduce a tree approximation technique to induce tree backbones from deep dependency graphs. Such tree backbones can be utilized to train a tree parser which provides pseudo tree features. To evaluate transition-based models for deep dependency parsing, we conduct experiments on CCG-grounded functor–argument analysis (Hockenmaier and Steedman 2007; Tse and Curran 2010), LFG-grounded grammatical relation analysis (Sun et al. 2014), and HPSG-grounded semantic dependency analysis (Miyao, Ninomiya, and ichi Tsujii 2004; Ivanova et al. 2012) for English and Chinese. Empirical evaluation indicates some non-obvious facts: 1. Data-driven models with appropriate transition systems and disambiguation techniques can produce high-quality deep dependency analysis, comparable to more complex grammar-driven models. 355 Computational Linguistics Volume 42, Number 3 2. Parsers built upon heterogeneous transition systems and decoding orders have complem"
J16-3001,P10-1110,0,0.116738,"Missing"
J16-3001,D10-1002,0,0.200019,"Missing"
J16-3001,W12-3602,0,0.630056,"2000), lexical-functional grammar (LFG; Bresnan and Kaplan 1982) and head-driven phrase structure grammar (HPSG; Pollard and Sag 1994), are able to produce rich linguistic information encoded as bilexical dependencies. Under CCG, this is done by relating the lexical heads of functor categories and their arguments (Clark, Hockenmaier, and Steedman 2002). Under LFG, bilexical grammatical relations can be easily derived as the backbone of F-structures (Sun et al. 2014). Under HPSG, predicate–argument structures (Miyao, Ninomiya, and ichi Tsujii 2004) or reduction of minimal recursion semantics (Ivanova et al. 2012) can be extracted from typed feature structures corresponding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations between word “people” and conjuncts “fight,” “eat,” as well as “drink.” Automatically building deep dependency structures is desirable for many practical"
J16-3001,P10-1001,0,0.071173,"Missing"
J16-3001,P13-1008,0,0.0289887,"Missing"
J16-3001,S14-2082,0,0.178503,"bination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, strictly less information is used. This result dem"
J16-3001,E06-1011,0,0.554556,"Missing"
J16-3001,J11-1007,0,0.0384152,"th of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. Another concern of our work"
J16-3001,P08-1006,0,0.0695221,"nding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations between word “people” and conjuncts “fight,” “eat,” as well as “drink.” Automatically building deep dependency structures is desirable for many practical NLP applications, for example, information extraction (Miyao et al. 2008) and question answering (Reddy, Lapata, and Steedman 2014). Traditionally, deep dependency graphs are generated as a by-product of grammar-guided parsers. The challenge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study o"
J16-3001,J08-1002,0,0.117885,"rsing quality can be significantly improved by system combination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, st"
J16-3001,J08-4003,0,0.710277,"lation r from head wi to dependent wj . A dependency graph G is thus a set of labeled dependency relations between the root and the words of x. To simplify the description in this section, we mainly consider unlabeled parsing and assume the relation set R is a singleton. Or, taking it another way, we assume A ⊆ V × V. It is straightforward to adapt the discussions in this article for labeled parsing. To do so, we can parameterize transitions with possible dependency relations. For empirical evaluation as discussed in Section 5, we will test both labeled and unlabeled parsing models. Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct ), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of dependency arcs, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration with β = [1, . . . , n], 4. Ct ⊆ C is a set of terminal configurations. Given a sentence x = w1 , . . . , wn and a graph G = (V, A) on it, if there is a sequence of transitions t1 , . . . , tm and a sequence of configurations"
J16-3001,P09-1040,0,0.1007,"nsition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic"
J16-3001,P08-1108,0,0.0930353,"Missing"
J16-3001,P05-1013,0,0.0991563,"Missing"
J16-3001,S14-2008,0,0.214654,"demonstrates the effectiveness of data-driven approaches to the deep linguistic processing problem. Compared to other types of data-driven parsers, our individual parser achieves equivalent performance to and our hybrid parser obtains slightly better results than factorization parsers based on dual decomposition (Martins and Almeida 2014; Du, Sun, and Wan 2015). This result highlights the effectiveness of the lightweight, transitionbased approach. Parsers based on the two new transition systems have been utilized as base components for parser ensemble (Du et al. 2014) for SemEval 2014 Task 8 (Oepen et al. 2014). Our hybrid system obtained the best overall performance of the closed track of this shared task. In this article, we re-implement all models, calibrate features more carefully, and thus obtain improved accuracy. The idea to extract tree-shaped backbone from a deep dependency graph has also been used to design other types of parsing models in our early work (Du et al. 2014, 2015; Du, Sun, and Wan 2015). Nevertheless, the idea to train a pseudo tree parser to serve a transition-based graph parser is new. The implementation of our parser is available at http://www.icst.pku.edu.cn/ lcwm/grass. 2"
J16-3001,J05-1004,0,0.185242,"Missing"
J16-3001,J08-2005,0,0.11006,"Missing"
J16-3001,Q14-1030,0,0.061084,"Missing"
J16-3001,N06-2033,0,0.036496,"and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run"
J16-3001,C08-1095,0,0.146021,"xample, information extraction (Miyao et al. 2008) and question answering (Reddy, Lapata, and Steedman 2014). Traditionally, deep dependency graphs are generated as a by-product of grammar-guided parsers. The challenge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study on applying transition-based methods to generate HPSG-style predicate–argument structures, and have obtained competitive results. Furthermore, Titov et al. (2009) and Henderson et al. (2013) have shown that more general graphs rather than planars can be produced by augmenting existing transition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCG"
J16-3001,P14-1042,1,0.903861,". 1. Introduction The derivations licensed by a grammar under deep grammar formalisms, for example, combinatory categorial grammar (CCG; Steedman 2000), lexical-functional grammar (LFG; Bresnan and Kaplan 1982) and head-driven phrase structure grammar (HPSG; Pollard and Sag 1994), are able to produce rich linguistic information encoded as bilexical dependencies. Under CCG, this is done by relating the lexical heads of functor categories and their arguments (Clark, Hockenmaier, and Steedman 2002). Under LFG, bilexical grammatical relations can be easily derived as the backbone of F-structures (Sun et al. 2014). Under HPSG, predicate–argument structures (Miyao, Ninomiya, and ichi Tsujii 2004) or reduction of minimal recursion semantics (Ivanova et al. 2012) can be extracted from typed feature structures corresponding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations betw"
J16-3001,Q13-1025,1,0.876415,"Missing"
J16-3001,W08-2121,0,0.158669,"Missing"
J16-3001,N10-1091,0,0.123101,"phs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. Another concern of our work is about the model diversity obtained by the"
J16-3001,D09-1058,0,0.0641133,"Missing"
J16-3001,P09-1039,0,0.451961,"Missing"
J16-3001,D08-1017,0,0.0819892,"Missing"
J16-3001,C10-1122,0,0.601432,"aging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been show"
J16-3001,N12-1030,0,0.124466,"Missing"
J16-3001,P15-1032,0,0.0507278,"Missing"
J16-3001,P14-1021,0,0.0275392,"Missing"
J16-3001,W03-3023,0,0.606489,"Missing"
J16-3001,D09-1161,0,0.0305667,"nsition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. An"
J16-3001,D08-1059,0,0.0498215,"ch for approximate decoding. During parsing, the parser keeps track of multiple yet a fixed number of partial outputs to avoid making decisions too early. Training a parser in the discriminative setting corresponds to estimating θ associated with rich features. Previous research on dependency parsing shows that structured perceptron (Collins 2002; Collins and Roark 2004) is one of the strongest learning algorithms. In all experiments, we use the averaged perceptron algorithm with early update to estimate parameters. The whole parser is very similar to the transition-based system introduced in Zhang and Clark (2008, 2011b). 3.2 Transition Combination In either THMM, SS , or S2S , the L EFT /R IGHT-A RC transition does not modify either the stack or the buffer. Only new edges are added to the target graph. When automatic classifiers are utilized to approximate an oracle, a majority of features for predicting an A RC transition will be overlapped with the features for the successive transition. Empirically, this property significantly decreases the parsing accuracy. A key observation of a linguistically motivated bilexical graph is that there is usually at most one edge between any two words, therefore an"
J16-3001,P11-1069,0,0.0751119,"Missing"
J16-3001,J11-1005,0,0.0682425,"Missing"
J16-3001,P11-2033,0,0.0719523,"Missing"
J16-3001,C10-1153,0,0.0379597,"Missing"
J16-3001,P13-1007,0,\N,Missing
J16-3001,N15-1006,0,\N,Missing
J16-3001,oepen-lonning-2006-discriminant,0,\N,Missing
J16-3002,D15-1159,0,0.0224378,"and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw texts to define new features for disambi"
J16-3002,D12-1133,0,0.0206208,"r, and Wang (2007) and Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw te"
J16-3002,J92-4003,0,0.254186,"s partially resolved by exploring the ability of discriminative learning to automatically identify the correspondence between the two types of “word classes.” In the literature, contexts have been defined as subjective and objective relations involving the word, as the documents containing the word, or as search engine snippets for the word as a query. We derive new features for POS tagging by applying two distributional clustering methods, which both take into account surrounding words as contexts. Brown Clustering. Our first choice is the bottom–up agglomerative word clustering algorithm of Brown et al. (1992), which derives a hierarchical clustering of words from unlabeled data. This algorithm generates a hard clustering—each word belongs to exactly one cluster. The input to the algorithm is sequences of words w1 , ..., wn . Initially, the algorithm starts with each word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two clusters that maximize the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. P(wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) (7) where the function C maps a wor"
J16-3002,A00-2018,0,0.339526,"two syntax-free sequential taggers and a state-of-the-art syntax-based parser, aiming at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing"
J16-3002,D14-1082,0,0.0555118,"s state-of-the-art for both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies a"
J16-3002,P15-1168,0,0.11677,"95.32 95.34 that such discriminative learning method achieves state-of-the-art for both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based sys"
J16-3002,W02-1001,0,0.845637,"Harper, and Wang 2007; 393 Computational Linguistics Volume 42, Number 3 Huang, Eidelman, and Harper 2009; Li et al. 2011). In this section, we give a brief introduction and a comparative analysis to several models that have been recently designed to resolve the Chinese POS tagging problem. 2.1 State-of-the-Art Tagging Models 2.1.1 Linear-Chain Global Linear Model (LGLM). All state-of-the-art English POS taggers are based on discriminative sequence labeling models—for example, maximum entropy (Toutanova et al. 2003), support vector machines (Gim´enez and M`arquez 2004), structure perceptron (Collins 2002; Shen, Satta, and Joshi 2007; Huang, Fayong, and Guo 2012), and conditional random fields (Sun 2014). A discriminative learner can be easily extended with arbitrary features and is therefore suitable to recognize more new words. Moreover, a majority of the POS tags are locally dependent on each other, so the Markov assumption can well capture the syntactic relations among words. A majority of discriminative POS taggers utilize Global Linear Models (GLMs) for learning and prediction. A GLM represents the sequence labeling task through a feature-vector representation of the whole observation an"
J16-3002,J03-4003,0,0.167651,"e sequential taggers and a state-of-the-art syntax-based parser, aiming at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing Tagging and Par"
J16-3002,P14-1129,0,0.0378166,"e between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies are all evaluated on CTB, but different training and test data sets"
J16-3002,gimenez-marquez-2004-svmtool,0,0.135284,"Missing"
J16-3002,I11-1136,0,0.0509337,"Missing"
J16-3002,N12-1015,0,0.0605964,"Missing"
J16-3002,P10-1110,0,0.202014,"Missing"
J16-3002,N09-2054,0,0.0530183,"Missing"
J16-3002,D07-1117,0,0.0830749,"Missing"
J16-3002,P08-1068,0,0.0942449,"Missing"
J16-3002,D11-1109,0,0.258362,"and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of our experimental settings. To deeply analyze the POS tagging problem for Chinese, we"
J16-3002,C12-1106,0,0.0177921,"Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw texts to define ne"
J16-3002,P15-1167,0,0.0409363,"Missing"
J16-3002,P05-1010,0,0.067805,"Missing"
J16-3002,N04-1043,0,0.155293,"Missing"
J16-3002,W04-3236,0,0.0814478,"Missing"
J16-3002,P08-1108,0,0.0290476,"ely trained in their solution. In other words, one key difference is whether to allow integration of base models at learning time. Second, the application of the decomposition technique is dependent on the solvability of sub-problems. This technique, therefore, is not as flexible as stacking. 4.4.1 Stacked Learning. Stacked generalization is a meta-learning algorithm that was first proposed in Wolpert (1992) and Breiman (1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun 2011), and dependency parsing (Nivre and McDonald 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Training is done as follows. The training data S = {(xt , yt ) : t ∈ [1, T]} are split into L equal-sized disjoint subsets S1 , ..., SL . Then functions g1 , ..., gL ("
J16-3002,E99-1010,0,0.056434,"...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) (7) where the function C maps a word w to its class C(w). We use a publicly available package2 (Liang, Collins, and Liang 2005) to train this model. MKCLS Clustering. We also do experiments by using another popular clustering method based on the exchange algorithm (Kneser and Ney 1993). The objective function is Q maximizing the likelihood ni=1 P(wi |w1 , ..., wi−1 ) of the training data given a partially class-based bigram model of the form P(wi |w1 , ...wi−1 ) ≈ p(C(wi )|wi−1 )p(wi |C(wi )) (8) We use the publicly available implementation MKCLS3 (Och 1999) to train this model. One downside of both Brown and MKCLS clustering is that they are based solely on bigram statistics, and do not consider word usage in a wider context. We choose to work with these two algorithms considering their prior success in other NLP applications. However, we expect that our approach can function with other clustering algorithms. 2 http://cs.stanford.edu/~pliang/software/brown-cluster-1.2.zip. 3 http://code.google.com/p/giza-pp/. 400 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 3.1.2 Data. Chinese Gigaword is a comprehensive archive of newswire tex"
J16-3002,P06-1055,0,0.0968083,"at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing Tagging and Parsing From a linguistic view, we can distinguish syntax-free and syntax-based"
J16-3002,D10-1069,0,0.0508594,"Missing"
J16-3002,N07-1051,0,0.0225375,"as “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1.” The objective of training a symbol-refined bigram tagger is to solve the LA-involved emission and transition parameters by maximizing the likelihood of the training data. In contrast with a non-symbol-refined HMM tagger, where the POS tags are observed, the latent annotations are unseen variables. In order to learn these parameters, a variant of EM algorithm is used. The objective function used for decoding is: yˆ[1:n] = arg max y[1:n] nY −1 P(yi , yi+1 |x[1:n] ) (5) i=1 This goal function is a variant of the MAX-RULE-PRODUCT algorithm in Petrov and Klein (2007), which maximizes the product of rule posteriors. This algorithm is not probabilistically correct but follows the instinct of choosing the tree with the greatest chance of having all rules correct. Similarly, the goal function used in the SR-HMM POS tagger tries to find the tag sequence with the greatest chance of having all bigrams correct. The bigram tag posterior is calculated by marginalizing out the latent annotations in the bigram latent tag posterior. P(yi = t, yi+1 = s|x[1:n] ) = X X P(ta , sb |x[1:n] ) (6) ta ∈S(t) sb ∈S(s) 395 Computational Linguistics Volume 42, Number 3 Table 1 Tra"
J16-3002,D10-1001,0,0.0296712,"ing when applying the C&C parser (Tse and Curran 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertagging. 4.4 Enhancing Tagging via Stacking We study a simple way of integrating multiple heterogeneous models in order to exploit their complementary strengths and thereby improve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of SR-HMM and the parser to define features for the LLM/LGLM. Similar to our work on combining a sequence model and a parser, Rush et al. (2010) proposed a principled decoding technique based on dual decomposition to take advantages of heterogeneous models. There are two differences between their model and ours. First, the base models for combining are separately trained in their solution. In other words, one key difference is whether to allow integration of base models at learning time. Second, the application of the decomposition technique is dependent on the solvability of sub-problems. This technique, therefore, is not as flexible as stacking. 4.4.1 Stacked Learning. Stacked generalization is a meta-learning algorithm that was fir"
J16-3002,P07-1096,0,0.0928353,"Missing"
J16-3002,P12-1046,0,0.0220779,"ve method. However, they did not deeply analyze the problem from a linguistic view. The global linear algorithm we adopt in this article is averaged perceptron (Collins 2002). 394 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 2.1.2 Symbol-Refined Hidden Markov Model (SR-HMM). Generative models with latent annotations (LAs) obtain state-of-the-art performance for a number of NLP tasks. For example, both context-free Grammar (CFG) and tree-substitution grammar (TSG) with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki, Miyao, and Tsujii 2005; Shindo et al. 2012). For Chinese POS tagging, Huang, Eidelman, and Harper (2009) described and evaluated a bigram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. An HMM POS tagger models the joint distribution of the observation sequence x[1:n] and the tag sequence y[1:n] . Under the first-order Markov assumption, the inference problem can be computed as: yˆ[1:n] = arg max P(x[1:n] , y[1:n] ) y[1:n] = arg max y[1:n] n Y (4) P(yi |yi−1 )P(xi |yi )"
J16-3002,C10-2139,1,0.832536,"news text, that is, Xinhua newswire. These data cover all news published by Xinhua News Agency (the largest news agency in China) from 1991 to 2004, which contains over 473 million characters. 3.1.3 Pre-processing: Word Segmentation. Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units (i.e., words), segmentation is a necessary pre-processing step for word clustering. Our previous research showed that character-based segmentation models trained on labeled data are reasonably accurate (Sun 2010). In this work, we use a supervised segmenter introduced in Sun and Xu (2011) to process raw texts. 3.2 Improving Tagging with Cluster Features Our discriminative sequential tagger is easy to be extended with arbitrary features and therefore suitable to explore additional features derived from other sources. We propose using word clusters as substitutes for word forms to assist the POS tagger. We are relying on the ability of the discriminative learning method to explore informative features, which play central role to boost the tagging performance. Five clustering-based features are added: r"
J16-3002,P11-1139,1,0.9335,"] ) = n X φ(hi , yi ) (2) i=1 where hi is the history correlated with yi . Using this local representation, we can use the Viterbi algorithm for inference, which finds the optimal tag sequence yˆ[1:n] that maximizes the following score: yˆ[1:n] = arg max y[1:n] n X wφ(hi , yi ) (3) i=1 Discriminative learning is also an appropriate solution for Chinese POS tagging, because of its flexibility to include knowledge from multiple linguistic sources. Tseng, Jurafsky, and Manning (2005) introduced a maximum entropy–based model, which includes morphological features for unknown word recognition, and Sun (2011) studied the joint word segmentation and POS tagging problem and developed a fully discriminative method. However, they did not deeply analyze the problem from a linguistic view. The global linear algorithm we adopt in this article is averaged perceptron (Collins 2002). 394 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 2.1.2 Symbol-Refined Hidden Markov Model (SR-HMM). Generative models with latent annotations (LAs) obtain state-of-the-art performance for a number of NLP tasks. For example, both context-free Grammar (CFG) and tree-substitution grammar (TSG) with refined latent"
J16-3002,I13-1021,1,0.857765,"Missing"
J16-3002,C08-1105,1,0.860064,"Missing"
J16-3002,D11-1090,1,0.958956,"rious features can be drawn upon information sources such as word forms and characters that constitute words. Previous study on many languages shows that local classification is inadequate to capture structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project,"
J16-3002,D08-1017,0,0.0796353,"Missing"
J16-3002,N03-1033,0,0.102768,"Missing"
J16-3002,N12-1030,0,0.291408,"gories. The numbers presented in the bottom block of Table 12 give a rough illustration. The results are obtained by providing the parser mixed POS tagging analysis: The tags of “的/得” predicted by the Berkeley parser and the tags of other words are utilized. We can see that though the overall parsing quality is still worse than Berkeley parser, it is better than sequence models. The performance change demonstrates the importance of prediction of these two particular words. Our linguistic analysis can also better explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertagging. 4.4 Enhancing Tagging via Stacking We study a simple way of integrating multiple heterogeneous models in order to exploit their complementary strengths and thereby improve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of SR-HMM and the parser to define features for the LLM/LGLM. Similar to our work on combining a sequence model and a parser, Rush et al. (2010) proposed a principled decoding tech"
J16-3002,I05-3005,0,0.0447556,"s designed for English have been applied to many other languages as well. In some cases, the methods work well without large modifications, such as for German POS tagging. But a number of augmentations and changes became necessary when dealing with highly inflected or agglutinative languages, as well as analytic languages, of which Chinese is the focus of this article. Both discriminative and generative models are explored for accurate Chinese POS tagging (Ng and Low 2004; Tseng, Jurafsky, and Manning 2005; Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009). Ng and Low (2004) and Tseng et al. (2005) introduced a maximum entropy–based model, which includes morphological features for unknown word recognition. Huang, Harper, and Wang (2007) and Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM mode"
J16-3002,P06-1054,0,0.324867,"Missing"
J16-3002,P15-1032,0,0.0289012,"both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies are all evaluated on"
J16-3002,D08-1059,0,0.086811,"re structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of our experimental settings. To deeply analyze t"
J16-3002,W09-3825,0,0.500508,"hat local classification is inadequate to capture structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of"
J16-3002,J11-1005,0,0.0692174,". Such pseudo training data could be of very largescale theoretically and practically. Together with gold standard training data, large-scale pseudo training data can be used to train SR-HMMs and LGLMs. We expect that the SR-HMM tagger can be improved by exploring better latent variables, and that the discriminative taggers can be improved by using features in a larger context. 5.3 Beam Decoding for LGLM For a number of NLP tasks, including tagging and parsing, the generic beam-search algorithmic technique has been shown to be very powerful to build efficient systems with comparable accuracy (Zhang and Clark 2011). In our model re-compilation case, to train a second order LGLM on a very large data set is quite time-consuming. Rather than the Viterbi algorithm, we here use the beam search algorithm for decoding. Beyond simple beam decoding that essentially implements the greedy search strategy, Huang and Sagae (2010) discuss how the state-merging strategy that is used by dynamic programming methods can be applied to enhance a beam decoder. Considering that the total number of possible tags is much larger than conventional tagging, we implement a beam-search algorithm with state merging for our discrimin"
J16-3002,D13-1061,0,0.057421,"Missing"
J16-3002,P12-1026,1,\N,Missing
J16-3002,I05-3027,0,\N,Missing
J19-1003,P16-1231,0,0.143503,"based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and Manning (2014) made the first successful attempt at incorporating deep learning into a transition-based dependency parser. A number of other researchers have attempted to address some limitations of their parser by augmenting it with additional complexity: Later it was enhanced with a more principled decoding algorithm, namely beam search, as well as a conditional random field loss objective function (Weiss et al. 2015; Watanabe and Sumita 2015; Zhou et al. 2015; Andor et al. 2016). A graph-based system explicitly parameterizes"
J19-1003,W13-2322,0,0.014542,"stic graphs beyond trees, we propose a new approach, namely graph merging, by constructing GR graphs from subgraphs. There are two key issues involved in the approach, that is, graph decomposition and merging. To solve these two problems in a principled way, we treat both problems as optimization problems and employ combinatorial optimization techniques. Experiments demonstrate the effectiveness of the graph merging framework, which can be adopted to other types of flexible representations, for example, semantic dependency graphs (Oepen et al. 2014, 2015) and abstract meaning representations (Banarescu et al. 2013). The study is significant in demonstrating a linguistically motivated and computationally effective way of parsing Chinese texts. Appendix In annotations of dependency structures, typical grammatical relations are subject, object, etc., which imply the role the dependent plays with regard to its head. In addition to phrasal categories, CTB also has functional labels to represent relations. The CTB 131 Computational Linguistics Volume 45, Number 1 Table 16 Illustration of major dependency relations in our corpus. ADV AMOD AUX COMP DET LOC NMOD OBJ PRT QUANT RELATIVE SUBJ TMP adverbial adjunct"
J19-1003,D11-1037,0,0.156954,"an essential role in boosting the parsing performance. It is generally believed that the representation format for parser outputs may greatly affect its impact on applications. Predicate–argument structures extracted from deep parses have been shown very helpful for NLP tasks such as information extraction (Miyao et al. 2008; Yakushiji et al. 2005). Partly due to their importance in information processing, deep dependencies are the de facto standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize tr"
J19-1003,C10-1011,0,0.0122984,"ier u in our algorithm can be regarded as additional weights for the score function. The update of u is to increase weights to the edges that are not covered by any tree-like subgraph, so it is more likely for them to be selected in the next iteration. 4.3 Graph Merging As explained above, the extraction algorithm gives three classes of trees for each graph. The algorithm is applied to the graph training set to deliver three training tree sets. After that, three parsing models can be trained with the three tree sets. The parsers used in this study to train models and parse trees include Mate (Bohnet 2010), a second-order graph-based dependency parser, and our implementation of the first-order factorization model proposed in Kiperwasser and Goldberg (2016). If the scores used by the three models are f1 , f2 , f3 , respectively, then the parsers can find trees with the highest scores for a sentence. That solves the following optimization problems: arg maxg 1 f1 (gg1 ), arg maxg 2 f2 (gg2 ), and arg maxg 2 f3 (gg3 ). We can 115 Computational Linguistics Volume 45, Number 1 parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs. We combine them tog"
J19-1003,P06-2006,0,0.0749123,"Missing"
J19-1003,D14-1082,0,0.0659172,"hich are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do not work well for coordinations, long-range dependencies i"
J19-1003,P07-1032,0,0.423293,"syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Ho"
J19-1003,J07-4004,0,0.602185,"syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Ho"
J19-1003,C96-1058,0,0.897898,"raph as well as cover all edges in y . The optimization problem can be written as max. s1 (gg1 ) + s2 (gg2 ) + s3 (gg3 ) s.t. g 1 , g 2 , g 3 are tree-like g 1 (i, j) + g 2 (i, j) + g 3 (i, j) ≥ y (i, j), ∀i, j Scoring a Subgraph. We score a subgraph in a first order arc-factored way, which first scores theP edges separately and then adds up the scores. Formally, the score function is sk (gg ) = ωk (i, j)ggk (i, j) (k = 1, 2, 3) where ωk (i, j) is the score of the edge from i to j. Under this score function, we can use the Maximum Spanning Tree (MST) algorithm (Chu and Liu 1965; Edmonds 1967; Eisner 1996) to decode the tree-like subgraph with the highest score. After the score function is defined, extracting a subgraph from a GR graph works in the following way: We first assign heuristic weights ωk (i, j) (1 ≤ i, j ≤ n) to the potential edges between all the pairs of words, then compute a best projective tree g k using the Eisner’s Algorithm (Eisner 1996): g k = arg max sk (gg ) = arg max g g X ωk (i, j)gg (i, j). g k is not exactly a subgraph of y , because there may be some edges in the tree but not in the graph. To guarantee a meaningful subgraph of the original graph, we add labels to the"
J19-1003,C12-1059,0,0.183579,"nodes to the secondary memory module, namely λ0 . Another key property of the oracle is building arcs as soon as possible to avoid further complication. 5.3 Bi-LSTM Based Scorer The neural model, which acts as a classifier of actions in this transition system, is similar to previous neural models. The Bi-LSTMs play the same role, but the feature vectors of the front of the buffer and the top of list λ are used to assign scores for actions. The structure is shown in Figure 16. W 1 · (rrlistλtop ⊕ r bufferfront ) + b 1 ) + b 2 Scores = W 2 · ReLU (W Two search methods, that is, dynamic oracle (Goldberg and Nivre 2012) and beam search, can be used with this transition system. 122 Sun et al. Parsing Chinese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate"
J19-1003,J13-4006,0,0.0728661,"Missing"
J19-1003,J07-3004,0,0.26688,"and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last de"
J19-1003,P10-1110,0,0.0606884,"Missing"
J19-1003,N04-1013,0,0.147685,"Missing"
J19-1003,W03-2401,0,0.435763,"machine translation (Oepen et al. 2007; Wu, Matsuzaki, and Tsujii 2010) and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing ha"
J19-1003,Q16-1023,0,0.109372,"es that are not covered by any tree-like subgraph, so it is more likely for them to be selected in the next iteration. 4.3 Graph Merging As explained above, the extraction algorithm gives three classes of trees for each graph. The algorithm is applied to the graph training set to deliver three training tree sets. After that, three parsing models can be trained with the three tree sets. The parsers used in this study to train models and parse trees include Mate (Bohnet 2010), a second-order graph-based dependency parser, and our implementation of the first-order factorization model proposed in Kiperwasser and Goldberg (2016). If the scores used by the three models are f1 , f2 , f3 , respectively, then the parsers can find trees with the highest scores for a sentence. That solves the following optimization problems: arg maxg 1 f1 (gg1 ), arg maxg 2 f2 (gg2 ), and arg maxg 2 f3 (gg3 ). We can 115 Computational Linguistics Volume 45, Number 1 parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs. We combine them together to obtain the graph parse of the sentence by putting all the edges in the three subgraphs together. That is to say, graph y = max{t2g(gg1 ), t2g(gg"
J19-1003,P10-1001,0,0.0323173,"Missing"
J19-1003,D10-1125,0,0.458865,"s represent various grammatical relations (GRs), which are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do no"
J19-1003,D11-1109,0,0.0273462,"VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical dependencies, which are defined as hwh , wd , li tuples, where wh is the head, wd is the de"
J19-1003,P05-1012,0,0.801363,"standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and"
J19-1003,E06-1011,0,0.413161,"Missing"
J19-1003,H05-1066,0,0.706592,"standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and"
J19-1003,P08-1006,0,0.219536,"Structure Grammar (HPSG) (Pollard and Sag [1994]), and Tree-adjoining Grammar (TAG) (Joshi and Schabes [1997]). These theories provide not only the description of the syntactic structures, but also the ways in which meanings are composed, and thus parsing in these formalisms provides an elegant way to simultaneously obtain both syntactic and semantic analyses, generating valuable and richer linguistic information. Deep linguistic annotators and processors are strongly demanded in NLP applications, such as machine translation (Oepen et al. 2007; Wu, Matsuzaki, and Tsujii 2010) and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) int"
J19-1003,J08-1002,0,0.539472,"bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Hockenmaier and Steedman 2007) and statistical deep parsing (Clark and Curran 2007b; Miyao and Tsujii 2008) have been well studied for English. Statistical models that are estimated on large-scale treebanks play an essential role in boosting the parsing performance. It is generally believed that the representation format for parser outputs may greatly affect its impact on applications. Predicate–argument structures extracted from deep parses have been shown very helpful for NLP tasks such as information extraction (Miyao et al. 2008; Yakushiji et al. 2005). Partly due to their importance in information processing, deep dependencies are the de facto standard to evaluate deep parsers (Kaplan et al. 9"
J19-1003,J08-4003,0,0.620576,"Volume 45, Number 1 labeled directed graph in the standard graph-theoretic sense and consists of nodes, V, and arcs, A, such that for sentence x = w0 w1 . . . wn and label set R the following holds: • V = {0, 1, . . . , n}, • A ⊆ V × V × R. The vertex −1 denotes a virtual root. The arc set A represents the labeled dependency relations of the particular analysis G. Specifically, an arc (wi , wj , r) ∈ A represents a dependency relation from head wi to dependent wj labeled with relation type r. A dependency graph G is thus a set of labeled dependency relations between the words of x. Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs , Ct ), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of arcs, 2. T is a set of transitions, each of which is a (partial) function t : C 7→ C, 3. cs is an initialization function, mapping a sentence x to a configuration, with β = [0, . . . , n], and 4. Ct ⊆ C is a set of terminal configurations. Given a sentence x = w1 , . . . , wn and a graph G = (V, A) on it, if there is a sequence of transitions t1 , . . . , tm and a sequence of configurations c0 ,"
J19-1003,W04-2407,0,0.247099,"Missing"
J19-1003,S15-2153,0,0.050761,"Missing"
J19-1003,S14-2008,0,0.0140807,"tituency and dependency formalisms. To construct complex linguistic graphs beyond trees, we propose a new approach, namely graph merging, by constructing GR graphs from subgraphs. There are two key issues involved in the approach, that is, graph decomposition and merging. To solve these two problems in a principled way, we treat both problems as optimization problems and employ combinatorial optimization techniques. Experiments demonstrate the effectiveness of the graph merging framework, which can be adopted to other types of flexible representations, for example, semantic dependency graphs (Oepen et al. 2014, 2015) and abstract meaning representations (Banarescu et al. 2013). The study is significant in demonstrating a linguistically motivated and computationally effective way of parsing Chinese texts. Appendix In annotations of dependency structures, typical grammatical relations are subject, object, etc., which imply the role the dependent plays with regard to its head. In addition to phrasal categories, CTB also has functional labels to represent relations. The CTB 131 Computational Linguistics Volume 45, Number 1 Table 16 Illustration of major dependency relations in our corpus. ADV AMOD AUX"
J19-1003,2007.tmi-papers.18,0,0.0195122,"Missing"
J19-1003,N18-1202,0,0.289677,"e proposed graph decomposition and merging methods. 3. The transition-based parser can be trained with either the dynamic oracle or the beam search method. According to our implementations, the dynamic oracle method performs better. Coupled with dynamic oracle, the transition-based parser reaches a labeled f-score of 85.51 when gold POS tags are utilized. 4. Gold-standard POS tags play a very important role in achieving the above accuracies. However, the automatic tagging quality of state-of-the-art Chinese POS taggers are still far from satisfactory. To deal with this problem, we apply ELMo (Peters et al. 2018), a contextualized word embedding producer, to obtain adequate lexical information. Experiments show that ELMo is extremely effective in harvesting lexical knowledge from raw texts. With the help of the ELMo vectors but not any POS tagging results, the graph merging parser achieves a labeled f-score of 84.90. This is a realistic set-up to evaluate how accurate the GR parsers could be for real-world Chinese Language Processing applications. The current study expands on the earlier and preliminary results, which have been published in Sun et al. (2014) and Sun, Du, and Wan (2017). The new findin"
J19-1003,Q13-1002,0,0.0519761,"Missing"
J19-1003,D09-1085,0,0.0857498,"Missing"
J19-1003,C08-1095,0,0.308052,"Missing"
J19-1003,P14-1042,1,0.643032,"To deal with this problem, we apply ELMo (Peters et al. 2018), a contextualized word embedding producer, to obtain adequate lexical information. Experiments show that ELMo is extremely effective in harvesting lexical knowledge from raw texts. With the help of the ELMo vectors but not any POS tagging results, the graph merging parser achieves a labeled f-score of 84.90. This is a realistic set-up to evaluate how accurate the GR parsers could be for real-world Chinese Language Processing applications. The current study expands on the earlier and preliminary results, which have been published in Sun et al. (2014) and Sun, Du, and Wan (2017). The new findings and contributions in this submission include: (1) a detailed comparison between our 98 Sun et al. Parsing Chinese Sentences with Grammatical Relations GR analysis and other syntactic/semantic analyses, including Universal Dependency, Semantic Role Labeling, and LFG’s f-structure, (2) the design of a new neural graph merging parser, (3) the design of a new neural transition-based parser, and (4) more comprehensive evaluations and analyses of the neural parsing models. The implementation of the corpus conversion algorithm, a corpus visualization too"
J19-1003,K17-1005,1,0.860347,"Missing"
J19-1003,P12-1026,1,0.783037,"inese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure"
J19-1003,J16-3002,1,0.860014,"07b; Miyao and Tsujii 2008). Previous work on Chinese CCG and HPSG parsing unanimously agree that obtaining the deep analysis of Chinese is more challenging (Yu et al. 2011; Tse and Curran 2012). The successful C&C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state of the art of deep linguistic processing for Chinese lags very much behind. 6.4.3 Impact of POS Tagging. According to our previous study (Sun and Wan 2016), the use of different POS taggers has a great impact on syntactic analysis. This is highly related to a prominent language-specific property of Mandarin Chinese: as an analytic language, Mandarin Chinese lacks morphosyntactic features to explicitly indicate lexical categories. To evaluate the parsing performance in a more realistic setup, we report parsing results based on two different POS taggers introduced in Sun and Wan (2016). Table 9 presents the results. We can see that automatic POS tagging has a great impact on deep dependency parsing. Even when assisted with a state-of-the-art tagge"
J19-1003,D11-1090,1,0.896865,"stem. 122 Sun et al. Parsing Chinese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, accordi"
J19-1003,P09-1039,0,0.0970611,"Missing"
J19-1003,C10-1122,0,0.0298864,"rin Chinese. 2.3 Initial Research on Chinese Deep Linguistic Processing In the last few years, study on deep linguistic processing for Chinese has been initialized. Treebank annotation for individual formalisms is prohibitively expensive. To quickly construct deep annotations, corpus-driven grammar engineering has been developed. Phrase structure trees in CTB have been semi-automatically converted to deep derivations in the CCG (Tse and Curran 2010), LFG (Guo, van Genabith, and Wang 2007), and HPSG (Yu et al. 2010) formalisms. To semi-automatically extract a largescale HPSG grammar, Yu et al. (2010) defined a skeleton, including the structure of sign, grammatical principles, and schemata, based on which, the CTB trees are converted into HPSG-style trees. The treebank conversion under the CCG formalism is relatively easier. Tse and Curran (2010) followed the method proposed for English Penn Treebank (Hockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependen"
J19-1003,N12-1030,0,0.127842,"ockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependency-based model, which extracts functional information from dependency trees. With the use of converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran 2007b) and Enju (Miyao and Tsujii 2008), have been evaluated on the Chinese annotations (Yu et al. 2011; Tse and Curran 2012). Although the discriminative learning architecture of both C&C and Enju parsers makes them relatively easy to be adapted to solve multilingual parsing, their performance on Chinese sentences is far from satisfactory. Yu et al. (2011) and Tse and Curran (2012) analyze the challenges and difficulties in Chinese deep parsing. In particular, some language-specific properties account for a large number of errors. 3. Representing Deep Linguistic Information Using Dependency Graphs In this section, we discuss the construction of the GR annotations. Basically, the annotations are automatically conver"
J19-1003,P06-1054,0,0.119024,"Missing"
J19-1003,P15-1113,0,0.0641625,"Missing"
J19-1003,P15-1032,0,0.333835,"traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do not work well for coordinations, long-range dependencies involved in raising,"
J19-1003,P10-1034,0,0.0673123,"Missing"
J19-1003,W03-3023,0,0.471219,"Missing"
J19-1003,W11-2907,0,0.176594,"Penn Treebank (Hockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependency-based model, which extracts functional information from dependency trees. With the use of converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran 2007b) and Enju (Miyao and Tsujii 2008), have been evaluated on the Chinese annotations (Yu et al. 2011; Tse and Curran 2012). Although the discriminative learning architecture of both C&C and Enju parsers makes them relatively easy to be adapted to solve multilingual parsing, their performance on Chinese sentences is far from satisfactory. Yu et al. (2011) and Tse and Curran (2012) analyze the challenges and difficulties in Chinese deep parsing. In particular, some language-specific properties account for a large number of errors. 3. Representing Deep Linguistic Information Using Dependency Graphs In this section, we discuss the construction of the GR annotations. Basically, the annotations ar"
J19-1003,C10-2162,0,0.0658995,"Missing"
J19-1003,J16-3001,1,0.948956,"architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transition-based parsers with linear scoring models (Zhang et al. 2016), and the result obtained by a shift-reduce CCG parser (Zhang and Clark 2011a). The functor-argument dependencies generated by the CCG parser also has a relatively high precision but considerably low recall. To build NLP application, for example, information extraction, and systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. The graph merging system coupled with neural tree parser performs quite differently. The precision and recall are quite harmonious. Figure 17 and Table 8 present detailed a"
J19-1003,D08-1059,0,0.0400365,"mbedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical dependencies, which are defined as hwh , wd , li tup"
J19-1003,W09-3825,0,0.0245686,"Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical depend"
J19-1003,P11-1069,0,0.0243552,"5 88.36 20.72 20.72 20.66 29.34 88.80 88.16 89.11 86.43 77.82 80.20 79.95 85.66 82.95 83.99 84.28 86.05 18.39 18.16 17.93 25.09 125 Computational Linguistics Volume 45, Number 1 Table 7 Accuracies of the transition-based parser on development set. Dynamic oracle Beam search UP UR UF UCM LP LR LF LCM 89.59 84.58 85.50 87.38 87.49 85.96 24.74 20.89 87.49 82.17 83.49 84.89 85.44 83.51 21.45 17.99 achieves excellent results for various NLP tasks, for example, Machine Translation. When coupled with linear models, beam search has shown to be a useful technique to improve both training and decoding (Zhang and Clark 2011b). However, in the particular neural parsing architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transit"
J19-1003,J11-1005,0,0.0312884,"5 88.36 20.72 20.72 20.66 29.34 88.80 88.16 89.11 86.43 77.82 80.20 79.95 85.66 82.95 83.99 84.28 86.05 18.39 18.16 17.93 25.09 125 Computational Linguistics Volume 45, Number 1 Table 7 Accuracies of the transition-based parser on development set. Dynamic oracle Beam search UP UR UF UCM LP LR LF LCM 89.59 84.58 85.50 87.38 87.49 85.96 24.74 20.89 87.49 82.17 83.49 84.89 85.44 83.51 21.45 17.99 achieves excellent results for various NLP tasks, for example, Machine Translation. When coupled with linear models, beam search has shown to be a useful technique to improve both training and decoding (Zhang and Clark 2011b). However, in the particular neural parsing architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transit"
J19-1003,P15-1117,0,0.0403883,"Missing"
K17-1005,W13-2322,0,0.0379397,"Missing"
K17-1005,C10-1011,0,0.100823,"consistency tags emerge, for convenience we index the graph and tree vector representation using three indices. g(i, j, t) denotes whether there is an edge from word wi to word wj with tag t in graph g. The joint decoding problem can be written as a constrained optimization problem as Graph Merging The extraction algorithm gives three classes of trees for each graph. We apply the algorithm to the graph training set, and get three training tree sets. After that, we can train three parsing models with the three tree sets. In this work, the parser we use to train models and parse trees is Mate (Bohnet, 2010), a second-order graph-based dependency parser. Let the scores the three models use be f1 , f2 , f3 respectively. Then the parsers can find trees with highest scores for a sentence. That is solving the following optimization problems: arg maxg1 f1 (g1 ), arg maxg2 f2 (g2 ) and arg maxg2 f3 (g3 ). We can parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs, and combine them together to obtain the graph parse of the sentence by putting all the edges in the three subgraphs together. That is to say, we obtain the graph y = max{t2g(g1 ), t2g(g2 ),"
K17-1005,P17-1193,1,0.746711,"cy graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need to develop a princi"
K17-1005,C02-1013,0,0.0856166,"nto simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging. Our parser reaches state-of-the-art performance and is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design"
K17-1005,J16-3001,1,0.910283,"d is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design26 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computationa"
K17-1005,N04-1013,0,0.654687,"Missing"
K17-1005,Q15-1040,0,0.013714,"ions, e.g. semantic dependency graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need t"
K17-1005,S15-2153,0,0.287007,"Missing"
K17-1005,S14-2008,0,0.136054,"Missing"
K17-1005,Q13-1002,0,0.22183,"flexible representations, e.g. semantic dependency graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under"
K17-1005,P17-1077,1,0.80564,"t al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need to develop a principled method to deco"
K17-1005,P14-1042,1,0.758478,"the-art performance and is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design26 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Asso"
K17-1005,D10-1001,0,\N,Missing
K17-1005,W03-3023,0,\N,Missing
K17-1005,E06-1011,0,\N,Missing
K17-1005,C96-1058,0,\N,Missing
K17-1005,J08-4003,0,\N,Missing
K17-1005,C08-1095,0,\N,Missing
K17-1005,S14-2082,0,\N,Missing
K17-1005,J13-4006,0,\N,Missing
K17-1005,P09-1039,0,\N,Missing
K17-1005,J05-1004,0,\N,Missing
K17-1005,P15-1149,1,\N,Missing
K17-1005,D10-1125,0,\N,Missing
K17-1005,P11-1008,0,\N,Missing
K17-1035,P16-1231,0,0.017546,"Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald an"
K17-1035,D12-1091,0,0.0131874,"r ones. When we take into account empty categories, more information is available. The empirical results suggest that deep linguistic information does not necessarily help surface analysis. 6.4 Results of Joint Decoding Table 5 lists the accuracy of different joint decoding models on the test sets. We can see that the joint decoding framework is effective to deal with structure-based overfitting. This time, the accuracy of analysis for overt words is consistently improved across a wide range of conditions. Especially, the third-order model is improved more. We use the Hypothesis Tests method (Berg-Kirkpatrick et al., 2012) to evaluate the improvements. When the p-value is set to 0.05, all improvements in Figure 5 is statistically significant. We separate all sentences in test data set into two subsets: One contains sentences that have no • arc features: funi (h), funi (c), fbi (h, c), fcontext (h, c). • sibling features: fsib (c, m0 ), fsib (h, c, mk ). • tri-sibling features: fsib (h, c, m1 ), ftsib (h, c, m, m1 ). 6.3 Chinese 89.16 89.20 (+0.04) 89.28 (+0.12) 90.00 89.82 (−0.18) Table 4: UASo of different individual models on test data. The upper and bottom blocks present results obtained by sibling and tri-s"
K17-1035,C96-1058,0,0.0354057,"ure 2. 3.2 Algorithm 2: Tri-sibling Factorization It is easy to extend the second-order sibling factorization to parts containing multiple siblings. For example, Koo and Collins (2010) introduced trisibling factorization in which a triple of three successive edges on the same side. Here, we consider parsing for tri-sibling factorization only. To this end, we augment the incomplete span structure with an internal index. The modified construction rule is specified graphically in Figure 3. Note that the presentation is slightly different from Koo and Collins’s. Algorithm 1: Sibling Factorization Eisner (1996) introduced a widely-used DP algorithm for first-order parsing. Their algorithm includes two interrelated types of DP structures: (1) complete spans, which consist of a head-word and its descendents on one side, and (2) incomplete spans, which consist of a dependency and the region between the head and modifier. To include 2 Comparing their numeric results with other papers’, we find that their model does not result in improved parsing. 345 ∅1 h a ... h1 ... ∅1 ... n ∅2 ... ∅3 Length 1 2 3 Total ∅2 ∅4 ... h2 defined in Section 6). We show the statistics in Table 1. The length indicates the num"
K17-1035,P02-1018,0,0.0662743,"an example. Detecting empty elements is important to the interpretation of the syntactic structure of a sentence. For example, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic empty element detection on statistical machine translation. There are three strategies to find empty categories. Dienes and Dubey (2003) introduced a model that utilizes clues from word forms and POS tags to predict the existence of empty categories. In their method, syntactic parsing was treated as a next-step task and therefore had no influence on finding empty elements. Johnson (2002) and Xue and Yang To ensure that predicting the empty elements helps parse the overt, we need to reduce the new estimation error. To this end, we propose to integrate scores from parsing models with and without empty elements and perform joint decoding. The intuition is to leverage parameters estimated without empty elements as a backoff, which exhibit better generalization ability. We evaluate two joint decoders: One is based on chart merging and the other is based on dual decomposition. Experiments demonstrate that information about the covert improves surface analysis in this way. Accuracy"
K17-1035,P11-2037,0,0.025696,"fact that empty category can help reduce the approximation error for surface analysis. The remaining part of the paper is organized as follows. Section 2 is a brief introduction to the problem. Section 3 describes existing algorithms for parsing for overt words only, while Section 4 gives the details of our new algorithms for parsing with empty elements. Section 5 describes the de344 (2013) proposed to identify empty categories after syntactic parsing. Different from the above preprocessing strategy, their post-processing models can not use information about empty category to improve parsing. Cai et al. (2011) introduced an integrated model, where empty category detection and phrase-structure parsing are combined in a single model. They, however, did not report any improvement for parsing.2 Seeker et al. (2012) evaluted all above strategies to include empty nodes in dependency parsing for German and Hungarian. To predict both empty nodes and dependency relations, they enriched the information encoded in dependency labels. They showed that both pre-processing and integrated strategies failed to leverage empty categories to improve parsing. Especially, their preprocessing method significantly descrea"
K17-1035,N04-1013,0,0.0650461,"ses the search space for inference and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings"
K17-1035,W03-2401,0,0.049369,"disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 343–353, c Vancouver, Canada, August 3 - August 4,"
K17-1035,D10-1062,0,0.0259654,"function tags, they make up the full syntactic representation of a sentence. Empty category is one key concept bridging SStructure and D-Structure, given that they contain essential information to trace movements, i.e. the transformation procedure to convert a D-Structure to an S-Structure. When representing empty categories in dependency trees, we can use a null symbol to depict the idea that there is a mental category at the level being represented. See Figure 1 for an example. Detecting empty elements is important to the interpretation of the syntactic structure of a sentence. For example, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic empty element detection on statistical machine translation. There are three strategies to find empty categories. Dienes and Dubey (2003) introduced a model that utilizes clues from word forms and POS tags to predict the existence of empty categories. In their method, syntactic parsing was treated as a next-step task and therefore had no influence on finding empty elements. Johnson (2002) and Xue and Yang To ensure that predicting the empty elements helps parse the overt, we need to reduce the new estimation error. To this"
K17-1035,P10-1001,0,0.0166792,"ended Eisner’s algorithm with a third structure, viz. (3) sibling spans, which represent the region between successive modifiers of same head. The second-order algorithm visits all the spans from bottom to top, finding the best combination of smaller structures to form a new one. Each type of span is created by recursively combining two smaller, adjacent spans. The DP structures and their constructions are specified graphically in Figure 2. 3.2 Algorithm 2: Tri-sibling Factorization It is easy to extend the second-order sibling factorization to parts containing multiple siblings. For example, Koo and Collins (2010) introduced trisibling factorization in which a triple of three successive edges on the same side. Here, we consider parsing for tri-sibling factorization only. To this end, we augment the incomplete span structure with an internal index. The modified construction rule is specified graphically in Figure 3. Note that the presentation is slightly different from Koo and Collins’s. Algorithm 1: Sibling Factorization Eisner (1996) introduced a widely-used DP algorithm for first-order parsing. Their algorithm includes two interrelated types of DP structures: (1) complete spans, which consist of a he"
K17-1035,D10-1125,0,0.0223262,"91.72 (−0.01) 92.23 92.41 (+0.18) Results of Individual Models Table 4 lists the accuracy of individual models coupled with different decoding algorithms on the test sets. We focus on the prediction for overt 350 English −EC +EC 92.50 91.53 92.83 91.77 92.82 91.48 92.84 91.74 93.68 92.13 93.99 92.43 Algo 3 1+3 4 1+4 5 2+5 Chinese −EC +EC 90.92 88.60 91.12 88.97 91.29 88.58 91.10 88.98 92.00 89.06 92.10 89.77 composition practically gives the exact solutions in a few iterations. One advantage relevant is that such a decoder can integrate parsing models that are somehow heterogeneous. Refer to (Koo et al., 2010) for example. 7 Can deep syntactic information help surface parsing, which is the mainstream focus of NLP research. In this paper, we investigate this topic under the umbrella of Transformational Grammar, GB in particular. We focused on empty category augmented dependency analysis. We demonstrate that on the one hand deep information helps reduce the approximation error for traditional (surface) parsing, while on the other hand traditional parsing helps reduce the estimation error for deep parsing. Coupling surface and deep information in an appropriate way is able to produce better syntactic"
K17-1035,W02-1001,0,0.0799215,"38 (+0.38) Table 5: UASo of different joint decoding models on test data. “CM” and “DD” are short for joint decoders based on chart merging and dual decomposition respectively. The upper and bottom blocks present results obtained by sibling and tri-sibling models respectively. All improvements are statistically significant. Statistical Disambiguation In the context of data-driven parsing, we still need an extra disambiguation model for building a practical parser. As with many other parsers, we employ a global linear model. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). Developing features has been shown crucial to advancing the state-of-the-art in dependency parsing. We adopt features from previous work. We refer to the head/child of the arc as h/c, the k-th inner split point as mk , and the grand point as g. We list features selected by different algorithm as follows, and all following features should be concatenated with direction and distance of the arc. words only. Models coupled with Algorithm 1, 3 and 4 are second-order models, while with 2 and 5 third-order ones. When we take into account empty categories, more information is available. The empirica"
K17-1035,P14-1130,0,0.0190081,"ling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third structure, viz. (3) si"
K17-1035,de-marneffe-etal-2006-generating,0,0.200503,"Missing"
K17-1035,E06-1011,0,0.168251,"Missing"
K17-1035,N13-1125,0,0.0665693,"cs, Peking University {zhangxunah,ws,wanxiaojun}@pku.edu.cn Abstract clear whether such additional information is helpful for surface syntactic analysis. This is partly because analysis grounded on different grammar formalisms, e.g. HPSG and CFG, are not directly comparable. In the Government and Binding (GB; Chomsky, 1981) theory, empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. Following the linguistic insights underlying GB, a traditional dependency analysis can be augmented with empty elements, viz. covert elements (Xue and Yang, 2013). See Figure 1 for an example. The new representation provides a considerable amount of deep syntactic information, while keeping intact all dependencies of overt words. Integrating both overt and covert elements in one unified representation provides an effective yet lightweight way to achieve deeper language understanding beyond surface syntax1 . Even more important, this modest way to modify tree analysis makes possible fair evaluation of the influence of deep syntactic elements on surface parsing. We study graph-based parsing models for this new representation with a particular focus on th"
K17-1035,H05-1066,0,0.137515,"m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third structure, viz. (3) sibling spans, which represent the region between successive modifiers of same head. The second-order algorithm visits all the spans from bottom to top, finding the best combination of smaller structures to form a new one. Each type of span is created by rec"
K17-1035,W03-3023,0,0.136041,"ations of the standard sibling algorithm. Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l secon"
K17-1035,J16-3001,1,0.854813,"or. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural Language Learning (Co"
K17-1035,J08-1002,0,0.0436971,"ingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural"
K17-1035,J08-4003,0,0.032539,"ng algorithm. Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibli"
K17-1035,C12-2105,0,0.0220798,"3 describes existing algorithms for parsing for overt words only, while Section 4 gives the details of our new algorithms for parsing with empty elements. Section 5 describes the de344 (2013) proposed to identify empty categories after syntactic parsing. Different from the above preprocessing strategy, their post-processing models can not use information about empty category to improve parsing. Cai et al. (2011) introduced an integrated model, where empty category detection and phrase-structure parsing are combined in a single model. They, however, did not report any improvement for parsing.2 Seeker et al. (2012) evaluted all above strategies to include empty nodes in dependency parsing for German and Hungarian. To predict both empty nodes and dependency relations, they enriched the information encoded in dependency labels. They showed that both pre-processing and integrated strategies failed to leverage empty categories to improve parsing. Especially, their preprocessing method significantly descreased parsing accuracy. Although empty categories are very important in theory, it is still unclear that they can help parsing in practice. 3 = l m = r l r l + m r m+1 r m r + l m l m = + Figure 2: The DP st"
K17-1035,P09-1039,0,0.0276317,"as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third str"
K17-1035,P03-1055,0,\N,Missing
K18-1054,W13-2322,0,0.0628345,"Missing"
K18-1054,P17-1193,1,0.269538,"ience and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes, e.g., noncrossing graphs. In this paper, we fill this gap in the literature"
K18-1054,D17-1003,1,0.59271,"ience and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes, e.g., noncrossing graphs. In this paper, we fill this gap in the literature"
K18-1054,W02-1001,0,0.179089,"Missing"
K18-1054,P16-2058,0,0.060807,"Missing"
K18-1054,P15-1149,1,0.908825,"Missing"
K18-1054,S14-2082,0,0.0313727,"ture of the network when processing He wants to go. The upper-left nonlinear transform is used for edge scoring while the upper right one is used for label scoring. L ABEL(i, j) = arg max W2 · ReLU(W1,1 · ri + W1,2 · rj + b) + b2 candidate dependencies as well as their relation types. Figure 4 shows the architecture of our system. We can see here the two local score functions explicitly utilize the positions of a semantic head and a semantic dependent. It is similar to the firstorder factorization as defined in a number of linear parsing models, e.g., the models defined by Martins and Almeida (2014) and Cao et al. (2017a). 3.3.2 Dense Representations We use words as well as POS tags as clues for scoring an individual arc. In particular, we transform all of them into continuous and dense vectors. Inspired by Costa-juss`a and Fonollosa (2016)’s work, we utilize character-based embedding for low-frequency words, i.e., words that appear more than k times in the training data, and word-based embeddings for other words. The word-based embedding module applies the common lookup-table mechanism, while the character-based word embedding wi is implemented by extracting the features (denoted as c1"
K18-1054,flickinger-etal-2010-wikiwoods,0,0.122158,"uracy, it usually performs rather poorly on the out-of-domain data (Oepen et al., 2015). How to build robust semantic dependency parsers that can learn across domains remains an under-addressed problem. To improve the cross-domain parsing performance, we propose a data-oriented model to explore the linguistic generality encoded in a hand-crafted, domainindependent, linguistically-precise English grammar, namely English Resource Grammar (ERG; Flickinger, 2000). In particular, we introduce a cost-sensitive training model to learn crossdomain semantic information implicitly encoded in WikiWoods (Flickinger et al., 2010), i.e., a corpus that collects the wikipedia1 texts as well as their automatic syntactico-semantic annotations produced by ERG. Evaluation demonstrates the usefulness of the imperfect annotations automatically created by ERG. Our parser is available at https://github. com/draplater/msg-parser. 2 ARG2 ARG1 ARG2 BV The company that Mark wants to buy is broken Figure 1: A fragment of a semantic dependency graph. of more than one predicate; (2) cycles are allowed if the direction of arcs are not taken into account. 2.2 Semantic Dependency Analysis SDP is the task of mapping a natural language sent"
K18-1054,S15-2153,0,0.159321,"Missing"
K18-1054,N09-2054,0,0.0141939,"select 480,564 sentences (5,346,703 tokens) from WikiWoods to train another model, and leave out other parts of Redwoods. The performance improvement is more remarkable when providing more data, even though such data contains annotation errors. For the second group of experiments, we use the RedwoodsWOD sentences for training and the DeepBank WSJ sentences for evaluation. For this set-up, consistent improvements of the parser quality are observed. 5.6 Chinese POS tagging has a great impact on parsing. In this paper, we consider two POS taggers: a symbol-refined generative HMM tagger (SR-HMM) (Huang et al., 2009) and a BiLSTMCRF model when assisting Chinese SDG. For the neural tagging model, in addition to a BiLSTM layer for encoding words, we set a BiLSTM layer for encoding characters, which supports us to derive character-level representations for all words. In particular, vectors from the characterlevel LSTM is concatenated with the pre-trained word embedding before feeding into the other word-level BiLSTM network to capture contextual information. The final module of our CRF tagger is a linear chain CRF which scores the output sequence by factoring it in local tag bi-grams. From Table 5, we can se"
K18-1054,S14-2008,0,0.0373675,"ment relationships for all content words. Such sentence-level semantic analysis of text is concerned with the characterization of events and is therefore important to understand the essential meaning of a natural language sentence. With the advent of many supporting resources, SDP has become a well-defined task with a substantial body of work and comparative evaluation. (Almeida and Martins, 2015; Du et al., 2015a; Zhang et al., 2016; Peng et al., 2017; Wang et al., 2018). Two SDP shared tasks have been run as part of the 2014 and 2015 International Workshops on Semantic Evaluation (SemEval) (Oepen et al., 2014, 2015). 562 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 562–572 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics ARG2 published result reported in Zhang et al. (2016) and Du et al. (2015b). Most studies on semantic parsing focused on the in-domain setting, meaning that both training and testing data are drawn from the same domain. Even a data-driven parsing system achieves a high in-domain accuracy, it usually performs rather poorly on the out-of-domain data (Oepen et al., 2015). How to bui"
K18-1054,C02-2025,0,0.0168485,"” annotations (RedwoodsWOD for short) as well as the official WikiWoods annotations. Note that the MaxEnt model used to obtain the official WikiWoods annotations are compatible with RedwoodswWOD. Due to the diversity of the RedwoodsWOD and DeepBank sentences, this set-up can also be viewed as an outof-domain evaluation. Data for Cross-Domain Experiments Since around 2001, the ERG has been accompanied by syntactico-semantic annotations, where for each sentence an annotator has selected the intended analysis among all alternatives licensed by the grammar. This derived resource, namly Redwoods6 (Oepen et al., 2002; Flickinger et al., 2017), is a collection of hand-annotated corpora and consists of data sets from several distinct domains. Redwoods also includes (re)treebanking results of the first 22 sections of the venerable Wall Street Journal (WSJ) text and the section of Brown Corpus in the Penn Treebank (Marcus et al., 1993). The WSJ part is also known as Deep6 5.5 Results of Cross-Domain Parsing Table 3 summarizes experimental results for different cross-domain evaluation set-ups. For the 7 8 http://moin.delph-in.net/RedwoodsTop 568 http://moin.delph-in.net/WikiWoods https://github.com/delph-in/py"
K18-1054,W13-5707,0,0.0606194,"Missing"
K18-1054,P17-1186,0,0.0639981,"Missing"
K18-1054,W12-3602,0,0.0252342,"ions automatically created by ERG. Our parser is available at https://github. com/draplater/msg-parser. 2 ARG2 ARG1 ARG2 BV The company that Mark wants to buy is broken Figure 1: A fragment of a semantic dependency graph. of more than one predicate; (2) cycles are allowed if the direction of arcs are not taken into account. 2.2 Semantic Dependency Analysis SDP is the task of mapping a natural language sentence into a formal meaning representation in the form of a dependency graph. Figure 1 shows an Minimal Recursion Semantics (MRS; Copestake et al., 2005) reduced semantic dependency analysis (Ivanova et al., 2012). In this example, the semantic analysis is represented as a labeled directed graph in which the vertices are tokens in the sentence. The graph abstracts away from syntactic analysis (e.g., the complementizer—that— and passive construction are excluded) and includes most semantically relevant non-anaphoric local (e.g., from “wants” to “Mark”) and longdistance (e.g., from “buy” to “company”) dependencies. The arc labels encode linguisticallymotivated, broadly-applicable semantic relations that are grounded under the type-driven semantics. It is worth noting that semantic dependency graphs are n"
K18-1054,Q16-1023,0,0.159873,"he Architecture A semantic graph mainly consists of two parts: the structural part and the label part. The former describes the predicate–argument relation in the sentence, and the latter describes the type of this relation. In our model, the structural part and the label part are regarded as independent of each other. We use a coarse-to-fine strategy: finding the maximum unlabeled subgraph first and assigning a label for every edge in this subgraph then. The motivation is to avoid the calculation of a number of unnecessary label scores in order to improve the processing efficiency. Following Kiperwasser and Goldberg (2016)’s successful experience on syntactic tree parsing and Peng et al. (2017)’s experience on semantic graph parsing, we employ a stacked bidirectional-LSTM (BiLSTM) based model to assign scores. In our system, the BiLSTM vectors associated with the input words are utilized to calculate scores for the Parsing to 1 EC / P 2 Graphs Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017a,b). In particular, 1 EC / P 2 graphs are an appropriate graph class for modeling semantic dependency struct"
K18-1054,Q13-1002,0,0.0223189,"ted with the input words are utilized to calculate scores for the Parsing to 1 EC / P 2 Graphs Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017a,b). In particular, 1 EC / P 2 graphs are an appropriate graph class for modeling semantic dependency structures (Cao et al., 2017a). Figure 2 presents an example to illustrate the 1-endpoint-crossing property, while Figure 3 shows a case for pagenumber-2. Below we present the formal description of the two properties that are adopted from Pitler et al. (2013) and Kuhlmann and Jonsson (2015) respectively. 564 LSTM LSTM ... LSTM ... 3.3.4 Factorized Scoring In our first order model, the S CORE function evaluates the preference of a semantic dependency graph by considering every bilexical relation in this graph one by one. In particular, the corresponding S CORE PART function assigns a score to a candidate arc between word i and word j using a non-linear transform from the two feature vectors, viz. ri and rj , associated to the two words: S CORE PART(i, j) = W2 · ReLU(W1,1 · ri + W1,2 · rj + b) He PRP wants VBZ go VB The assignment task for dependenc"
K18-1054,C10-1122,0,0.0859428,"he BiLSTM based disambiguation model. The preciExperiments Set-up for the Baseline System To evaluate neural Maximum Subgraph parsing in practice, we first conduct experiments on the three English data sets, namely DM, PAS and PSD4 , which are from the SemEval 2015 Task18 (Oepen et al., 2015). We use the “standard” training, validation, and test splits to facilitate comparisons. In other words, the data splitting policy follows the shared task. In addition to English parsing, we consider Chinese SDP and use two data sets: (1) Chinese PAS data provided by SemEval 2015, and (2) Chinese CCGBank (Tse and Curran, 2010) to evaluate the cross-lingual ability of our model. All the SemEval data sets are publicly available from 4 DM, PAS and PSD are short for DeepBank, Enju HPSGBank and Prague Dependency Treebank. 5 567 https://github.com/clab/dynet 94 90 Labeled F-score Bank (Flickinger et al., 2012). The Brown corpus part is used as the out-of-domain test data by SemEval 2015. The DM data sets for both SemEval 2014 and 2015 SDP shared tasks are based on the RedWoods corpus. Besides gold standard annoations, Flickinger et al. (2010) built the WikiWoods corpus7 , which provides automatically created annotations"
K18-1054,C10-2162,0,0.0319857,"eepBank S 85.70 85.02 Redwoods S 86.28 84.85 DeepBank+WikiWoods-ACE S 88.30 86.42 DeepBank+WikiWoods-ACE E[3] 89.53 87.57 O UT- OF -D OMAIN (R EDWOODS WOD) DeepBank S 90.74 90.40 RedwoodsWOD S 81.40 78.99 RedwoodsWOD+WikiWoods S 84.05 79.86 RedwoodsWOD+WikiWoods E[3] 84.84 81.02 LF 90.57 91.03 91.32 92.11 85.37 85.56 87.35 88.54 90.57 80.18 81.90 82.88 Table 3: Labeled F1 on the DM test sets. “S” denotes single model, while “E[3]” denotes ensemble model with 3 sub-models. set-up as Zhang et al. (2016). Both data sets are transformed from Chinese TreeBank with two rich sets of heuristic rules (Yu et al., 2010; Tse and Curran, 2010). Table 4 and 5 presents all results. Our parser significantly outperforms Zhang et al. (2016)’s Zhang et al. (2016) system on Chinese CCGBank, which achieved best reported performance. first group of experiments, we test the parser using different training data sets. The baseline utilizes the WSJ portion only. While more reliable training data is added, the performances increase consistently. We notice that the improvement extending the training data from DeepBank to Redwoods is quite limited for the out-of-domain evaluation. One reason is that the amount of enlarged go"
K18-1054,J16-3001,1,0.852396,"Analysis Yufei Chen♠ , Sheng Huang♠ , Fang Wang♠ , Weiwei Sun♠♥ and Xiaojun Wan♠ ♠ Institute of Computer Science and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for hig"
K18-1054,P09-1043,0,0.0903088,"Missing"
K19-2016,P12-2074,0,0.0591315,"sets short-term short-term misinterpreted misinterpreted a sequence of basic elements with simply defined regular expressions. The core part of our tokenizer is a sequence labeling model over this sequence. In particular, each element is assigned with a positional label that indicates token boundaries. The labels can be either B, which means the unit is at the begining of a target token, or I, which means the unit is inside a token. For sequential classification, we utilize a multi-layer BiLSTM network. Tokens can be retrieved from the predicted labels. See Figure 1 for an example. Note that, Dridan and Oepen (2012) showed that regular expressions are quite powerful to deal with the tokenizaiton problem for different styles. Table 2: Examples to illustrate the relationships between surface concepts and word-level units. ‘ ’ is an escape character for whitespace. and punctuation markers. In an EDS graph, a surface concept may be aligned with a sub-unit, a single unit or multiple units. Table 2 shows some examples. During concept identification (§3.4), there should exist a surjection from surface concepts to the input tokens. Therefore, tokenization is important for obtaining a reasonable alignment between"
K19-2016,W15-0128,0,0.237873,"ks (Chen et al., 2018b; Groschwitz et al., 2018; Lindemann et al., 2019). A composition-based parser explicitly models derivations that yield semantic graphs by defining a score function S CORE D. Assume a derivation D = r1 , r2 , . . . , rm is a sequence of rules. Formally, we have the following optimization problem: X G0 = arg max S CORE D(D) (4) a large set. To the best of our knowledge, McDonald and Pereira (2006) present the first graph-based syntactic dependency parsing algorithm that removes the tree-shape constraint. In the scenario of semantic dependency parsing, Kuhlmann and Jonsson (2015) generalize the graph-based framework (aka Maximum Spanning Tree parsing) and propose Maximum Subgraph parsing. Given a directed graph G = (V, E) that corresponds to an input sentence x = w0 , . . . wn−1 and a score function S CORE G. The string-to-graph parsing is formulated as a problem of searching for a subset E 0 ⊆ E with the maximum score. Formally, we have the following optimization problem: (V, E 0 ) = arg max S CORE G(G∗ ) (1) G∗ =(V,E ∗ ⊆E) For semantic dependency parsing, V is the set of surface tokens, and G is, usually, the corresponding complete graph. It is relatively straightfo"
K19-2016,P17-1112,0,0.173131,"Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries. Take the output structure in Figure 1 for example: go v 1 and want v 1 indicate surface concepts, while proper q and named indicate abstract concepts. To avoid proliferation of concepts, some concepts are parameterized. The"
K19-2016,flickinger-etal-2014-towards,0,0.549444,"ramework Meaning Representation Parsing. The detailed evaluation of the two parsers gives us a new perception about parsing into linguistically enriched meaning representations: current neural EDS parsers are able to reach an accuracy at the interannotator agreement level in the same-epochand-domain setup. 1 Introduction For the CoNLL 2019 Shared Task on CrossFramework Meaning Representation Parsing (MRP; Oepen et al., 2019), we concentrate on Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), the graph-based meaning representations derived from English Resource Semantics1 (ERS; Flickinger et al., 2014b) that is the richly detailed semantic annotation associated to English Resource Grammar (ERG; Flickinger, 2000), a domain-independent, linguistically deep and broad-coverage HPSG grammar. The full ERS and EDS annotations include not only basic predicate–argument structures, but also information about quantifiers and scopal operators, e.g. negation, as well as analyses of linguistically complex phenomena such as time and date expressions, conditionals, and comparatives. Following Koller et al. (2019)’s practice, we divide existing work on string-to-semantic-graph parsing into four types, name"
K19-2016,P18-1038,1,0.470361,"inger, 2000), a domain-independent, linguistically deep and broad-coverage HPSG grammar. The full ERS and EDS annotations include not only basic predicate–argument structures, but also information about quantifiers and scopal operators, e.g. negation, as well as analyses of linguistically complex phenomena such as time and date expressions, conditionals, and comparatives. Following Koller et al. (2019)’s practice, we divide existing work on string-to-semantic-graph parsing into four types, namely factorization-, composition-, transition- and translation-based approaches. Our previous studies (Chen et al., 2018b; Cao et al., 2019) as well as other investigations on other graph banks indicate that the 1 2 Parsing to Semantic Graphs In this section, we present a summary of factorization-, composition-, transition- and translation-based parsing approaches. Factorization-Based Approach. This type of approach is inspired by the successful design of graph-based dependency tree parsing (McDonald, 2006). A factorization-based parser explicitly models the target semantic structures by defining a score function that is able to evaluate the goodness of any candidate graph. Usually, the set of possible graphs t"
K19-2016,J18-1004,0,0.0164629,"ph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries. Take the output structure in Figure 1 for example: go v 1 and want v 1 indicate surface concepts, while proper q and named indicate abstract concepts. To avoid proliferation of concepts, some concepts are parameterized. The parameters can be vi"
K19-2016,W17-6810,0,0.0270202,"onality is a cornerstone for many formal semantic theories. Following a principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. On the linguistic side, such rules extensively encode explicit knowledge about natural languages. On the computational side, such rules must be governed by a well-defined grammar formalism. In particular, to manipulate graph construction in a principled way, Hyperedge Replacement Grammar (HRG; Drewes et al., 1997) and AM Algebra (Groschwitz et al., 2017) have been applied to build semantic parsers for various graph banks (Chen et al., 2018b; Groschwitz et al., 2018; Lindemann et al., 2019). A composition-based parser explicitly models derivations that yield semantic graphs by defining a score function S CORE D. Assume a derivation D = r1 , r2 , . . . , rm is a sequence of rules. Formally, we have the following optimization problem: X G0 = arg max S CORE D(D) (4) a large set. To the best of our knowledge, McDonald and Pereira (2006) present the first graph-based syntactic dependency parsing algorithm that removes the tree-shape constraint. In"
K19-2016,P18-1170,0,0.0398488,"Missing"
K19-2016,P81-1022,0,0.564559,"Missing"
K19-2016,P18-2077,0,0.0221512,"he general form of (4) is a very complex combinatorial optimization problem. The approximating strategy to search for the best derivation instead has been shown practical yet effective for ERS parsing (Chen et al., 2018b). Formally, we solve the below problem, G∗ =(V,E ∗ ⊆E) e∈E ∗ The essential computational module in this architecture is the score function, which is usually induced based on moderate-sized annotated sentences. Various deep learning models together with vector-based encodings induced from largescale raw texts have been making advances in shaping a score function significantly (Dozat and Manning, 2018). We will detail our factorizationbased parser in §3. D0 = arg max m X D∗ ∈GEN DERIV (x) i=1 D∗ =r1 r2 ···rm S CORE RULE(ri ) (6) where GEN DERIV (x) denotes all sound derivations that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures"
K19-2016,P18-1249,0,0.0223103,"c recursive derivations that are governed by a synchronous HRG (SHRG). See Figure 4 for an example. Our parser constructs EDS graphs by explicitly modeling such derivations. In particular, it utilizes a constituent parser to build a syntactic derivation, and then selects semantic HRG rules associated to syntactic CFG rules to generate a graph. When multiple rules are applicable for a single phrase, a neural network is used to rank them. One main difference between our submission parer and the parser introduced in Chen et al. (2018b) is that the syntactic parsing model is a reimplementation of Kitaev and Klein (2018). It utilizes transformer layers to capture words’ contextual information, denoted as ri . After encoding an input sentence, a multiple-layer peceptron (MLP) is employed to get span scores. The score of span (i, j) with label L is calculated from its embedding si,j , which is from the contextual vector of Here P ROJ(·) represents a feed-forward network with L EAKY R E LU activation. The anchors provided by training dataset are all character-based, so transformation is required before training this model. In the same manner, after retrieving the start/end word of a concept, we need to convert w"
K19-2016,P19-4002,1,0.84895,"6), the graph-based meaning representations derived from English Resource Semantics1 (ERS; Flickinger et al., 2014b) that is the richly detailed semantic annotation associated to English Resource Grammar (ERG; Flickinger, 2000), a domain-independent, linguistically deep and broad-coverage HPSG grammar. The full ERS and EDS annotations include not only basic predicate–argument structures, but also information about quantifiers and scopal operators, e.g. negation, as well as analyses of linguistically complex phenomena such as time and date expressions, conditionals, and comparatives. Following Koller et al. (2019)’s practice, we divide existing work on string-to-semantic-graph parsing into four types, namely factorization-, composition-, transition- and translation-based approaches. Our previous studies (Chen et al., 2018b; Cao et al., 2019) as well as other investigations on other graph banks indicate that the 1 2 Parsing to Semantic Graphs In this section, we present a summary of factorization-, composition-, transition- and translation-based parsing approaches. Factorization-Based Approach. This type of approach is inspired by the successful design of graph-based dependency tree parsing (McDonald, 2"
K19-2016,P17-1186,0,0.116278,"Missing"
K19-2016,P17-1014,0,0.0390699,"either true or false. Edges are called relations. An edge links exactly two nodes and mainly reflects predicate– argument relations. Edges are assigned with a small, fixed inventory of role labels (e.g. ARG1, ARG2, . . . ). Translation-Based Approach. This type of approach is inspired by the success of sequence-tosequence (seq2seq for short) models that are the heart of modern Neural Machine Translation. A translation-based parser takes a family of semantic graphs as a foreign language, in that a semantic graph is encoded and then viewed as a string from another language (Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017). A parser knows how to linearize a graph. Data augmentation has been shown very helpful (Konstas et al., 2017), partially reflecting the data-hungry nature of seq2seq models. Simple application of seq2seq models is not sucessful. However, some basic models can be integrated with other types of approaches. Peng et al. (2018) propose to combine the translation- and transition-based approaches. Zhang et al. (2018) combined the translation- and factorization-based approaches. 3 3.2 We employ a four-stage pipeline to incrementally construct an EDS graph. Figure 1 illustrat"
K19-2016,Q15-1040,0,0.0272743,"for various graph banks (Chen et al., 2018b; Groschwitz et al., 2018; Lindemann et al., 2019). A composition-based parser explicitly models derivations that yield semantic graphs by defining a score function S CORE D. Assume a derivation D = r1 , r2 , . . . , rm is a sequence of rules. Formally, we have the following optimization problem: X G0 = arg max S CORE D(D) (4) a large set. To the best of our knowledge, McDonald and Pereira (2006) present the first graph-based syntactic dependency parsing algorithm that removes the tree-shape constraint. In the scenario of semantic dependency parsing, Kuhlmann and Jonsson (2015) generalize the graph-based framework (aka Maximum Spanning Tree parsing) and propose Maximum Subgraph parsing. Given a directed graph G = (V, E) that corresponds to an input sentence x = w0 , . . . wn−1 and a score function S CORE G. The string-to-graph parsing is formulated as a problem of searching for a subset E 0 ⊆ E with the maximum score. Formally, we have the following optimization problem: (V, E 0 ) = arg max S CORE G(G∗ ) (1) G∗ =(V,E ∗ ⊆E) For semantic dependency parsing, V is the set of surface tokens, and G is, usually, the corresponding complete graph. It is relatively straightfo"
K19-2016,P18-1171,0,0.0146456,"hat are the heart of modern Neural Machine Translation. A translation-based parser takes a family of semantic graphs as a foreign language, in that a semantic graph is encoded and then viewed as a string from another language (Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017). A parser knows how to linearize a graph. Data augmentation has been shown very helpful (Konstas et al., 2017), partially reflecting the data-hungry nature of seq2seq models. Simple application of seq2seq models is not sucessful. However, some basic models can be integrated with other types of approaches. Peng et al. (2018) propose to combine the translation- and transition-based approaches. Zhang et al. (2018) combined the translation- and factorization-based approaches. 3 3.2 We employ a four-stage pipeline to incrementally construct an EDS graph. Figure 1 illustrates the four steps with a simple sentence. The core idea is to identify concepts from surface strings, and then detect the relations between them. 3.3 The Factorization-Based Parser 3.1 The Architecture Tokenization Automatic tokenization for English has been widely viewed as a solved problem for quite a long time. Taking the risk of oversimplifying"
K19-2016,J16-4009,0,0.237215,"hnical details of our parser, we roughly sketch key elements in EDS graphs. Refer to Flickinger et al. (2014a) for more information about the design of ERS. We distinguish three kinds of elements: (1) labeled nodes, (2) node properties and (3) labeled edges. Nodes are sometimes called concepts2 , where their labels reflect conceptual meaning. The 2 Considering the original design and especially the logic foundation of ERS, the seemly more standard name is predicate. In this paper, we call them concepts, mainly because we want to follow the new tradition of graph-based meaning representations (Kuhlmann and Oepen, 2016). 3 168 We purposely avoid using words here. But when we inConcept Identification Input Tokenization Tom wants to go. Tom / wants / to / go / . proper q named Tom *v1 wants Output ARG2 ∅ . Top h4:9i want v 1 go v 1h13:15i ARG1 *v1 go Relation Detection proper qh0:3i BV ∅ to Property Prediction want v 1 go v 1 BV ARG1 ARG2 proper q ARG1 ARG1 named named(""Tom"")h0:3i Figure 1: The workflow of our factorization-based parser. Tokenization: To separate an input sentence into semantic parsing-oriented tokens. Concept Identification: To generate concepts with a sequence labeling model. Relation Detect"
K19-2016,E17-1035,0,0.152264,"y whose value can be either true or false. Edges are called relations. An edge links exactly two nodes and mainly reflects predicate– argument relations. Edges are assigned with a small, fixed inventory of role labels (e.g. ARG1, ARG2, . . . ). Translation-Based Approach. This type of approach is inspired by the success of sequence-tosequence (seq2seq for short) models that are the heart of modern Neural Machine Translation. A translation-based parser takes a family of semantic graphs as a foreign language, in that a semantic graph is encoded and then viewed as a string from another language (Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017). A parser knows how to linearize a graph. Data augmentation has been shown very helpful (Konstas et al., 2017), partially reflecting the data-hungry nature of seq2seq models. Simple application of seq2seq models is not sucessful. However, some basic models can be integrated with other types of approaches. Peng et al. (2018) propose to combine the translation- and transition-based approaches. Zhang et al. (2018) combined the translation- and factorization-based approaches. 3 3.2 We employ a four-stage pipeline to incrementally construct an EDS gr"
K19-2016,N18-1202,0,0.0317943,"rd of a compound. Re-aligning these concepts means discarding their original anchors. To fully fit the MRP goals, we treat anchors as properties of concepts, and recover them by predicting the start/end boundaries with a classification model, as to be described in §3.6. We employ a neural sequence labeling model to predict concepts. A multi-layer BiLSTM is utilized to encode tokens and another two softmax layers to predict concept-related labels: One for lexicalized concepts and the other for the rest. We also use recently widely-used contextualized word representation models, including ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). Figure 2 shows the neural network for concept identication. 3.5 c1 c4 B IAFFINE 2:pronoun q 1:pron 3:* v 1 arg max φ 4:* v 1 r1 encoder encoder encoder r4 encoder He wants to go Figure 2: The network architecture for our concept identification and relation detection models which share the same architecture in word embedding and contextual encoder layers but with the same sets of parameters. A softmax layer is used for concept identification. To determine whether the dependency pron ← go v 1 exists, i.e. unlabeled dependency parsing, the corresponding embeddings"
K19-2016,P19-1450,0,0.0566105,"e result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. On the linguistic side, such rules extensively encode explicit knowledge about natural languages. On the computational side, such rules must be governed by a well-defined grammar formalism. In particular, to manipulate graph construction in a principled way, Hyperedge Replacement Grammar (HRG; Drewes et al., 1997) and AM Algebra (Groschwitz et al., 2017) have been applied to build semantic parsers for various graph banks (Chen et al., 2018b; Groschwitz et al., 2018; Lindemann et al., 2019). A composition-based parser explicitly models derivations that yield semantic graphs by defining a score function S CORE D. Assume a derivation D = r1 , r2 , . . . , rm is a sequence of rules. Formally, we have the following optimization problem: X G0 = arg max S CORE D(D) (4) a large set. To the best of our knowledge, McDonald and Pereira (2006) present the first graph-based syntactic dependency parsing algorithm that removes the tree-shape constraint. In the scenario of semantic dependency parsing, Kuhlmann and Jonsson (2015) generalize the graph-based framework (aka Maximum Spanning Tree p"
K19-2016,J93-2004,0,0.066093,"Missing"
K19-2016,C08-1095,0,0.044721,"aw texts have been making advances in shaping a score function significantly (Dozat and Manning, 2018). We will detail our factorizationbased parser in §3. D0 = arg max m X D∗ ∈GEN DERIV (x) i=1 D∗ =r1 r2 ···rm S CORE RULE(ri ) (6) where GEN DERIV (x) denotes all sound derivations that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contrib"
K19-2016,P18-1150,0,0.0406691,"Missing"
K19-2016,E06-1011,0,0.150637,"te graph construction in a principled way, Hyperedge Replacement Grammar (HRG; Drewes et al., 1997) and AM Algebra (Groschwitz et al., 2017) have been applied to build semantic parsers for various graph banks (Chen et al., 2018b; Groschwitz et al., 2018; Lindemann et al., 2019). A composition-based parser explicitly models derivations that yield semantic graphs by defining a score function S CORE D. Assume a derivation D = r1 , r2 , . . . , rm is a sequence of rules. Formally, we have the following optimization problem: X G0 = arg max S CORE D(D) (4) a large set. To the best of our knowledge, McDonald and Pereira (2006) present the first graph-based syntactic dependency parsing algorithm that removes the tree-shape constraint. In the scenario of semantic dependency parsing, Kuhlmann and Jonsson (2015) generalize the graph-based framework (aka Maximum Spanning Tree parsing) and propose Maximum Subgraph parsing. Given a directed graph G = (V, E) that corresponds to an input sentence x = w0 , . . . wn−1 and a score function S CORE G. The string-to-graph parsing is formulated as a problem of searching for a subset E 0 ⊆ E with the maximum score. Formally, we have the following optimization problem: (V, E 0 ) = a"
K19-2016,C10-2139,1,0.739309,"ferent styles. Table 2: Examples to illustrate the relationships between surface concepts and word-level units. ‘ ’ is an escape character for whitespace. and punctuation markers. In an EDS graph, a surface concept may be aligned with a sub-unit, a single unit or multiple units. Table 2 shows some examples. During concept identification (§3.4), there should exist a surjection from surface concepts to the input tokens. Therefore, tokenization is important for obtaining a reasonable alignment between concepts and input tokens. We adopt the character-based word segmentation approach for Chinese (Sun, 2010) to find suitable tokens. We first split an input sentence into 3.4 Concept Identification Surface concepts (e.g. quantifier some q) and some of the abstract concepts (e.g. named entity named) have a more transparent connection to surface forms and are relatively easier to identify. We call such concepts lexicalized concepts, which include all but are not limited to surface concepts. We cast identification of lexicalized concepts as a token-based tagging problem. The lexicalized concepts usually include lemma information in its troduce our neural parsing models, we still use word to relate the"
K19-2016,J19-1003,1,0.836111,"We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries. Take the output structure in Figure 1 for example: go v 1 and want v 1 indicate surface concepts, while proper q and named indicate abstract concepts. To avoid proliferation of concepts, some concepts are parameterized. The parameters can be viewed as properties"
K19-2016,J08-4003,0,0.0357178,"or-based encodings induced from largescale raw texts have been making advances in shaping a score function significantly (Dozat and Manning, 2018). We will detail our factorizationbased parser in §3. D0 = arg max m X D∗ ∈GEN DERIV (x) i=1 D∗ =r1 r2 ···rm S CORE RULE(ri ) (6) where GEN DERIV (x) denotes all sound derivations that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract c"
K19-2016,K19-2001,0,0.135412,"Missing"
K19-2016,N15-1040,0,0.051432,"es all sound derivations that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries. Take the output structure in Figure 1 for example: go v 1 and want v 1 indicate surface concepts, while proper q and named indicate abstract concepts. To avoid proliferation of co"
K19-2016,W03-3023,0,0.0798391,"ng models together with vector-based encodings induced from largescale raw texts have been making advances in shaping a score function significantly (Dozat and Manning, 2018). We will detail our factorizationbased parser in §3. D0 = arg max m X D∗ ∈GEN DERIV (x) i=1 D∗ =r1 r2 ···rm S CORE RULE(ri ) (6) where GEN DERIV (x) denotes all sound derivations that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and"
K19-2016,D18-1194,0,0.0444899,"Missing"
K19-2016,J16-3001,1,0.855014,"tions that yield x. Then we get a target graph by evaluating D0 . We will detail our composition-based parser in §4. 167 Transition-Based Approach. This type of approach is inspired by the successful design of transition-based dependency tree parsing (Yamada and Matsumoto, 2003; Nivre, 2008). To the best of our knowledge, Sagae and Tsujii (2008) firstly apply this type of approach to predict predicate– argument structures grounded in HPSG (Miyao et al., 2005). A number of new transition systems and disambiguation models have been discussed for parsing into different graphs (Wang et al., 2015; Zhang et al., 2016; Buys and Blunsom, 2017; Gildea et al., 2018; Sun et al., 2019) node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries. Take the output structure in Figure 1 for example: go v 1 and want v 1 indicate surface concepts, while proper q and named indicate abstract concepts. To avoid proliferation of concepts, some concept"
P09-2064,J02-3001,0,0.0202575,"ai , aj , r) · w} ST H (ai , aj , r) = P where ψ is the feature map and w is the parameter vector to learn. Note that the model predicts the rank of ai and aj through calculating ST H (ai , aj , r) rather than ST H (aj , ai , r), where ai precedes aj . In other words, the position information is implicitly encoded in the model rather than explicitly as a feature. The system extracts a number of features to represent various aspects of the syntactic structure of a pair of arguments. All features are listed in Table 1. The Path features are designed as a sequential collection of phrase tags by (Gildea and Jurafsky, 2002). We also use Single Character Category Path, in which each phrase tag is clustered to a category defined by its first character (Pradhan et al., 2005). To characterize the relation between two constituents, we combine features of the two individual arguments as new features (i.e. conjunction features). For example, if the category of the first argument is NP and the category of the second is S, then the conjunction of category feature is NP-S. Prediction Method Assigning different labels to possible rank between two arguments ai and aj , such as labeling ai  aj as ””, identification of them"
P09-2064,W05-0625,0,0.0579635,"Missing"
P09-2064,J08-2002,0,0.0380799,"Missing"
P09-2064,J05-1004,0,0.0404489,"inguistic theories but the lowest in some other theories (Levin and Hovav, 2005). In this paper, the proto-role theory (Dowty, 1991) is taken into account to rank PropBank arguments, partially resolving the two problems above. There are three key points in our solution. First, the rank of Arg0 is the highest. The Agent is almost without exception the highest role in proposed hierarchies. Though PropBank defines semantic roles on a verb by verb basis, for a particular verb, Arg0 is generally the argument exhibiting features of a prototypical Agent while Arg1 is a prototypical Patient or Theme (Palmer et al., 2005). As being the proto-Agent, the rank of Arg0 is higher than other numbered arguments. Second, the rank of the Arg1 is second highest or lowest. Both hierarchy of Arg1 are tested and discussed in section 4. Third, we do not rank other arguments. Two sets of roles closely correspond to numbered arguments: 1) referenced arguments and 2) continuation arguments. To adapt the relation to help these two kinds of arguments, the equivalence relation is divided into several sub-categories. In summary, relations of two arguments ai and aj in this paper include: 1) ai  aj : ai is higher than aj , 2) ai ≺"
P09-2064,J08-2005,0,0.0178431,"ents as new features (i.e. conjunction features). For example, if the category of the first argument is NP and the category of the second is S, then the conjunction of category feature is NP-S. Prediction Method Assigning different labels to possible rank between two arguments ai and aj , such as labeling ai  aj as ””, identification of thematic rank can be formulated as a classification problem. De3 Re-ranking Models for SRC Toutanova et al. (2008) empirically showed that global information is important for SRL and that 254 4 structured solutions outperform local semantic role classifiers. Punyakanok et al. (2008) raised an inference procedure with integer linear programming model, which also showed promising results. Identifying relations among arguments can provide structural information for SRL. Take the sentence ”[Arg0 She] [V addressed] [Arg1 her husband] [ArgM −M N R with her favorite nickname].” for example, if the thematic rank of she and her husband is predicted as that she is higher than her husband, then her husband should not be assigned the highest role. To incorporate the relation information to local classification results, we employ re-ranking approach. Assuming that the local semantic"
P09-2064,A00-2018,0,\N,Missing
P10-2019,P10-2031,1,0.797416,"n the traditional ones. An interesting phenomenon is that though the second kind of chunk type definition increases the complexity of the parsing job, it achieves better bracketing performance. Experiments and Analysis Experimental Setting Experiments in previous work are mainly based on CPB 1.0 and CTB 5.0. We use CoNLL-2005 shared task software to process CPB and CTB. To facilitate comparison with previous work, we use the same data setting with (Xue, 2008). Nearly all previous research on Chinese SRL evaluation use this setting, also including (Ding and Chang, 2008, 2009; Sun et al., 2009; Sun, 2010). The data is divided into three parts: files from chtb 081 to chtb 899 are used as training set; files from chtb 041 to chtb 080 as development set; files from chtb 001 to chtb 040, and chtb 900 to chtb 931 as test set. Both syntactic chunkers and semantic chunkers are trained and evaluated by using the same data set. By using CPB and CTB, we can extract gold standard semantics-driven shallow chunks according to our definition. We use this kind of gold chunks automatically generated from training data to train syntactic chunkers. For both syntactic and semantic chunking, we used conditional r"
P10-2019,Y09-2011,1,0.462392,"is simply the phrase type, such as NP, PP, of current chunk. The column CHUNK 1 illustrates this kind of chunk type definition. The second is more complicated. Inspired by (Klein and Manning, 2003), we split one phrase type into several subsymbols, which contain category information of current constituent’s parent. For example, an NP immediately dominated by a S, will be substituted by NPˆS. This strategy severely increases the number of chunk types and make it hard to train chunking models. To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in (Sun and Sui, 2009). The column CHUNK 2 illustrates this definition. E.g., NPˆS implicitly represents Subject while NPˆVP represents Object. 1. ∀c[i : j] ∈ C, c[i : j] ∈ Ts ; 2. ∀c[i : j] ∈ C, ∀c[iv : j v ] ∈ ∪Ps , j < iv or i &gt; j v or iv ≤ i ≤ j ≤ j v ; 3. ∀c[i : j] ∈ C, the parent of c[i : j] does not satisfy the condition 2. 4. ∀C 0 satisfies above conditions, C 0 ⊂ C. The first condition guarantees that every chunk is a constituent. The second condition means that chunks do not overlap with arguments, and further guarantees that semantic chunking can recover all arguments with the last condition. The third c"
P10-2019,P06-2013,0,0.336097,"tagging semantic chunks in one-stage, and 2) identifying argument boundaries as a chunking task and labeling their semantic types as a classification task. On the basis of syntactic chunks, they define semantic chunks which do not overlap nor embed using IOB2 representation. Syntactic chunks outside a chunk receive the tag O (Outside). For syntactic chunks forming a chunk of type A*, the first chunk receives the B-A* tag (Begin), and the remaining ones receive the tag I-A* (Inside). Then a SRL system can work directly by using sequence tagging technique. Shallow chunk definition presented in (Chen et al., 2006) is used in their experiments. The definition of syntactic and semantic chunks is illustrated Figure 1. For example, “保险公司/the insurance company”, consisting of two nouns, is a noun phrase; in the syntactic chunking stage, its two components “保 险” and “公司” should be labeled as B-NP and I-NP. Because this phrase is the Agent of the predicate “提 供/provide”, it takes a semantic chunk label B-A0. In the semantic chunking stage, this phrase should be labeled as B-A0. 3 3.1 Semantics-Driven Shallow Parsing Motivation There are two main jobs of semantic chunking: 1) grouping words as argument candida"
P10-2019,W08-2121,0,0.0805661,"Missing"
P10-2019,J08-2004,0,0.143347,"he traditional shallow parsing (Chen et al., 2006) and ours. We think one main reason is that syntactic chunks in our new definition are larger than the traditional ones. An interesting phenomenon is that though the second kind of chunk type definition increases the complexity of the parsing job, it achieves better bracketing performance. Experiments and Analysis Experimental Setting Experiments in previous work are mainly based on CPB 1.0 and CTB 5.0. We use CoNLL-2005 shared task software to process CPB and CTB. To facilitate comparison with previous work, we use the same data setting with (Xue, 2008). Nearly all previous research on Chinese SRL evaluation use this setting, also including (Ding and Chang, 2008, 2009; Sun et al., 2009; Sun, 2010). The data is divided into three parts: files from chtb 081 to chtb 899 are used as training set; files from chtb 041 to chtb 080 as development set; files from chtb 001 to chtb 040, and chtb 900 to chtb 931 as test set. Both syntactic chunkers and semantic chunkers are trained and evaluated by using the same data set. By using CPB and CTB, we can extract gold standard semantics-driven shallow chunks according to our definition. We use this kind of"
P10-2019,D08-1034,0,\N,Missing
P10-2019,D09-1153,1,\N,Missing
P10-2019,P03-1054,0,\N,Missing
P10-2019,N01-1025,0,\N,Missing
P10-2031,N04-1032,0,0.690954,"gnificant improvement over the best reported performance 92.0. We are further concerned with the effect of parsing in Chinese SRL. We empirically analyze the two-fold effect, grouping words into constituents and providing syntactic information. We also give some preliminary linguistic explanations. 1 Introduction 2 Previous work on Chinese Semantic Role Labeling (SRL) mainly focused on how to implement SRL methods which are successful on English. Similar to English, parsing is a standard pre-processing for Chinese SRL. Many features are extracted to represent constituents in the input parses (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). By using these features, semantic classifiers are trained to predict whether a constituent fills a semantic role. Developing features that capture the right kind of information encoded in the input parses has been shown crucial to advancing the state-of-the-art. Though there has been some work on feature design in Chinese SRL, information encoded in the syntactic trees is not fully exploited and requires more research effort. In this paper, we propose a set of additional Chinese SRL The Chinese PropBank (CPB) is a semantic annotation for the syntactic trees"
P10-2031,P10-2019,1,0.862317,"uous sequence of integers, in the form of AN (N is a natural number); the adjuncts are annotated as such with the label AM followed by a secondary tag that represents the semantic classification of the adjunct. The assignment of semantic roles is illustrated in Figure 1, where the predicate is the verb “调查/investigate”. E.g., the NP “事故原因/the cause of the accident” is labeled as A1, meaning that it is the Patient. In previous research, SRL methods that are successful on English are adopted to resolve Chinese SRL (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun et al., 2009; Sun, 2010). Xue (2008) produced complete and systematic research on full parsing based methods. ∗ The work was partially completed while this author was at Peking University. 168 Proceedings of the ACL 2010 Conference Short Papers, pages 168–172, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3 IP bbbbbbbbb bbbbbbbbbb bb bb bb bb bbbbbb A0 NP A majority of features used in our system are a combination of features described in (Xue, 2008; Ding and Chang, 2008) as well as the word formation and coarse frame features introduced in (Sun et al., 2009), the c-command thread"
P10-2031,Y09-2011,1,0.74747,"ncode the structural information. Category of ck ’s parent; head word and POS of head word of parent, left sibling and right sibling of ck . Lexicalized Rewrite rules: Conjuction of rewrite rule and head word of its corresponding RHS. These features of candidate (lrw-c) and its parent (lrw-p) are used. For example, this lrwc feature of the NP “事 故 原 因” in Figure 1 is N P → N N + N N (原因). Partial Path: Path from the ck or wv to the lowest common ancestor of ck and wv . One path feature, hence, is divided into left path and right path. Clustered Path: We use the manually created clusters (see (Sun and Sui, 2009)) of categories of all nodes in the path (cpath) and right path. C-commander thread between ck and wv (cct): (proposed by (Sun et al., 2008)). For example, this feature of the NP “警方” in Figure 1 is N P + ADV P + ADV P + V V . Head Trace: The sequential container of the head down upon the phrase (from (Sun and Sui, 2009)). We design two kinds of traces (htr-p, htrw): one uses POS of the head word; the other uses the head word word itself. E.g., the head word of 事故原因 is “原因” therefore these feature of this NP are NP↓NN and NP↓原因. Combination features: verb class+ck , wh +wv , wh +Position, wh +"
P10-2031,C08-1105,1,0.828321,"omplete and systematic research on full parsing based methods. ∗ The work was partially completed while this author was at Peking University. 168 Proceedings of the ACL 2010 Conference Short Papers, pages 168–172, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3 IP bbbbbbbbb bbbbbbbbbb bb bb bb bb bbbbbb A0 NP A majority of features used in our system are a combination of features described in (Xue, 2008; Ding and Chang, 2008) as well as the word formation and coarse frame features introduced in (Sun et al., 2009), the c-command thread features proposed in (Sun et al., 2008). We give a brief description of features used in previous work, but explain new features in details. For more information, readers can refer to relevant papers and our source codes2 that are well commented. To conveniently illustrate, we denote a candidate constituent ck with a fixed context wi−1 [ck wi ...wh ...wj ]wj+1 , where wh is the head word of ck , and denote predicate in focus with v w v w v w v w v , where w v is the a context w−2 −1 +1 +2 predicate in focus. VP d dddddididii dddddididiiiii ddddddd i AM-TMP AM-MNR VP ZZ ZZZZZZZ ZZZZZZZ ZZZZZZ NN ADVP ADVP Rel 警方 police AD AD VV 正在 n"
P10-2031,D09-1153,1,0.809375,"eled with a contiguous sequence of integers, in the form of AN (N is a natural number); the adjuncts are annotated as such with the label AM followed by a secondary tag that represents the semantic classification of the adjunct. The assignment of semantic roles is illustrated in Figure 1, where the predicate is the verb “调查/investigate”. E.g., the NP “事故原因/the cause of the accident” is labeled as A1, meaning that it is the Patient. In previous research, SRL methods that are successful on English are adopted to resolve Chinese SRL (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun et al., 2009; Sun, 2010). Xue (2008) produced complete and systematic research on full parsing based methods. ∗ The work was partially completed while this author was at Peking University. 168 Proceedings of the ACL 2010 Conference Short Papers, pages 168–172, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3 IP bbbbbbbbb bbbbbbbbbb bb bb bb bb bbbbbb A0 NP A majority of features used in our system are a combination of features described in (Xue, 2008; Ding and Chang, 2008) as well as the word formation and coarse frame features introduced in (Sun et al., 2009), the c-co"
P10-2031,W04-3224,0,0.0250017,"Missing"
P10-2031,J08-2004,0,0.494365,"er the best reported performance 92.0. We are further concerned with the effect of parsing in Chinese SRL. We empirically analyze the two-fold effect, grouping words into constituents and providing syntactic information. We also give some preliminary linguistic explanations. 1 Introduction 2 Previous work on Chinese Semantic Role Labeling (SRL) mainly focused on how to implement SRL methods which are successful on English. Similar to English, parsing is a standard pre-processing for Chinese SRL. Many features are extracted to represent constituents in the input parses (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). By using these features, semantic classifiers are trained to predict whether a constituent fills a semantic role. Developing features that capture the right kind of information encoded in the input parses has been shown crucial to advancing the state-of-the-art. Though there has been some work on feature design in Chinese SRL, information encoded in the syntactic trees is not fully exploited and requires more research effort. In this paper, we propose a set of additional Chinese SRL The Chinese PropBank (CPB) is a semantic annotation for the syntactic trees of the Chin"
P10-2031,D08-1034,0,0.264882,"reported performance 92.0. We are further concerned with the effect of parsing in Chinese SRL. We empirically analyze the two-fold effect, grouping words into constituents and providing syntactic information. We also give some preliminary linguistic explanations. 1 Introduction 2 Previous work on Chinese Semantic Role Labeling (SRL) mainly focused on how to implement SRL methods which are successful on English. Similar to English, parsing is a standard pre-processing for Chinese SRL. Many features are extracted to represent constituents in the input parses (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). By using these features, semantic classifiers are trained to predict whether a constituent fills a semantic role. Developing features that capture the right kind of information encoded in the input parses has been shown crucial to advancing the state-of-the-art. Though there has been some work on feature design in Chinese SRL, information encoded in the syntactic trees is not fully exploited and requires more research effort. In this paper, we propose a set of additional Chinese SRL The Chinese PropBank (CPB) is a semantic annotation for the syntactic trees of the Chinese TreeBank (CTB). The"
P10-2031,D08-1059,0,0.0896866,"Missing"
P11-1139,D07-1117,0,0.267232,"ters designed with different views have complementary strength. We argue that the agreements and disagreements of different solvers can be used to construct an intermediate sub-word structure for joint segmentation and tagging. Since the sub-words are large enough in practice, the decoding for POS tagging over subwords is efficient. Finally, the Chinese language is characterized by the lack of morphology that often provides important clues for POS tagging, and the POS tags contain much syntactic information, which need context information within a large window for disambiguation. For example, Huang et al. (2007) showed the effectiveness of utilizing syntactic information to rerank POS tagging results. As a result, the capability to represent rich contextual features is crucial to a POS tagger. In this work, we use a representation-efficiency tradeoff through stacked learning, a way of approximating rich non-local feaProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1385–1394, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics tures. This paper describes a novel stacked sub-word model. Given multiple word segmentations of o"
P11-1139,P08-1102,0,0.661509,"Missing"
P11-1139,C08-1049,0,0.801277,"d to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation. A challenge for joint approaches is the large combined search 1385 space, which makes efficient decoding and structured learning of parameters very hard. Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable. To overcome such efficiency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work (Zhang and Clark, 2010; Jiang et al., 2008b). In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging. Our work is motivated by several characteristics of this problem. First of all, a majority of words are easy to identify in the segmentation problem. For example, a simple maximum matching segmenter can achieve an f-score of about 90. We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words. Second, segmenters designed with different views have complementary strength. We argue that the agreements and disagreement"
P11-1139,P09-1058,0,0.632328,"solvers SegW l , SegC l and SegTagL l , which process the Sl and provide inaccurate predictions. Then the inaccurate predictions are merged into subword sequences and Sl is extended to Sl0 . Finally, the sub-word tagger is trained on the whole extended data set S 0 . 4 Experiments 4.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank (CTB) in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010). Table 2 shows the statistics of our experimental settings. Data set Training Devel. Test CTB files 1-270 400-931 1001-1151 301-325 271-300 # of sent. 18,089 # of words 493,939 350 348 6821 8008 Table 2: Training, development and test data on CTB 5.0 Three metrics are used for evaluation: precision (P), recall (R) and balanced f-score (F) defined by 2PR/(P+R). Precision is the relative amount of correct words in the system output. Recall is the relative amount of correct words compared to the gold standard annotations. For segmentation, a token is considered to be corr"
P11-1139,N01-1025,0,0.0473866,"revious research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage of this model is the difficulty in incorporating the whole"
P11-1139,W04-3236,0,0.289102,"nd empirical analyses. Section 5 concludes the paper. 2 2.1 Background Problem Definition Given a sequence of characters c = (c1 , ..., c#c ), the task of word segmentation and POS tagging is 1386 to predict a sequence of word and POS tag pairs y = (hw1 , p1 i, hw#y , p#y i), where wi is a word, pi is its POS tag, and a “#” symbol denotes the number of elements in each variable. In order to avoid error propagation and make use of POS information for word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is locate"
P11-1139,P08-1108,0,0.0283415,"yt ) : y ˆ tk = gkl (xt ) dataset Sˆ = {(hxt , y Segmented Segmented Segmented and xt ∈ Sl }. Finally, each gk is trained on the origisentences sentences sentences nal dataset and the second level predictor h is trained ˆ The intent of the cross-validation scheme is on S. that ytk is similar to the prediction produced by a Merging predictor which is learned on a sample that does not include xt . Sub-word Sub-word tagStacked learning has been applied as a system ensequences ger SubTag semble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This frameFigure 1: Workflow of the stacked sub-word model. work is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In In our model, segmentation and POS tagging inthe machine learning research, stacked learning has teract with each other in two processes. First, albeen applied to structured prediction (Cohen and though SegT agL is locally trained, it resolves the 1387 work, we introduce two simple but important refinements: (1) to shuffle the sample orders in each iteration and (2) to average the parameters in each iteration as the final paramete"
P11-1139,W95-0107,0,0.022246,"word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage"
P11-1139,C10-2139,1,0.663927,"a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solvers may use previously predicted words and their POS information as clues to find a new word. After one word is found and classified, solvers move on and search for the next possible word. This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), and was then further used in POS tagging in (Zhang and Clark, 2008). In our previous work(Sun, 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation. We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previous focus, the diversity of different models designed with different views is utilized to construct sub-"
P11-1139,D08-1017,0,0.14968,"Missing"
P11-1139,W03-0433,0,0.0597527,"ruct the augmented ˆ t1 , ..., y ˆ tK i, yt ) : y ˆ tk = gkl (xt ) dataset Sˆ = {(hxt , y Segmented Segmented Segmented and xt ∈ Sl }. Finally, each gk is trained on the origisentences sentences sentences nal dataset and the second level predictor h is trained ˆ The intent of the cross-validation scheme is on S. that ytk is similar to the prediction produced by a Merging predictor which is learned on a sample that does not include xt . Sub-word Sub-word tagStacked learning has been applied as a system ensequences ger SubTag semble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This frameFigure 1: Workflow of the stacked sub-word model. work is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In In our model, segmentation and POS tagging inthe machine learning research, stacked learning has teract with each other in two processes. First, albeen applied to structured prediction (Cohen and though SegT agL is locally trained, it resolves the 1387 work, we introduce two simple but important refinements: (1) to shuffle the sample orders in each iteration and (2) to average the"
P11-1139,O03-4002,0,0.801715,"into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage of this model is the difficulty in incorporating the whole word information. The second kind of solution is the “word-based” method, where the basic predicting units are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solver"
P11-1139,P07-1106,0,0.272353,"s are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solvers may use previously predicted words and their POS information as clues to find a new word. After one word is found and classified, solvers move on and search for the next possible word. This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), and was then further used in POS tagging in (Zhang and Clark, 2008). In our previous work(Sun, 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation. We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previou"
P11-1139,P08-1101,0,0.526653,"des the paper. 2 2.1 Background Problem Definition Given a sequence of characters c = (c1 , ..., c#c ), the task of word segmentation and POS tagging is 1386 to predict a sequence of word and POS tag pairs y = (hw1 , p1 i, hw#y , p#y i), where wi is a word, pi is its POS tag, and a “#” symbol denotes the number of elements in each variable. In order to avoid error propagation and make use of POS information for word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method"
P11-1139,D10-1082,0,0.722078,"that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation. A challenge for joint approaches is the large combined search 1385 space, which makes efficient decoding and structured learning of parameters very hard. Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable. To overcome such efficiency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work (Zhang and Clark, 2010; Jiang et al., 2008b). In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging. Our work is motivated by several characteristics of this problem. First of all, a majority of words are easy to identify in the segmentation problem. For example, a simple maximum matching segmenter can achieve an f-score of about 90. We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words. Second, segmenters designed with different views have complementary strength. We argue that the agreeme"
P11-1139,N06-2049,0,0.018365,"words from a subword sequence is very high. The quality of the final tagger relies on the quality of the sub-word tagger. If a high performance sub-word tagger can be constructed, the whole task can be well resolved. The statistics will also empirically show that subwords are significantly larger than characters and only slightly smaller than words. As a result, the search space of the sub-word tagging is significantly shrunken, and exact Viterbi decoding without approximately pruning can be efficiently processed. This property makes nearly all popular sequence labeling algorithms applicable. Zhang et al. (2006) described a sub-word based tagging model to resolve word segmentation. To get the pieces which are larger than characters but smaller than words, they combine a character-based segmenter and a dictionary matching segmenter. Our contributions include (1) providing a formal definition of our sub-word structure that is based on multiple segmentations and (2) proposing a stacking method to acquire sub-words. To resolve the classification problem, we use the linear SVM classifier LIBLINEAR2 . 3.2 3.3 The Coarse-grained Solvers We systematically described the implementation of two state-of-the-art"
P11-1139,W03-1728,0,\N,Missing
P12-1025,J07-3004,0,0.00703274,"acquire 00 = D ST agctb . We can a new labeled data set Dctb ppd→ctb 00 . If re-train the STagctb model with Dctb ∪ Dctb 00 we use the gold PPD-style labels of Dctb to extract sub-words, the new model will overfit to the gold PPD-style labels, which are unavailable at test time. To avoid this training/test mismatch problem, we also employ a 10-fold cross validation procedure to add noise. It is not a new topic to convert corpus from one formalism to another. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al., 2004) and LFG (Cahill et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training"
P12-1025,P08-1102,0,0.324178,"Missing"
P12-1025,C08-1049,0,0.107848,"itted as a supplemental material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method"
P12-1025,P09-1059,0,0.816229,"acked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their method"
P12-1025,P09-1058,0,0.0932823,"ersion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4 . In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling problems. Among several parameter estimation methods provided by wapiti, our aux"
P12-1025,P10-1052,0,0.0183469,"Missing"
P12-1025,W04-3236,0,0.08008,"s data set is submitted as a supplemental material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the"
P12-1025,P09-1006,0,0.053446,"re trained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heter"
P12-1025,P08-1108,0,0.101577,"erent training data, we can construct multiple heterogeneous systems. These systems produce similar linguistic analysis which holds the same high level linguistic principles but differ in details. A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based on the input, the less accurate target structure and complementary structures. This idea is very close to stacked learning (Wolpert, 1992), which is well developed for ensemble learning, and successfully applied to some NLP tasks, e.g. dependency parsing (Nivre and McDonald, 2008; Torres Martins et al., 2008). Formally speaking, our idea is to include two “levels” of processing. The first level includes one 235 AS ⇒ u:44; DEC ⇒ u:83; DEG ⇒ u:123; LB ⇒ p:1; OD ⇒ m:41; SP ⇒ u:1; VE ⇒ v:13; CS ⇒ c:3; d:1; MSP ⇒ c:2; u:1; CC ⇒ c:73; p:5; v:2; LC ⇒ f:51; Ng:3; v:1; u:1; VA ⇒ a:57; i:4; z:2; ad:1; b:1; VV ⇒ v:382; i:5; a:3; Vg:2; vn:2; n:2; p:2; w:1; AD ⇒ d:149; c:11; ad:6; z:4; a:3; v:2; n:1; r:1; m:1; f:1; t:1; CD ⇒ m:134; DEV ⇒ u:7; ETC ⇒ u:9; NT ⇒ t:98; PU ⇒ w:552; VC ⇒ v:32; BA ⇒ p:2; d:1; DT ⇒ r:15; b:1; PN ⇒ r:53; n:2; M ⇒ q:101; n:11; v:1; P ⇒ p:133; v:4; c:2; Vg:1;"
P12-1025,W95-0107,0,0.0773981,"independently built on different training data. The second level processing consists of an inference function h that takes as input hx, f1 (x), ..., fK (x)i3 and outputs a final prediction h(x, f1 (x), ..., fK (x)). The only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensemble are different. This framework is general and flexible, in the sense that it assumes almost nothing about the individual systems and take them as black boxes. 4.2 A Character-based Tagger With IOB2 representation (Ramshaw and Marcus, 1995), the problem of joint segmentation and tagging can be regarded as a character classification task. Previous work shows that the character-based approach is an effective method for Chinese lexical processing. Both of our feature- and structure-based stacking models employ base character-based taggers to generate multiple segmentation and tagging results. Our base tagger use a discriminative sequential classifier to predict the POS tag with positional information for each character. Each character can be assigned one of two possible boundary tags: “B” for a character that begins a word and “I”"
P12-1025,D11-1090,1,0.0791492,"finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniqu"
P12-1025,W10-4144,1,0.594589,"uce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotation"
P12-1025,C10-2139,1,0.656916,"tence of multiple resources, such data cannot be simply put together for training systems, because almost all of statistical NLP systems assume homogeneous annotation. Therefore, it is not only interesting but also important to study how to fully utilize heterogeneous resources to improve Chinese lexical processing. There are two main types of errors in statistical NLP: (1) the approximation error that is due to the intrinsic suboptimality of a model and (2) the estimation error that is due to having only finite training data. Take Chinese word segmentation for example. Our previous analysis (Sun, 2010) shows that one main intrinsic disadvantage of characterbased model is the difficulty in incorporating the whole word information, while one main disadvantage of word-based model is the weak ability to express word formation. In both models, the significant decrease of the prediction accuracy of out-ofvocabulary (OOV) words indicates the impact of the estimation error. The two essential characteristics about systematic diversity of heterogeneous annota234 tions can be utilized to reduce both approximation and estimation errors. 3.1 Analysis of the CTB and PPD Standards This paper focuses on tw"
P12-1025,P11-1139,1,0.549812,"processing tasks. We empirically analyze the diversity between two representative popular heterogeneous corpora, i.e. 232 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232–241, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). To that end, we manually label 200 sentences from CTB with PPD-style annotations.1 Our analysis confirms the aforementioned two properties of heterogeneous annotations. Inspired by the sub-word tagging method introduced in (Sun, 2011), we propose a structure-based stacking model to fully utilize heterogeneous word structures to reduce the approximation error. In particular, joint word segmentation and POS tagging is addressed as a two step process. First, character-based taggers are respectively trained on heterogeneous annotations to produce multiple analysis. The outputs of these taggers are then merged into sub-word sequences, which are further re-segmented and tagged by a sub-word tagger. The sub-word tagger is designed to refine the tagging result with the help of heterogeneous annotations. To reduce the estimation er"
P12-1025,D08-1017,0,0.0484194,"Missing"
P12-1025,I11-1035,0,0.00729695,"agger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniques. Although several in"
P12-1025,P08-1101,0,0.0237511,"al material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Mar"
P12-1025,D10-1082,0,0.264343,"s are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Markov tagging (Zhang and Clark, 2008; Zhang and Clark, 2010), where the basic predicting units are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. Solvers may use previously predicted words and their POS information as clues to process a new word. In addition, we proposed an effective and efficient stacked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs"
P12-1026,J92-4003,0,0.053679,"h other clustering algorithms. 3.1.2 Data Chinese Gigaword is a comprehensive archive of newswire text data that has been acquired over several years by the Linguistic Data Consortium (LDC). The large-scale unlabeled data we use in our experiments comes from the Chinese Gigaword (LDC2005T14). We choose the Mandarin news text, i.e. Xinhua newswire. This data covers all news published by Xinhua News Agency (the largest news agency in China) from 1991 to 2004, which contains over 473 million characters. Brown Clustering Our first choice is the bottomup agglomerative word clustering algorithm of (Brown et al., 1992) which derives a hierarchical clustering of words from unlabeled data. This algorithm generates a hard clustering – each word belongs to exactly one cluster. The input to the algorithm is sequences of words w1 , ..., wn . Initially, the algorithm starts with each word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languag"
P12-1026,A00-2018,0,0.748666,"Missing"
P12-1026,W02-1001,0,0.220434,"Missing"
P12-1026,J03-4003,0,0.494678,"Missing"
P12-1026,gimenez-marquez-2004-svmtool,0,0.060584,"Missing"
P12-1026,P10-1110,0,0.109628,"es includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the prin"
P12-1026,N09-2054,0,0.505726,"011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. punctuations. From this table, we can see that words with low frequency, especially the out-of-vocabulary (OOV) words, are hard to label. However, when a word is very frequently used, its behavior is very complicated and therefore hard to predict. A typical example of such words is"
P12-1026,D07-1117,0,0.056847,"rds. To conveniently illustrate, we denote a word in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed ana"
P12-1026,P08-1068,0,0.128971,"Missing"
P12-1026,P10-1052,0,0.0840783,"Missing"
P12-1026,D11-1109,0,0.173648,"Missing"
P12-1026,P05-1010,0,0.184469,"Missing"
P12-1026,N04-1043,0,0.145546,"Missing"
P12-1026,E99-1010,0,0.0303481,"matic or substitutional similarity among words. 3.1.1 Clustering Algorithms Various clustering techniques have been proposed, some of which, for example, perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms. In this paper, we focus on distributional word clustering that is based on the assumption that words that appear in similar contexts (especially surrounding words) tend to have similar meanings. They have been successfully applied to many NLP problems, such as language modeling. We use the publicly available implementation MKCLS3 (Och, 1999) to train this model. We choose to work with these two algorithms considering their prior success in other NLP applications. However, we expect that our approach can function with other clustering algorithms. 3.1.2 Data Chinese Gigaword is a comprehensive archive of newswire text data that has been acquired over several years by the Linguistic Data Consortium (LDC). The large-scale unlabeled data we use in our experiments comes from the Chinese Gigaword (LDC2005T14). We choose the Mandarin news text, i.e. Xinhua newswire. This data covers all news published by Xinhua News Agency (the largest n"
P12-1026,P06-1055,0,0.305586,"Missing"
P12-1026,N07-1051,0,0.0474247,"Missing"
P12-1026,P07-1096,0,0.0619281,"Missing"
P12-1026,C10-2139,1,0.854465,"t least two clusters left, the algorithm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units, i.e. words, segmentation is a necessary pre-processing step for word clustering. Previous research shows that character-based segmentation models trained on labeled data are reasonably accurate (Sun, 2010). Furthermore, as shown in (Sun and Xu, 2011), appropriate string knowledge acquired from large-scale unlabeled data can significantly enhance a supervised model, especially for the prediction of out-of-vocabulary (OOV) words. P (wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) In this paper, we employ such supervised and semisupervised segmenters4 to process raw texts. where the function C maps a word w to its class 3.2 Improving Tagging with Cluster Features C(w). We use a publicly available package2 (Liang Our discriminative sequential tagger is easy to be exet al., 2005) to train this"
P12-1026,D11-1090,1,0.847189,"hm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units, i.e. words, segmentation is a necessary pre-processing step for word clustering. Previous research shows that character-based segmentation models trained on labeled data are reasonably accurate (Sun, 2010). Furthermore, as shown in (Sun and Xu, 2011), appropriate string knowledge acquired from large-scale unlabeled data can significantly enhance a supervised model, especially for the prediction of out-of-vocabulary (OOV) words. P (wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) In this paper, we employ such supervised and semisupervised segmenters4 to process raw texts. where the function C maps a word w to its class 3.2 Improving Tagging with Cluster Features C(w). We use a publicly available package2 (Liang Our discriminative sequential tagger is easy to be exet al., 2005) to train this model. tended with arbitrary features and the"
P12-1026,N03-1033,0,0.0975865,"Missing"
P12-1026,I05-3005,0,0.155187,"Missing"
P12-1026,P06-1054,0,0.104993,"window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setti"
P12-1026,D08-1059,0,0.0687714,"rrent token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting"
P12-1026,W09-3825,0,0.244173,"in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments"
P12-1026,I05-3027,0,\N,Missing
P14-1042,D11-1037,0,0.150172,"Missing"
P14-1042,P08-1006,0,0.451573,"Missing"
P14-1042,P06-2006,0,0.170001,"Missing"
P14-1042,J08-1002,0,0.0834445,"latively high precision but considerably low recall. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. Local vs. Non-local Although the micro accuracy of all dep"
P14-1042,P07-1032,0,0.219132,"Missing"
P14-1042,J08-4003,0,0.138881,"say the sequence of transitions is an oracle sequence. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) ind"
P14-1042,P09-1040,0,0.0718205,"Missing"
P14-1042,J07-4004,0,0.510125,"ortant role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗ Recent years have seen the introduction of a number of treebank-guided statistical parsers capable of generating considerably accurate parses for Chinese. With the high-quality GR resource at hand, we study data-driven GR parsing. Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually studied two transition systems that can generate more general graphs rather than trees. Inspired by their work, we study transition-based m"
P14-1042,W04-2407,0,0.0954544,"ing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ|i, β, A) ⇒ (σ, β, A) (σ|ik |. . . |i2 |i1 , β, A) ⇒ (σ|i1 |ik |. . . |i2 , β, A) 3.3 On"
P14-1042,D09-1085,0,0.138373,"Missing"
P14-1042,J07-3004,0,0.342597,"Missing"
P14-1042,P80-1024,0,0.634562,"Missing"
P14-1042,C08-1095,0,0.778542,"tionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ|i, β, A) ⇒ (σ, β, A) (σ|ik |. . . |i2 |i1 , β, A) ⇒ (σ|i1 |ik |. . . |i2 , β, A) 3.3 Online Reordering Among existing systems, Sagae and Tsujii’s is designed for projective graphs (denoted by G1 in Definition 1), and Titov et al.’s handles only a specific subset of non-proje"
P14-1042,W03-2401,0,0.0722133,"ction Algorithm Our treebank conversion algorithm borrows key insights from Lexical Functional Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001). LFG posits two levels of representation: c(onstituent)-structure and f(unctional)-structure minimally. C-structure is represented by phrasestructure trees, and captures surface syntactic configurations such as word order, while f-structure encodes grammatical functions. It is easy to extract a dependency backbone which approximates basic predicate-argument-adjunct structures from f-structures. The construction of the widely used PARC DepBank (King et al., 2003) is a good example. Beyond CTB annotations: tracing more. Natural languages do not always interpret linguistic material locally. In order to obtain accurate and complete GR, predicate-argument, or logical form representations, a hallmark of deep grammars is that they usually involve a non-local dependency resolution mechanism. CTB trees utilize ECs and coindexed materials to represent long-distance dependencies. An EC is a nominal element that does not have any phonological content and is therefore unpronounced. Two kinds of anaphoric ECs, i.e. big PRO and trace, are annotated in CTB. Theoreti"
P14-1042,P12-1026,1,0.830759,"0 ) strictly precedes any L(j), j ∈ σ). If k > 0, we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also"
P14-1042,P10-1001,0,0.0800084,"is the set of all graphs without self-loop. Proof. It follows immediately from the fact that G ∈ G|V |, ∀G = hV, Ei. The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S1 1 and S2 . 1 Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1 . 451 weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. w1 w2 w3 w4 w5 w6 w7 w8 w9 Figure 5: A graph that can be parsed with S3 with a transition sequence SSSSR3 SR3 APAPR2 R3 SR3 SR3 APAPAPAPAP, where S stands for S HIFT, R for ROTATE, A for L EFT-A RC, and P for P OP. But the approximate procedure fails to find the"
P14-1042,Q13-1025,1,0.829862,"else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also hold for G’s. not equal to GK in non-trivial cases. Theorem 4. Gˆ"
P14-1042,D11-1090,1,0.801829,"phical order (here we assume L(j0 ) strictly precedes any L(j), j ∈ σ). If k > 0, we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 a"
P14-1042,C10-2162,0,0.0740776,"igure 1: An example: Pudong recently enacted regulatory documents involving the economic field. The symbol “*ldd” indicates long-distance dependencies; “subj*ldd” between the word “涉及/involve” and the word “文件/documents” represents a long-range subject-predicate relation. The arguments and adjuncts of the coordinated verbs, namely “颁布/issue” and “实行/practice,” are separately yet distributively linked the two heads. structure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. our problem, we extend Titov et al.’s work and study what we cal"
P14-1042,W09-3825,0,0.0567214,"we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also hold for G’s. not equal to GK in non-trivial"
P14-1042,P09-1039,0,0.103764,"e. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate"
P14-1042,P11-1069,0,0.149804,"le the improved numeric accuracies practically certify the benefits. The two points merit further exploration to more expressive transition systems for deep dependency parsing, at least for Chinese. The labeled evaluation scores on the final test data are presented in Tab. 4. Test S5 UP 83.93 UR 79.82 UF 81.82 LRL 80.94 LRNL 54.38 Table 4: Performance on the test data. 4.3 Precision vs. Recall A noteworthy thing about the overall performance is that the precision is promising but the recall is too low behind. This difference is consistent with the result obtained by a shift-reduce CCG parser (Zhang and Clark, 2011). The functor-argument dependencies generated by that parser also has a relatively high precision but considerably low recall. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, success"
P14-1042,C10-1122,0,0.261191,"gulatory 文件 document Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field. The symbol “*ldd” indicates long-distance dependencies; “subj*ldd” between the word “涉及/involve” and the word “文件/documents” represents a long-range subject-predicate relation. The arguments and adjuncts of the coordinated verbs, namely “颁布/issue” and “实行/practice,” are separately yet distributively linked the two heads. structure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. our problem, we extend Titov et al.’s work and"
P14-1042,P11-2033,0,0.0356007,"s without self-loop. Proof. It follows immediately from the fact that G ∈ G|V |, ∀G = hV, Ei. The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S1 1 and S2 . 1 Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1 . 451 weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. w1 w2 w3 w4 w5 w6 w7 w8 w9 Figure 5: A graph that can be parsed with S3 with a transition sequence SSSSR3 SR3 APAPR2 R3 SR3 SR3 APAPAPAPAP, where S stands for S HIFT, R for ROTATE, A for L EFT-A RC, and P for P OP. But the approximate procedure fails to find the oracle, since R2 R3 in"
P14-1042,N12-1030,0,0.379065,"two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. Local vs. Non-local Although the micro accuracy of all dependencies are considerably good, the ability of current state"
P14-1042,P10-1034,0,0.268424,"Missing"
P14-1042,W03-3023,0,0.200374,"), cm ∈ Ct , and Acm = A, we say the sequence of transitions is an oracle sequence. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et"
P14-1042,W11-2907,0,0.194823,"Missing"
P14-1042,N04-1013,0,\N,Missing
P15-1149,D11-1031,0,0.388421,"ic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3 . Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3 The unlabeled parsing results are not reported in the original"
P15-1149,W13-2322,0,0.182655,"Missing"
P15-1149,D11-1037,0,0.0404513,"Missing"
P15-1149,C10-1011,0,0.146901,"ncy trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of different models, we utilize the averaged perceptron algorithm. W"
P15-1149,P07-1032,0,0.195199,"natory Categorial Grammar (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicab"
P15-1149,C04-1041,0,0.100725,"Missing"
P15-1149,J07-4004,0,0.619934,"natory Categorial Grammar (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicab"
P15-1149,P02-1042,0,0.0866988,"nk, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to"
P15-1149,W02-1001,0,0.704001,"ted by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of different models, we utilize the averaged perceptron algorithm. We implement our own the predicate- and argument-centric models. To perform tree parsing, we re-use the open-source implementation provided by the mate-tool. See the source code attached for details. We set iteration 5 to train predicate- and argument-centric models and 10 for the tree approximation model. To perform dual decomposition, we set the maximum iteration 200. Experimental Setup 1 CCGbank is a translation of the Penn Treebank into a corpus of"
P15-1149,C96-1058,0,0.436886,"Missing"
P15-1149,P10-1035,0,0.517446,", providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres M"
P15-1149,J07-3004,0,0.682088,"Missing"
P15-1149,D10-1002,0,0.192969,"nt structure. Our experiments were performed using CCGBank which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous resear"
P15-1149,W03-2401,0,0.0496616,"underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the"
P15-1149,P10-1001,0,0.24472,"Missing"
P15-1149,D10-1125,0,0.608752,"is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently produces globally optimal CCG dependency graphs acc"
P15-1149,Q13-1018,0,0.0403024,"Missing"
P15-1149,C12-2077,0,0.0413808,"Missing"
P15-1149,J93-2004,0,0.0536153,"general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role. The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars. Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis (Miyao et al., 2004). 2 Related Work Hockenmaier and Steedman (2007) developed linguistic resources, namely CCGBank, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler a"
P15-1149,S14-2082,0,0.247471,"ls as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Llu´ıs et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic parsing and semantic role labeling under 1storder factorization, we are interested in designing higher-order factorization models for more dense and general linguistic graphs. 3 3.1 Graph Factorization Background Notations Consider a sentence s = hw, pi with words w = w1 w2 · · · wn and POS-tags p = p1 p2 · · · pn . Fi"
P15-1149,P05-1012,0,0.61802,"rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars"
P15-1149,E06-1011,0,0.927776,"014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled d"
P15-1149,H05-1066,0,0.802292,"rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars"
P15-1149,W04-2407,0,0.109566,"Missing"
P15-1149,P08-1108,0,0.0413116,"PC+AC+TA Gr PC+AC+TA Tr PC+AC+TA Auli and Lopez Xu et al. UP 93.03 93.71 93.63 93.08 93.15 UR 92.03 92.72 92.83 92.44 91.06 UF 92.53 93.21 93.23 92.76 92.09 UEM 32.61 38.14 37.47 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improv"
P15-1149,Q13-1002,0,0.360478,"Missing"
P15-1149,Q14-1030,0,0.15802,"lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade"
P15-1149,P11-1008,0,0.0345487,"C+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3 . Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3 The unlabeled parsing results are not reported in the original paper. The figures presented in are provided b"
P15-1149,D10-1001,0,0.256493,"dencies. First, all arguments associated with the same predicate are highly correlated due to the nature that they approximates type-logical semantics. Second, all predicates govern the same argument exhibit the hybrid syntactic/semantic, i.e. head-complementadjunct, relationships. Finally, the CCG dependency graphs are not but look very much like trees, which have many good computational properties. Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding. Inspired by (Koo et al., 2010; Rush et al., 2010), we employ dual decomposition to perform principled decoding. Though not always, we can obtain the optimal solution most of time. The time complexity of our parser is O(n3 ) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps sur"
P15-1149,P07-1079,0,0.0702333,"Missing"
P15-1149,C08-1095,0,0.420854,"the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been suc"
P15-1149,P09-1039,0,0.331719,"Missing"
P15-1149,D08-1017,0,0.0403206,"Auli and Lopez Xu et al. UP 93.03 93.71 93.63 93.08 93.15 UR 92.03 92.72 92.83 92.44 91.06 UF 92.53 93.21 93.23 92.76 92.09 UEM 32.61 38.14 37.47 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with resp"
P15-1149,P14-1021,0,0.166683,"res have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDo"
P15-1149,P11-1069,0,0.104639,"Missing"
P15-1149,P11-2033,0,0.036617,"and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of differe"
P17-1077,C10-1011,0,0.0580287,"B 833 B IND T WO PAGES(gA , gB ) 1 u(0) ← 0 2 for k ← 0..T do 3 gA ← arg maxg fA (g) − u(k)⊤ Ag 4 gB ← arg maxg fB (g) − u(k)⊤ Bg 5 if AgA + BgB ≤ 0 then 6 return gA , gB 7 else 8 u(k+1) ← u(k) + α(k) (AgA + BgB ) 9 return gA , gB 2010). For the other three data sets we use POStags provided by the shared task. We also use features extracted from trees. We consider two types of trees: (1) syntactic trees provided as a companion analysis by the shared task and CCGBank, (2) pseudo trees (Zhang et al., 2016) automatically extracted from semantic dependency annotations. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees for all data sets and also syntactic trees for CCG analysis, and use the companion syntactic analysis provided by the shared task for the other three data sets. Figure 7: The page binding algorithm. 5.2 Statistical Disambiguation We instead try to find the solution for maxu L(u). By using a subgradient method to calculate maxu L(u), we have an algorithm for joint decoding (see Figure 7). L(u) is divided into two optimization problems which can be decoded easily. Each sub-problem is still a parsing problem for noncrossing graphs. Only the scores of factors are modified"
P17-1077,P17-1193,1,0.833873,"onditions. Our parser obtains comparable results with a state-of-the-art transition-based parser. 1 Introduction Dependency analysis provides a lightweight and effective way to encode syntactic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P17-1077,W02-1001,0,0.0661282,"is “yes,” we stop and return the merged graph. Otherwise, we update u in a way to increase L(u) (see Line 8). 5 Our parsing algorithms can be applied to scores originated from any source, but in our experiments we chose to use the framework of global linear models, deriving our scores as: S CORE PART(s, p) = w⊤ ϕ(s, p) ϕ is a feature-vector mapping and w is a parameter vector. p may refer to a single arc, a pair of neighboring arcs, or a general tuple of arcs, according to the definition of a parsing model. For details we refer to the source code. We chose the averaged structured perceptron (Collins, 2002) for parameter estimation. 5.3 Results of Practical Parsing We evaluate five decoding algorithms: Experiments M1 first-order exact algorithm, 5.1 Data Sets M2 second-order exact algorithm with singleside factorization, To evaluate the effectiveness of book embedding in practice, we conduct experiments on unlabeled parsing using four corpora: CCGBank (Hockenmaier and Steedman, 2007), DeepBank (Flickinger et al., 2012), Enju HPSGBank (EnjuBank; Miyao et al., 2004) and Prague Dependency TreeBank (PCEDT; Hajic et al., 2012), We use “standard” training, validation, and test splits to facilitate com"
P17-1077,P15-1149,1,0.796967,"Missing"
P17-1077,S14-2080,1,0.893308,"92.83 92.83 92.87 91.61 93.02 UP 93.45 93.66 92.49 -- CCGBank UR UF 92.51 92.98 92.06 92.85 92.30 92.40 --- UP 89.58 89.27 -91.79 PCEDT UR UF 87.73 88.65 87.37 88.31 --86.02 88.81 Table 3: Parsing accuracy evaluated on the test sets. 5.4 Comparison with Other Parsers Acknowledgments We show the parsing results on the test data together with some relevant results from related work. We compare our parser with two other systems: (1) ZDSW (Zhang et al., 2016) is a transition-based system that obtains state-of-theart accuracy; we present the results of their best single parsing model; (2) Peking (Du et al., 2014) is the best-performing system in the shared task; it is a hybrid system that integrate more than ten submodels to achieve high accuracy. Our parser can be taken as a graph-based parser. It reaches stateof-the-art performance produced by the transitionbased system. On DeepBank and EnjuBank, the accuracy of our parser is equivalent to ZDSW, while on CCGBank, our parser is significantly better. There is still a gap between our single parsing model and Peking hybrid model. For a majority of NLP tasks, e.g. parsing (Surdeanu and Manning, 2010), semantic role labeling (Koomen et al., 2005), hybrid"
P17-1077,E06-1011,0,0.613142,"Missing"
P17-1077,J11-1007,0,0.0121486,"different from the intuition of the design of the open structure when we consider first-order factorization. For the last combination, we need to search for two best separating words, namely sr and le , and two best labels, namely l′ and l′ , so the time complexity of this second-order algorithm is O(n4 |L|2 ). 3.3 Generalized Higher-Order Parsing Both of the above two algorithms are exact decoding algorithms. Solutions allow for exact decoding with higher-order features typically at a high cost in terms of efficiency. A trade-off between rich features and exact decoding benefit tree parsing (McDonald and Nivre, 2011). In particular, Zhang and McDonald (2012) proposed a generalized higher-order model that abandons exact search in graph-based parsing in favor of freedom in feature scope. They kept intact Eisner’s algorithm for first-order parsing problems, while enhanced the scoring function in an approximate way by introducing higher-order features. We borrow Zhang and McDonald’s idea and develop a generalized parsing model for noncrossing dependency representations. The sub-problems and their decomposition are much like the firstorder algorithm. The difference is that we expand 4 Finding and Binding Pages"
P17-1077,P10-1151,0,0.49585,"Missing"
P17-1077,hajic-etal-2012-announcing,0,0.118887,"Missing"
P17-1077,S15-2153,0,0.138924,"Missing"
P17-1077,S14-2008,0,0.52016,"new exact second- and approximate higher-order algorithms. Our algorithms facilitate building with high accuracy the partial semantic dependency graphs on each page. To produce a full semantic analysis, we also need to integrate partial graphs on all pages into one coherent book. To this end, we formulate the problem as a combinatorial optimization problem, and propose a Lagrangian Relaxation-based algorithm for solutions. We implement a practical parser in the new framework with a statistical disambiguation model. We evaluate this parser on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from We model a dependency graph as a book, a particular kind of topological space, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm to combine pages into a book. Experiments demonstrate the effectiveness of the book embedding framework across a wide range of conditions. Our parser obtain"
P17-1077,J07-3004,0,0.304208,"i.org/10.18653/v1/P17-1077 Kuhlmann and Jonsson (2015) proposed to generalize the MST model to other types of subgraphs. In general, dependency parsing is formulated as the search for Maximum Subgraph for graph class G: Given a graph G = (V, A), find a subset A′ ⊆ A with maximum total weight such that the induced subgraph G′ = (V, A′ ) belongs to G. Formally, we have the following optimization problem: ∑ G′ (s) = arg max S CORE PART(s, p) arg2 arg1 arg1 arg1 arg2 arg1 arg2 arg1 arg2 . . The . company . that . Mark wants . to. buy . Figure 1: A fragment of a semantic dependency graph. CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our higher-order parsing models are more accurate than the first-order baseline. Experiments also demonstrate the effectiveness of our page binding algorithm. Our new parser can be taken as a graph-based parser extended for more general dependency graphs. It parallels the state-of-the-art transition-based system of Zhang et al. (2016) in performance. The implementation of our parser is available at http://www.icst.pku.edu.cn/ lcwm/grass. 2 H∈G(s,G) p∈H Here, G(s, G) is the set of all graphs that belong to G and are compatible with s and G. For parsing, G is usu"
P17-1077,D10-1002,0,0.341865,"Missing"
P17-1077,W12-3602,0,0.0240379,"ctic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1077 Kuhlmann and Jonsson (2015) proposed to generalize the MST model to other types of subgraphs. In general, dependency parsing is fo"
P17-1077,W05-0625,0,0.0449306,"Missing"
P17-1077,D12-1030,0,0.053895,"of the open structure when we consider first-order factorization. For the last combination, we need to search for two best separating words, namely sr and le , and two best labels, namely l′ and l′ , so the time complexity of this second-order algorithm is O(n4 |L|2 ). 3.3 Generalized Higher-Order Parsing Both of the above two algorithms are exact decoding algorithms. Solutions allow for exact decoding with higher-order features typically at a high cost in terms of efficiency. A trade-off between rich features and exact decoding benefit tree parsing (McDonald and Nivre, 2011). In particular, Zhang and McDonald (2012) proposed a generalized higher-order model that abandons exact search in graph-based parsing in favor of freedom in feature scope. They kept intact Eisner’s algorithm for first-order parsing problems, while enhanced the scoring function in an approximate way by introducing higher-order features. We borrow Zhang and McDonald’s idea and develop a generalized parsing model for noncrossing dependency representations. The sub-problems and their decomposition are much like the firstorder algorithm. The difference is that we expand 4 Finding and Binding Pages Statistics presented in Table 1 indicate"
P17-1077,J16-3001,1,0.676361,"ss a wide range of conditions. Our parser obtains comparable results with a state-of-the-art transition-based parser. 1 Introduction Dependency analysis provides a lightweight and effective way to encode syntactic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 20"
P17-1077,N10-1091,0,\N,Missing
P17-1193,C10-1011,0,0.0780292,"92.40 --92.03 92.53 UP 90.09 -90.21 -- PCEDT UR UF 85.90 87.95 --85.51 87.80 --- Table 3: Parsing accuracy evaluated on the test sets. 2014), and the data splitting policy follows the shared task. All the four data sets are publicly available from LDC (Oepen et al., 2016). Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. The pre-processing for CCGBank, DeepBank and EnjuBank are exactly the same as in experiments reported in (Zhang et al., 2016). 5.3 Accuracy We evaluate two parsing algorithms, the algorithm for noncrossing dependency graphs (Kuhlmann and Jonsson, 2015), i.e. pagenumber-1 (denoted as P1) graphs, and our quartic-time algorithm (denoted as 1ECP2d ). Table 2 summerizes the accuracy obtained our parser. Same feature templates are applied for disambiguation. We can see that our new algorithm yields significant improvements on all data sets, as expected. Especially, due to"
P17-1193,D07-1101,0,0.0702152,"processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restrictions. For all o"
P17-1193,W02-1001,0,0.220605,"expressiveness in terms of linguistic data. This degenerated version algorithm requires O(n4 ) time and O(n3 ) space. 5 Practical Parsing 5.1 Disambiguation We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. For details we refer to the source code. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). 4.8.3 Spurious Ambiguity 5.2 Data To generate the same graph, even a maximal 1 EC / P 2 graph, we may have different derivations. Figure 8 is an example. This is similar to syntactic analysis licensed by Combinatory Categorial Grammar (CCG; Steedman, 1996, 2000). To derive one surface string, there usually exists multiple CCG derivations. A practice of CCG parsing is defining one particular derivation as the standard one, namely normal form (Eisner, 1996a). The spurious ambiguity in our algorithm does not affect the correctness of first-order parsing, because scores are assigned to individua"
P17-1193,P15-1149,1,0.904732,"Missing"
P17-1193,S14-2080,1,0.878685,"w1 · · · wn−1 of length n, the vertices, i.e. words, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projec"
P17-1193,S15-2154,1,0.948367,"e. words, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in tim"
P17-1193,P96-1011,0,0.657272,"nor P 2 2116 cies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voi"
P17-1193,C96-1058,0,0.644214,"nor P 2 2116 cies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voi"
P17-1193,P10-1151,0,0.277089,"Missing"
P17-1193,hajic-etal-2012-announcing,0,0.15452,"Missing"
P17-1193,J13-4006,0,0.113669,"Missing"
P17-1193,J07-3004,0,0.105502,"one linguistically-rare structure descreases the complexity to O(n4 ). We also extend our quartic-time algorithm into a practical parser with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing. 1 Introduction Dependency parsing has long been studied as a central issue in developing syntactic or semantic analysis. Recently, some linguistic projects grounded on deep grammar formalisms, including CCG, LFG, and HPSG, draw attentions to rich syntactic and semantic dependency annotations that are not limited to trees (Hockenmaier and Steedman, 2007; Sun et al., 2014; Ivanova et al., 2012). Parsing for these deep dependency representations can be viewed as the search for Maximum Subgraphs (Kuhlmann and Jonsson, 2015). This is a natural extension of the Maximum Spanning Tree (MST) perspective (McDonald et al., 2005) for dependency tree parisng. One main challenge of the Maximum Subgraph perspective is to design tracTable algorithms for certain graph classes that have good empirical coverage for linguistic annotations. Unfortunately, no previously defined class simultaneously has high ∗ The first two authors contribute equally. coverage an"
P17-1193,D10-1002,0,0.0926707,"Bank UR UF 86.98 88.90 88.85 88.95 88.65 89.39 --- UP 93.83 92.92 93.18 -- EnjuBank UR UF 91.49 92.64 92.83 92.87 91.12 92.14 --- UP 94.23 92.49 -93.03 CCGBank UR UF 91.13 92.66 92.30 92.40 --92.03 92.53 UP 90.09 -90.21 -- PCEDT UR UF 85.90 87.95 --85.51 87.80 --- Table 3: Parsing accuracy evaluated on the test sets. 2014), and the data splitting policy follows the shared task. All the four data sets are publicly available from LDC (Oepen et al., 2016). Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. The pre-processing for CCGBank, DeepBank and EnjuBank are exactly the same as in experiments reported in (Zhang et al., 2016). 5.3 Accuracy We evaluate two parsing algorithms, the algorithm for noncrossing dependency graphs (Kuhlmann and Jonsson, 2015), i.e. pagenumber-1 (denoted as P1) graphs, and our quartic-time algorithm (denoted as 1ECP2d ). Table 2 summerizes the accuracy obtained our p"
P17-1193,W12-3602,0,0.345787,"Missing"
P17-1193,P10-1001,0,0.0363537,"is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restrictions. For all other aforementioned algo"
P17-1193,Q15-1040,0,0.537517,"an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncrossing dependency graphs, the complexity is O(|V |3 ). Unfortunately, no previously defined class simultaneously has high coverage on deep dependency ann"
P17-1193,S14-2082,0,0.238393,"Missing"
P17-1193,E06-1011,0,0.181954,"ies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restric"
P17-1193,H05-1066,0,0.41364,"Missing"
P17-1193,J16-3001,1,0.885044,"d with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G i"
P17-1193,S14-2008,0,0.219665,"Missing"
P17-1193,Q13-1002,0,0.711438,"ted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncrossing dependency graphs, the complexity is O(|V |3 ). Unfortunately, no previously defined class simultaneously has high coverage on deep dependency annotations and low-degree polynomial decoding algorithms for practical parsing. In this paper, we study well-motivated restrictions: 1 EC (Pitler et al., 2013) and P 2 (Kuhlmann and Jonsson, 2015). We will show that relatively satisfactory coverage and parsing complexity can be obtained for graphs that satisfy both restrictions. 3 The 1 EC, P 2 Graphs 3.1 The 1 EC Restriction Pitler et al. (2013) introduced a very nice property for modelling non-projective dependency trees, i.e. 1 EC. This property not only covers a large amount of tree annotations in natural language treebanks, but also allows the corresponding MST problem to bo solved in time of O(n4 ). The formal description of the 1 EC property is adopted from (Pitler et al., 2013). Definition 1"
P17-1193,P17-1077,1,0.833825,"arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncro"
P17-1193,P14-1042,1,0.920004,"Missing"
P18-1038,P13-1023,0,0.0624778,"arsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 -"
P18-1038,W13-2322,0,0.101456,"t over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408"
P18-1038,P02-1042,0,0.24527,"Missing"
P18-1038,E09-1001,0,0.35333,"rtance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computa"
P18-1038,W15-0128,0,0.177066,"HS (syntax) D+N V + PUNCT CM + HD↓V V + HD-CMP SP-HD + HD-CMP HD-CMP arg2 V HD-CMP arg1 arg1 SP-HD N bv D V HD↓V V(2,3) arg1 HD-CMP(3,6) ¬ RHS (semantics)  Figure 2: The grammar extraction process of the running example. Conceptual edges which are directly aligned with the syntactic rules are painted in green. The span-based alignment is shown in the parentheses. Structural edges that connect conceptual edges are painted in brown. Green edges and brown edges together form the subgraph, which acts as RHS in the HRG rule. External nodes are represented as solid dots. ciple of compositionality (Bender et al., 2015). A precise syntax-semantics interface is introduced to guarantee compositionality and therefore all meaning units can be traced back to linguistic signals, including both lexical and constructional ones. Take Figure 2 for example. Every concept, e.g. the existence quantifier some q, is associated with a surface string. We favor such correspondence not because it eases extraction of SHRGs, but because we emphasize sentence meanings that are from forms. The connection between syntax (sentence form) and semantics (word and sentence meaning) is fundamental to the study of language. 3.2 results ba"
P18-1038,D16-1001,0,0.017707,"divided into two steps: syntactic parsing and semantic interpretation. Syntactic parsing utilizes the CFG part to get a derivation that is shared by the HRG part. At one derivation step, there may be more than one HRG rule applicable. In this case, we need a semantic disambiguation model to choose a good one. 4.1 Semantic Interpretation ˆ = argmaxR∈R(T ) S CORE(R|T ) R (1) To solve this optimization problem, we implement a greedy search decoder and a bottom-up beam search decoder. The final semantic graph G is read ˆ off from R. Syntactic Parsing Following the LSTM-Minus approach proposed by Cross and Huang (2016), we build a constituent parser with a CKY decoder. We denote the output vectors of forward and backward LSTM as fi and bi . The feature si,j of a span (i, j) can be calculated from the differences of LSTM encodings: 4.2.1 Greedy Search Model In this model, we assume that each HRG rule is selected independently of the others. The score of G is defined as the sum of all rule scores: si,j = (fj − fi ) ⊕ (bi − bj ) S CORE(R = {r1 , r2 , ...}|T ) = The operator ⊕ indicates the concatenation of two vectors. Constituency parsing can be regarded as predicting scores for spans and labels, and getting"
P18-1038,P17-1112,0,0.733501,"oin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification Rewriting 89.4 92.80 -- 85.48 89.58 90.39 84.16 89.64 89.51 HD-CMP HD-CMP arg1 S arg1 arg1 bv arg1 D SP-HD Table 1: Parsing accuracy of the best existing grammar-free and -based models as well as our SHRG-based model. Results are copied from (Oepen et al., 2015; Peng et al., 2017a; Buys and Blunsom, 2017). arg2 _some_q ing techniques, we build a robust SHRG parser that is able to produce semantic analysis for all sentences. Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of EL EMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1). This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing. On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps between ERG-guided parsers and"
P18-1038,W11-2927,0,0.45783,"are the original labels, namely fine-grained construction types. We use the part before the first underscore of each label, e.g. SP-HD, as a coarse-grained label. The coarse-grained labels are more like the highly generalized rule schemata proposed by Pollard and Sag (1994). Some statistics are shown in Table 3. Instead of using gold-standard trees to extract a synchronous grammar, we also tried randomlygenerated alignment-compatible trees. The result is shown in Table 4. Gold standard trees exhibit a low entropy, indicating a high regularity. 5.3 Condensed Results of Semantic Interpretation Dridan and Oepen (2011) proposed the EDM metric to evaluate the performance the ERS-based graphs. EDM uses the alignment between the nodes in a graph and the spans in a string to detect the common parts between two graphs. It converts the predicate and predicate–argument relationship to comparable triples and calculates the correctness in these triples. A predicate of label L and span S is denoted as triple (S, NAME, L) and a relationship R between the predicate labelled P and argument labelled A is denoted as triple (P, R, A). We calculate the F1 value of the total triples as EDM score. Similarity, we compute the F"
P18-1038,P17-1193,1,0.828312,"to a sequence-to-sequence model and leverage the power of deep learning technologies to obtain auxiliary symbols to transform the output sequence into a graph (Peng et al., 2017b; Konstas et al., 2017). The strategy of the second type is to gradually generate a graph in a greedy search fashion (Zhang et al., 2016; Buys and Blunsom, 2017). Usually, a transition system is defined to handle graph construction. The last solution explicitly associates each basic part with a target graph score, and casts parsing as the search for the graphs with highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that t"
P18-1038,P14-1134,0,0.04233,"tract input sentence into a sequence-to-sequence model and leverage the power of deep learning technologies to obtain auxiliary symbols to transform the output sequence into a graph (Peng et al., 2017b; Konstas et al., 2017). The strategy of the second type is to gradually generate a graph in a greedy search fashion (Zhang et al., 2016; Buys and Blunsom, 2017). Usually, a transition system is defined to handle graph construction. The last solution explicitly associates each basic part with a target graph score, and casts parsing as the search for the graphs with highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this pa"
P18-1038,P13-1091,0,0.100031,"when replacing a hyperedge. An HRG G = hN, T, P, Si is a graph rewriting system, where N and T are two disjoint finite sets of nonterminal and terminal symbols respectively. S ∈ N is the start symbol. P is a finite set of productions of the form A → R, where the left hand side (LHS) A ∈ N , and the right hand side (RHS) R is a hypergraph with edge labels over N ∪ T . The rewriting process replaces a nonterminal hyperedge with the graph fragment specified by a production’s RHS, attaching each external node to the matched node of the corresponding LHS. An example is shown in Figure 1. Following Chiang et al. (2013), we make the nodes only describe connections between edges and store no other information. A synchronous grammar defines mappings between different grammars. Here we focus on relating a string grammar, CFG in our case, to a graph grammar, i.e., HRG. SHRG can be represented as tuple G = hN, T, T 0 , P, Si. N is a finite set of nonterminal symbols in both CFG and HRG. T 0 and T are finite sets of terminal symbols in CFG and HRG, respectively. S ∈ N is the start symbol. P is a finite set of productions of the form A → hR, R0 , ∼i, where A ∈ N , R is a hypergraph 3 3.1 Grammar Extraction Graph Re"
P18-1038,W12-3602,0,0.177345,"Missing"
P18-1038,Q16-1023,0,0.0692475,"pa ⊕ pb )[L] 5 We assign a beam to each node in the syntactic tree. To ensure that we always get a subgraph which does not contain any nonterminal edges during the search process, we perform the beam search in the bottom-up direction. We only reserve top k subgraphs in each beam. Figure 3 illustrates the process. 4.3 5.1 Training The objective of training is to make the score of the correct graph higher than incorrect graphs. We use the score difference between the correct graph Rg and the highest scoring incorrect graph as the loss: ˆ )−S CORE(Rg |T ) loss = maxR6ˆ =Rg S CORE(R|T Following (Kiperwasser and Goldberg, 2016)’s experience of loss augmented inference, in order to update graphs which have high model scores but are very wrong, we augment each factor belonging to the gold graph by adding a penalty term c to its score. Finally the loss term is: loss = S CORE(Rg |T ) − X Results of Grammar Extraction DeepBank provides fine-grained syntactic trees with rich information. For example, the label SP-HD HC C denotes that this is a “head+specifier” construction, where the semantic head is also the syntactic head. But there c− X Set-up DeepBank is an annotation of the Penn TreeBank Wall Street Journal which is"
P18-1038,K15-1004,0,0.259315,"highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that the performance that can be achieved by an SHRG-based parser is far higher than what has previously been demonstrated. We focus here on relating SHRG rules to the syntactosemantic composition process because we feel that information about syntax-semantics interface has been underexploited in the data-driven parsing architecture. We demonstrate the feasibility of inducing a high-quality, linguistically-informed SHRG from compositional semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000), dubbed English Resource S"
P18-1038,P17-1014,0,0.097929,"Missing"
P18-1038,E17-1035,0,0.0249248,"throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification Rewriting 89.4 92.80 -- 85.48 89.58 90.39 84.16 89.64 89.51 HD-CMP HD-CMP arg1 S arg1 arg1 bv arg1 D SP-HD Table 1: Parsing accuracy of the best existing grammar-free and -based models as well as our SHRG-based model. Results are copied from (Oepen et al., 2015; Peng et al., 2017a; Buys and Blunsom, 2017). arg2 _some_q ing techniques, we build a robust SHRG parser that is able to produce semantic analysis for all sentences. Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of EL EMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1). This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing. On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps betw"
P18-1038,J16-4009,0,0.144393,"s are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification R"
P18-1038,J93-2004,0,0.0641098,"Missing"
P18-1038,P17-1076,0,0.0224556,"e the output vectors of forward and backward LSTM as fi and bi . The feature si,j of a span (i, j) can be calculated from the differences of LSTM encodings: 4.2.1 Greedy Search Model In this model, we assume that each HRG rule is selected independently of the others. The score of G is defined as the sum of all rule scores: si,j = (fj − fi ) ⊕ (bi − bj ) S CORE(R = {r1 , r2 , ...}|T ) = The operator ⊕ indicates the concatenation of two vectors. Constituency parsing can be regarded as predicting scores for spans and labels, and getting the best syntactic tree with dynamic programming. Following Stern et al. (2017)’s approach, We calculate the span scores S COREspan (i, j) and labels scores S CORElabel (i, j, l) from si,j with multilayer perceptrons (MLPs): X S CORE(r|T ) r∈R The maximization of the graph score can be decomposed into the maximization of each rule score. S CORE(r|T ) can be calculated in many ways. Count-based approach is the simplest one, where the rule score is estimated by its frequency in the training data. We also evaluate a sophisticated scoring method, i.e., training a classifier based on rule embedding: S COREspan (i, j) = MLPspan (si,j ) S CORElabel (i, j, l) = MLPlabel (si,j )["
P18-1038,J16-3001,1,0.913456,"Missing"
P18-1038,S15-2153,0,0.474919,"Missing"
P18-1038,oepen-lonning-2006-discriminant,0,0.705721,"Missing"
P18-1038,P17-1186,0,0.0801732,"Missing"
P18-1038,S16-1183,0,0.0152412,"tial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that the performance that can be achieved by an SHRG-based parser is far higher than what has previously been demonstrated. We focus here on relating SHRG rules to the syntactosemantic composition process because we feel that information about syntax-semantics interface has been underexploited in the data-driven parsing architecture. We demonstrate the feasibility of inducing a high-quality, linguistically-informed SHRG from compositional semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000), dubbed English Resource Semantics1 (ERS). Coupled"
P18-1179,W13-2322,0,0.0288581,"igned especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design. 1 Introduction The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in dep"
P18-1179,bohnet-wanner-2010-open,0,0.0149683,"edges. In this paper, we only consider DAGs that are unordered, node-labeled, multi-rooted1 and connected. Conceptual graphs, including AMR and EDS, are both node-labeled and edge-labeled. It seems that without edge labels, a DAG is inadequate, but this problem can be solved easily by using the strategies introduced in (Chiang et al., 2018). Take BV a labeled edge proper q −→ named for example2 . We can represent the same information by replacing it with two unlabeled edges and a new labeled node: proper q → BV → named. 2.2 Previous Work DAG automata are the core engines of graph transducers (Bohnet and Wanner, 2010; Quernheim and Knight, 2012). In this work, we adopt Chiang et al. (2018)’s design and define a weighted DAG automaton as a tuple M = ⟨Σ, Q, δ, K⟩: • Σ is an alphabet of node labels. • Q is a finite set of states. • (K, ⊕, ⊗, 0, 1) is a semiring of weights. • δ : Θ → K{0} is a weight function that assigns nonzero weights to a finite transition set Θ. Every transition t ∈ Θ is of the form σ {q1 , · · · , qm } − → {r1 , · · · , rn } where qi and rj are states in Q. A transition t gets m states on the incoming edges of a node and puts n states on the outgoing edges. A transition that does not b"
P18-1179,J18-1005,0,0.362951,", e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems. The meaning representation studied in this work is what we call type-logical semantic graphs, i.e. semantic graphs grounded under type-logical semantics (Carpenter, 1997), one dominant theoretical framework for modeling natural language semantics. In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments (Kratzer and Heim, 1998). In the sam"
P18-1179,P97-1003,0,0.0227035,"namic rule on-the-fly. Our rule creator builds a new rule following the Markov assumption: P (O|C) = P (q1 |C) n ∏ P (qi |C)P (qi |qi−1 , C) i=2 C = ⟨I, D⟩ represents the context. O = {q1 , · · · , qn } denotes the outgoing states and I, D have the same meaning as before. Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels. There is also a group of hard constraints to make sure that the predicted rules are well-formed as the definition in §5 requires. This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers (Collins, 1997; Klein and Manning, 2003). For a dynamic rule, all variables in this rule will appear in the statement. We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order. 6 Evaluation and Analysis 5.4 Extended Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation"
P18-1179,W02-1001,0,0.0482386,"1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development and test data sets. We use a small portion of wikiwoods data, c.a. 300K sentences, for experiments. 37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obtained. For DAG recognition, at one particular position, there may be more than one rule applicable. In this case, we need a disambiguation model as well as a decoder to search for a globally optimal solution. In this work, we train a structured perceptron model (Collins, 2002) for disambiguation and employ a beam decoder. The perceptron model used by our dynamic rule generator are trained with the induced rules. To get a sequence-to-sequence model, we use the open source tool—OpenNMT4 . 6.2 The Decoder We implement a fine-to-coarse beam search decoder. Given a DAG D, our goal is to find the highest scored labeling function ρ: ρ = arg max ρ n ∑ ∏ wj · fj (rule(vi ), D) i=1 j ℓ(vi ) s.t. rule(vi ) = ρ(in(vi )) −−−→ ⟨ρ(out(vi )), Ei ⟩ where n is the node count and fj (·, ·) and wj represent a feature and the corresponding weight, respectively. The features are chosen"
P18-1179,P03-1054,0,0.00869505,"he-fly. Our rule creator builds a new rule following the Markov assumption: P (O|C) = P (q1 |C) n ∏ P (qi |C)P (qi |qi−1 , C) i=2 C = ⟨I, D⟩ represents the context. O = {q1 , · · · , qn } denotes the outgoing states and I, D have the same meaning as before. Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels. There is also a group of hard constraints to make sure that the predicted rules are well-formed as the definition in §5 requires. This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers (Collins, 1997; Klein and Manning, 2003). For a dynamic rule, all variables in this rule will appear in the statement. We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order. 6 Evaluation and Analysis 5.4 Extended Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation here is to deal with the d"
P18-1179,P17-1014,0,0.112577,"Missing"
P18-1179,J93-2004,0,0.0652256,"ons licensed by English Resource Grammar (ERG; Flickinger, 2000). We introduce a principled method to derive transduction rules from DeepBank (Flickinger et al., 2012). Furthermore, we introduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph. Taking EDS graphs, a variable-free ERS format, as input, our NLG system achieves a BLEU-4 score of 68.07. On average, it produces more than 5 sentences in a second on an x86 64 GNU/Linux platform with two Intel Xeon E5-2620 CPUs. Since the data for experiments is newswire data, i.e. WSJ sentences from PTB (Marcus et al., 1993), the input graphs are quite large on average. The remarkable accuracy, efficiency and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design. 2 Previous Work and Challenges 2.1 Preliminaries A node-labeled simple graph over alphabet Σ is a triple G = (V, E, ℓ), where V is a finite set of nodes, E ⊆ V × V is an finite set of edges and ℓ : V → Σ is a labeling function. For a node v ∈ V , sets of its incoming and outgoing edges are denoted by in(v) and out(v) respectively. For an edge e ∈ E, its source node and ta"
P18-1179,E09-1001,0,0.401001,"lish Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design. 1 Introduction The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems. The meaning representation studied"
P18-1179,W12-4209,0,0.435466,"function of which is to consume complex arguments (Kratzer and Heim, 1998). In the same spirit, generalized quantifiers, prepositions and function words in many languages other than English are also analyzed as higher-order functions. Accordingly, all the linguistic elements are treated as roots in type-logical semantic graphs, such as EDS and DMRS. This makes the typological structure quite flat rather than hierachical, which is an essential distinction between natural language semantics and syntax. To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by Quernheim and Knight (2012). Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation. This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics. Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and reported no result of practical generation. It is still unknown to what extent a DAG transducer suits realistic NLG. The design for string and tree transducers 1928 Proceedings of the 56th An"
P18-1179,P17-2002,0,0.0540524,"Missing"
P18-1179,flickinger-etal-2010-wikiwoods,0,0.0158063,"ed Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation here is to deal with the data sparseness problem. We use DeepBank 1.1 (Flickinger et al., 2012), i.e. gold-standard ERS annotations, as our main experimental data set to train a DAG transducer as well as a sequence-to-sequence morpholyzer, and wikiwoods (Flickinger et al., 2010), i.e. automatically-generated ERS annotations by ERG, as additional data set to enhance the sequence-to-sequence morpholyzer. The training, 1934 development and test data sets are from DeepBank and split according to DeepBank’s recommendation. There are 34,505, 1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development and test data sets. We use a small portion of wikiwoods data, c.a. 300K sentences, for experiments. 37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obt"
P18-1250,P11-2037,0,0.163599,"terest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e.g. Johnson (2002); Wang et al. (2015)) where parser outputs are utilized as clues to determine the existence of empty categories. For disambiguation, while early work on dependency parsing focused on linear models, recent work started exploring deep learning techniques for the post-parsing approach (Wang et al., 2015). From the above two dimensions, we show all existing systems for ECD in Table 1. Neural models for pre- and in-parsing ECD have not been studied yet. In this paper, we fil"
P18-1250,P03-1055,0,0.345223,"s constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e.g. Johnson (2002); Wang et al. (2015)) where parser outputs are utilized as clues to determine the existence of empty categories. For disambiguation, while early work on dependency parsing focused on linear models, recent work started exploring deep learning techniques for the post-parsing approach (Wang et al., 2015). From the above two dimensions, we show all existi"
P18-1250,P02-1018,0,0.234613,"of machinery in representing the (deep) syntactic structure of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is inte"
P18-1250,N13-1125,0,0.0398982,"is 3. We calculate labeled recall scores for enumerated Dependency Distance. A higher score means greater capability to catch and to represent long-distance details. 4 4.2 Results of Pre-Parsing Models Experiments 4.1 Experimental Setup 4.1.1 Data We conduct experiments on a subset of Penn Chinese Treebank (CTB; Xue et al., 2005) 9.0. As a pro-drop language, the empty category is a very useful method for representing the (deep) syntactic analysis in Chinese language. Empty categories in CTB is divided into six classes: pro, PRO, OP, T, RNR and *, which were described in detail in Xue and Yang (2013); Wang et al. (2015). For comparability with the state-of-the-art, the division of training, development and testing data is coincident with the previous work (Xue and Yang, 2013). Our experiments can be divided into two groups. The first group is conducted on the linear conditional random field (Linear-CRF) model and LSTM-CRF tagging model to evaluate gains from the introduction of neural structures. The second group is designed for the dependency-based inparsing models. 4.1.2 Evaluation Metrics We adopt two kinds of metrics for the evaluation of our experiments. The first one focuses on EC’s"
P18-1250,E17-1063,0,0.0594007,"Missing"
P18-1250,C12-2105,0,0.0136955,"n representing the (deep) syntactic structure of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing"
P18-1250,N15-1030,0,0.138237,"ture of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e"
P19-4002,W13-2322,0,0.637356,"ive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the sub"
P19-4002,C16-1056,0,0.056597,"Missing"
P19-4002,basile-etal-2012-developing,0,0.0527628,"hen apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013); • Graph-Based Minimal Recursion Semantics (EDS and DMRS; Oepen and Lønning, 2006; Copestake, 2009); • Abstract Meaning Representation (AMR; Banarescu et al., 2013); • Non-Graph Representations: Discourse Representation Structures (DRS; Basile et al., 2012); Compositionality Semantic parsers also differ with respect to whether they assume that the graph-based semantic representations are constructed compositionally. Some approaches follow standard linguistic practice in assuming that the graphs have a latent compositional structure and try to reconstruct it explicitly or implicitly during parsing. Others are more agnostic and simply predict the edges of the target graph without regard to such linguistic assumptions. • Contrastive review of selected examples across frameworks; • Availability of training and evaluation data; shared tasks; state-of"
P19-4002,P14-1134,0,0.0312129,"state-of-the-art empirical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexan"
P19-4002,C04-1180,0,0.239598,"Missing"
P19-4002,P18-1170,1,0.908645,"Missing"
P19-4002,P17-1112,0,0.504659,"enabling lexical decomposition (e.g. of causatives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the su"
P19-4002,hajic-etal-2012-announcing,0,0.136582,"Missing"
P19-4002,S16-1167,0,0.0477873,"tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natura"
P19-4002,J94-1007,0,0.785093,"Missing"
P19-4002,P17-1104,0,0.0797035,"ed methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the"
P19-4002,S15-2153,1,0.918102,"Missing"
P19-4002,P18-1035,0,0.283731,"satives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release"
P19-4002,S14-2008,1,0.898615,"Missing"
P19-4002,J07-3004,0,0.0229765,"in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natural-language sentences to graphs. There is now a dizzying array of different semantic parsing algorithms, and it is a challenge to keep trac"
P19-4002,W12-3602,1,0.935246,"almost every Semantic Evaluation (SemEval) exercise since 2014. These shared tasks were based on a variety of different corpora with graph-based meaning annotations (graph banks), which differ both in their formal properties and in the facets of meaning they aim to represent. The relevance of this tutorial is to clarify this landscape 6 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 6–11 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2008), DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) and Prague Semantic Dependencies (PSD; a simplification of the tectogrammatical structures of Hajiˇc et al., 2012). Tutorial slides and additional materials are available at the following address: https://github.com/cfmrp/tutorial 2 Semantic Graph Banks In the first part of the tutorial, we will give a systematic overview of the available semantic graph banks. On the one hand, we will distinguish graph banks with respect to the facets of natural language meaning they aim to represent. For instance, some graph banks focus on predicate–argument structure, perhaps with some extensions for polari"
P19-4002,P10-5006,0,0.0128625,"cipants will be enabled to identify genuine content differences between frameworks as well as to tease apart more superficial variation, for example in terminology or packaging. Furthermore, major current processing techniques for semantic graphs will be reviewed against a highlevel inventory of families of approaches. This part of the tutorial will emphasize reflections on codependencies with specific graph flavors or frameworks, on worst-case and typical time and space complexity, as well as on what guarantees (if any) are obtained on the wellformedness and correctness of output structures. Kate and Wong (2010) suggest a definition of semantic parsing as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” This view brings along a tacit expectation to map (more or less) directly from a linguistic surface form to an actionable encoding of its intended meaning, e.g. in a database query or even programming language. In this tutorial, we embrace a broader perspective on semantic parsing as it has come to be viewed commonly in recent years. We will review graph-based meaning representations that ai"
P19-4002,P17-1186,0,0.115805,"Missing"
P19-4002,P17-1014,0,0.0545964,"nderspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philolo"
P19-4002,P18-1171,0,0.187888,"ng of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Scienc"
P19-4002,Q15-1040,0,0.0475384,"ical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD"
P19-4002,C08-1095,0,0.198112,"odels. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic par"
P19-4002,J16-4009,1,0.825702,"icate–argument structure, perhaps with some extensions for polarity or tense, whereas others capture (some) scopal phenomena. Furthermore, while the graphs in most graph banks do not have a precisely defined model theory in the sense of classical linguistic semantics, there are still underlying intuitions about what the nodes of the graphs mean (individual entities and eventualities in the world vs. more abstract objects to which statements about scope and presupposition can attach). We will discuss the different intuitions that underly different graph banks. On the other hand, we will follow Kuhlmann and Oepen (2016) in classifying graph banks with respect to the relationship they assume between the tokens of the sentence and the nodes of the graph (called anchoring of graph fragments onto input sub-strings). We will distinguish three flavors of semantic graphs, which by degree of anchoring we will call type (0) to type (2). While we use ‘flavor’ to refer to formally defined sub-classes of semantic graphs, we will reserve the term ‘framework’ for a specific linguistic approach to graph-based meaning representation (typically cast in a particular graph flavor, of course). Type (1) A more general form of an"
P19-4002,D18-1263,0,0.0131,"guities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Science, and Computational Linguis"
P19-4002,P19-1450,1,0.642351,"tic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he"
P19-4002,N15-1040,0,0.0436829,"on: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for A"
P19-4002,S16-1166,0,0.0365445,"unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic f"
P19-4002,D18-1194,0,0.126774,"Missing"
P19-4002,S17-2090,0,0.0283489,"in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this"
P19-4002,J08-1002,0,0.0102092,", and it is a challenge to keep track of their respective strengths and weaknesses. Different parsing approaches are, of course, more or less effective for graph banks of different flavors (and, at times, even specific frameworks). We will discuss these interactions in the tutorial and organize the research landscape on graph-based semantic parsing along three dimensions. (1) Linguistic Foundations: Layers of Sentence Meaning (2) Formal Foundations: Graphs Labeled Directed (3) Meaning Representation Frameworks and Graph Banks • Bi-Lexical semantic dependencies (Hockenmaier and Steedman, 2007; Miyao and Tsujii, 2008; Hajiˇc et al., 2012; Ivanova et al., 2012; Che et al., 2016); Decoding strategy Semantic parsers differ with respect to the type of algorithm that is used to compute the graph. These include factorizationbased methods, which factorize the score of a graph into parts for smaller substrings and can then apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Co"
Q13-1025,C10-1011,0,0.0273249,"otations also includes functional information and empty categories. Modern parsers, e.g. Collins and Berkeley parsers, ignore these types of linguistic knowledge. To train a constituent parser, we perform a heuristic procedure on the treebank data to delete function tags and empty categories as well as its associated redundant ancestors. Many papers reported parsing results of an older version CTB (namely CTB 5). To compare with systems introduced in these papers, we evaluate our final ensemble model on CTB5 in Section 5.4. For dependency parsing, we choose a second order graph-based parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2 code.google.com/p/mate-tools/ code.google.com/p/berkeleyparser/ 4 cis.upenn.edu/˜dbikel/software.html 3 304 a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012"
Q13-1025,P04-1041,0,0.087956,"Missing"
Q13-1025,A00-2018,0,0.0793168,"ions normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from da"
Q13-1025,P05-1022,0,0.0129527,"ntain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCF"
Q13-1025,P12-2003,0,0.221451,"model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure"
Q13-1025,J03-4003,0,0.160915,"ructure annotations normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-ca"
Q13-1025,P99-1065,0,0.199812,"es are two automatically converted pseudo constituency trees. By applying DS to CS rules, we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position"
Q13-1025,C96-1058,0,0.159863,"re superior to each of the individual parsers. We implement their method to aggregate models. Once we have obtained multiple dependency trees respectively from base parsers, we can build a graph where each word in the sentence is a node. We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures. The weights are the word-by-word voting results of sub-models. Based on this graph, the sentence can be reparsed by a graph-based algorithm. Taking Chinese as a projective language, we use Eisner’s algorithm (Eisner, 1996) to combine multiple dependency parses. Surdeanu and Manning (2010) indicates that reparsing performs essentially as well as other simpler or more complex models. 86.5 G raph[-lab] 85.5 Tran U nlex 84.5 G raph[-lab]+U nlex 83.5 Tran+U nlex 82 .5 G raph[-lab]+Tran 81.5 3 4 5 6 7 8 9 4.2 Parameter tuning We evaluate our combination model on the same data set used in the last section. The two hyperparameters (λ and m) of our Bagging model are tuned on the development (validation) set. On one hand, with the increase of the size of sub-samples, i.e. λ, the performance of sub-models is improved. How"
Q13-1025,P10-1035,0,0.0195225,"verted pseudo constituency trees. By applying DS to CS rules, we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing m"
Q13-1025,I11-1136,0,0.0454682,"t dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond deterministic greedy search, principled dynam"
Q13-1025,W99-0623,0,0.0760128,"is of the CoNLL 2009 shared task data. By applying this conversion procedure on the outputs of an automatic phrase structure parser, we can build a PCFG-based dependency parser. 2.3 Parser ensemble NLP systems built on particular single views normally capture different properties of an original problem, and therefore differ in predictive powers. As a result, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and 1 For example, as two popular dependenc"
Q13-1025,A00-2005,0,0.0431167,"ines (http://www.cis.upenn. edu/˜chinese/posguide.3rd.ch.pdf). 306 4.1 Applying Bagging to dependency parsing Bagging is a machine learning ensemble metaalgorithm to improve classification and regression models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsing. Since training even one single parser takes hours (if not days), experiments on Bagging is time-consuming. To save time, we conduct data-driven parsing experiments based on simple configuration. More specifically, the beam size of the transition-based parser is set to 16, and the simple feature set is utilized; dependency relations are not incor"
Q13-1025,P10-1110,0,0.103321,"ainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond deterministic greedy sea"
Q13-1025,N09-2054,0,0.0160455,"discriminative Markov and semi-Markov tagging models are reported. Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models with latent annotations can also achieve good performance for Chinese POS tagging. Huang et al. (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. Different from negative results of Bagging discriminative models, our auxiliary experiment shows that Bagging Huang et al.’s tagger can help Chinese POS tagging. In other words, Bagging substantially improves both HMMLA and PCFGLA models, at least for Chinese POS tagging and constituency parsing. It seems that Bagging favors the use of latent variables. denote graph-based, transition-based and PCFGbased parsers as g, t and c; denote the reparsing procedure as reparse and the Bagging procedure as bagging. The two ex"
Q13-1025,P12-1071,0,0.0291299,"Missing"
Q13-1025,D11-1109,0,0.0457477,"Missing"
Q13-1025,P05-1010,0,0.0380651,"hat the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-th"
Q13-1025,D07-1013,0,0.0875169,"Missing"
Q13-1025,J08-4003,0,0.362412,"Missing"
Q13-1025,P08-1108,0,0.246077,"mpact on dependency parsing and PCFGbased models have complementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. c Submitted 6/2012; Revised 10/2012; Published 7/2013. 2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the"
Q13-1025,P06-1055,0,0.0554414,"atively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and"
Q13-1025,N06-2033,0,0.583141,"omplementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. c Submitted 6/2012; Revised 10/2012; Published 7/2013. 2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the CoNLL 09 shared task data demonstrate its effectivene"
Q13-1025,P10-2031,1,0.90609,"Missing"
Q13-1025,C10-2139,1,0.914673,"labels to a model. • For every word h that governs at least two children (d1 , ..., dn ), we consider every word triple hh, di , di+1 i, among h and its sibling dependents di as well as di+1 (0 ≤ i &lt; n). Similar to the grandparent dependencies, we can define evaluation metrics for sibling dependencies. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. From Table 1, we can see that the grammar-based model parses relatively better for slightly larger fragments. For example, the UAS of the graph-based model is significantly higher than the grammarbased one, but their sibling and grandparent scores are similar. In the next section, we will introduce a general parser enhancement technique and pre"
Q13-1025,Y09-2011,1,0.791843,". +/-lab means whether to incorporate relation labels to a model. • For every word h that governs at least two children (d1 , ..., dn ), we consider every word triple hh, di , di+1 i, among h and its sibling dependents di as well as di+1 (0 ≤ i &lt; n). Similar to the grandparent dependencies, we can define evaluation metrics for sibling dependencies. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. From Table 1, we can see that the grammar-based model parses relatively better for slightly larger fragments. For example, the UAS of the graph-based model is significantly higher than the grammarbased one, but their sibling and grandparent scores are similar. In the next section, we will introduce a genera"
Q13-1025,P12-1026,1,0.925684,"parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2 code.google.com/p/mate-tools/ code.google.com/p/berkeleyparser/ 4 cis.upenn.edu/˜dbikel/software.html 3 304 a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results"
Q13-1025,P09-1039,0,0.18528,"Missing"
Q13-1025,D08-1017,0,0.120811,"Missing"
Q13-1025,W11-2923,0,0.0238692,", we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing most phrases, we only need sequentially combine the right or le"
Q13-1025,D08-1059,0,0.112678,"S tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to"
Q13-1025,W09-3825,0,0.08432,"represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure (DS) conversion for the Chinese Treebank data, which is the basis o"
Q13-1025,J11-1005,0,0.0444095,"work 2.1 Data-driven dependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre ("
Q13-1025,P11-2033,0,0.431697,"ependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond d"
Q13-1025,C10-1153,0,0.0914276,"le feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to dependency parsing, Zhuang and Zong (2010) found that Berkeley parser produce much more accurate syntactic analyses to assist a Chinese semantic role labeler than Bikel parser. Charniak and Stanford parsers are two other wellknown and frequently used tools that can provide lexicalized parsing results. According to (Che et al., 2012), they perform even worse than Bikel parser, at least for Stanford dependencies. Due to the poor parsing performance, we only concentrate on the unlexicalized model in the remainder of this paper. The performance of labeled dependency prediction of"
Q13-1025,N10-1091,0,\N,Missing
S14-2080,P10-1035,0,0.0309837,"ify some dependency relations in order to make the graph a tree. Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 4.2 Auxiliary Labels In the transformed trees, we use auxiliary labels to carry out information of the original graphs. To encode multiple edges to one, we keep the original label on the dir"
S14-2080,S14-2008,0,0.15362,"Missing"
S14-2080,C08-1095,0,0.179036,"ollowing several well-established syntactic theories, SemEval-2014 task 8 (Oepen et al., 2014) proposes using graphs to represent semantics. Considering that semantic dependency parsing is a quite new topic and there is little previous work, we think it worth appropriately profiling successful tree parsing techniques for graph parsing. To this end, we build a hybrid system Architecture We explore two kinds of basic models: One is transition-based, and the other is tree approximation. Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009). Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system. The motivation of developing tree approximation models is to apply existing graph-based tree parsers to generate graphs. At the training time, we convert the dependency graphs from the training data into dependency trees, and train secondorder arc-factored models1 . At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. We think tree approximation can appropriately evaluate the possi"
S14-2080,D07-1111,0,0.0168913,"nd a set A of arcs, denoted by hσ, β, Ai. The initial configuration of a sentence x = w1 w2 · · · wn is cs (x) = h[0], [1, 2, · · · , n], {}i, and the terminal configuration set Ct is the set of all configurations with empty buffer. These two transition systems are shown in 1. The transitions of the Titov system are: 3.3 • L EFT-A RCl adds an arc from the front of the buffer to the top of the stack, labeled l, into A. Sentence Reversal Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007). In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences. Therefore we can get 10 parse of a sentence based on 5 transition systems. • R IGHT-A RCl adds an arc from the top of the stack to the front of the buffer, labeled l, into A. • S HIFT removes the front of the buffer and push it onto the stack; 460 L EFT-A RCl R IGHT-A RCl S HIFT P OP S WAP L EFT-A RCkl R IGHT-A RCkl S HIFT P OPk (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i,"
S14-2080,N10-1091,0,0.150502,"Missing"
S14-2080,W11-2923,0,0.0269142,"ations in order to make the graph a tree. Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 4.2 Auxiliary Labels In the transformed trees, we use auxiliary labels to carry out information of the original graphs. To encode multiple edges to one, we keep the original label on the directed edge but may add oth"
S14-2080,P11-2033,0,0.0350252,"Statistical Disambiguation First of all, we derive oracle transition sequences for every sentence, and train Passive-Aggressive models (Crammer et al., 2006) to predict next transition given a configuration. When it comes to parsing, we start with the initial configuration, predicting next transition and updating the configuration with the transition iteratively. And finally we will get a terminal configuration, we then stop and output the arcs of the graph contained in the final configuration. We extracted rich feature for we utilize a set of rich features for disambiguation, referencing to Zhang and Nivre (2011). We examine the several tops of the stack and the one or more fronts of the buffer, and combine the lemmas and POS tags of them in many ways as the features. Additionally, we also derive features from partial parses such as heads and dependents of these nodes. Our Transition Systems We implemented 5 different transition systems for graph parsing. Here we describe two of them in detail, one is the Titov system proposed in (Titov et al., 2009), and the other is our Naive system. The configurations of the two systems each contain a stack σ, a buffer β, and a set A of arcs, denoted by hσ, β, Ai."
S15-2154,C10-1011,0,0.0195341,"transition-based and tree approximation approaches. The transitionbased model use transitions on configurations to obtain graph parses, while the tree approximation model transform graphs into trees for training and test. To further combine the complementary prediction power, DZWS14 applied a voting-based ensemble method. 2.1 Transition-Based Models Tree Approximation Models The core of tree approximation is transformations between graphs and trees. At the training time, we convert the dependency graphs from the training data into dependency trees, and train second-order arc-factored models1 (Bohnet, 2010). At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. In DZSW14, We develop several different methods to convert a semantic graph into a tree. The main idea is to apply graph traversal algorithms to convert a directed graph to a directed tree. During the traversal, we may lose or modify some dependency relations in order to make a tree. 2.3 • The model ensemble is quite effective, resulting in a boost in performance. 3 Weighted Tree Approximation Models In our system for SemEval-2015, we develop more tree approximation models fo"
S15-2154,S14-2080,1,0.710053,"eked to stimulate the dependency parsing community to move towards more general graph processing. Quite a number of teams all over the world participated in this shared task, which suggests a growing community interest in parsing into graph-shaped dependency representations. ∗ Email correspondence. SemEval-2015 Task 18 is a subsequent task of SemEval-2014 Task 8. Following several wellestablished syntactic theories, this task proposes using graphs to represent semantics and provides highquality annotations for three typologically different languages. We have developed a system, dubbed DZSW14 (Du et al., 2014) for the task last year. The system employed a hybrid architecture which benefits from both transition-based and graph-based parsing approaches. Evaluation on multiple English data sets provided by SemEval-2014 indicated that DZSW14 is able to obtain high-quality parsing results. Following the key idea to employ heterogeneous models to enhance hybrid parsing, we extend DZSW14 by developing more tree approximation models, namely the weighted tree approximation models. Evaluation on multilingual data sets provided by this year’s task confirms the effectiveness of the techniques we have studied."
S15-2154,S14-2008,0,0.315983,"Missing"
S19-2012,D14-1082,0,0.0332372,"(2017a), while French and German parsing is based on Hershcovich et al. (2018). It has been shown that the choice of model plays an important role in transition-based parsing (Hershcovich et al., 2017b). For TUPA, we built parsers with different models: MLP, BiLSTM, and also train MLP based on BiLSTM, viz. Cascaded BiLSTM. The three single parsers are described as the following: The MLP parser (Hershcovich et al., 2017b) applies a feedforward neural network with dense embedding features to predict optimal transitions given particular parser states. This parser adopts a similar architecture to Chen and Manning (2014). The BiLSTM parser (Hershcovich et al., 2018) applies a bidirectional LSTM to learn contextualized vector-based representations for words that are then utilized for encoding a parser state, similarly to Kiperwasser and Goldberg (2016). The red box in Figure 1 shows the architecture of BiLSTM model, indicating that the representations after BiLSTM are fed into a Multiple-layer perceptron. The Cascaded BiLSTM parser combines the above two parsing models, which contains a multistage training process. First, we use BiLSTM TUPA model to train 100 epochs, then retrain built via interpreting auxilia"
S19-2012,P17-1104,0,0.186141,"du ‡ Institute of Computer Science and Technology, Peking University {huangsheng,ws,sq.zhang}@pku.edu.cn Abstract This paper describes the systems of the CUNY-PKU team in “SemEval 2019 Task 1: Cross-lingual Semantic Parsing with UCCA”1 . We introduce a novel method by applying a cascaded MLP and BiLSTM model. Then, we ensemble multiple system-outputs by reparsing. Our system won the second places in German-20K-Closed track, and third place in English-20K-Closed track. 1 Introduction We participate in Cross-lingual Semantic Parsing at SemEval 2019, and our submission systems are based on TUPA (Hershcovich et al., 2017a, 2018). A shared task summary paper (Hershcovich et al., 2019) by competition organizers summaries the results. We built three single parser using BiLSTM (Bidirectional LSTM) and Multi-Layer Perceptron (MLP) with TUPA (Hershcovich et al., 2017a, 2018). Most importantly, we introduce a new training method Cascaded BiLSTM by first pretraining the BiLSTM model and then training another MLP model based on pre-trained BiLSTM model. The cascaded BiLSTM parser enhances the parsing accuracy on all tasks. We also complete a Self-Attentive Constituency Parser (Kitaev and Klein, 2018a,b) as comparison."
S19-2012,P18-1035,0,0.299865,"luding three single parsers in Section 2 and a voter in 1 https://competitions.codalab.org/ competitions/19160 Section 3. We focus on two novel technical contributions: the Cascaded BiLSTM model and the Reparsing strategy. In Section 4 we will present experimental setup and results. 2 2.1 Single Parsers TUPA Parsers The TUPA parser (Hershcovich et al., 2017a) builds on discontinuous constituency and dependency graph parsing and makes some improvements especially for the UCCA representation. The English parsing is based on Hershcovich et al. (2017a), while French and German parsing is based on Hershcovich et al. (2018). It has been shown that the choice of model plays an important role in transition-based parsing (Hershcovich et al., 2017b). For TUPA, we built parsers with different models: MLP, BiLSTM, and also train MLP based on BiLSTM, viz. Cascaded BiLSTM. The three single parsers are described as the following: The MLP parser (Hershcovich et al., 2017b) applies a feedforward neural network with dense embedding features to predict optimal transitions given particular parser states. This parser adopts a similar architecture to Chen and Manning (2014). The BiLSTM parser (Hershcovich et al., 2018) applies"
S19-2012,Q16-1023,0,0.0569875,"ers with different models: MLP, BiLSTM, and also train MLP based on BiLSTM, viz. Cascaded BiLSTM. The three single parsers are described as the following: The MLP parser (Hershcovich et al., 2017b) applies a feedforward neural network with dense embedding features to predict optimal transitions given particular parser states. This parser adopts a similar architecture to Chen and Manning (2014). The BiLSTM parser (Hershcovich et al., 2018) applies a bidirectional LSTM to learn contextualized vector-based representations for words that are then utilized for encoding a parser state, similarly to Kiperwasser and Goldberg (2016). The red box in Figure 1 shows the architecture of BiLSTM model, indicating that the representations after BiLSTM are fed into a Multiple-layer perceptron. The Cascaded BiLSTM parser combines the above two parsing models, which contains a multistage training process. First, we use BiLSTM TUPA model to train 100 epochs, then retrain built via interpreting auxiliary labels. Span representation Graph nodes in a UCCA graph naturally create a hierarchical structure through the use of primary edges. Following this tree structure, we give the definition of span of nodes. Definition 1. The span of no"
S19-2012,P18-1249,0,0.101335,"e based on TUPA (Hershcovich et al., 2017a, 2018). A shared task summary paper (Hershcovich et al., 2019) by competition organizers summaries the results. We built three single parser using BiLSTM (Bidirectional LSTM) and Multi-Layer Perceptron (MLP) with TUPA (Hershcovich et al., 2017a, 2018). Most importantly, we introduce a new training method Cascaded BiLSTM by first pretraining the BiLSTM model and then training another MLP model based on pre-trained BiLSTM model. The cascaded BiLSTM parser enhances the parsing accuracy on all tasks. We also complete a Self-Attentive Constituency Parser (Kitaev and Klein, 2018a,b) as comparison. Finally, we ensemble different parsers with a reparsing strategy (Sagae and Lavie, 2006). In particular, we introduce an algorithm based on dynamic programming to perform inference for the UCCA representation. This decoder can also be utilized as a core engine for a single parser. We will describe our systems in detail, including three single parsers in Section 2 and a voter in 1 https://competitions.codalab.org/ competitions/19160 Section 3. We focus on two novel technical contributions: the Cascaded BiLSTM model and the Reparsing strategy. In Section 4 we will present exp"
S19-2012,N06-2033,0,0.0770633,"competition organizers summaries the results. We built three single parser using BiLSTM (Bidirectional LSTM) and Multi-Layer Perceptron (MLP) with TUPA (Hershcovich et al., 2017a, 2018). Most importantly, we introduce a new training method Cascaded BiLSTM by first pretraining the BiLSTM model and then training another MLP model based on pre-trained BiLSTM model. The cascaded BiLSTM parser enhances the parsing accuracy on all tasks. We also complete a Self-Attentive Constituency Parser (Kitaev and Klein, 2018a,b) as comparison. Finally, we ensemble different parsers with a reparsing strategy (Sagae and Lavie, 2006). In particular, we introduce an algorithm based on dynamic programming to perform inference for the UCCA representation. This decoder can also be utilized as a core engine for a single parser. We will describe our systems in detail, including three single parsers in Section 2 and a voter in 1 https://competitions.codalab.org/ competitions/19160 Section 3. We focus on two novel technical contributions: the Cascaded BiLSTM model and the Reparsing strategy. In Section 4 we will present experimental setup and results. 2 2.1 Single Parsers TUPA Parsers The TUPA parser (Hershcovich et al., 2017a) b"
W08-2135,J96-1002,0,0.00943365,"S tag of the candidate Lemma and POS of Neighboring words of the candidate Lemma and POS of sibling words of the candidate Length of the constituent headed by the candidate Lemma and POS of the left and right most words of the constituent of the candidate Conjunction of lemma of candidates and predicates; Conjunction of POS of candidates and predicates POS Pattern of all candidates Table 3: Features used to predict syntactic dependency parsing 3.3 Probability Estimation The system defines the conditional probability p(si |si−1 , di ; x) and p(di |si−1 , di−1 ; x) by using the maximum entropy (Berger et al., 1996) framework Denote the tag set of syntactic dependency relations D and the tag set of semantic dependency relations S. Formally, given a feature map φs and a weight vector ws , lists the features used to predict semantic dependency relations, whereas table 3 lists the features used to predict the syntactic dependency relations. The features used for syntactic dependency relation classification are strongly based on previous works (McDonald et al., 2006; Nakagawa, 2007). We just integrate syntactic dependency Relation classification and semantic dependency relation here. If one combines identifi"
W08-2135,W05-0602,0,0.0618226,"Missing"
W08-2135,J02-3001,0,0.0165755,"ered to a category defined by its first character POS Pattern (string of POS tags) of all candidates Single Character POS Pattern of all candidates Table 2: Features used for semantic role labeling 2.4 Semantic Dependency Relation Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the previous stage. Table 2 lists the features used. It is clear that the general type of features used here is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process"
W08-2135,W05-0625,0,0.0143518,"RL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process is modeled as an integer Linear Programming Problem (ILP). It takes the predicted probability over each type of the arguments as inputs, and takes the optimal solution that maximizes the linear sum of the probability subject to linguistic constraints as outputs. The constraints are a subset of constraints raised by Koomen et al. (2005) and encoded as following: 1) No overlapping or embedding arguments; 2) No duplicate argument classes for A0-A5; 3) If there is an R-arg argument, then there has to be an arg argument; 4) If there is a C-arg argument, there must be an arg argument;"
W08-2135,H05-1066,0,0.124756,"Missing"
W08-2135,W06-2932,0,0.0225129,"obability Estimation The system defines the conditional probability p(si |si−1 , di ; x) and p(di |si−1 , di−1 ; x) by using the maximum entropy (Berger et al., 1996) framework Denote the tag set of syntactic dependency relations D and the tag set of semantic dependency relations S. Formally, given a feature map φs and a weight vector ws , lists the features used to predict semantic dependency relations, whereas table 3 lists the features used to predict the syntactic dependency relations. The features used for syntactic dependency relation classification are strongly based on previous works (McDonald et al., 2006; Nakagawa, 2007). We just integrate syntactic dependency Relation classification and semantic dependency relation here. If one combines identification and classification of semantic roles as one multi-class classification, the tag set of the second layer can be substituted by the tag set of semantic roles plus a NULL (”not an argument”) label. 3.4 Inference The ”argmax problem” in structured prediction is not tractable in the general case. However, the bilayer graphical model presented in form sections admits efficient search using dynamic programming solution. Searching for the highest proba"
W08-2135,D07-1100,0,0.0296719,"Missing"
W08-2135,C04-1197,0,0.0652386,"Missing"
W08-2135,W04-3212,0,0.162513,"POS Pattern (string of POS tags) of all candidates Single Character POS Pattern of all candidates Table 2: Features used for semantic role labeling 2.4 Semantic Dependency Relation Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the previous stage. Table 2 lists the features used. It is clear that the general type of features used here is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process is modeled as an integer Linear Programming P"
W08-2135,W05-0639,0,0.0372408,"Missing"
W08-2135,W08-2121,0,\N,Missing
W08-2135,D07-1096,0,\N,Missing
W10-4144,P05-1022,0,0.225381,"Missing"
W10-4144,P02-1034,0,0.0384164,"best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to sho"
W10-4144,J05-1003,0,0.0744491,"e of the same sentence. For example, we can check whether the boundary predictions given by the TCT parser are agreed by the PCTB parser. Since the PCTB parser is trained on a different treebank from TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w)"
W10-4144,W02-1001,0,0.300956,"rom TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w +"
W10-4144,P07-1104,0,0.0134447,"the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to show the features we e"
W10-4144,N07-1051,0,0.0495543,"Missing"
W10-4144,P06-1055,0,0.202281,"Missing"
wang-etal-2010-automatic,J90-1003,0,\N,Missing
wang-etal-2010-automatic,E03-1073,0,\N,Missing
wang-etal-2010-automatic,J09-1005,0,\N,Missing
wang-etal-2010-automatic,P95-1007,0,\N,Missing
wang-etal-2010-automatic,P94-1033,0,\N,Missing
wang-etal-2010-automatic,chen-etal-2006-study,0,\N,Missing
Y09-2011,J96-1002,0,0.00651777,"n all the system improvements, we perform a binominal test of significance at p=0.05, and all significant improvements are marked with a *. From this table, we can see that the most effective features are those so called structural features. 5.3 Classifier Performance Table 5: F-measures for different classifiers. SNoW MaxEnt SVM Syntactic devel. test 96.92 96.60 97.15 96.53 97.44 96.94 Semantic devel. test 82.28 84.19 85.74 86.72 85.46 87.19 We experimented with three popular machine learning algorithms: Support Vector Machine classifier (SVM) (Vapnik, 1998), Maximum Entropy classifier (ME) (Berger et al., 1996), and Sparse Network of Winnows (SNoW) (Roth, 1998). For each algorithm we use the same set of features. In terms of SVM, we used TinySVM3 . All SVM classifiers were realized with default parameters. One-Vs-All strategy is used to solve multi-class classification problem. For ME model, we use maxent4 . For SNoW model, we use UIUC SNoW toolkit5 . In training, SNoW’s default parameters are used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. Table 5 shows the classification performance of different classifiers, both on test and developm"
Y09-2011,A00-2031,0,0.0239594,"Missing"
Y09-2011,P05-1022,0,0.0277018,". Some sparse tags cannot be accurately identified, such as Q. The recall, in general, performs worse than precision. This is mainly for that the negative samples (syntactic nodes are not assigned any function label) is much more than the positive sample. 5.5 Using Automatic Parses The results in former experiments are based on the use of hand-crafted parses. In practical use, of course, automatic parses will not be as accurate. To gauge the tagging performance in realistic situation, in this section, we report experiments on function tag labeling with automatic parsing information. Charniak (Charniak and Johnson, 2005) parser, which is ported to Chinese, are used to produce full parses. The parser is re-trained using the same training and development data in function tagging. Table 8 summarizes the parsing performance of Charniak parser. Table 9 shows the parsing performance and the function tag labeling performance (evaluation metric was described in (Blaheta and Charniak, 2000)). 6 Conclusion Function tag assignment has been studied for English and Spanish. In this paper, we address the question of assigning function tags to parsed sentences in Chinese. We describe a Chinese 537 Table 8: Parsing performan"
Y09-2011,P06-2018,0,0.0395342,"Missing"
Y09-2011,N06-1024,0,0.016019,"BJ 43701 47410 EXT 1485 1544 FOC 247 247 PRD 6113 6117 SBJ 38995 65335 TPC 2291 3149 Miscellaneous Label APP 6762 6762 HLN 1501 1501 PN 28559 28581 SHORT* 60 60 TTL 887 891 WH 832 832 Semantic Label BNF 645 645 CND 624 624 DIR 2724 2739 IJ* 28 29 LGS 398 399 LOC 4684 4799 MNR 2424 2618 PRP 1109 1173 TMP 8254 8475 VOC* 20 20 Clause Type IMP* 49 49 Q 931 935 Discrepancy Type ADV 4400 4430 3 Method 3.1 Previous Work There are two main kinds of function assignment methods, which we call parsing method and labeling method. Parsing methods integrate function tag assignment into the parsing process (Gabbard et al., 2006; Merlo and Musillo, 2005), whereas labeling approaches take syntactic parsing as pre-processing and label function tags or NULL tag (which indicates the given constituent does not represent any function tags) to each syntax tree node. (Blaheta, 2004; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006). Gabbard et al. (2006) modify Collins parser’ model 2 to allow it to produce function tags without decreasing the parsing performance. In the original model function tags is deleted after being used to identify and mark arguments. Collins parser use function tags as part of the heuristi"
Y09-2011,P04-1040,0,0.0377766,"Missing"
Y09-2011,D07-1062,0,0.0250633,"4.89 95.06 94.87 95.07 95.03 95.03 95.26 95.23 94.96 *95.33 *95.41 94.94 94.92 94.98 *95.39 *96.87 94.97 95.08 94.89 94.83 94.86 94.73 94.98 95.06 Semantic 75.82 *78.24 77.22 76.49 *81.87 *78.48 *81.10 *76.17 76.98 *78.55 76.84 *81.26 *80.22 77.23 76.55 *78.23 *77.92 *78.09 *80.95 76.82 *78.79 *78.38 75.95 76.92 *77.36 *80.32 *78.02 *79.00 *78.34 *77.69 • Head Trace (htr) The sequential container of the head down upon the phrase. For example, the head word of IP-IPC is “d d/opening”; therefore this feature of IP-TPC is IP↓VP↓VP↓VV. This feature is very similar to etree feature in TAG grammar (Liu and Sarkar, 2007). • C-commander thread of the head C-commander2 thread features, raised by (Sun et al., 2008), are sequential containers of constituents which C-command the head word of the constituent. We design two C-commander threads: 1. all items in the thread are categories of the C-commanders (cct-c); 2. using the word content to occupy the head position (cct-w). For instance, in Figure 1, the noun phrase “dd/Guangxi” and the preposition phrase “d d/opening” are two left c-commanders of the head “dd/opening”, so the cct-w feature for IP-TPC is NP←PP←dd. 2 C-command is a concept in X-bar syntax theory. A"
Y09-2011,H05-1078,0,0.0176235,"5 1544 FOC 247 247 PRD 6113 6117 SBJ 38995 65335 TPC 2291 3149 Miscellaneous Label APP 6762 6762 HLN 1501 1501 PN 28559 28581 SHORT* 60 60 TTL 887 891 WH 832 832 Semantic Label BNF 645 645 CND 624 624 DIR 2724 2739 IJ* 28 29 LGS 398 399 LOC 4684 4799 MNR 2424 2618 PRP 1109 1173 TMP 8254 8475 VOC* 20 20 Clause Type IMP* 49 49 Q 931 935 Discrepancy Type ADV 4400 4430 3 Method 3.1 Previous Work There are two main kinds of function assignment methods, which we call parsing method and labeling method. Parsing methods integrate function tag assignment into the parsing process (Gabbard et al., 2006; Merlo and Musillo, 2005), whereas labeling approaches take syntactic parsing as pre-processing and label function tags or NULL tag (which indicates the given constituent does not represent any function tags) to each syntax tree node. (Blaheta, 2004; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006). Gabbard et al. (2006) modify Collins parser’ model 2 to allow it to produce function tags without decreasing the parsing performance. In the original model function tags is deleted after being used to identify and mark arguments. Collins parser use function tags as part of the heuristics for doing so. A followi"
Y09-2011,N04-1032,0,0.0851794,"hereas the proper noun classifier recognizes that phrase as a PN. Other classifiers, such as HLN predictor, should assign NULL label to this phrase. 4 Features 4.1 Baseline Features Our baseline system uses features introduced by Blaheta (2004): category, cc-category, head, head POS, alt head, alt head POS, category clusters. • Category This is the syntactic category (NP, VP, IP, etc.) of the constituent. • cc-Category If a candidate phrase is comprised of the conjunction of two or more XP, this feature is CCXP. • Head To extract the syntactic head of a phrase, we use head rules described in (Sun and Jurafsky, 2004). This set of head rules are very popular in Chinese parsing research, such as in (Duan et al., 2007; Zhang and Clark, 2008). • Head Word POS The part-of-speech of syntactic head. • Alt Head Word Many kinds of function tags, such as temporals and locatives, occur as prepositional phrases in a sentence, and it is often the case that the head words of those phrase, which are always prepositions, are not very discriminative, for example, “ddd/in the last year”, “ddd/in Beijing”, both share the same head word “d”, but the former is TMP whereas the latter is LOC. Alt Head Word feature is the head o"
Y09-2011,C08-1105,1,0.744974,"7 94.97 95.08 94.89 94.83 94.86 94.73 94.98 95.06 Semantic 75.82 *78.24 77.22 76.49 *81.87 *78.48 *81.10 *76.17 76.98 *78.55 76.84 *81.26 *80.22 77.23 76.55 *78.23 *77.92 *78.09 *80.95 76.82 *78.79 *78.38 75.95 76.92 *77.36 *80.32 *78.02 *79.00 *78.34 *77.69 • Head Trace (htr) The sequential container of the head down upon the phrase. For example, the head word of IP-IPC is “d d/opening”; therefore this feature of IP-TPC is IP↓VP↓VP↓VV. This feature is very similar to etree feature in TAG grammar (Liu and Sarkar, 2007). • C-commander thread of the head C-commander2 thread features, raised by (Sun et al., 2008), are sequential containers of constituents which C-command the head word of the constituent. We design two C-commander threads: 1. all items in the thread are categories of the C-commanders (cct-c); 2. using the word content to occupy the head position (cct-w). For instance, in Figure 1, the noun phrase “dd/Guangxi” and the preposition phrase “d d/opening” are two left c-commanders of the head “dd/opening”, so the cct-w feature for IP-TPC is NP←PP←dd. 2 C-command is a concept in X-bar syntax theory. Assuming α and β are two nodes in a syntax tree: α C-commands β means every parent of α is anc"
Y09-2011,D08-1059,0,0.0500208,"L label to this phrase. 4 Features 4.1 Baseline Features Our baseline system uses features introduced by Blaheta (2004): category, cc-category, head, head POS, alt head, alt head POS, category clusters. • Category This is the syntactic category (NP, VP, IP, etc.) of the constituent. • cc-Category If a candidate phrase is comprised of the conjunction of two or more XP, this feature is CCXP. • Head To extract the syntactic head of a phrase, we use head rules described in (Sun and Jurafsky, 2004). This set of head rules are very popular in Chinese parsing research, such as in (Duan et al., 2007; Zhang and Clark, 2008). • Head Word POS The part-of-speech of syntactic head. • Alt Head Word Many kinds of function tags, such as temporals and locatives, occur as prepositional phrases in a sentence, and it is often the case that the head words of those phrase, which are always prepositions, are not very discriminative, for example, “ddd/in the last year”, “ddd/in Beijing”, both share the same head word “d”, but the former is TMP whereas the latter is LOC. Alt Head Word feature is the head of the object of a prepositional phrase (and undefined for other sorts of constituents), which is designed to capture more in"
