2019.icon-1.25,L16-1521,0,0.0199059,"ited accessibility of up-to-date information. Providing more individuals access to the online repositories of information can often help them improve their wellbeing. There are some situations particularly during natural calamities where the absence of notifications about potentially disaster-prone areas can result in life and death situations of individuals. People in regions with sparse connectivity often fall victim to these incidents due to lack of timely updates. Using technical platforms to support the spread of information to these regions is an important goal to keep in mind. LORELEI (Strassel and Tracey, 2016) is a DARPA funded initiative with the goal of the building of technologies for dealing and responding to disasters in low resource language communities. Similar initiatives in India would be capable of saving lives. The daily function and health of individuals in a community can be influenced positively by the dissemination of relevant information. For example, healthcare and agricultural knowledge We use Gondi, a South-Central Dravidian language in the vulnerable category on UNESCO’s Atlas of the Worlds Languages in Danger (Moseley, 2010), as an example wherever possible. Spoken by nearly 3"
2019.icon-1.25,D08-1058,0,0.0250619,"is considerable difficulty in adopting these tools. Machine Translation can potentially be used as a fix to bridge the gap. Translation engines can help in translating documents from minority languages to majority languages. This allows the pool of data to be used in a number of NLP tasks like sentiment analysis and summarization. Doing so allows us to leverage the existing body of work in NLP done on resource-rich languages and subsequently apply it to the resource-poor languages, thereby foregoing any attempt to reinvent the wheel for these languages. This ensures a quicker and wider impact.Wan (2008) performs sentiment analysis on Chinese customer reviews by translating them to English. They observe that the quality of machine translation systems are sufficient for sentiment analysis to be performed on the automatically translated texts without a substantial tradeoff in accuracy. Making more digital content available The process of enabling more low-resource language communities with tools to access online information alone is not sufficient. There need to be steps taken to make more of the content which exists online interpretable to people in these communities. For example, The Indian C"
2019.icon-1.25,C12-1164,0,0.0234494,"1) in the Indian states of Chhattisgarh, Andhra, Odisha, Maharashtra and Karnataka, it is heavily influenced by the dominant state language. However, it is also one of the least resourced languages in India, with very little available data and technology. We believe that the components discussed in the sections below encapsulate the spectrum of issues surrounding this field and that all future discussions in this area will also fall under the umbrella 212 lation tools for low resource languages can help. Cross-language information retrieval makes extensive use of these translation mechanisms (Zhou et al., 2012) where information is retrieved in a language different from the language of the user’s query. McNamee and Mayfield (2002) describes a system making use of minimal resources to perform the same. There is huge potential for language technologies to be involved in content creation and information access. Further, more accurate retrieval methods can help the user get relevant information specific to their needs and context in their own language. can affect the prosperity of a rural household, making them aware of potential solutions and remedies which can be acquired. There has been a considerabl"
2020.acl-main.329,W18-3219,0,0.0939614,"Missing"
2020.acl-main.329,W18-3200,0,0.0686392,"ral tasks, so that tasks with less training data can benefit from others. Although our current work does not include models evaluated in a multi-task setting, we plan to implement this in subsequent versions of the benchmark. There have been shared tasks conducted in the past as part of code-switching workshops co-located with notable NLP conferences. The first and second workshops on Computational Approaches to Code Switching (Diab et al., 2014, 2016) conducted a shared task on Language Identification for several language pairs (Solorio et al., 2014; Molina et al., 2016). The third workshop (Aguilar et al., 2018) included a shared task on Named Entity Recognition for the English-Spanish and Modern Standard Arabic-Egyptian Arabic language pairs(Aguilar et al., 2019). The Forum for Information Retrieval Evaluation (FIRE) aims to meet new challenges in multilingual information access and has conducted several shared tasks on code-switching. These include tasks on transliterated search, (Roy et al., 2013; Choudhury et al., 2014) code-mixed entity extraction (Rao and Devi, 2016) and mixed script information retrieval (Sequiera et al., 2015; Banerjee et al., 2016). Other notable shared tasks include the Too"
2020.acl-main.329,W16-5812,0,0.0485338,"tains a transliterated version, where Hindi is in the Roman script, and also a corrected version in which Hindi has been manually converted back to Devanagari. We report the highest score obtained by (Bhat et al., 2018) as the SOTA for this task. The second English-Hindi dataset we use was part of the ICON 2016 Tool Contest on POS Tagging for Code-Mixed Indian Social Media Text (Jamatia et al., 2016) (FG POS). We report the highest score obtained by (Anupam Jamatia, 2016)- (report communicated directly by authors) as the SOTA for this task. For English-Spanish, of the two corpora utilised in (AlGhamdi et al., 2016), we choose the Bangor Miami corpus (Bangor POS) owing to the larger size of the corpus. We report the highest score 3578 obtained by (AlGhamdi et al., 2016) as the SOTA for this task. 3.3 Named Entity Recognition (NER) NER involves recognizing named entities such as person, location, organization etc. in a segment of text. For English-Hindi we use the Twitter NER corpus provided by (Singh et al., 2018) (IIITH NER). We report the highest score obtained by (Singh et al., 2018) as the SOTA for this task. For English-Spanish, we use the Twitter NER corpus provided as part of the CALCS 2018 shared"
2020.acl-main.329,N18-1090,0,0.0322344,"use a POS tagging dataset (Jamatia et al., 2016) which also contains language labels. For English-Spanish we choose the dataset in (Solorio et al., 2014), provided as part of the LID shared task at EMNLP 2014. We report the highest score obtained for SPA-EN (Solorio et al., 2014) as the SOTA for this task. 3.2 Part of Speech (POS) tagging POS tagging includes labelling at the word level, grammatical part of speech tags such as noun, verb, adjective, pronoun, prepositions etc. For EnglishHindi, we use two datasets. The first is the codeswitched Universal Dependency parsing dataset provided by (Bhat et al., 2018) (UD POS). This corpus contains a transliterated version, where Hindi is in the Roman script, and also a corrected version in which Hindi has been manually converted back to Devanagari. We report the highest score obtained by (Bhat et al., 2018) as the SOTA for this task. The second English-Hindi dataset we use was part of the ICON 2016 Tool Contest on POS Tagging for Code-Mixed Indian Social Media Text (Jamatia et al., 2016) (FG POS). We report the highest score obtained by (Anupam Jamatia, 2016)- (report communicated directly by authors) as the SOTA for this task. For English-Spanish, of the"
2020.acl-main.329,W18-3204,0,0.107072,"Missing"
2020.acl-main.329,W18-3211,0,0.34418,"enges in multilingual information access and has conducted several shared tasks on code-switching. These include tasks on transliterated search, (Roy et al., 2013; Choudhury et al., 2014) code-mixed entity extraction (Rao and Devi, 2016) and mixed script information retrieval (Sequiera et al., 2015; Banerjee et al., 2016). Other notable shared tasks include the Tool Contest on POS Tagging for Code-Mixed Indian Social Media at ICON 2016 (Jamatia et al., 2016), Sentiment Analysis for Indian Languages (Code-Mixed) at ICON 2017 (Patra et al., 2018) and the Code-Mixed Question Answering Challenge (Chandu et al., 2018a). Each of the shared tasks mentioned above attracted several participants and have led to follow up research in these problems. However, all tasks have focused on a single NLP problem and so far, there has not been an evaluation of models across 3576 several code-switched NLP tasks. Our objective with proposing GLUECoS is to address this gap, and determine which models best generalize across different tasks, languages and datasets. 3 Tasks and Datasets 1 shows all the datasets that we use, with their statistics, while Table 2 shows the code-switching statistics of the data in terms of standa"
2020.acl-main.329,W14-3900,0,0.0700911,"erGLUE (Wang et al., 2019) once models beat the human baseline for GLUE. The motivation behind GLUE is to evaluate models in a multi-task learning framework across several tasks, so that tasks with less training data can benefit from others. Although our current work does not include models evaluated in a multi-task setting, we plan to implement this in subsequent versions of the benchmark. There have been shared tasks conducted in the past as part of code-switching workshops co-located with notable NLP conferences. The first and second workshops on Computational Approaches to Code Switching (Diab et al., 2014, 2016) conducted a shared task on Language Identification for several language pairs (Solorio et al., 2014; Molina et al., 2016). The third workshop (Aguilar et al., 2018) included a shared task on Named Entity Recognition for the English-Spanish and Modern Standard Arabic-Egyptian Arabic language pairs(Aguilar et al., 2019). The Forum for Information Retrieval Evaluation (FIRE) aims to meet new challenges in multilingual information access and has conducted several shared tasks on code-switching. These include tasks on transliterated search, (Roy et al., 2013; Choudhury et al., 2014) code-mi"
2020.acl-main.329,P19-1070,0,0.0255249,"rms best. We do not experiment with baseline or cross-lingual embedding techniques for NLI, since we find that mBERT surpasses the other techniques for all other tasks. For NLI, as in the other cases, we find that modified mBERT performs better than mBERT. We hypothesize that this happens because code-switched languages are not just a union of two monolingual languages. The distributions and usage of words in code-switched languages differ from their monolingual counterparts, and can only be captured with real code-switched data, or synthetically generated data that closely mimics real data. (Glavas et al., 2019) point out how all crosslingual word embedding methods optimize for bilingual lexicon induction. Each model is trained using different language pairs and different training and evaluation dictionaries, leading to it overfitting to the task it is optimizing for and failing in other cross-lingual scenarios. Also, the loss function in training cross-lingual word embeddings has a component where w1 in one language predicts the context of its aligned word w2 in the other language. However, in the case of code-switching, w1 appear9 (Sun et al., 2019) show that fine-tuning BERT with in-domain data on"
2020.acl-main.329,P14-1006,0,0.0191507,"training them. 4.1 MUSE Embeddings We use the MUSE library3 to train both supervised and unsupervised word embeddings. The unsupervised word embeddings are learnt without any parallel data or anchor point. It learns a mapping from the source to the target space using adversarial training and (iterative) Procrustes refinement (Conneau et al., 2017). The supervised method leverages a bilingual dictionary (or identical character strings as anchor points), to learn a mapping from the source to the target space using (iterative) Procrustes alignment. 4.2 BiCVM Embeddings This method, proposed by (Hermann and Blunsom, 2014), leverages parallel data, based on the assumption that parallel sentences are equivalent in meaning and subsequently have similar sentence 3 https://github.com/facebookresearch/DrQA 3579 https://github.com/facebookresearch/MUSE representations. We use the BiCVM toolkit4 to learn these embeddings. The parallel corpus we use for English-Spanish consists of 4.5M parallel sentences from Twitter. For English-Hindi, we make use of an internal parallel corpus consisting of roughly 5M parallel sentences. 4.3 BiSkip Embeddings This method makes use of parallel corpora as well as word alignments to lea"
2020.acl-main.329,2020.calcs-1.2,1,0.685424,"in zero-shot cross lingual transfer and code-switched settings. Though comprehensively validated by (Pires et al., 2019) in the case of zero-shot transfer, the probing in codeswitched settings was limited to one dataset of one task, namely POS Tagging. To address all these issues and inspired by the GLUE (Wang et al., 2018) benchmark, we propose GLUECoS, a language understanding evaluation framework for Code-Switched NLP. We include five tasks from previously conducted evaluations and shared tasks, and propose a sixth, Natural Language Inference task for code-switching, using a new dataset1 (Khanuja et al., 2020). We include tasks varying in complexity ranging from wordlevel tasks [Language Identification (LID); Named Entity Recognition (NER)], syntactic tasks [POS 1 we use a subset of the original corpus as available to us at the time of experimentation 3575 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3575–3585 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tagging], semantic tasks [Sentiment Analysis; Question Answering] and finally a Natural Language Inference task. Where available, we include multiple datasets for each task in"
2020.acl-main.329,L18-1548,0,0.217573,"Missing"
2020.acl-main.329,N16-1030,0,0.0893728,"Missing"
2020.acl-main.329,W15-1521,0,0.0433761,"ed on the assumption that parallel sentences are equivalent in meaning and subsequently have similar sentence 3 https://github.com/facebookresearch/DrQA 3579 https://github.com/facebookresearch/MUSE representations. We use the BiCVM toolkit4 to learn these embeddings. The parallel corpus we use for English-Spanish consists of 4.5M parallel sentences from Twitter. For English-Hindi, we make use of an internal parallel corpus consisting of roughly 5M parallel sentences. 4.3 BiSkip Embeddings This method makes use of parallel corpora as well as word alignments to learn cross-lingual embeddings. (Luong et al., 2015) adapt the skip-gram objective originally proposed by (Mikolov et al., 2013) to a bilingual setting wherein a model learns to predict words cross-lingually along with the monolingual objectives. We make use of the fastalign toolkit5 to learn word alignments given parallel corpora and use the BiVec toolkit6 to learn the final BiSkip embeddings given the parallel corpora and the word alignments. The parallel corpora utilised to learn these are the same as those used to learn the BiCVM embeddings. 4.4 Synthetic Data (GCM) Embeddings We also experiment with skip-gram embeddings learnt from synthet"
2020.acl-main.329,W16-5805,0,0.132551,"multi-task learning framework across several tasks, so that tasks with less training data can benefit from others. Although our current work does not include models evaluated in a multi-task setting, we plan to implement this in subsequent versions of the benchmark. There have been shared tasks conducted in the past as part of code-switching workshops co-located with notable NLP conferences. The first and second workshops on Computational Approaches to Code Switching (Diab et al., 2014, 2016) conducted a shared task on Language Identification for several language pairs (Solorio et al., 2014; Molina et al., 2016). The third workshop (Aguilar et al., 2018) included a shared task on Named Entity Recognition for the English-Spanish and Modern Standard Arabic-Egyptian Arabic language pairs(Aguilar et al., 2019). The Forum for Information Retrieval Evaluation (FIRE) aims to meet new challenges in multilingual information access and has conducted several shared tasks on code-switching. These include tasks on transliterated search, (Roy et al., 2013; Choudhury et al., 2014) code-mixed entity extraction (Rao and Devi, 2016) and mixed script information retrieval (Sequiera et al., 2015; Banerjee et al., 2016)."
2020.acl-main.329,W18-2405,0,0.0466526,"OS). We report the highest score obtained by (Anupam Jamatia, 2016)- (report communicated directly by authors) as the SOTA for this task. For English-Spanish, of the two corpora utilised in (AlGhamdi et al., 2016), we choose the Bangor Miami corpus (Bangor POS) owing to the larger size of the corpus. We report the highest score 3578 obtained by (AlGhamdi et al., 2016) as the SOTA for this task. 3.3 Named Entity Recognition (NER) NER involves recognizing named entities such as person, location, organization etc. in a segment of text. For English-Hindi we use the Twitter NER corpus provided by (Singh et al., 2018) (IIITH NER). We report the highest score obtained by (Singh et al., 2018) as the SOTA for this task. For English-Spanish, we use the Twitter NER corpus provided as part of the CALCS 2018 shared task on NER for code-switched data (Aguilar et al., 2019) (CALCS NER). We report the highest score obtained by (Winata et al., 2019) as the SOTA for this task. 3.4 Sentiment Analysis Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment. For English-Hindi we choose the sentiment annotated social media corpus used"
2020.acl-main.329,P19-1493,0,0.0758506,"Missing"
2020.acl-main.329,P18-1143,1,0.772422,"ed Language Evaluation Benchmark (GLUE) to evaluate embedding models on a wide variety of language understanding tasks. This benchmark has spurred research in monolingual transfer learning settings. Data and annotated resources are scarce for codeswitched languages, even if one or both languages being mixed are high resource. Due to this, there is a lack of standardized datasets in code-switched languages other than those used in shared tasks in a few language pairs. Although models using synthetic code-switched data and cross-lingual embedding techniques have been proposed for codeswitching (Pratapa et al., 2018a), there has not been a comprehensive evaluation of embedding models across different types of tasks. Furthermore, there have been claims that multilingual models such as mBERT are competent in zero-shot cross lingual transfer and code-switched settings. Though comprehensively validated by (Pires et al., 2019) in the case of zero-shot transfer, the probing in codeswitched settings was limited to one dataset of one task, namely POS Tagging. To address all these issues and inspired by the GLUE (Wang et al., 2018) benchmark, we propose GLUECoS, a language understanding evaluation framework for C"
2020.acl-main.329,D18-1344,1,0.939966,"ed Language Evaluation Benchmark (GLUE) to evaluate embedding models on a wide variety of language understanding tasks. This benchmark has spurred research in monolingual transfer learning settings. Data and annotated resources are scarce for codeswitched languages, even if one or both languages being mixed are high resource. Due to this, there is a lack of standardized datasets in code-switched languages other than those used in shared tasks in a few language pairs. Although models using synthetic code-switched data and cross-lingual embedding techniques have been proposed for codeswitching (Pratapa et al., 2018a), there has not been a comprehensive evaluation of embedding models across different types of tasks. Furthermore, there have been claims that multilingual models such as mBERT are competent in zero-shot cross lingual transfer and code-switched settings. Though comprehensively validated by (Pires et al., 2019) in the case of zero-shot transfer, the probing in codeswitched settings was limited to one dataset of one task, namely POS Tagging. To address all these issues and inspired by the GLUE (Wang et al., 2018) benchmark, we propose GLUECoS, a language understanding evaluation framework for C"
2020.acl-main.329,P17-1180,1,0.910378,"Missing"
2020.acl-main.329,L16-1655,0,0.119432,"Missing"
2020.acl-main.329,W18-5446,0,0.115451,"Missing"
2020.acl-main.329,D19-1360,0,0.0526171,"ore 3578 obtained by (AlGhamdi et al., 2016) as the SOTA for this task. 3.3 Named Entity Recognition (NER) NER involves recognizing named entities such as person, location, organization etc. in a segment of text. For English-Hindi we use the Twitter NER corpus provided by (Singh et al., 2018) (IIITH NER). We report the highest score obtained by (Singh et al., 2018) as the SOTA for this task. For English-Spanish, we use the Twitter NER corpus provided as part of the CALCS 2018 shared task on NER for code-switched data (Aguilar et al., 2019) (CALCS NER). We report the highest score obtained by (Winata et al., 2019) as the SOTA for this task. 3.4 Sentiment Analysis Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment. For English-Hindi we choose the sentiment annotated social media corpus used in the ICON 2017 shared task; Sentiment Analysis for Indian Languages (SAIL) (Patra et al., 2018). This corpus is originally language tagged at the word level with Hindi in the Roman script. We report the highest score obtained for HI-EN (Patra et al., 2018) as the SOTA for this task. For English-Spanish we choose the sentim"
2020.acl-main.329,W14-3907,0,\N,Missing
2020.acl-main.560,Q19-1038,0,0.0286891,"th Annual Meeting of the Association for Computational Linguistics, pages 6282–6293 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Nevertheless, it would be prudent to re-examine these issues in the light of recent advances in deep learning. Neural systems, on one hand, require a lot more data for training than rule-based or traditional ML systems, creating a bigger technological divide between the Xs and Ys; yet, some of the most recent techniques on zero-shot learning of massively multilingual systems (Devlin et al., 2019; Conneau and Lample, 2019; Aharoni et al., 2019; Artetxe and Schwenk, 2019) bridge this gap by obliterating the need for large labeled datasets in all languages. Instead, they need only large unlabeled corpora across languages and labeled data in only some languages. Assuming that this approach can be taken to its promising end, how does the fate of different languages change? We break down this complex prescient question into the following more tractable and quantifiable questions on Linguistic Diversity and Inclusion: 1. How many resources, labeled and unlabeled, are available across the World’s languages? How does this distribution correlate to their number of nat"
2020.acl-main.560,P19-1493,0,0.0864641,"Missing"
2020.acl-main.560,J19-3005,0,0.16808,"Missing"
2020.acl-main.560,bird-etal-2008-acl,0,0.137618,"Missing"
2020.acl-main.560,D16-1264,0,0.014204,"the assumption that the collection of one unit is proportional to a certain extent of effort being invested towards the resource improvement of that language. Moreover, this feature discretization is unambiguous and concrete. Other units such as the total number of datapoints across datasets can be misleading because different NLP tasks have different data requirements. For example, while Machine Translation (MT) models require datapoints to the order of millions (Koehn and Knowles, 2017) to perform competitively, competent models in Question Answering require around 100 thousand datapoints (Rajpurkar et al., 2016). Moreover, the unit of datapoints vary across different technologies (e.g. Speech data measured in hours, MT data measured in number of parallel sentences). 6283 to categorize languages into 6 unique positions in the language resource ‘race’: 0 - The Left-Behinds These languages have been and are still ignored in the aspect of language technologies. With exceptionally limited resources, it will be a monumentous, probably impossible effort to lift them up in the digital space. Unsupervised pre-training methods only make the ‘poor poorer’, since there is virtually no unlabeled data to use. Figu"
2020.acl-main.560,N19-1423,0,0.015212,"n by our NLP systems (Ponti et al., 2019). 6282 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282–6293 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Nevertheless, it would be prudent to re-examine these issues in the light of recent advances in deep learning. Neural systems, on one hand, require a lot more data for training than rule-based or traditional ML systems, creating a bigger technological divide between the Xs and Ys; yet, some of the most recent techniques on zero-shot learning of massively multilingual systems (Devlin et al., 2019; Conneau and Lample, 2019; Aharoni et al., 2019; Artetxe and Schwenk, 2019) bridge this gap by obliterating the need for large labeled datasets in all languages. Instead, they need only large unlabeled corpora across languages and labeled data in only some languages. Assuming that this approach can be taken to its promising end, how does the fate of different languages change? We break down this complex prescient question into the following more tractable and quantifiable questions on Linguistic Diversity and Inclusion: 1. How many resources, labeled and unlabeled, are available across the Wo"
2020.acl-main.560,kamholz-etal-2014-panlex,0,0.0442972,"VIBGYOR, represents the total speaker population size from low to high. Bounding curves used to demonstrate covered points by that language class. 2.2 Repositories We focus our attention on the LDC catalog1 and the ELRA Map2 for labeled datasets. Although there are other repositories of data available online, we found it practical to treat these organized collections as a representation of labeled dataset availability. This way, we look at standardized datasets that have established data quality and consistency, and which have been used in prior work. There are strong efforts such as PanLex (Kamholz et al., 2014), which is a large lexical database of a wide range of languages being used for a lexical translator, and OLAC (Simons and Bird, 2003), which contains a range of information for different languages (e.g. text collections, audio recordings, and dictionaries). However, keeping within the purview of NLP datasets used in *CL conferences, we decided to focus on popular repositories such as the above-mentioned. We look at Wikipedia pages as a measure for unlabeled data resources. With regards to language technologies, Wikipedia pages represent a strong source of unsupervised training data which are"
2020.acl-main.560,W17-3204,0,0.0127623,"res such as number of hours required to collect data aren’t available. We treat each data resource as a fundamental unit, based on the assumption that the collection of one unit is proportional to a certain extent of effort being invested towards the resource improvement of that language. Moreover, this feature discretization is unambiguous and concrete. Other units such as the total number of datapoints across datasets can be misleading because different NLP tasks have different data requirements. For example, while Machine Translation (MT) models require datapoints to the order of millions (Koehn and Knowles, 2017) to perform competitively, competent models in Question Answering require around 100 thousand datapoints (Rajpurkar et al., 2016). Moreover, the unit of datapoints vary across different technologies (e.g. Speech data measured in hours, MT data measured in number of parallel sentences). 6283 to categorize languages into 6 unique positions in the language resource ‘race’: 0 - The Left-Behinds These languages have been and are still ignored in the aspect of language technologies. With exceptionally limited resources, it will be a monumentous, probably impossible effort to lift them up in the digi"
2020.acl-main.560,D18-1269,0,\N,Missing
2020.calcs-1.2,P19-1493,0,0.155402,"dels today (McCoy et al., 2019), are expected to correctly predict examples involving Negation, Numeral Changes, Swapping Roles or Paraphrasing. However, they are hypothesized to fail in examples requiring deeper semantic knowledge, for instance, the examples involving Sarcasm, Word Sense Disambiguation, Inter-dependent Inference or Speaker Conflict. With the recent upsurge of multilingual models, and claims that they can be used to solve code-mixed tasks as well, we evaluate the multilingual BERT model on our dataset. Previously, it has been shown to perform well on codemixed POS tagging by (Pires et al., 2019). Our results are as shown in Table 5. We make use of the transformers library1 for the experiment. We use the AdamW optimizer with a learning rate of 5e-5, epsilon of 1e-8, and a batch size of 16, as suggested by (Devlin et al., 2018). We train for 5 epochs. We report the average result of training on 5 random seed values. Note that the dataset contains Hindi in Roman script while mBERT is trained on Hindi in Devanagari, and we report this number as a mere baseline. To put our numbers in perspective, we have included accuracies achieved by the BERT base model, as shown in (Talman and Chatziky"
2020.calcs-1.2,W17-7510,1,0.818709,"first stage is conducted to fulfil two objectives: Dataset Creation Code-mixing is primarily a spoken language phenomenon, so it is challenging to find naturally occurring code-mixed text on the web, or in standard monolingual corpora. Social Media and Instant Messaging data from multilingual users can be a source of code-mixed conversational data, but cannot be used due to privacy concerns. For this reason, we choose scripts of Hindi movies, also referred to as “Bollywood” movies. Bollywood movies, from certain time periods and genres, contain varying amounts of code-mixing, as described in (Pratapa and Choudhury, 2017). Although the movie data is not artificially generated, it is scripted, which makes it a less natural source of data than conversations between real people. 3.1. Task Paradigm • It acts as an initial filter to make sure that the annotators are well versed in both languages and have a good understanding of the task. If they fail to assign gold labels to more than 80 percent of the hypotheses, they will not be assigned the second stage of annotation. Data Preparation The Bollywood data consists of scenes taken from 18 movies. The data is in Romanized form, so both Hindi and English parts of the"
2020.calcs-1.2,P18-1143,1,0.855215,"otheses based on them for code-mixed language. Based on the approaches used for creating the datasets mentioned above, there are three main approaches that can be taken while creating a code-mixed NLI dataset. One approach is to translate an existing NLI dataset into a code-mixed language. Since there do not exist good Machine Translation systems for code-mixed languages, that can capture the nuances of the language necessary for an NLI dataset, this would need to be done manually to ensure high quality. Another approach is to synthesize code-mixed data artificially, using approaches such as (Pratapa et al., 2018). However, this cannot be done for a conversational dataset, and will not be natural enough to create good hypotheses. The third approach, which we take, is to use a naturally occurring source of conversational data as premises, and get the hypotheses manually annotated. 3. be done automatically using a transliteration system if desired. 3.2. The data annotation process involves the formulation of one or more true and false hypothesis, given a scene from the categories above as a premise. Subsequently, the NLI task is to classify whether the conversation entails the hypothesis or contradicts i"
2020.calcs-1.2,W17-1609,0,0.0559582,"Missing"
2020.calcs-1.2,W18-5446,0,0.0466868,"because it has several practical applications, but also because it tests the language understanding abilities of machines beyond pattern recognition. NLI tasks usually involve inferring the logical relationship, such as entailment or contradiction, between a pair of sentences. In some cases, instead of a sentence, a document, paragraph or a dialogue snippet might be provided as the premise; the task then is to infer whether a given hypothesis is entailed in (or implied by) the premise. There are several monolingual NLI datasets available, with the most notable ones being included in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. There are also multilingual and crosslingual NLI datasets, such as XNLI (Conneau et al., 2018). These datasets have successfully spurred and facilitated research in this area. In this paper, we introduce, for the first time, a new NLI dataset for code-mixing. Code-mixing or code-switching refers to the use of more than one language in a single conversation or utterance. It is prevalent in almost all multilingual societies across the world. Monolingual as well as multilingual NLP systems typically fail to handle code-mixed inputs. Therefore, recent"
2020.calcs-1.2,P19-1363,0,0.0370259,"Missing"
2020.calcs-1.2,W09-3930,0,0.0143645,"ates our work in their context. Section 3 describes the creation of the data for annotation. Section 4 describes the data annotation, including results from the pilot and the final annotation scheme. Section 5 presents an extensive analysis and a baseline evaluation. Section 6 concludes with a discussion of future work. 2. NLI Datasets NLI is a concept central to natural language understanding models. Most of the prominent datasets that are used to solve NLI problems involve learning textual entailment wherein we determine whether a hypothesis is entailed in or contradicts a textual document (Zhang and Chai, 2009). • NLI is an important requirement for chatbots and conversational agents, and since code-mixing is a spoken and conversational phenomenon, it is crucial that such 9 Conversation MRS.KAPOOR: Kitna old fashion hairstyle hai tumhara, new hair cut kyun nahin try karte .. Go to the Vidal Sasoon salon tomorrow .. Aur thoda product use karo .. You’ll get some texture. Translation MRS KAPOOR: Your hairstyle is so old fashioned, why don’t you try a new hair cut .. Go to the Vidal Sasoon salon tomorrow .. And use some product .. You’ll get some texture. MR.KAPOOR: Tumhari maa ko bahut pata hai, MBA ki"
2020.calcs-1.2,D10-1074,0,0.0263292,"ing the PersonaChat dataset (Zhang et al., 2018). Each human labeled triple is first associated to each persona sentence and then pairs of such triple; persona sentences are labeled as entailment, neutral or contradiction. The corpus consists of around 33k examples. Types of NLI Datasets We briefly outline the prominent NLI datasets that have been well researched upon, to suitably place our contribution in context of the same. • The FraCaS test suite (Consortium and others, 1996) consists of 346 manually curated premises followed by a Yes/No/Don’t Know question. • The Conversation Entailment (Zhang and Chai, 2010) dataset consists of 50 dialogues from the Switchboard corpus (Godfrey et al., 1992). 15 volunteer annotators read the dialogues and manually created hypotheses to obtain a total of 1096 entailment annotated examples. • The RTE datasets (Dagan et al., 2005) include naturally occurring data as premises and construct hypotheses based on them. All datasets have fewer than 1000 examples for training. A limitation of these datasets is that many examples assume world knowledge which is not explicitly labeled with each example. While most of the datasets described above benefit information extraction"
2020.calcs-1.2,P18-1205,0,0.0310776,"like the Soonawallas, you already look like you are 55! MR.KAPOOR: main 57 ka hun. MR.KAPOOR: I am 57 years old. Table 1: Example Conversation from the Bollywood data Even so, each dataset is severely limited in the reasoning it represents and cannot be generalised outside of its domain. (Bernardy and Chatzikyriakidis, 2019) 2.1. randomly selecting a subset of sentence pairs from two sources - the 8k ImageFlickr dataset and the SemEval2012 STS MSR-Video Description dataset. • The Dialogue NLI Corpus (Welleck et al., 2018) consists of pairs of sentences generated using the PersonaChat dataset (Zhang et al., 2018). Each human labeled triple is first associated to each persona sentence and then pairs of such triple; persona sentences are labeled as entailment, neutral or contradiction. The corpus consists of around 33k examples. Types of NLI Datasets We briefly outline the prominent NLI datasets that have been well researched upon, to suitably place our contribution in context of the same. • The FraCaS test suite (Consortium and others, 1996) consists of 346 manually curated premises followed by a Yes/No/Don’t Know question. • The Conversation Entailment (Zhang and Chai, 2010) dataset consists of 50 dia"
2020.calcs-1.2,D15-1075,0,0.142194,"Missing"
2020.calcs-1.2,D18-1269,0,0.199117,"ecognition. NLI tasks usually involve inferring the logical relationship, such as entailment or contradiction, between a pair of sentences. In some cases, instead of a sentence, a document, paragraph or a dialogue snippet might be provided as the premise; the task then is to infer whether a given hypothesis is entailed in (or implied by) the premise. There are several monolingual NLI datasets available, with the most notable ones being included in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. There are also multilingual and crosslingual NLI datasets, such as XNLI (Conneau et al., 2018). These datasets have successfully spurred and facilitated research in this area. In this paper, we introduce, for the first time, a new NLI dataset for code-mixing. Code-mixing or code-switching refers to the use of more than one language in a single conversation or utterance. It is prevalent in almost all multilingual societies across the world. Monolingual as well as multilingual NLP systems typically fail to handle code-mixed inputs. Therefore, recently, code-mixing has attained considerable attention from the speech and NLP communities. Consequently, there have been several shared tasks o"
2020.calcs-1.2,S14-2001,0,0.0336258,"s (Williams et al., 2017) is also a crowd-sourced collection of 433k sentence pairs annotated with entailment information. Although it is modeled on SNLI, it differs from it as it covers a variety of genres in both written and spoken English. XNLI (Conneau et al., 2018) is a multilingual extension of MultiNLI wherein 5k (train) and 2.5k (dev) examples are translated into 14 languages. A. Mont Blanc is higher than B. Mt. Ararat? A. Yes. B. No, this is not correct. It is the other way around. A. Are you... B. Sure? Yes, I am. A. Ok, then • The SICK (Sentences Involving Compositional Knowledge) (Marelli et al., 2014) dataset consists of 9840 examples of inference patterns primarily to test distributional semantics. It is constructed by 10 Further, with the exception of the XNLI dataset, all other NLI datasets are in English. This motivates us to use dialogue, or conversation, as a premise, and build hypotheses based on them for code-mixed language. Based on the approaches used for creating the datasets mentioned above, there are three main approaches that can be taken while creating a code-mixed NLI dataset. One approach is to translate an existing NLI dataset into a code-mixed language. Since there do no"
2020.calcs-1.2,P19-1334,0,0.061426,"Missing"
2020.calcs-1.2,N19-1423,0,\N,Missing
2020.calcs-1.5,b-etal-2010-resource,1,0.795276,"Missing"
2020.calcs-1.5,W14-3914,1,0.918593,"n our dataset too, we find examples of tweets that are entirely in Devanagari but for the hashtag, which is in Roman. While the hashtags can be suggestive of certain information such as whether the tweet is spam, an advertisement, or contains sarcasm (Davidov et al., 2010), it could have been added just to insert the post within the global discussion of other posts using the same hashtag (Letierce et al., 2010). Therefore, script alternation using hashtags may not be suggestive of much information and we only analyse the script-mixing that occurs within the grammatical boundary of a sentence. Bali et al. (2014) analyse English-Hindi code-mixed posts on Facebook to study whether a word is an instance of actual code-mixing or just borrowing. They segregate the codemixed sentences on the basis of the matrix2 or embedding language and analyse them individually. However, they only consider the language aspect and limit themselves to Roman sentences. Our work differs from others because we take the script axis into consideration. We consider all the permutations of script and language and present a rich case study containing qualitative and quantitative analyses. To the best of our knowledge, this is the"
2020.calcs-1.5,I08-6014,0,0.0182636,"Missing"
2020.calcs-1.5,W14-5152,0,0.0265061,"ontext (EN) (a) गुड मॉ नग इं डया Translation: Good morning, India. (b) गरेट लीडर Translation: Great leader. • English context (EN) 2. Hindi context (HI) • Hindi context (HI) (a) जनमिदन क हा दक शुभकामनाएं और ढेरो बधाईयां Translation: Happy birthday and lots of well wishes. (b) कया तुम सही कर रहे हो? Translation: Are you doing the right thing? • Code-mixed context (CM) 3.4.1. Roman Script While typing English-Hindi code-mixed text, Roman script is most frequently used (Virga and Khudanpur, 2003; B. et al., 2010), and as a result, there are many LID tools available for it (Gella et al., 2014; Das and Gambäck, 2014; Rijhwani et al., 2017). We use the LID tool by Gella et al. (2014) and tag all our Roman tweets at word-level. After comparing the count of language tags, we divide the tweets into three categories. Here are a few examples of the tweets, 3. Code-mixed context (CM) (a) आपको फर ज करना है तो टेमपरेचर कम क रए Translation: Reduce the temperature if you want to freeze. (b) गलत लॉ जक Translation: Wrong logic. 1. English context (EN) 3.4.3. Mixed-Script Unlike Roman and Devanagari, a LID tool for mixed-script text has to look at all the possible variations of a word (Table 2). Contextual informati"
2020.calcs-1.5,W14-5151,1,0.840648,"gories, 1. English context (EN) (a) गुड मॉ नग इं डया Translation: Good morning, India. (b) गरेट लीडर Translation: Great leader. • English context (EN) 2. Hindi context (HI) • Hindi context (HI) (a) जनमिदन क हा दक शुभकामनाएं और ढेरो बधाईयां Translation: Happy birthday and lots of well wishes. (b) कया तुम सही कर रहे हो? Translation: Are you doing the right thing? • Code-mixed context (CM) 3.4.1. Roman Script While typing English-Hindi code-mixed text, Roman script is most frequently used (Virga and Khudanpur, 2003; B. et al., 2010), and as a result, there are many LID tools available for it (Gella et al., 2014; Das and Gambäck, 2014; Rijhwani et al., 2017). We use the LID tool by Gella et al. (2014) and tag all our Roman tweets at word-level. After comparing the count of language tags, we divide the tweets into three categories. Here are a few examples of the tweets, 3. Code-mixed context (CM) (a) आपको फर ज करना है तो टेमपरेचर कम क रए Translation: Reduce the temperature if you want to freeze. (b) गलत लॉ जक Translation: Wrong logic. 1. English context (EN) 3.4.3. Mixed-Script Unlike Roman and Devanagari, a LID tool for mixed-script text has to look at all the possible variations of a word (Table 2"
2020.calcs-1.5,W14-3906,0,0.0236224,"ter detail, and can inspire the development of appropriate NLP tools that can harness mixed1 We define nominal entities as phrases that behave either as a noun phrase or a named entity. 36 language/script data in the future. Tweets 2. Related Work Mixed-script information retrieval deals with cases in which the query and documents are in different scripts. The shared tasks in FIRE 2015 (Sequiera et al., 2015) and FIRE 2016 (Banerjee et al., 2016) present an overview of these approaches. However, they do not work for queries or documents that are in itself represented in the mixed-script text. Jurgens et al. (2014) study the tweets that have codeswitched (and possibly mixed-script) hashtags. They observe that authors fluent in non-Latin writing systems often use Latin-transliterated hashtags. In our dataset too, we find examples of tweets that are entirely in Devanagari but for the hashtag, which is in Roman. While the hashtags can be suggestive of certain information such as whether the tweet is spam, an advertisement, or contains sarcasm (Davidov et al., 2010), it could have been added just to insert the post within the global discussion of other posts using the same hashtag (Letierce et al., 2010). T"
2020.calcs-1.5,P17-2009,0,0.0192624,"that of data scarcity as it appears very less in formal texts which are usually spread across the World Wide Web. Code-mixing is primarily observed in informal settings like spoken conversations. However, with the advent of social media, it has pervaded to mediums that are set in informal contexts like forums and messaging platforms. Often these platforms are behind privacy walls that prohibit the use or scraping of such data. We resort to Twitter because studies have shown that a large number of bilingual/multilingual users code-mix on the platform (Carter et al., 2013; Solorio et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017) and the data is easily accessible for analysis. There are two ways of representing a code-mixed utterance in textual form, • Entire utterance is written in one script (single-script case) • It is written in more than one script (mixed-script case) The second phenomenon is known as script-mixing which occurs when the languages used for code-mixing have different native scripts (such as English-Hindi, French-Arabic, etc). This poses a key challenge for handling code-mixed data collected from social media and other such informal settings. As there is no laid out rule of h"
2020.calcs-1.5,P17-1180,1,0.908234,"as it appears very less in formal texts which are usually spread across the World Wide Web. Code-mixing is primarily observed in informal settings like spoken conversations. However, with the advent of social media, it has pervaded to mediums that are set in informal contexts like forums and messaging platforms. Often these platforms are behind privacy walls that prohibit the use or scraping of such data. We resort to Twitter because studies have shown that a large number of bilingual/multilingual users code-mix on the platform (Carter et al., 2013; Solorio et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017) and the data is easily accessible for analysis. There are two ways of representing a code-mixed utterance in textual form, • Entire utterance is written in one script (single-script case) • It is written in more than one script (mixed-script case) The second phenomenon is known as script-mixing which occurs when the languages used for code-mixing have different native scripts (such as English-Hindi, French-Arabic, etc). This poses a key challenge for handling code-mixed data collected from social media and other such informal settings. As there is no laid out rule of how someone should write"
2020.calcs-1.5,W14-3907,0,0.0269396,"primary reason being that of data scarcity as it appears very less in formal texts which are usually spread across the World Wide Web. Code-mixing is primarily observed in informal settings like spoken conversations. However, with the advent of social media, it has pervaded to mediums that are set in informal contexts like forums and messaging platforms. Often these platforms are behind privacy walls that prohibit the use or scraping of such data. We resort to Twitter because studies have shown that a large number of bilingual/multilingual users code-mix on the platform (Carter et al., 2013; Solorio et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017) and the data is easily accessible for analysis. There are two ways of representing a code-mixed utterance in textual form, • Entire utterance is written in one script (single-script case) • It is written in more than one script (mixed-script case) The second phenomenon is known as script-mixing which occurs when the languages used for code-mixing have different native scripts (such as English-Hindi, French-Arabic, etc). This poses a key challenge for handling code-mixed data collected from social media and other such informal settings. As there is"
2020.calcs-1.5,D14-1105,1,0.832269,"ntential and inter-sentential script-mixing are present on Twitter and show different behavior in different contexts. Examples suggest that script can be employed as a tool for emphasizing certain phrases within a sentence or disambiguating the meaning of a word. Script choice can also be an indicator of whether a word is borrowed or not. We present our analysis along with examples that bring out the nuances of the different cases. Keywords: Mixed-script, Code-mixing, Script-mixing 1. Introduction due to transliteration based loosely on the phonetic structure of the words (Singh et al., 2018; Vyas et al., 2014). The primary contribution of this paper lies in analyzing mixed-script texts present on Twitter and uncovering the underlying patterns as to when and where they are seen. While past studies have thoroughly studied linguistic functions of code-mixing (and language alternation) in speech and text (Poplack, 1980; Woolford, 1983; Alvarez-Cáccamo, 1990; Muysken et al., 2000; Sebba et al., 2012), we examine the functions of script alternation in mixed-script text. Our analysis shows that most cases of script-mixing are intentional. We find examples which suggest that script can be used as a tool fo"
2020.calcs-1.8,N19-1423,0,0.00663794,"eves near state of the art performance on the Penn Treebank WSJ set. This model runs the embedding of each token in the sentence through a transformer layer to produce contextual embeddings for each token, which are used to compute embeddings for each span in the sentence. This is then run through a scoring layer to produce scores for each span. These scores are used in a modiﬁed CKY-style parser to build up the most probable tree. For computing initial embedding of each token, we experiment with word embeddings over the combined vocabulary space of both languages and with multilingual BERT2 (Devlin et al., 2019) (mBERT), which produces subword level embeddings. Lastly, the parsing model also learns to predict POS tags of tokens (using it while parsing), so we also report the POS tagging accuracy. 3. Constituency Parsing 3.1. Background Constituency parsing is the task of generating a valid parse tree given a sentence as input. One of the simplest methods for this task is the CKY algorithm (Younger, 1967). This algorithm takes in a set of CFG productions and builds up a tree for a sentence using a dynamic programming algorithm. There are variations of this algorithm that work with a Probabilistic CFG"
2020.calcs-1.8,P15-1030,0,0.0214514,"h NN_h N_e एक today पेपर Height 7.05 (1.96) 7.10 (2.02) 5.40 (1.06) 8.16 (2.31) 8.13 (2.24) RHS Length 2.22 (2.34) 2.21 (2.28) 3.38 (1.60) 1.75 (1.16) 1.73 (1.12) Table 1: Statistics about Train and Test Datasets. Mean and Standard deviation (in brackets) reported for tree height and length of right hand size of productions. NP_e NP Size 421710 2740 1381 421710 2542 the tree given these scores. Finkel et al. (2008) use Conditional Random Fields (CRFs) for the scoring purpose. More recently, there has been a line of work where neural networks have been used to score the spans, starting oﬀ with Durrett and Klein (2015) where a ﬁxed-window based method is used. Stern et al. (2017) build upon this work by using RNNs instead of a ﬁxed-window for the scoring and Kitaev and Klein (2018) use a transformer instead of the RNN. These methods achieve performance that is superior to the early neural network parsers without the complex feature engineering associated with most of them. Figure 2: The ﬁnal annotated code-mixed parse tree for tree in Figure 1e. 2.4. Annotating CM Parse Trees We obtain the code-mixed trees directly each time we make substitutions in the monolingual trees. However, these simple trees do not"
2020.calcs-1.8,N16-1024,0,0.0313614,"builds up a tree for a sentence using a dynamic programming algorithm. There are variations of this algorithm that work with a Probabilistic CFG as well (Booth and Thompson, 1973). Most of the early neural network parsers were simple encoder-decoder approaches where the sentence would be taken in by the encoder and the decoder would have to output the tree with no extra information being provided about a tree structure (Vinyals et al., 2015). These later evolved into methods where the decoder was constrained to output tokens that conformed to a valid tree structure (Ballesteros et al., 2015; Dyer et al., 2016). One negative aspect of these early neural methods is that they required extensive feature engineering to perform well (Thang et al., 2015). 4. Evaluation We created train, dev and test sets of synthetic trees from independent sets of parallel sentences, so there is no overlap in the trees between the 3 sets. Table 1 contains some statistics about the datasets. For both language pairs, we obtained parallel corpora by taking English sentences and running them through Bing Translator to obtain the parallel sentences. This allows us to perform this technique for languages that do not have large"
2020.calcs-1.8,P08-1109,0,0.0825489,"th the productions are the same and the RHS of both the pro58 Dataset En-Hi Train En-Hi Synth. Test En-Hi Real. Test En-Es Train En-Es Synth. Test ROOT S NP_h VP_e PRP_h MD_e हम will VP VB_e present DT_h NN_h N_e एक today पेपर Height 7.05 (1.96) 7.10 (2.02) 5.40 (1.06) 8.16 (2.31) 8.13 (2.24) RHS Length 2.22 (2.34) 2.21 (2.28) 3.38 (1.60) 1.75 (1.16) 1.73 (1.12) Table 1: Statistics about Train and Test Datasets. Mean and Standard deviation (in brackets) reported for tree height and length of right hand size of productions. NP_e NP Size 421710 2740 1381 421710 2542 the tree given these scores. Finkel et al. (2008) use Conditional Random Fields (CRFs) for the scoring purpose. More recently, there has been a line of work where neural networks have been used to score the spans, starting oﬀ with Durrett and Klein (2015) where a ﬁxed-window based method is used. Stern et al. (2017) build upon this work by using RNNs instead of a ﬁxed-window for the scoring and Kitaev and Klein (2018) use a transformer instead of the RNN. These methods achieve performance that is superior to the early neural network parsers without the complex feature engineering associated with most of them. Figure 2: The ﬁnal annotated cod"
2020.calcs-1.8,C82-1023,0,0.510695,"able to parse these as well. The rest of the paper is organized as follows. Section 2 talks about linguistic theories for code-mixing and how they can be used to obtain code-mixed parse trees. Section 3 talks about neural constituency parsers and the parsing technique chosen for our testing. Section 4 describes the evaluation method for the parser and results on synthetic data. Section 5 talks 2. 2.1. Obtaining Code-Mixed Parse Trees Background Researchers in linguistics have proposed multiple theories that aim to model code-mixing from a linguistics perspective. (Poplack, 1980; Sankoﬀ, 1998; Joshi, 1982; Milroy, 1995; Di Sciullo et al., 1986; Belazi et al., 1994). On the whole, these theories draw parallels between the parse trees of a pair of parallel sentences in 2 languages and model codemixing as the substitution of a subtree in one language with its equivalent in the other language, assuming that a set of conditions are satisﬁed. One of these theories is the Equivalence Constraint (EC) theory (Poplack, 1980; Sankoﬀ, 1998). The work of Bhat et al. (2016) and Pratapa et al. (2018) make use of the EC theory to generate synthetic code-mixed sentences given a pair of parallel sentences. They"
2020.calcs-1.8,P18-1249,0,0.0736465,"e 1: Statistics about Train and Test Datasets. Mean and Standard deviation (in brackets) reported for tree height and length of right hand size of productions. NP_e NP Size 421710 2740 1381 421710 2542 the tree given these scores. Finkel et al. (2008) use Conditional Random Fields (CRFs) for the scoring purpose. More recently, there has been a line of work where neural networks have been used to score the spans, starting oﬀ with Durrett and Klein (2015) where a ﬁxed-window based method is used. Stern et al. (2017) build upon this work by using RNNs instead of a ﬁxed-window for the scoring and Kitaev and Klein (2018) use a transformer instead of the RNN. These methods achieve performance that is superior to the early neural network parsers without the complex feature engineering associated with most of them. Figure 2: The ﬁnal annotated code-mixed parse tree for tree in Figure 1e. 2.4. Annotating CM Parse Trees We obtain the code-mixed trees directly each time we make substitutions in the monolingual trees. However, these simple trees do not capture the information provided by EC theory used in generating the tree. To address this, we annotate each non-terminal in the tree with a tag. This tag is determin"
2020.calcs-1.8,P03-1054,0,0.128692,"-synthetic sentences. 2.2. Generating Synthetic Code-Mixed Sentences The method for generating code-mixed sentences in Bhat et al. (2016) ﬁrst obtains equivalent parse trees of the parallel sentences in their respective languages. The method is brieﬂy described here. The paper can be referred to for a detailed explanation. Assuming that L1 and L2 are the 2 languages, 1. Obtain a sentence in L1, its equivalent in L2 and a word level alignment between the two sentences 2. Obtain the parse tree of either of the sentences. If L1, is English, this can be done using a tool like the Stanford Parser (Klein and Manning, 2003) 3. Once the L1 tree is obtained, use the word level alignments to project the L2 words onto the L1 tree in a bottom-up manner. In this manner, the L2 tree is obtained Figure 1a and 1b show monolingual trees for L1 and L2. Having these equivalent trees, EC theory allows the substitution of any number of words in the L1 tree with their equivalents in L2 tree as long as set of constraints are satisﬁed. Author can be contacted at anirudhsriniv@gmail.com 57 ROOT ROOT S S NP NP VP PRP MD we will PRP VP NP VB present DT a NN हम NP NP NN NN VP MD NP VB करेंगे DT (b) Hindi tree ROOT ROOT ROOT S S S NP"
2020.calcs-1.8,W18-3219,0,0.0156237,"mixing 1. Introduction about further evaluating the parser using non-synthetic data and Section 6 concludes our discussion. Code-mixing is a phenomenon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid or not. Parse trees can also be used to build a probabilistic context free grammar (PCFG) that would help us understand the usage of diﬀerent grammatical elements"
2020.calcs-1.8,D15-1041,0,0.016616,"set of CFG productions and builds up a tree for a sentence using a dynamic programming algorithm. There are variations of this algorithm that work with a Probabilistic CFG as well (Booth and Thompson, 1973). Most of the early neural network parsers were simple encoder-decoder approaches where the sentence would be taken in by the encoder and the decoder would have to output the tree with no extra information being provided about a tree structure (Vinyals et al., 2015). These later evolved into methods where the decoder was constrained to output tokens that conformed to a valid tree structure (Ballesteros et al., 2015; Dyer et al., 2016). One negative aspect of these early neural methods is that they required extensive feature engineering to perform well (Thang et al., 2015). 4. Evaluation We created train, dev and test sets of synthetic trees from independent sets of parallel sentences, so there is no overlap in the trees between the 3 sets. Table 1 contains some statistics about the datasets. For both language pairs, we obtained parallel corpora by taking English sentences and running them through Bing Translator to obtain the parallel sentences. This allows us to perform this technique for languages tha"
2020.calcs-1.8,N18-1090,0,0.0539806,"menon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid or not. Parse trees can also be used to build a probabilistic context free grammar (PCFG) that would help us understand the usage of diﬀerent grammatical elements. The work in this paper makes 3 contributions to the area of code-mixed parsing. Firstly, we propose a technique that modiﬁes existing linguis"
2020.calcs-1.8,W18-0201,0,0.0158356,"Code-mixing is a phenomenon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid or not. Parse trees can also be used to build a probabilistic context free grammar (PCFG) that would help us understand the usage of diﬀerent grammatical elements. The work in this paper makes 3 contributions to the area of code-mixed parsing. Firstly, we propose a technique that modi"
2020.calcs-1.8,P18-1143,1,0.84214,"proposed multiple theories that aim to model code-mixing from a linguistics perspective. (Poplack, 1980; Sankoﬀ, 1998; Joshi, 1982; Milroy, 1995; Di Sciullo et al., 1986; Belazi et al., 1994). On the whole, these theories draw parallels between the parse trees of a pair of parallel sentences in 2 languages and model codemixing as the substitution of a subtree in one language with its equivalent in the other language, assuming that a set of conditions are satisﬁed. One of these theories is the Equivalence Constraint (EC) theory (Poplack, 1980; Sankoﬀ, 1998). The work of Bhat et al. (2016) and Pratapa et al. (2018) make use of the EC theory to generate synthetic code-mixed sentences given a pair of parallel sentences. They show that using these generated sentences in language modeling showed an improvement in perplexity on a test set of non-synthetic sentences. 2.2. Generating Synthetic Code-Mixed Sentences The method for generating code-mixed sentences in Bhat et al. (2016) ﬁrst obtains equivalent parse trees of the parallel sentences in their respective languages. The method is brieﬂy described here. The paper can be referred to for a detailed explanation. Assuming that L1 and L2 are the 2 languages,"
2020.calcs-1.8,P17-1180,1,0.80294,"y trees” and ﬁnd that a parser trained on synthetically generated trees is able to decently parse these as well. Keywords: Parse Trees, Constituency Parsing, Code-mixing 1. Introduction about further evaluating the parser using non-synthetic data and Section 6 concludes our discussion. Code-mixing is a phenomenon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid"
2020.calcs-1.8,P98-1002,0,0.230208,"the parser is able to parse these as well. The rest of the paper is organized as follows. Section 2 talks about linguistic theories for code-mixing and how they can be used to obtain code-mixed parse trees. Section 3 talks about neural constituency parsers and the parsing technique chosen for our testing. Section 4 describes the evaluation method for the parser and results on synthetic data. Section 5 talks 2. 2.1. Obtaining Code-Mixed Parse Trees Background Researchers in linguistics have proposed multiple theories that aim to model code-mixing from a linguistics perspective. (Poplack, 1980; Sankoﬀ, 1998; Joshi, 1982; Milroy, 1995; Di Sciullo et al., 1986; Belazi et al., 1994). On the whole, these theories draw parallels between the parse trees of a pair of parallel sentences in 2 languages and model codemixing as the substitution of a subtree in one language with its equivalent in the other language, assuming that a set of conditions are satisﬁed. One of these theories is the Equivalence Constraint (EC) theory (Poplack, 1980; Sankoﬀ, 1998). The work of Bhat et al. (2016) and Pratapa et al. (2018) make use of the EC theory to generate synthetic code-mixed sentences given a pair of parallel se"
2020.calcs-1.8,W14-3907,0,0.014535,"Keywords: Parse Trees, Constituency Parsing, Code-mixing 1. Introduction about further evaluating the parser using non-synthetic data and Section 6 concludes our discussion. Code-mixing is a phenomenon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid or not. Parse trees can also be used to build a probabilistic context free grammar (PCFG) that would help us u"
2020.calcs-1.8,P17-1076,0,0.0128885,"8.16 (2.31) 8.13 (2.24) RHS Length 2.22 (2.34) 2.21 (2.28) 3.38 (1.60) 1.75 (1.16) 1.73 (1.12) Table 1: Statistics about Train and Test Datasets. Mean and Standard deviation (in brackets) reported for tree height and length of right hand size of productions. NP_e NP Size 421710 2740 1381 421710 2542 the tree given these scores. Finkel et al. (2008) use Conditional Random Fields (CRFs) for the scoring purpose. More recently, there has been a line of work where neural networks have been used to score the spans, starting oﬀ with Durrett and Klein (2015) where a ﬁxed-window based method is used. Stern et al. (2017) build upon this work by using RNNs instead of a ﬁxed-window for the scoring and Kitaev and Klein (2018) use a transformer instead of the RNN. These methods achieve performance that is superior to the early neural network parsers without the complex feature engineering associated with most of them. Figure 2: The ﬁnal annotated code-mixed parse tree for tree in Figure 1e. 2.4. Annotating CM Parse Trees We obtain the code-mixed trees directly each time we make substitutions in the monolingual trees. However, these simple trees do not capture the information provided by EC theory used in generati"
2020.calcs-1.8,P15-1148,0,0.015473,"ic CFG as well (Booth and Thompson, 1973). Most of the early neural network parsers were simple encoder-decoder approaches where the sentence would be taken in by the encoder and the decoder would have to output the tree with no extra information being provided about a tree structure (Vinyals et al., 2015). These later evolved into methods where the decoder was constrained to output tokens that conformed to a valid tree structure (Ballesteros et al., 2015; Dyer et al., 2016). One negative aspect of these early neural methods is that they required extensive feature engineering to perform well (Thang et al., 2015). 4. Evaluation We created train, dev and test sets of synthetic trees from independent sets of parallel sentences, so there is no overlap in the trees between the 3 sets. Table 1 contains some statistics about the datasets. For both language pairs, we obtained parallel corpora by taking English sentences and running them through Bing Translator to obtain the parallel sentences. This allows us to perform this technique for languages that do not have large parallel corpora available for them. Since we are using an MT system, we end up with parallel sentences that are less likely to be semantic"
2020.calcs-1.8,D14-1105,1,0.806658,"aluating the parser using non-synthetic data and Section 6 concludes our discussion. Code-mixing is a phenomenon observed in multilingual societies all throughout the world. Although it started oﬀ as mainly a spoken phenomenon, the need for computational methods for processing code-mixed text is ever growing as people are now code-mixing on social media and other online platforms more and more (Rijhwani et al., 2017). Most work focusing on computational methods for codemixing have been on tasks like LID (Solorio et al., 2014; Sequiera et al., 2015), NER (Aguilar et al., 2018) and POS tagging (Vyas et al., 2014). Although there are some works on code-mixed dependency parsing (Partanen et al., 2018; Bhat et al., 2018), there is no work that has focused on obtaining parse trees and the task of constituency parsing. Having parse trees and a constituency parser for any language is extremely useful. It can be used for understanding the syntax of a sentence and checking whether a sentence is grammatically valid or not. Parse trees can also be used to build a probabilistic context free grammar (PCFG) that would help us understand the usage of diﬀerent grammatical elements. The work in this paper makes 3 con"
2020.calcs-1.8,C10-2148,0,0.0266807,"tituency trees and evaluate the parser with them. 60 D D root nmod case YouTube PROPN case में ADP speed NOUN nmod वाला ADP nsubj thing NOUN is AUX PROPN ADP YouTube में D AUX ADJ is useful cop useful ADJ (a) A dependency parse tree D NOUN NOUN ADP thing speed वाला (b) The converted constituency parse tree Figure 3: A dependency parse tree and the constituency tree obtained by the conversion technique used. Model 5.2. Converting Dependency Trees to Constituency Trees Word mBERT Although there has been a lot of recent work on converting dependency trees to constituency trees (Xia et al., 2008; Wang and Zong, 2010; Lee and Wang, 2016), most of these works require having the golden constituency tree for a dependency tree and train a machine learning based algorithm on such a golden tree set to learn the conversion. Since these resources are something not available for our case, we focus on the works of Collins et al. (1999) and Xia and Palmer (2001). These works propose an algorithm that will convert a dependency tree to a constituency tree in a deterministic fashion, outputting the structure alone of the constituency tree. This algorithm does not assign labels to intermediate nodes in the tree, a step"
2020.calcs-1.8,H01-1014,0,0.437685,"ee and the constituency tree obtained by the conversion technique used. Model 5.2. Converting Dependency Trees to Constituency Trees Word mBERT Although there has been a lot of recent work on converting dependency trees to constituency trees (Xia et al., 2008; Wang and Zong, 2010; Lee and Wang, 2016), most of these works require having the golden constituency tree for a dependency tree and train a machine learning based algorithm on such a golden tree set to learn the conversion. Since these resources are something not available for our case, we focus on the works of Collins et al. (1999) and Xia and Palmer (2001). These works propose an algorithm that will convert a dependency tree to a constituency tree in a deterministic fashion, outputting the structure alone of the constituency tree. This algorithm does not assign labels to intermediate nodes in the tree, a step that the aforementioned machine learning based algorithms try to achieve using labelled data. Real Test F1* POS 30.32 40.25 30.60 41.31 Synth. Test F1* POS 46.93 96.07 49.98 99.18 Table 4: *Unlabelled F1 and POS accuracies on the HindiEnglish set of converted dependency trees. belled F1 score for our Synth. Test set and report it and the P"
2020.conll-1.4,2020.acl-main.463,0,0.0131124,"e Road, Vigyan, Bengaluru, India 2 Google Research, Carina East Tower Bagmane Constellation Business Park, Bengaluru, India 3 University of Richmond, 410 Westhampton Way, Richmond, VA, USA pratikmjoshi123@gmail.com, aalok.sathe@richmond.edu, {t-soadit,monojitc}@microsoft.com 2019) to achieve state-of-the-art (SOTA) performance in these tasks. Despite the wide adoption of NLI datasets, a growing concern in the community has been the lack of clarity as to which linguistic or reasoning concepts these trained NLI systems are truly able to learn and generalize (see, for example (Linzen, 2020) and (Bender and Koller, 2020), for a discussion). Over the years, as models have shown steady performance increases in NLI tasks, many authors (Nie et al., 2019; Kaushik et al., 2019) demonstrate steep drops in performance when these models are tested against adversarially (or counterfactually) created examples by non-experts. Richardson et al. (2019) use templated examples to show trained NLI systems fail to capture essential logical (negation, boolean, quantifier) and semantic (monotonicity) phenomena. Herein lie the central questions of our work: 1) what is the distribution of various categories of reasoning tasks in t"
2020.conll-1.4,2020.acl-main.465,0,0.170152,"rch India, 9 Lavelle Road, Vigyan, Bengaluru, India 2 Google Research, Carina East Tower Bagmane Constellation Business Park, Bengaluru, India 3 University of Richmond, 410 Westhampton Way, Richmond, VA, USA pratikmjoshi123@gmail.com, aalok.sathe@richmond.edu, {t-soadit,monojitc}@microsoft.com 2019) to achieve state-of-the-art (SOTA) performance in these tasks. Despite the wide adoption of NLI datasets, a growing concern in the community has been the lack of clarity as to which linguistic or reasoning concepts these trained NLI systems are truly able to learn and generalize (see, for example (Linzen, 2020) and (Bender and Koller, 2020), for a discussion). Over the years, as models have shown steady performance increases in NLI tasks, many authors (Nie et al., 2019; Kaushik et al., 2019) demonstrate steep drops in performance when these models are tested against adversarially (or counterfactually) created examples by non-experts. Richardson et al. (2019) use templated examples to show trained NLI systems fail to capture essential logical (negation, boolean, quantifier) and semantic (monotonicity) phenomena. Herein lie the central questions of our work: 1) what is the distribution of various cate"
2020.conll-1.4,D15-1075,0,0.0567035,"serve that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies—a large jump over the previous models—some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories. 1 Introduction The Natural Language Inference (NLI) task tests whether a hypothesis (H) in text contradicts with, is entailed by, or is neutral with respect to a given premise (P) text. This 3-way classification task, popularized by Bowman et al. (2015), which was in turn inspired by Dagan et al. (2005), now serves as a benchmark for evaluation of natural language understanding (NLU) capability of models; for example, NLI datasets (Bowman et al., 2015; Williams et al., 2018) are included in all NLU benchmarks such as GLUE and SuperGLUE (Wang et al., 2018). These corpora, in turn, have been successfully used to train models such as BERT (Devlin et al., † denotes equal contribution. ∗ Work was done while Authors were at Microsoft Research India. 41 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 41–55 c Onl"
2020.conll-1.4,N19-1112,0,0.111634,"e premise. An example is “P: Another influence was that patrician politician Franklin Roosevelt, who was, like John D. Rockefeller, the focus of Nelson’s relentless sycophancy and black-belt bureaucratic infighting. H: Nelson targeted Roosevelt in order to gain political favor.”. Figure 4: The correlation matrix between the taxonomic categories. 4 (Re)Evaluation of SOTA Models We re-evaluate two Transformer-based and two standard baseline machine learning models on TAXI NLI, under the lens of the taxonomic categories. As baselines, we choose BERT-base (Devlin et al., 2019), and RoBERTa-large (Liu et al., 2019b) as two state-of-the-art NLI systems. For our experiments, we use the pre-trained BERTbase and RoBERTa models from HuggingFace’s Transformers implementation (Wolf et al., 2019). As pre-Transformer baselines, we use the bidirectional LSTM-based Enhanced Sequential Inference model (ESIM) (Chen et al., 2017). We also train a Naive Bayes (NB) model using bag-of-words features for the P-H pairs after removing stop words4 . Categorical Correlations Fig. 4 shows correlations among categories in our dataset. We observe that most categories show weak correlation in the MNLI dataset, hinting at a poss"
2020.conll-1.4,P17-1152,0,0.0286594,"matrix between the taxonomic categories. 4 (Re)Evaluation of SOTA Models We re-evaluate two Transformer-based and two standard baseline machine learning models on TAXI NLI, under the lens of the taxonomic categories. As baselines, we choose BERT-base (Devlin et al., 2019), and RoBERTa-large (Liu et al., 2019b) as two state-of-the-art NLI systems. For our experiments, we use the pre-trained BERTbase and RoBERTa models from HuggingFace’s Transformers implementation (Wolf et al., 2019). As pre-Transformer baselines, we use the bidirectional LSTM-based Enhanced Sequential Inference model (ESIM) (Chen et al., 2017). We also train a Naive Bayes (NB) model using bag-of-words features for the P-H pairs after removing stop words4 . Categorical Correlations Fig. 4 shows correlations among categories in our dataset. We observe that most categories show weak correlation in the MNLI dataset, hinting at a possible independence of categories with respect to each other. Relatively stronger positive correlations are seen between boolean-quantifier, and boolean-comparative categories. We specifically looked at the genre-wise split of datapoints containing boolean-quantifier and saw that 4.1 TAXI NLI Error Analysis W"
2020.conll-1.4,W18-5441,0,0.165199,"Missing"
2020.conll-1.4,2020.acl-main.442,0,0.0239353,"o serve as a set of necessary inferencing capabilities that one would expect a competing NLI system to possess; thereby promoting more probing tasks along unexplored categories. Existing categorization efforts have centred around informing feature creation in the preTransformer era, and model-specific error analysis in more recent times. Previously, (LoBue and Yates, 2011) enumerated the type of commonsense knowledge required for NLI. Among recent error analysis efforts, the GLUE diagnostic dataset (Wang et al., 2018), inference types for Adversarial NLI (Nie et al., 2019), the new CheckList (Ribeiro et al., 2020) system and the Stress Tests (Naik et al., 2018) are mentionworthy. As we attempted to group the categorizations in Nie et al. (2019) and Wang et al. (2018) into four high-level categories (lexical, syntactic, semantic, and pragmatic)2 , we observe that there is a lack of consensus, nonuniformity and repetitiveness of these categories. For example, the Tricky label in Nie et al. (2019) groups examples that involve “wordplay, linguistic strategies such as syntactic transformations, or inferring writer intentions from contexts”; thereby spanning aspects of syntax and pragmatics. Similarly, Refer"
2020.conll-1.4,N19-1131,0,0.0527142,"Missing"
2020.lrec-1.343,W10-0710,0,0.022466,"rom our participants. We conduct qualitycheck experiments and error analysis of the data collected, and discuss how such a platform might be realized as a source of supplementary income generation for workers in rural India. Our results support the viability of crowdsourcing speech data collection to poor areas in rural and urban India. All collected data is released to the public, free of cost, to motivate future research. 2. Crowdsourcing for Data Collection There have been a number of prior efforts that have looked at crowdsourcing as a way to collect speech data for lowresource languages. Ambati and Vogel (2010) probed Amazon Mechanical Turk workers to see if they are capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found. Novotney and Callison-Burch (2010) showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost (Callison-Burch and Dredze, 2010). Even though Mechanical Turk has a high representation of workers from India (and thus, could be used for speech data collection for local Indian la"
2020.lrec-1.343,W10-0701,0,0.0280674,"ng for Data Collection There have been a number of prior efforts that have looked at crowdsourcing as a way to collect speech data for lowresource languages. Ambati and Vogel (2010) probed Amazon Mechanical Turk workers to see if they are capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found. Novotney and Callison-Burch (2010) showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost (Callison-Burch and Dredze, 2010). Even though Mechanical Turk has a high representation of workers from India (and thus, could be used for speech data collection for local Indian languages), most Mechanical Turk workers in India are urban, highly educated, speak fluent English and have access to a computer and highspeed Internet (Khanna et al., 2010). Datasets, collected using Mechanical Turk, would not be very diverse. We strongly believe that traditionally underrepresented rural Indians also have much to gain, and much to contribute, as participants in crowdsourcing platforms. Takamichi and Saruwatari (2018) used web-based"
2020.lrec-1.343,N10-1024,0,0.0413425,". Our results support the viability of crowdsourcing speech data collection to poor areas in rural and urban India. All collected data is released to the public, free of cost, to motivate future research. 2. Crowdsourcing for Data Collection There have been a number of prior efforts that have looked at crowdsourcing as a way to collect speech data for lowresource languages. Ambati and Vogel (2010) probed Amazon Mechanical Turk workers to see if they are capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found. Novotney and Callison-Burch (2010) showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost (Callison-Burch and Dredze, 2010). Even though Mechanical Turk has a high representation of workers from India (and thus, could be used for speech data collection for local Indian languages), most Mechanical Turk workers in India are urban, highly educated, speak fluent English and have access to a computer and highspeed Internet (Khanna et al., 2010). Datasets, collected using Mechanical Turk, would not be very"
2020.lrec-1.343,L18-1067,0,0.0190669,"y lower cost (Callison-Burch and Dredze, 2010). Even though Mechanical Turk has a high representation of workers from India (and thus, could be used for speech data collection for local Indian languages), most Mechanical Turk workers in India are urban, highly educated, speak fluent English and have access to a computer and highspeed Internet (Khanna et al., 2010). Datasets, collected using Mechanical Turk, would not be very diverse. We strongly believe that traditionally underrepresented rural Indians also have much to gain, and much to contribute, as participants in crowdsourcing platforms. Takamichi and Saruwatari (2018) used web-based recording and crowdsourcing platforms to construct a parallel speech corpus of Japanese dialects. Hughes et al. (2010) suggest creating an Android application to help build transcribed speech corpora quickly and cheaply for many languages. They mainly engage with university students, and mention that university students require less technical support. BSpeak is an accessible crowdsourcing marketplace that enables visually challenged people in an urban setting to earn money by transcribing audio files in English through speech (Vashistha et al., 2018). To the best of our knowled"
2021.adaptnlp-1.12,2020.lrec-1.223,0,0.269601,"(b) shows that CM sentence representations align better in mBERT fine-tuned on any CM data regardless of the language of mixing. Introduction Massive multilingual models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have recently become very popular as they cover over 100 languages and are capable of zero-shot transfer of performance in downstream tasks across languages. As these models serve as good multilingual representations of sentences (Pires et al., 2019), there have been attempts at using these representations for encoding code-mixed sentences (Srinivasan, 2020; Aguilar et al., 2020; Khanuja et al., 2020). Code-Mixing (CM) is the mixing of words belonging two or more languages within a ∗ (b) mBERT fine-tuned on En-Hi CM The word BERTologiCoMix is a portmanteau of BERTology and Code-Mixing, and is inspired from the title of the graphic novel: Logicomix: An Epic Search for Truth by Apostolos Doxiadis and Christos Papadimitriou (2009). † The authors contributed equally to the work. 1 In this paper, unless specifically stated, finetuning refers to MLM finetuning/continued pretraining and not downstream task finetuning single sentence and is a commonly observed phenomenon in"
2021.adaptnlp-1.12,W19-1909,0,0.0320727,"Missing"
2021.adaptnlp-1.12,D19-1371,0,0.0215077,"ture directions. 2 2.1 Related Work Domain Adaptation of BERT Pre-trained Language Models trained on generic data such as BERT and RoBERTa are often adapted to the domain where it is required to be used. Domain adaptation benefits BERT in two ways (i) it gives exposure to text in the domain specific contexts and (ii) adds domain specific terms to the vocabulary. BERT has been adapted to several domains especially once which have its own complex jargon of communication such as the biomedical domain (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019), scientific texts or publications (Beltagy et al., 2019), legal domain (Chalkidis et al., 2020) and financial document processing (Yang et al., 2020b). Most of these works employ sophisticated techniques for mining large quantities of domain specific text from the internet and thus prefer to train the BERT model from scratch rather than fine-tuning the available BERT checkpoints. This is because they don’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives"
2021.adaptnlp-1.12,W18-3211,0,0.021312,"ware of certain nuances of realworld code-mixing which are still not completely known. 3.2 Training Procedure There are 3 [types] × 2 [language-pairs] = 6 combinations of data which can be obtained based on the previous specifications. For l-CM and g-CM, we use the same set of parallel sentences: en-es from Rijhwani et al. (2017) and en-hi from Kunchukuttan et al. (2018). As CM is prominently used in informal contexts, it is difficult to procure textual r-CM data. We use twitter data from Rijhwani et al. (2017) for en-es; for en-hi, we use data from online forums and Twitter respectively from Chandu et al. (2018) and Patro et al. (2017). For each of the 6 combinations, we randomly sample 100,000 sentences which is then used to further train mBERT with the masked language modelling objective. We use layer-wise scaled learning rate while finetuning the models. Sun et al. (2019) Model Notation: Let mhi be the vanilla mBERT, then mhp,qi are the mBERTs further trained on hp, qi data, where p ∈ {l, g, r} is the complexity of mixing and q ∈ {enes, enhi} is the language of mixing. For example, a model trained on EnglishHindi lexical code-mixed data will be represented as mhl,enhii . means that the model used"
2021.adaptnlp-1.12,W19-4828,0,0.0900156,"ll as to develop CM embeddings as a better alternative to standard cross-lingual embeddings for CM tasks (Pratapa et al., 2018b). In this work, we use grammatical theories to generate synthetic CM data from parallel sentences analogous to the aforementioned techniques. 2.3 BERT Attention based probing Given the complex black-box nature of the BERT model, there have been a large number of works that propose experiments to probe and understand the working of different components of the BERT model. A large portion of these methods have focused on the attention mechanism of the transformer model. Clark et al. (2019); Htut et al. (2019) find that certain attention heads encode linguistic dependencies between words of the sentence. Kovaleva et al. (2019) report on the patterns in the attention heads of BERT and find that a large number of heads just attend to the [CLS] or [SEP] tokens and do not encode any relation between the words of the sentence. Michel et al. (2019); Prasanna et al. (2020) also show that many of BERT’s attention heads are redundant and pruning heads does not affect downstream task performance. In this paper, we borrow ideas from these works and propose a technique for visualizing the a"
2021.adaptnlp-1.12,2020.acl-main.747,0,0.0992378,"Missing"
2021.adaptnlp-1.12,N19-1423,0,0.0482449,"and that finetuning with any type of code-mixed text improves the responsivity of it’s attention heads to code-mixed text inputs. 1 (a) Vanilla mBERT (without fine-tuning) Figure 1: t-SNE representations of En-Es CM sentences on the respective models. Each color represents CM sentences with the same meaning but with different amounts of mixing generated based on the g-CM method in Sec 3.1. The tight clusters in (b) shows that CM sentence representations align better in mBERT fine-tuned on any CM data regardless of the language of mixing. Introduction Massive multilingual models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have recently become very popular as they cover over 100 languages and are capable of zero-shot transfer of performance in downstream tasks across languages. As these models serve as good multilingual representations of sentences (Pires et al., 2019), there have been attempts at using these representations for encoding code-mixed sentences (Srinivasan, 2020; Aguilar et al., 2020; Khanuja et al., 2020). Code-Mixing (CM) is the mixing of words belonging two or more languages within a ∗ (b) mBERT fine-tuned on En-Hi CM The word BERTologiCoMix is a portmanteau of B"
2021.adaptnlp-1.12,N13-1073,0,0.0155069,"ble shared vocabulary with English, Hindi has a different word order and no shared vocabulary by virtue of using a different script. Thus, investigating through these two diverse pairs is expected to help us understand the representational variance. The linguistic complexity of code-mixing can be categorized into the following three types: Lexical Code-Mixing (l-CM): The simplest form of code-mixing is to substitute lexical units within a monolingual sentence with its counterpart from the other language. This can be achieved by using parallel sentences, and aligning the words with an aligner (Dyer et al., 2013). Grammatical Code-Mixing (g-CM): There are grammatical constraints (Joshi, 1982; Poplack, 2000; Belazi et al., 1994) on word-order changes and lexical substitution during code-mixing that the l-CM does not take into account. Pratapa et al. (2018a) propose a technique to generate all grammatically valid CM sentences from a pair of parallel sentences. Here, we use this generated dataset as our g-CM2 . Parse trees are generated for parallel sentences (between two languages L1 and L2 ), and common nodes between these parse trees are then replaced based on certain conditions specified by Equivalen"
2021.adaptnlp-1.12,2020.acl-main.560,1,0.836868,"itten form of communication (Tay, 1989). However, they mostly occur in an informal setting and hence such CM data is not publicly available in large quantities. Such scarcity of data would mean that building independent CM models can be unfeasible. With the onset of pre-trained multilingual models, further training with CM data can help in adapting these models for CM processing. However, even for further training, there is a requirement for a significant amount of data albeit lesser than starting from scratch. The amount of data available even for their monolingual counterparts is very less (Joshi et al., 2020) let alone the amount of real-world CM data. This can prove to be a bottleneck. Rightly so, 111 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 111–121 April 20, 2021. ©2021 Association for Computational Linguistics there have been previous works exploring synthesis of CM data for the purpose of data augmentation (Bhat et al., 2016; Pratapa et al., 2018a). Synthesis of CM mostly rely on certain linguistic theories (Poplack, 2000) to construct grammatically plausible sentences. These works have shown that using the synthetic and real CM data in a curriculum setting while"
2021.adaptnlp-1.12,2020.acl-main.329,1,0.931857,"tence representations align better in mBERT fine-tuned on any CM data regardless of the language of mixing. Introduction Massive multilingual models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have recently become very popular as they cover over 100 languages and are capable of zero-shot transfer of performance in downstream tasks across languages. As these models serve as good multilingual representations of sentences (Pires et al., 2019), there have been attempts at using these representations for encoding code-mixed sentences (Srinivasan, 2020; Aguilar et al., 2020; Khanuja et al., 2020). Code-Mixing (CM) is the mixing of words belonging two or more languages within a ∗ (b) mBERT fine-tuned on En-Hi CM The word BERTologiCoMix is a portmanteau of BERTology and Code-Mixing, and is inspired from the title of the graphic novel: Logicomix: An Epic Search for Truth by Apostolos Doxiadis and Christos Papadimitriou (2009). † The authors contributed equally to the work. 1 In this paper, unless specifically stated, finetuning refers to MLM finetuning/continued pretraining and not downstream task finetuning single sentence and is a commonly observed phenomenon in societies with multiple"
2021.adaptnlp-1.12,D19-1445,0,0.301882,"s work, we use grammatical theories to generate synthetic CM data from parallel sentences analogous to the aforementioned techniques. 2.3 BERT Attention based probing Given the complex black-box nature of the BERT model, there have been a large number of works that propose experiments to probe and understand the working of different components of the BERT model. A large portion of these methods have focused on the attention mechanism of the transformer model. Clark et al. (2019); Htut et al. (2019) find that certain attention heads encode linguistic dependencies between words of the sentence. Kovaleva et al. (2019) report on the patterns in the attention heads of BERT and find that a large number of heads just attend to the [CLS] or [SEP] tokens and do not encode any relation between the words of the sentence. Michel et al. (2019); Prasanna et al. (2020) also show that many of BERT’s attention heads are redundant and pruning heads does not affect downstream task performance. In this paper, we borrow ideas from these works and propose a technique for visualizing the attention heads and how their behaviour changes during finetuning. Models In this section, we describe the mBERT models, the modifications w"
2021.adaptnlp-1.12,L18-1548,0,0.0266562,"cs space of language understanding. Though rCM is a subset of g-CM, there does not exist any method which can sample realistic CM from such synthetic data, hence we rely on real-world CM datasets. Fine-tuning with this form should let the model become aware of certain nuances of realworld code-mixing which are still not completely known. 3.2 Training Procedure There are 3 [types] × 2 [language-pairs] = 6 combinations of data which can be obtained based on the previous specifications. For l-CM and g-CM, we use the same set of parallel sentences: en-es from Rijhwani et al. (2017) and en-hi from Kunchukuttan et al. (2018). As CM is prominently used in informal contexts, it is difficult to procure textual r-CM data. We use twitter data from Rijhwani et al. (2017) for en-es; for en-hi, we use data from online forums and Twitter respectively from Chandu et al. (2018) and Patro et al. (2017). For each of the 6 combinations, we randomly sample 100,000 sentences which is then used to further train mBERT with the masked language modelling objective. We use layer-wise scaled learning rate while finetuning the models. Sun et al. (2019) Model Notation: Let mhi be the vanilla mBERT, then mhp,qi are the mBERTs further tra"
2021.adaptnlp-1.12,N19-1112,0,0.0201972,"h r-CM data helps with SENT, NER, LID enes as well as QA tasks. While for POS, the performance remains almost same regardless of which data the model is fine-tuned with. 4 These differences are also reflected in the layerwise performance of these models as shown in Figure 2. The tasks are considered solved at the knee point where the performances start plateauing. The performances of different models start at the same note, and after a certain point mhr, i diverges to plateau at a higher performance than others. This can be attributed to final layers adapting the most during MLM fine-tuning. (Liu et al., 2019; Kovaleva et al., 2019). LID gets solved around 2nd layer. enhi LID gives a relatively high performance at the 0th layer indicating that it only needs the token+positional embeddings. This is because enhi 3 LID task has en and hi words in different scripts, which means it can be solved even with a simple unicode classification rule. POS gets solved at around 4th layer. The indifference to fine-tuning observed in case of POS is reflected here as well, as all the models are performing equally at all the layers for both the languages. As we use just 100k sentences as opposed to 3M sentences, we"
2021.adaptnlp-1.12,D19-6109,0,0.0282711,"0b). Most of these works employ sophisticated techniques for mining large quantities of domain specific text from the internet and thus prefer to train the BERT model from scratch rather than fine-tuning the available BERT checkpoints. This is because they don’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives, some works have explored on different techniques to improve downstream task performance. Ma et al. (2019) uses curriculum learning and domain-discriminative data selection for domain adaptation. Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixin"
2021.adaptnlp-1.12,2020.acl-main.681,0,0.0229711,"BERT checkpoints. This is because they don’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives, some works have explored on different techniques to improve downstream task performance. Ma et al. (2019) uses curriculum learning and domain-discriminative data selection for domain adaptation. Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM"
2021.adaptnlp-1.12,D17-1240,1,0.777213,"f realworld code-mixing which are still not completely known. 3.2 Training Procedure There are 3 [types] × 2 [language-pairs] = 6 combinations of data which can be obtained based on the previous specifications. For l-CM and g-CM, we use the same set of parallel sentences: en-es from Rijhwani et al. (2017) and en-hi from Kunchukuttan et al. (2018). As CM is prominently used in informal contexts, it is difficult to procure textual r-CM data. We use twitter data from Rijhwani et al. (2017) for en-es; for en-hi, we use data from online forums and Twitter respectively from Chandu et al. (2018) and Patro et al. (2017). For each of the 6 combinations, we randomly sample 100,000 sentences which is then used to further train mBERT with the masked language modelling objective. We use layer-wise scaled learning rate while finetuning the models. Sun et al. (2019) Model Notation: Let mhi be the vanilla mBERT, then mhp,qi are the mBERTs further trained on hp, qi data, where p ∈ {l, g, r} is the complexity of mixing and q ∈ {enes, enhi} is the language of mixing. For example, a model trained on EnglishHindi lexical code-mixed data will be represented as mhl,enhii . means that the model used depends on the configura"
2021.adaptnlp-1.12,W19-5006,0,0.023234,"dings. Section 6 concludes the paper by summarizing the work and laying out future directions. 2 2.1 Related Work Domain Adaptation of BERT Pre-trained Language Models trained on generic data such as BERT and RoBERTa are often adapted to the domain where it is required to be used. Domain adaptation benefits BERT in two ways (i) it gives exposure to text in the domain specific contexts and (ii) adds domain specific terms to the vocabulary. BERT has been adapted to several domains especially once which have its own complex jargon of communication such as the biomedical domain (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019), scientific texts or publications (Beltagy et al., 2019), legal domain (Chalkidis et al., 2020) and financial document processing (Yang et al., 2020b). Most of these works employ sophisticated techniques for mining large quantities of domain specific text from the internet and thus prefer to train the BERT model from scratch rather than fine-tuning the available BERT checkpoints. This is because they don’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have l"
2021.adaptnlp-1.12,P19-1493,0,0.0214194,"es with the same meaning but with different amounts of mixing generated based on the g-CM method in Sec 3.1. The tight clusters in (b) shows that CM sentence representations align better in mBERT fine-tuned on any CM data regardless of the language of mixing. Introduction Massive multilingual models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have recently become very popular as they cover over 100 languages and are capable of zero-shot transfer of performance in downstream tasks across languages. As these models serve as good multilingual representations of sentences (Pires et al., 2019), there have been attempts at using these representations for encoding code-mixed sentences (Srinivasan, 2020; Aguilar et al., 2020; Khanuja et al., 2020). Code-Mixing (CM) is the mixing of words belonging two or more languages within a ∗ (b) mBERT fine-tuned on En-Hi CM The word BERTologiCoMix is a portmanteau of BERTology and Code-Mixing, and is inspired from the title of the graphic novel: Logicomix: An Epic Search for Truth by Apostolos Doxiadis and Christos Papadimitriou (2009). † The authors contributed equally to the work. 1 In this paper, unless specifically stated, finetuning refers t"
2021.adaptnlp-1.12,2020.emnlp-main.259,0,0.0178725,"number of works that propose experiments to probe and understand the working of different components of the BERT model. A large portion of these methods have focused on the attention mechanism of the transformer model. Clark et al. (2019); Htut et al. (2019) find that certain attention heads encode linguistic dependencies between words of the sentence. Kovaleva et al. (2019) report on the patterns in the attention heads of BERT and find that a large number of heads just attend to the [CLS] or [SEP] tokens and do not encode any relation between the words of the sentence. Michel et al. (2019); Prasanna et al. (2020) also show that many of BERT’s attention heads are redundant and pruning heads does not affect downstream task performance. In this paper, we borrow ideas from these works and propose a technique for visualizing the attention heads and how their behaviour changes during finetuning. Models In this section, we describe the mBERT models, the modifications we make to them, and the types of CM data that we use for training. 3.1 Types of Code-Mixing For the purpose of this study, we characterize CM data across two dimensions: linguistic complexity and languages involved. Here, we experiment with CM"
2021.adaptnlp-1.12,P18-1143,1,0.856891,"sing. However, even for further training, there is a requirement for a significant amount of data albeit lesser than starting from scratch. The amount of data available even for their monolingual counterparts is very less (Joshi et al., 2020) let alone the amount of real-world CM data. This can prove to be a bottleneck. Rightly so, 111 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 111–121 April 20, 2021. ©2021 Association for Computational Linguistics there have been previous works exploring synthesis of CM data for the purpose of data augmentation (Bhat et al., 2016; Pratapa et al., 2018a). Synthesis of CM mostly rely on certain linguistic theories (Poplack, 2000) to construct grammatically plausible sentences. These works have shown that using the synthetic and real CM data in a curriculum setting while fine-tuning can help with achieving better performances on the downstream CM tasks. Though this is analogous to adapting models to new domains, CM differs in that the adaptation is not purely at vocabulary or style level but rather at a grammatical level. Although it is known such adaptation techniques can bring an improvement, it is not well understood how exactly fine-tunin"
2021.adaptnlp-1.12,D18-1344,1,0.85173,"sing. However, even for further training, there is a requirement for a significant amount of data albeit lesser than starting from scratch. The amount of data available even for their monolingual counterparts is very less (Joshi et al., 2020) let alone the amount of real-world CM data. This can prove to be a bottleneck. Rightly so, 111 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 111–121 April 20, 2021. ©2021 Association for Computational Linguistics there have been previous works exploring synthesis of CM data for the purpose of data augmentation (Bhat et al., 2016; Pratapa et al., 2018a). Synthesis of CM mostly rely on certain linguistic theories (Poplack, 2000) to construct grammatically plausible sentences. These works have shown that using the synthetic and real CM data in a curriculum setting while fine-tuning can help with achieving better performances on the downstream CM tasks. Though this is analogous to adapting models to new domains, CM differs in that the adaptation is not purely at vocabulary or style level but rather at a grammatical level. Although it is known such adaptation techniques can bring an improvement, it is not well understood how exactly fine-tunin"
2021.adaptnlp-1.12,P17-1180,1,0.925315,"Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. (2016); Pratapa et al. (2018a) demonstrate a technique to generate code-mixed sentences using parallel sentences and show that using these synthetic sentences can improve language model perplexity. A similar method is also proposed by Samanta et al. (2019) which uses parse trees to generate synthetic sentences. Yang et al. (2020a) generates CM sentences by using phrase tables to align and mix parts of a parallel sentence. Winata et al. (2019) proposes a technique to g"
2021.adaptnlp-1.12,P19-1343,0,0.0189254,"are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. (2016); Pratapa et al. (2018a) demonstrate a technique to generate code-mixed sentences using parallel sentences and show that using these synthetic sentences can improve language model perplexity. A similar method is also proposed by Samanta et al. (2019) which uses parse trees to generate synthetic sentences. Yang et al. (2020a) generates CM sentences by using phrase tables to align and mix parts of a parallel sentence. Winata et al. (2019) proposes a technique to generate codemixed sentences using pointer generator networks. The efficacy of synthetic CM data is evident from these works where they have been used in a curriculum setting for CM language modelling (Pratapa et al., 2018a), cross-lingual training of multilingual transformer models (Yang et al., 2020a) as well as to develop CM embeddings as a better alternative to standard cross-li"
2021.adaptnlp-1.12,P98-1002,0,0.0724762,"e grammatical constraints (Joshi, 1982; Poplack, 2000; Belazi et al., 1994) on word-order changes and lexical substitution during code-mixing that the l-CM does not take into account. Pratapa et al. (2018a) propose a technique to generate all grammatically valid CM sentences from a pair of parallel sentences. Here, we use this generated dataset as our g-CM2 . Parse trees are generated for parallel sentences (between two languages L1 and L2 ), and common nodes between these parse trees are then replaced based on certain conditions specified by Equivalence Constraint (EC) theory (Poplack, 2000; Sankoff, 1998), thereby producing a grammatically sound code-mixing. Fine-tuning with this form of CM should ideally impart the knowledge of grammatical boundaries for CM and would let us know whether a grammatically correct CM sentence is required to improve the performance. 113 Real Code-Mixing (r-CM): While g-CM considers purely the syntactic structure of CM, real-world 2 It is important to note that Pratapa et al. (2018a) uses GCM to denote “Generated CM” data, and not for “grammatical” as is used here. SENT model mhi enes enhi NER enes POS ehi enes enhi LID enhi enes enhi QA NLI enhi enhi 67.81±2.5 58."
2021.adaptnlp-1.12,2020.semeval-1.122,1,0.767101,"tight clusters in (b) shows that CM sentence representations align better in mBERT fine-tuned on any CM data regardless of the language of mixing. Introduction Massive multilingual models such as mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have recently become very popular as they cover over 100 languages and are capable of zero-shot transfer of performance in downstream tasks across languages. As these models serve as good multilingual representations of sentences (Pires et al., 2019), there have been attempts at using these representations for encoding code-mixed sentences (Srinivasan, 2020; Aguilar et al., 2020; Khanuja et al., 2020). Code-Mixing (CM) is the mixing of words belonging two or more languages within a ∗ (b) mBERT fine-tuned on En-Hi CM The word BERTologiCoMix is a portmanteau of BERTology and Code-Mixing, and is inspired from the title of the graphic novel: Logicomix: An Epic Search for Truth by Apostolos Doxiadis and Christos Papadimitriou (2009). † The authors contributed equally to the work. 1 In this paper, unless specifically stated, finetuning refers to MLM finetuning/continued pretraining and not downstream task finetuning single sentence and is a commonly o"
2021.adaptnlp-1.12,P19-1452,0,0.0302091,"t Analysis (SENT) for both enes and enhi, and Question Answering (QA) and Natural Language Inference (NLI) for only enhi. 114 4.2 Method We first measure the performance of these models on the aforementioned tasks. For each task, we fine-tune the models further after attaching a task specific classification layer. We report the average performances and standard deviations of each model run for 5 seeds in Table 1. 3 In addition to getting absolute performances, we want to get an insight of how much each layer of the different models contribute to the performance of a particular task. Following Tenney et al. (2019), we measure the solvability of a task by finding out the expected layer at which the model is able to correctly solve the task. Here the mBERT weights are kept frozen and a weighted sum of representations from each layer are passed to the task specific layer. Figure 2 shows the layer-wise F1 scores for the tasks for different models and language pairs. We additionally calculate scalar mixing weights which lets us know the contribution of each layer by calculating the attention paid to each layer for the task. 4.3 NER gets solved around the 5th layer. Here, rCM training seems to help for enhi,"
2021.adaptnlp-1.12,D19-1254,0,0.0214712,"s is because they don’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives, some works have explored on different techniques to improve downstream task performance. Ma et al. (2019) uses curriculum learning and domain-discriminative data selection for domain adaptation. Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. ("
2021.adaptnlp-1.12,K19-1026,0,0.0201247,"in in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. (2016); Pratapa et al. (2018a) demonstrate a technique to generate code-mixed sentences using parallel sentences and show that using these synthetic sentences can improve language model perplexity. A similar method is also proposed by Samanta et al. (2019) which uses parse trees to generate synthetic sentences. Yang et al. (2020a) generates CM sentences by using phrase tables to align and mix parts of a parallel sentence. Winata et al. (2019) proposes a technique to generate codemixed sentences using pointer generator networks. The efficacy of synthetic CM data is evident from these works where they have been used in a curriculum setting for CM language modelling (Pratapa et al., 2018a), cross-lingual training of multilingual transformer models (Yang et al., 2020a) as well as to develop CM embeddings as a better alternative to standard cross-lingual embeddings for CM tasks (Pratapa et al., 2018b). In this work, we use grammatical theories to generate synthetic CM data from parallel sentences analogous to the aforementioned techniq"
2021.adaptnlp-1.12,2020.emnlp-main.599,0,0.0295866,"ate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives, some works have explored on different techniques to improve downstream task performance. Ma et al. (2019) uses curriculum learning and domain-discriminative data selection for domain adaptation. Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. (2016); Pratapa et al. (2018a) demonstra"
2021.adaptnlp-1.12,2020.sustainlp-1.14,0,0.0193767,"on’t have to accommodate existing vocabulary along with the domain specific vocabulary which can lead to further fragmentation (Gu et al., 2020). While most works have looked at domain adaptation by plainly continuing the training using MLM objectives, some works have explored on different techniques to improve downstream task performance. Ma et al. (2019) uses curriculum learning and domain-discriminative data selection for domain adaptation. Adversarial techniques have been used for enforce domain-invariant learning and thus improve on generalization (Naik and Rose, 2020; Wang et al., 2019; Zhang et al., 2020). Ye et al. (2020) explores adapting BERT across languages. However, domain adaptation is not always effective and can lead to worse performances. This depends on several factors such as how different the domains are (Kashyap et al., 2020) or how much data is available (Zhang et al., 2020). 112 2.2 3 Code-Mixing Traditionally, Code-Mixing has been used in informal contexts and can be difficult to obtain in large quantities (Rijhwani et al., 2017). This scarcity of data has been previously tackled by generation of synthetic CM data to augment the real CM data. Bhat et al. (2016); Pratapa et al."
2021.eacl-demos.24,N19-1423,0,0.00866874,"omising direction. Various linguistic theories have been proposed that can determine how languages are mixed together, and in prior work we presented the first computational implementation (Bhat et al., 2016) of the Matrix-language (MyersScotton, 1993) and Equivalence Constraint theories (Poplack, 1980). We also showed that generating synthetic data using our computational implementations improved word embeddings leading to better downstream performance on sentiment analysis and POS tagging (Pratapa et al., 2018b), as well as RNN language models (Pratapa et al., 2018a). The multilingual BERT (Devlin et al., 2019) model finetuned with synthetic code-mixed data outperformed all prior techniques on the GLUECoS benchmark (Khanuja et al., 2020) for code-switching, which spans 11 NLP tasks in two language pairs. The approach of generating synthetic code-mixed data has gained traction following our work, with other approaches including using Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework with transfer learning (Gupta et al., 2020), using parallel data with a small amount of real code-mixed data to learn code-mixing patterns (Winata et al., 2019) and a novel two-level varia"
2021.eacl-demos.24,N13-1073,0,0.0657219,"e 4), which requires parallel sentences in the two languages being mixed as input data. Three maFigure 2: Parse-trees of (a) sentences [1E] and (b) [1H], and (c) of [1CM] according to the EC Theory Figure 3: Parse-trees of (a) sentences [2E] and (b) [1S], and (c) of [2CM] according to the EC Theory 206 2 Figure 4: The CM Generation Process jor components play a part in the process and the stages occur in the following order: The first stage is the “Alignment stage”. In this stage, the Aligner is used to generate word level alignments for input pair of sentences. We currently use “fast align” (Dyer et al., 2013) which performs well compared to other aligners in terms of both speed and accuracy. The second stage is the Pre-GCM stage which is responsible for pre-processing the input. This stage combines the aligner outputs along with constituent parse trees generated by the parser and “Pseudo Fuzzy-match Score” (Pratapa et al., 2018a) for each sentence pair to make one row of input data for the GCM stage. The Parser is used to generate a sentence level constituent parse tree for one of the source languages. Previously in (Pratapa et al., 2018a) we used the Stanford Parser (Klein and Manning, 2003) but"
2021.eacl-demos.24,2020.findings-emnlp.206,0,0.484801,"e on sentiment analysis and POS tagging (Pratapa et al., 2018b), as well as RNN language models (Pratapa et al., 2018a). The multilingual BERT (Devlin et al., 2019) model finetuned with synthetic code-mixed data outperformed all prior techniques on the GLUECoS benchmark (Khanuja et al., 2020) for code-switching, which spans 11 NLP tasks in two language pairs. The approach of generating synthetic code-mixed data has gained traction following our work, with other approaches including using Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework with transfer learning (Gupta et al., 2020), using parallel data with a small amount of real code-mixed data to learn code-mixing patterns (Winata et al., 2019) and a novel two-level variational autoencoder approach (Samanta et al., 2019). In this work, we present a tool GCM that can automatically generate synthetic code-mixed data given parallel data or a Machine Translation system between the languages that are being mixed. Our tool is intended for use by NLP practitioners who would like to generate training data to train models that can handle code-mixing, as well as linguists and language experts who would like to visualize how cod"
2021.eacl-demos.24,P18-1249,0,0.016188,"peed and accuracy. The second stage is the Pre-GCM stage which is responsible for pre-processing the input. This stage combines the aligner outputs along with constituent parse trees generated by the parser and “Pseudo Fuzzy-match Score” (Pratapa et al., 2018a) for each sentence pair to make one row of input data for the GCM stage. The Parser is used to generate a sentence level constituent parse tree for one of the source languages. Previously in (Pratapa et al., 2018a) we used the Stanford Parser (Klein and Manning, 2003) but we now also provide the option to use the Berkeley Neural Parser (Kitaev and Klein, 2018). This stage is also responsible for creating appropriate batches of data to be consumed by the next stage. The final GCM stage, processes each batch of data, applying linguistic theories in order to generate CM sentences as output. 2.3 Figure 5: Need for Sampling: Not all generated CM sentences feel natural ated data is crucial. We experimented with various sampling techniques and showed that training an RNN Language Model with sampled synthetic data reduces the perplexity of the model by an amount which is equivalent to doubling the amount of real CM data available (Pratapa et al., 2018a). S"
2021.eacl-demos.24,P03-1054,0,0.0274374,"st align” (Dyer et al., 2013) which performs well compared to other aligners in terms of both speed and accuracy. The second stage is the Pre-GCM stage which is responsible for pre-processing the input. This stage combines the aligner outputs along with constituent parse trees generated by the parser and “Pseudo Fuzzy-match Score” (Pratapa et al., 2018a) for each sentence pair to make one row of input data for the GCM stage. The Parser is used to generate a sentence level constituent parse tree for one of the source languages. Previously in (Pratapa et al., 2018a) we used the Stanford Parser (Klein and Manning, 2003) but we now also provide the option to use the Berkeley Neural Parser (Kitaev and Klein, 2018). This stage is also responsible for creating appropriate batches of data to be consumed by the next stage. The final GCM stage, processes each batch of data, applying linguistic theories in order to generate CM sentences as output. 2.3 Figure 5: Need for Sampling: Not all generated CM sentences feel natural ated data is crucial. We experimented with various sampling techniques and showed that training an RNN Language Model with sampled synthetic data reduces the perplexity of the model by an amount w"
2021.eacl-demos.24,P18-1143,1,0.953496,"hat can use unlabeled data for pre-training, we see the generation of synthetic code-mixed data as a promising direction. Various linguistic theories have been proposed that can determine how languages are mixed together, and in prior work we presented the first computational implementation (Bhat et al., 2016) of the Matrix-language (MyersScotton, 1993) and Equivalence Constraint theories (Poplack, 1980). We also showed that generating synthetic data using our computational implementations improved word embeddings leading to better downstream performance on sentiment analysis and POS tagging (Pratapa et al., 2018b), as well as RNN language models (Pratapa et al., 2018a). The multilingual BERT (Devlin et al., 2019) model finetuned with synthetic code-mixed data outperformed all prior techniques on the GLUECoS benchmark (Khanuja et al., 2020) for code-switching, which spans 11 NLP tasks in two language pairs. The approach of generating synthetic code-mixed data has gained traction following our work, with other approaches including using Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework with transfer learning (Gupta et al., 2020), using parallel data with a small amount"
2021.eacl-demos.24,D18-1344,1,0.899992,"hat can use unlabeled data for pre-training, we see the generation of synthetic code-mixed data as a promising direction. Various linguistic theories have been proposed that can determine how languages are mixed together, and in prior work we presented the first computational implementation (Bhat et al., 2016) of the Matrix-language (MyersScotton, 1993) and Equivalence Constraint theories (Poplack, 1980). We also showed that generating synthetic data using our computational implementations improved word embeddings leading to better downstream performance on sentiment analysis and POS tagging (Pratapa et al., 2018b), as well as RNN language models (Pratapa et al., 2018a). The multilingual BERT (Devlin et al., 2019) model finetuned with synthetic code-mixed data outperformed all prior techniques on the GLUECoS benchmark (Khanuja et al., 2020) for code-switching, which spans 11 NLP tasks in two language pairs. The approach of generating synthetic code-mixed data has gained traction following our work, with other approaches including using Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework with transfer learning (Gupta et al., 2020), using parallel data with a small amount"
2021.eacl-demos.24,K19-1026,0,0.22114,"8a). The multilingual BERT (Devlin et al., 2019) model finetuned with synthetic code-mixed data outperformed all prior techniques on the GLUECoS benchmark (Khanuja et al., 2020) for code-switching, which spans 11 NLP tasks in two language pairs. The approach of generating synthetic code-mixed data has gained traction following our work, with other approaches including using Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework with transfer learning (Gupta et al., 2020), using parallel data with a small amount of real code-mixed data to learn code-mixing patterns (Winata et al., 2019) and a novel two-level variational autoencoder approach (Samanta et al., 2019). In this work, we present a tool GCM that can automatically generate synthetic code-mixed data given parallel data or a Machine Translation system between the languages that are being mixed. Our tool is intended for use by NLP practitioners who would like to generate training data to train models that can handle code-mixing, as well as linguists and language experts who would like to visualize how code-mixing occurs between languages given 205 Proceedings of the 16th Conference of the European Chapter of the Associa"
2021.findings-acl.414,Q18-1041,0,0.119633,"NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. Several other works have proposed best practices for dealing with ethical implications of NLP research and deployment (Prabhumoye et al., 2019; Leidner and Plachouras, 2017; Bender and Friedman, 2018; Schnoebelen, 2017). There is now an increased awareness around this topic with a number of workshops and tutorials on ethics at NLP conferences (Tsvetkov et al., 2018; Hovy et al., 2017; Alfano et al., 2018). Such discussions have resulted in a number of reforms at NLP conferences. NLP conferences now have new track called Ethics in NLP. Furthermore, several ML and NLP conferences such as NeurIPS 2020, 4704 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4704–4710 August 1–6, 2021. ©2021 Association for Computational Linguistics 2021 and ACL 20211 now recomm"
2021.findings-acl.414,W17-1612,0,0.0212885,"to the daily fabric of our lives and the society. Since “language is a portal of emotions, a proxy of human behavior, and a strong signal of individual characteristics” (Hovy and Spruit, 2016), large-scale deployment of language technology has potential risks that require early detection and mitigation. Naturally, there have been several discussions about the potential harms and ethical issues concerning NLP (Hovy and Spruit, 2016; Conway and O’Connor, 2016). They have mostly revolved around building or deploying systems in sensitive areas such as hate speech (Sap et al., 2019), social media (Benton et al., 2017), clinical NLP and menˇ tal health (Suster et al., 2017; Mikal et al., 2016) and use of sensitive or personal information (Larson, 2017). While building NLP systems, there are The awareness of the dangers of the existing and new NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore"
2021.findings-acl.414,L18-1361,0,0.0574852,"Missing"
2021.findings-acl.414,couillault-etal-2014-evaluating,0,0.0658835,"Missing"
2021.findings-acl.414,P16-2096,0,0.0640507,"Missing"
2021.findings-acl.414,2020.acl-main.560,1,0.884135,"Missing"
2021.findings-acl.414,W17-1604,0,0.0207481,"angers of the existing and new NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. Several other works have proposed best practices for dealing with ethical implications of NLP research and deployment (Prabhumoye et al., 2019; Leidner and Plachouras, 2017; Bender and Friedman, 2018; Schnoebelen, 2017). There is now an increased awareness around this topic with a number of workshops and tutorials on ethics at NLP conferences (Tsvetkov et al., 2018; Hovy et al., 2017; Alfano et al., 2018). Such discussions have resulted in a number of reforms at NLP conferences. NLP conferences now have new track called Ethics in NLP. Furthermore, several ML and NLP conferences such as NeurIPS 2020, 4704 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4704–4710 August 1–6, 2021. ©2021 Association for Computational Linguistics 20"
2021.findings-acl.414,W19-3637,0,0.0169119,"re The awareness of the dangers of the existing and new NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. Several other works have proposed best practices for dealing with ethical implications of NLP research and deployment (Prabhumoye et al., 2019; Leidner and Plachouras, 2017; Bender and Friedman, 2018; Schnoebelen, 2017). There is now an increased awareness around this topic with a number of workshops and tutorials on ethics at NLP conferences (Tsvetkov et al., 2018; Hovy et al., 2017; Alfano et al., 2018). Such discussions have resulted in a number of reforms at NLP conferences. NLP conferences now have new track called Ethics in NLP. Furthermore, several ML and NLP conferences such as NeurIPS 2020, 4704 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4704–4710 August 1–6, 2021. ©2021 Association fo"
2021.findings-acl.414,C18-1064,0,0.0396249,"Missing"
2021.findings-acl.414,P19-1163,0,0.0168165,"technologies are getting woven into the daily fabric of our lives and the society. Since “language is a portal of emotions, a proxy of human behavior, and a strong signal of individual characteristics” (Hovy and Spruit, 2016), large-scale deployment of language technology has potential risks that require early detection and mitigation. Naturally, there have been several discussions about the potential harms and ethical issues concerning NLP (Hovy and Spruit, 2016; Conway and O’Connor, 2016). They have mostly revolved around building or deploying systems in sensitive areas such as hate speech (Sap et al., 2019), social media (Benton et al., 2017), clinical NLP and menˇ tal health (Suster et al., 2017; Mikal et al., 2016) and use of sensitive or personal information (Larson, 2017). While building NLP systems, there are The awareness of the dangers of the existing and new NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014)"
2021.findings-acl.414,W17-1611,0,0.0269462,"o the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. Several other works have proposed best practices for dealing with ethical implications of NLP research and deployment (Prabhumoye et al., 2019; Leidner and Plachouras, 2017; Bender and Friedman, 2018; Schnoebelen, 2017). There is now an increased awareness around this topic with a number of workshops and tutorials on ethics at NLP conferences (Tsvetkov et al., 2018; Hovy et al., 2017; Alfano et al., 2018). Such discussions have resulted in a number of reforms at NLP conferences. NLP conferences now have new track called Ethics in NLP. Furthermore, several ML and NLP conferences such as NeurIPS 2020, 4704 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4704–4710 August 1–6, 2021. ©2021 Association for Computational Linguistics 2021 and ACL 20211 now recommend the inclusion of"
2021.findings-acl.414,2020.acl-demos.3,0,0.0260929,"Missing"
2021.findings-acl.414,2021.naacl-main.295,0,0.306552,"Missing"
2021.findings-acl.414,W17-1610,0,0.0206064,"“language is a portal of emotions, a proxy of human behavior, and a strong signal of individual characteristics” (Hovy and Spruit, 2016), large-scale deployment of language technology has potential risks that require early detection and mitigation. Naturally, there have been several discussions about the potential harms and ethical issues concerning NLP (Hovy and Spruit, 2016; Conway and O’Connor, 2016). They have mostly revolved around building or deploying systems in sensitive areas such as hate speech (Sap et al., 2019), social media (Benton et al., 2017), clinical NLP and menˇ tal health (Suster et al., 2017; Mikal et al., 2016) and use of sensitive or personal information (Larson, 2017). While building NLP systems, there are The awareness of the dangers of the existing and new NLP applications has led to the curation of several ethical guidelines and frameworks. Undergirded by lessons from the past, these guidelines and frameworks help researchers consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. S"
2021.findings-acl.414,2020.bionlp-1.3,0,0.0184483,"Missing"
2021.findings-acl.414,N18-6005,0,0.017234,"rs consider and contextualize critical ethical concerns. Most of the ethical issues in NLP are rooted in the data being used for research. Couillault et al. (2014) is one of the first works to explore the ethics of data collection and evaluation in NLP. Several other works have proposed best practices for dealing with ethical implications of NLP research and deployment (Prabhumoye et al., 2019; Leidner and Plachouras, 2017; Bender and Friedman, 2018; Schnoebelen, 2017). There is now an increased awareness around this topic with a number of workshops and tutorials on ethics at NLP conferences (Tsvetkov et al., 2018; Hovy et al., 2017; Alfano et al., 2018). Such discussions have resulted in a number of reforms at NLP conferences. NLP conferences now have new track called Ethics in NLP. Furthermore, several ML and NLP conferences such as NeurIPS 2020, 4704 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4704–4710 August 1–6, 2021. ©2021 Association for Computational Linguistics 2021 and ACL 20211 now recommend the inclusion of broader impact statement in their papers, which allows for authors to introspect and be mindful of the ethical implications their research poses. A"
2021.law-1.7,P12-2012,0,0.0346959,"2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained momentum, especially in the Global South. Though prior work has studied interaction pat"
2021.law-1.7,W14-3902,0,0.0197428,"ated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 1994). Such analysis has proven useful in the design of computer systems as it reveals the practices and methods of those who w"
2021.law-1.7,L16-1260,1,0.809995,"estamp, original message and English-translated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 1994). Such analysis has proven useful in the design of computer systems as it reve"
2021.law-1.7,W12-2108,0,0.0149637,"a contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 1994). Such analysis has proven useful in the design of computer systems as it reveals the practices and methods of those who will use these systems a"
2021.law-1.7,D18-1507,0,0.0130977,"e in providing patient-centered care, they can be taxing for already overworked healthcare providers (Viswanathan et al., 2020). There is therefore a necessity for designing technology to support both the providers and patients using such groups. Prior work on online health forums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in sma"
2021.law-1.7,W18-3817,0,0.0154779,"g information: anonymized speaker ID, timestamp, original message and English-translated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 1994). Such analysis has proven useful in"
2021.law-1.7,W12-0602,0,0.0608824,", 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained momentum, especially in the Global South. Though prior work has studi"
2021.law-1.7,2020.sltu-1.28,0,0.0334439,"Missing"
2021.law-1.7,W11-0704,0,0.0155623,"ng user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained momentum, especially in the Global South. Though prior work has studied interaction patterns in online (heal"
2021.law-1.7,W17-5217,0,0.0281199,", 2021; Bhat et al., 2021; Karusala et al., 2020). Even though these peer support groups are effective in providing patient-centered care, they can be taxing for already overworked healthcare providers (Viswanathan et al., 2020). There is therefore a necessity for designing technology to support both the providers and patients using such groups. Prior work on online health forums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak e"
2021.law-1.7,W15-1608,0,0.0264415,"sage and English-translated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 1994). Such analysis has proven useful in the design of computer systems as it reveals the practices and m"
2021.law-1.7,P17-1131,0,0.0249401,"an be taxing for already overworked healthcare providers (Viswanathan et al., 2020). There is therefore a necessity for designing technology to support both the providers and patients using such groups. Prior work on online health forums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this pap"
2021.law-1.7,P17-1180,1,0.84188,"nt (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained"
2021.law-1.7,D16-1121,1,0.840293,"0; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained momentum, especially in the Global South. Thou"
2021.law-1.7,J18-4007,0,0.0280075,"connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using o"
2021.law-1.7,W15-2602,0,0.0211652,"at are being increasingly used to ensure smooth access to both patient-provider communication and peer support beyond formal healthcare system (Karusala et al., 2021; Bhat et al., 2021; Karusala et al., 2020). Even though these peer support groups are effective in providing patient-centered care, they can be taxing for already overworked healthcare providers (Viswanathan et al., 2020). There is therefore a necessity for designing technology to support both the providers and patients using such groups. Prior work on online health forums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zo"
2021.law-1.7,N18-4018,0,0.0131653,"message in the dataset had the following information: anonymized speaker ID, timestamp, original message and English-translated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 1997; Hughes et al., 19"
2021.law-1.7,2020.emnlp-main.425,1,0.751622,"ication and peer support beyond formal healthcare system (Karusala et al., 2021; Bhat et al., 2021; Karusala et al., 2020). Even though these peer support groups are effective in providing patient-centered care, they can be taxing for already overworked healthcare providers (Viswanathan et al., 2020). There is therefore a necessity for designing technology to support both the providers and patients using such groups. Prior work on online health forums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017;"
2021.law-1.7,D14-1105,1,0.797028,"alyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Addit"
2021.law-1.7,2020.sltu-1.24,0,0.0327896,"ative speaker. Each chat message in the dataset had the following information: anonymized speaker ID, timestamp, original message and English-translated message. The data contains sensitive content dealing with people’s deeply personal lives and tragedies, and therefore, even with anonymization there are ethical reasons for not making the dataset public. However, anyone wanting access to the data for research purposes can get in touch with the authors. Several annotation schemas have been proposed for multilingual code-mixed messages on social media platforms (Chakravarthi et al., 2021, 2020; Sirajzade et al., 2020; Vijay et al., 2018; Swami et al., 2018; Dhar et al., 2018; Jamatia et al., 2016; Begum et al., 2016; Maharjan et al., 2015; Barman et al., 2014; Bergsma et al., 2012). However, to the best of our knowledge, none of these look into instant-messaging based interactions among peer supporters using code-mixed African languages like Kiswahili, Sheng. We developed our annotation framework to support an ethnomethodologically-informed ethnographic analysis of the chat: a qualitative approach used in Human-Computer Interaction (HCI) research to understand social interactions (Button and Sharrock, 199"
2021.law-1.7,W19-3009,0,0.0364495,"15; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In rece"
2021.law-1.7,W17-1102,0,0.0159198,"orums focuses on analyzing participation (Schultz et al., 2019; Sadeque et al., 2015; van Campen and Iedema, 2007), social connection and engagement (Sharma et al., 2020a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level senti"
2021.law-1.7,D16-1108,0,0.0275532,"a; Kushner and Sharma, 2020; SmithMerry et al., 2019; Halder et al., 2017), and modelling user behavior (Hosseini and Caragea, 2021; Sharma et al., 2020b; Buechel et al., 2018; Elliott et al., 2018; Pérez-Rosas et al., 2017; Choudhury et al., 2016; Gibson et al., 2016; Choudhury and De, 2014). Further, existing research focuses on the linguistic analysis of conversations in social media platforms like Facebook (Dehouche, 2020; Tian et al., 2017; Vyas et al., 2014; Das and Gambäck, 2013), Reddit and Twitter (Zomick et al., 2019; Rudra et al., 2019; Kiesling et al., 2018; Rijhwani et al., 2017; Tran and Ostendorf, 2016; Rudra et al., 2016; Celli and Rossi, 2012; Bak et al., 2012; Gouws et al., 2011), but has paid little attention to analyzing the conversations in small, close-knit WhatsApp communities. In this paper, we propose a hierarchical annotation framework to facilitate the analysis of health-focused WhatsApp groups. Our proposed hierarchical framework consists of fine-grained peer support categorization and message-level sentiment tagging. Additionally, we address the prevalence of code-mixing in such In recent years, remote digital healthcare using online chats has gained momentum, especially in th"
2021.mrl-1.8,2020.acl-main.421,0,0.0315419,"nce, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance. 1 Introduction Recent work has shown that masked language models such as XLM (Lample and Conneau, 2019), mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2019) can achieve efficient crosslingual transfer, in both zero-shot and few-shot settings. Such results have motivated researchers (K et al., 2020; Artetxe et al., 2020; Conneau et al., 2020) to investigate the underlying influencing factors behind transfer efficiency along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require different types of reasoning and linguistic phenomena. In the monolingual context, recently, authors in Joshi et al. (2020a) extended ∗ 2 TaxiXNLI: Towards Multilingual E"
2021.mrl-1.8,P19-4007,0,0.0164398,"ervations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance. 1 Introduction Recent work has shown that masked language models such as XLM (Lample and Conneau, 2019), mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2019) can achieve efficient crosslingual transfer, in both zero-shot and few-shot settings. Such results have motivated researchers (K et al., 2020; Artetxe et al., 2020; Conneau et al., 2020) to investigate the underlying influencing factors behind transfer efficiency along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require differ"
2021.mrl-1.8,2020.emnlp-main.363,0,0.0134349,"pre-training data, XLM-R does not learn the correspondence between separate negative particles and fused morphemes. Negation Min Max ar es fr hi ru sw ur vi zh -0.73 -0.912 0.162 -0.23 -0.55 0.96 0.06 -0.797 0.39 -0.02 -0.75 0.56 0.005 -0.54 0.16 -1.3 -1.3 -0.03 -0.64 -1.009 -0.01 -0.46 -0.74 0.27 -0.55 -0.88 0.35 Table 6: Difference between L2 distances of sentencepairs for each categories before and after few-shot. For negation, there is large decrease in distances, indicating higher crosslingual alignment. Language features or Reasoning categories – which affects transfer more? Inspired by Lauscher et al. (2020), we extend their regression analysis to include category features. Since, multiple categories can exist per example, we create a feature-set for each example in TaxiXNLI (translated) test set. We include Lang2Vec features of its corresponding language (syntax, phonology, inventory, family, and GEO), and one-hot category vector (1 if it belongs to the category otherwise zero). We then use logistic regression (LR) and linear discriminant analysis (LDA) to predict which factors influence XLM-R’s correct prediction (0/1) on TaxiXNLI (translated) dataset. As shown in Figure 3, with high p-values,"
2021.mrl-1.8,D18-1269,0,0.0265775,"xt section), aligns with the intuition from the definitions that, translation to different languages does not change the reasoning categories in the LOGICAL and KNOWLEDGE bucket. Table 1: We show the percentage of full and partial noisy samples; F1, Accuracy scores with respect to the English TAXINLI annotations for each language on the sampled set of examples. We also include percentage of category annotations where the annotated target category is zero, while the corresponding category in English is 1. 2.1 TaxiXNLI (diagnostic) Motivated by the above study, we look towards the XNLI dataset (Conneau et al., 2018), that provides parallel P-H pairs in 15 languages. We sample 1.4k XNLI examples and annotate with a few selected interesting categories (Negation, Boolean, Spatial, Causal, Temporal, Knowledge). This is used as a diagnostic gold TaxiXNLI Dataset TaxiXNLI (translated) The TAXINLI dataset (Joshi et al., 2020a) consists of 10071 premise, hypothesis (P-H) pairs drawn from the MultiNLI (MNLI) dataset (Williams et al., 2018a). Each pair is annotated with the types of reasoning (among 15 categories) that are required to make the inference. To account for the asymmetric distribution of exam1 Logic: {"
2021.mrl-1.8,2020.acl-main.536,0,0.0328802,"gory-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance. 1 Introduction Recent work has shown that masked language models such as XLM (Lample and Conneau, 2019), mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2019) can achieve efficient crosslingual transfer, in both zero-shot and few-shot settings. Such results have motivated researchers (K et al., 2020; Artetxe et al., 2020; Conneau et al., 2020) to investigate the underlying influencing factors behind transfer efficiency along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require different types of reasoning and linguistic phenomena. In the monolingual context, recently, authors in Joshi et al. (2020a) extended ∗ 2 TaxiXNLI: Towards Multilingual Extension of TaxiNLI In"
2021.mrl-1.8,2020.acl-main.442,0,0.0129696,"y along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require different types of reasoning and linguistic phenomena. In the monolingual context, recently, authors in Joshi et al. (2020a) extended ∗ 2 TaxiXNLI: Towards Multilingual Extension of TaxiNLI In monolingual settings, various authors (Nie et al., 2019; Wang et al., 2018; Ribeiro et al., 2020) have proposed a shift from tracking end-to-end task accuracy to explicit categorizations of fundamental capabilities and to track such individual categorywise errors. While some of the proposed categories seem to capture relevant reasoning capabilities, they are often incomplete as they are tuned towards analyzing errors. Joshi et al. (2020a) recently proposed a categorization of the reasoning tasks involved in NLI. It takes inspiration from earlier literature in linguistics (Wittgenstein, 1922) Work done during internship at Microsoft Research India. 86 Proceedings of the 1st Workshop on Mul"
2021.mrl-1.8,N19-1423,0,0.0796911,"Karthikeyan K∗ Duke University karthikeyan.k@duke.edu Aalok Sathe∗ Massachusetts Institute of Technology asathe@mit.edu Somak Aditya and Monojit Choudhury Microsoft Research India {t-soadit,monojitc}@microsoft.com Abstract previous work (Richardson et al., 2020; Salvatore et al., 2019) and listed a comprehensive list of types of reasoning capabilities required to solve the NLI examples in large public NLI datasets. Authors show that NLI requires a mix of 15 different types of reasoning capabilities (TAXINLI), categorized broadly into LANGUAGE, LOGIC and KNOWLEDGE. They observe that both BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) perform poorly in reasoning tasks such as causal and coreference, whereas they pick up negation easily; and observe that some categories are intrinsically harder for these models (both Transformers-based and preTransformers LSTM-based) to understand. Our central hypothesis of this work is, crosslingual transfer gap (Hu et al., 2020) and few-shot performance may depend on the type of reasoning required. To this end, we propose multilingual extension of the TAXINLI dataset. Our zero-shot analysis strongly suggests that reasoning types play a critical role in trans"
2021.mrl-1.8,D18-1029,0,0.0608487,"Missing"
2021.mrl-1.8,D19-6103,0,0.0376233,"Missing"
2021.mrl-1.8,2020.conll-1.4,1,0.844902,"Missing"
2021.mrl-1.8,2020.acl-main.560,1,0.703518,"s have motivated researchers (K et al., 2020; Artetxe et al., 2020; Conneau et al., 2020) to investigate the underlying influencing factors behind transfer efficiency along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require different types of reasoning and linguistic phenomena. In the monolingual context, recently, authors in Joshi et al. (2020a) extended ∗ 2 TaxiXNLI: Towards Multilingual Extension of TaxiNLI In monolingual settings, various authors (Nie et al., 2019; Wang et al., 2018; Ribeiro et al., 2020) have proposed a shift from tracking end-to-end task accuracy to explicit categorizations of fundamental capabilities and to track such individual categorywise errors. While some of the proposed categories seem to capture relevant reasoning capabilities, they are often incomplete as they are tuned towards analyzing errors. Joshi et al. (2020a) recently proposed a categorization of the reasoning tasks involved in NLI. It takes in"
2021.mrl-1.8,W18-5446,0,0.0223014,"transfer efficiency along different dimensions such as model capacity, language similarity and learning objective. However, such transfer efficiency is measured for natural language understanding (NLU) tasks which often cover a wide range of linguistic phenomena. Natural Language Inference is one such representative task which is known to require different types of reasoning and linguistic phenomena. In the monolingual context, recently, authors in Joshi et al. (2020a) extended ∗ 2 TaxiXNLI: Towards Multilingual Extension of TaxiNLI In monolingual settings, various authors (Nie et al., 2019; Wang et al., 2018; Ribeiro et al., 2020) have proposed a shift from tracking end-to-end task accuracy to explicit categorizations of fundamental capabilities and to track such individual categorywise errors. While some of the proposed categories seem to capture relevant reasoning capabilities, they are often incomplete as they are tuned towards analyzing errors. Joshi et al. (2020a) recently proposed a categorization of the reasoning tasks involved in NLI. It takes inspiration from earlier literature in linguistics (Wittgenstein, 1922) Work done during internship at Microsoft Research India. 86 Proceedings of"
2021.mrl-1.8,N18-1101,0,0.429011,"he annotated target category is zero, while the corresponding category in English is 1. 2.1 TaxiXNLI (diagnostic) Motivated by the above study, we look towards the XNLI dataset (Conneau et al., 2018), that provides parallel P-H pairs in 15 languages. We sample 1.4k XNLI examples and annotate with a few selected interesting categories (Negation, Boolean, Spatial, Causal, Temporal, Knowledge). This is used as a diagnostic gold TaxiXNLI Dataset TaxiXNLI (translated) The TAXINLI dataset (Joshi et al., 2020a) consists of 10071 premise, hypothesis (P-H) pairs drawn from the MultiNLI (MNLI) dataset (Williams et al., 2018a). Each pair is annotated with the types of reasoning (among 15 categories) that are required to make the inference. To account for the asymmetric distribution of exam1 Logic: {Quantifier, Conditional, Comparative}; Deductions: {Relational, Spatial, Coreference, Temporal}; Knowledge: {World, Taxonomic} 2 Category-wise splits are in Tab. 4. 87 Category P (eng) H (eng) Swahili The time period during which the Black Plague happened. E1 world (eng) Middle Ages E2 taxonomic (swa) Carmel Man, a relation of the Neanderthal family, lived here 600,000 years ago. E3 spatial (swa) E4 temporal (eng) E5 C"
2021.sigmorphon-1.7,J96-4003,0,0.613836,"1) , CopyReplace (x , w , 1)) The T OKEN system does not synthesize these 67 the rule for long vowels applies, and one where the rule for words without long vowels applies. 6 we hope to apply it to learning more complex types of lingusitic rules in the future. In addition to being a way to learn rules from data, the ability to explicity control the generalization behaviour of the model allows for the use of program synthesis to understand the kinds of learning biases and operations that are required to model various linguistic processes. We leave this exploration to future work. Related work Gildea and Jurafsky (1996) also study the problem of learning phonological rules from data, and explicitly controlling generalization behaviour. We pursue a similar goal, but in a few-shot setting. Barke et al. (2019) and Ellis et al. (2015) study program synthesis applied to linguistic rule learning. They make much stronger assumptions about the data (the existence of an underlying form, and the availability of additional information like IPA features). We take a different approach, and study program synthesis models that can work only on the tokens in the word (like N O F EATURE), and also explore the effect of provi"
2021.sigmorphon-1.7,2020.sigmorphon-1.2,0,0.0154612,"2019) and Ellis et al. (2015) study program synthesis applied to linguistic rule learning. They make much stronger assumptions about the data (the existence of an underlying form, and the availability of additional information like IPA features). We take a different approach, and study program synthesis models that can work only on the tokens in the word (like N O F EATURE), and also explore the effect of providing features in these cases. We also test our approach on a more varied set of problems that involves aspects of morphology, transliteration, multilinguality, and stress. S¸ahin et al. (2020) also present a set of Linguistics Olympiad problems as a test of the metalinguistic reasoning abilities of NLP models. While problems in their set involve finding phonological rules, they also require the knowledge of syntax and semantics that are out of the scope of our study. We present a set of problems that only requires reasoning about surface word forms, and without requiring the meanings. 7 Acknowledgements We would like to thank Partho Sarthi for invaluable help with PROSE and NDSyn. We would also like to thank the authors of the ProLinguist paper for their assistance. Finally, we wou"
2021.sigmorphon-1.7,W15-3049,0,0.0176907,"Missing"
2021.sigmorphon-1.7,2020.acl-main.115,0,0.0613396,"Missing"
2021.sigmorphon-1.7,N07-1047,0,0.0659117,"ther aspects of the model remain the same across variants. Morphophonology and multilingual problems: For every pair of columns (s, t) in the problem matrix M , we synthesize the program M:s → M:t . To predict the form of a test sample Mij , we find a column k such that the program M:k → M:j has the best ranking score, and evaluate it on Mik . Transliteration problems: Given a problem matrix M , we construct a new matrix M 0 for each pair of columns (s, t) such that all entries in M 0 are in the same script. We align word pairs (Mis , Mit ) using the Phonetisaurus many-to-many alignment tool (Jiampojamarn et al., 2007), and build a simple mapping f for each source token to the target token with which it is most frequently aligned. We 0 by applying f to each token of M and fill in Mis is Dataset statistics The dataset we present is highly multilingual. The 34 problems contain samples from 38 languages, drawn from across 19 language families. There are 15 morphophonology problems, 7 multilingual problems, 6 stress, and 6 transliteration problems. The set contains 1452 training words with an average of 43 words per problem, and 319 test words with an average of 9 per problem. Each problem has a matrix that has"
2021.sigmorphon-1.7,P84-1070,0,0.728015,"ng programming language that satisfy (user) intent expressed in some form of constraints” (Gulwani et al., 2017). This method allows us to specify domain-specific assumptions as a language, and use generic synthesis approches like FlashMeta (Polozov and Gulwani, 2015) to synthesize programs. The ability to explicitly encode domain-specific assumptions gives program synthesis broad applicability to various tasks. In this paper, we explore applying it to the task of learning phonological rules. Whereas previous work on rule-learning has focused on learning rules of a specific type (Brill, 1992; Johnson, 1984), the DSL in program synthesis allows learning rules of different types, and in different rule formalisms. In this work, we explore learning rules similar to rewrite rules (Chomsky and Halle, 1968) that are used extensively to describe phonology. Sequences of rules are learnt using a noisy disjunctive synthesis algorithm NDSyn (Iyer et al., 2019) extended to learn stateful multi-pass rules (Sarthi et al., 2021). 2.1 2.2 Domain-specific language The domain-specific language (DSL) is the declarative language which defines the allowable string transformation operations. The DSL is defined by a se"
2021.sigmorphon-1.7,2020.lrec-1.521,0,0.0527236,"Missing"
2021.wnut-1.18,2020.scil-1.32,0,0.029254,"Missing"
2021.wnut-1.18,2020.acl-main.80,0,0.0116237,"#4 is disallowed by EC, ML, aligned and dictionary models, #5 is disallowed under all the grammatical and heuristic models. Words from embedded language are italicized in each example. may not be considered natural by bilingual speakers. In this work, we compare the two theories (EC and ML) for their perceived naturalness. Due to the unavailability of a computational model for FHC theory, we leave the comparison of FHC to EC and ML theories to future work. There is another line of work involving sequenceto-sequence models that do not directly rely on grammatical theories (Winata et al., 2019; Lee and Li, 2020). While this is an exciting line of research, we restrict the focus of this work to grammatical models and leave the comparison with neural models to future work. 2.1 Grammatical Models Equivalence Constraint (EC): Proposed by Poplack (1980), this theory imposes an equivalence constraint at each switch point between the two languages in a code-mixed sentence. The two constituent languages are assumed to follow contextfree grammar (CFG). Each non-terminal (and terminal) in one CFG has a counterpart in the other CFG. In cases where the two language parses are not identical, we follow prior work"
2021.wnut-1.18,J08-4004,0,0.158117,"alues in place of absolute scores (Equation 6). This methodology allows for analysis over a much larger space of sentence pairs. (4) We construct a partial order between Ti and Tj 7 M1 see A.1 in Appendix for the questions used in the test. We acknowledge that understandability is a requirement for a code-mixed text generation model. But in the scope of this work, we focus primarily on the naturalness evaluation. xi,k − µk zi,k = σ X k X Ti &gt; Tj , if zi,k &gt; zj,k k (6) k 8 To measure the inter-coder agreement, we have used Krippendorff’s α as it allows for missing data 162 and multiple coders (Artstein and Poesio, 2008). With d(x, y) = max(0, −sgn(x ∗ y)) as the distance metric, i.e., penalizing only if x and y are of different signs, we observed the α to be 0.59, indicating moderate to substantial agreement (Landis and Koch, 1977). We use the partial order in the sentences to learn a partial order for the feature variants. When comparing the impact of two feature variants, we kept the other feature values constant for consistency. Table 2 and Table 3 present a comparison of original theories (ML and EC) with their relaxed variants. The results show that the original ML theory is better than its relaxation(s"
2021.wnut-1.18,C12-1102,0,0.108454,"become commonplace in informal text conversations with the increasing use of social media (Rijhwani et al., 2017). Prior work has focused on various code-mixed natural language processing (NLP) tasks, including part-ofspeech (POS) tagging (Soto and Hirschberg, 2017; Singh et al., 2018), sentiment analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream NLP tasks (Pratapa et al., 2018b). These generation techniques have also been applied in human-machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generati"
2021.wnut-1.18,W93-0231,0,0.258016,"machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generation models. Such comparison is necessary to understand the relevance of these generation models to individual downstream NLP applications. In this work, we present one such comparison involving two linguistic theory-based code-mixed text generation systems, one based on Equivalence Constraint theory (Poplack, 1980) and the other on MatrixEmbedded language theory (Myers-Scotton, 1993). Specifically, we crowdsource human judgments on the understandability and naturalness of text generated by the two systems. Our results show that grammatical models are considerably better than heuristic counterparts. Additionally, we find the two grammatical models are equally preferable by our human judges. We evaluate the importance of individual constraints in each grammatical theory. In the following sections, we first briefly describe the grammatical models (§2.1) and related heuristic models (§2.2). In §3, we describe our human evaluation setup, and present the results in §4. 2 Genera"
2021.wnut-1.18,N13-1073,0,0.0350229,"To evaluate the effectiveness of the above described models of code-mixing (§2), we first sample Spanglish (English-Spanish) sentences using selected criterion (§3.1), and then collect human judgments on their understandability and naturalness (§3.2). 3.1 Preparing Data The generative models of code-mixing take parallel sentences and their word-level alignments as their inputs. We follow the same strategy as Pratapa et al. (2018a) to collect monolingual tweets in English and Spanish.2 We then translate the tweets to the other language using Microsoft Translator API. We use fast_align toolkit (Dyer et al., 2013) to extract word-level alignments between the source and target sentences.3 For the grammatical models, we extract constituent parses using Stanford PCFG parser (Klein and Manning, 2003). For generating code-mixed sentences using the grammatical theories, we follow the prior work (Bhat et al., 2016; Pratapa et al., 2018a). Note that both these systems follow the grammatical theories exactly and do not exclude code-mixed sentences based on their naturalness. For the heuristic models, we follow the methodologies described in §2.2. Most models take parallel sentences as input and generate a large"
2021.wnut-1.18,2021.naacl-main.459,0,0.0203549,"and Hirschberg, 2017; Singh et al., 2018), sentiment analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream NLP tasks (Pratapa et al., 2018b). These generation techniques have also been applied in human-machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generation models. Such comparison is necessary to understand the relevance of these generation models to individual downstream NLP applications. In this work, we present one such comparison involving two linguistic theory-based code-mixed text generation"
2021.wnut-1.18,P03-1054,0,0.0557696,"lect human judgments on their understandability and naturalness (§3.2). 3.1 Preparing Data The generative models of code-mixing take parallel sentences and their word-level alignments as their inputs. We follow the same strategy as Pratapa et al. (2018a) to collect monolingual tweets in English and Spanish.2 We then translate the tweets to the other language using Microsoft Translator API. We use fast_align toolkit (Dyer et al., 2013) to extract word-level alignments between the source and target sentences.3 For the grammatical models, we extract constituent parses using Stanford PCFG parser (Klein and Manning, 2003). For generating code-mixed sentences using the grammatical theories, we follow the prior work (Bhat et al., 2016; Pratapa et al., 2018a). Note that both these systems follow the grammatical theories exactly and do not exclude code-mixed sentences based on their naturalness. For the heuristic models, we follow the methodologies described in §2.2. Most models take parallel sentences as input and generate a large number of code-mixed sentences. For this study, we sample code-mixed sentences by carefully controlling for specific features. As we describe below, we identify features relevant to the"
2021.wnut-1.18,P18-1143,1,0.492355,"analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream NLP tasks (Pratapa et al., 2018b). These generation techniques have also been applied in human-machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generation models. Such comparison is necessary to understand the relevance of these generation models to individual downstream NLP applications. In this work, we present one such comparison involving two linguistic theory-based code-mixed text generation systems, one based on Equivalence Constraint theory ("
2021.wnut-1.18,D18-1344,1,0.491342,"analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream NLP tasks (Pratapa et al., 2018b). These generation techniques have also been applied in human-machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generation models. Such comparison is necessary to understand the relevance of these generation models to individual downstream NLP applications. In this work, we present one such comparison involving two linguistic theory-based code-mixed text generation systems, one based on Equivalence Constraint theory ("
2021.wnut-1.18,P17-1180,1,0.679958,"mixing, and there is a lack of comparison of these theories. We present a largescale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two grammatical theories. 1 Introduction Code-mixing is the phenomenon of mixing multiple languages within a single conversation. While widely observed in spoken language, mixing languages has also become commonplace in informal text conversations with the increasing use of social media (Rijhwani et al., 2017). Prior work has focused on various code-mixed natural language processing (NLP) tasks, including part-ofspeech (POS) tagging (Soto and Hirschberg, 2017; Singh et al., 2018), sentiment analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate reali"
2021.wnut-1.18,2021.eacl-demos.24,1,0.687084,"from one of the two languages in a given aligned pair. To generate a sentence, we first sort the alignment indices in the order of L1 tokens (similarly for L2) and then iterate through the alignment indices. Dictionary (Hdict ): In this model, we use an external bilingual dictionary that maps words from L1 Matrix-Embedded Language (ML): Proposed and L2. We take a monolingual sentence, and for by Myers-Scotton (1993), this theory allows for every word, we uniformly sample one of the words inserting grammatical constituents of an embedded or its counterpart from the other language (if one 1 See Rizvi et al. (2021) for the implementation. exists in the dictionary). This model only needs a 159 pre-built dictionary to synthesize code-mixed sentences from monolingual corpora. We then group them into the four buckets, {SL1 : (5, 8], SL2 : (8, 11], SL3 : (11, 15], SL4 : (15, 20]}. Parallel (Hparallel ): In the case of missing wordlevel alignments, we can still generate code-mixed sentences by assuming a linear mapping between L1 and L2 word order. This assumption might work well for languages with similar word order, like the English-Spanish pair used in this study. Figure 1 illustrates the validity of examp"
2021.wnut-1.18,W18-3503,0,0.0123717,"ivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two grammatical theories. 1 Introduction Code-mixing is the phenomenon of mixing multiple languages within a single conversation. While widely observed in spoken language, mixing languages has also become commonplace in informal text conversations with the increasing use of social media (Rijhwani et al., 2017). Prior work has focused on various code-mixed natural language processing (NLP) tasks, including part-ofspeech (POS) tagging (Soto and Hirschberg, 2017; Singh et al., 2018), sentiment analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream"
2021.wnut-1.18,2021.acl-long.245,0,0.0120528,"sks of speech recognition, human-machine dialogue, and translation. Prior work has primarily focused on three grammatical theories, Equivalence Constraint (EC) (Poplack, 1980), MatrixEmbedded Language (ML) (Myers-Scotton, 1993) and Functional Head Constraint (FHC) (Belazi et al., 1994). Each of these theories presents an account of the grammatical constraints of codemixing. Bhat et al. (2016) presented computational models for EC and ML theories. Such computational models allow for generation of grammatically correct code-mixed text (Li and Fung, 2012; Pratapa et al., 2018a; Lee et al., 2019; Tarunesh et al., 2021). However, the resulting text may or 158 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 158–167 November 11, 2021. ©2021 Association for Computational Linguistics S NP DT VP NN VBZ PP IN NP DT JJ NN 1 2 3 4 5 Un superconductor levita sobre a magnetic track Un superconductor levita sobre una pista magnetic Un superconductor levita sobre a pista magnética A superconductor levitates on a magnetic magnética Levita un superconductor sobre a magnetic track A superconductor levitates on a magnetic track (b) Candidate code-mixed sentences. Un sup"
2021.wnut-1.18,K19-1026,0,0.0560712,"f social media (Rijhwani et al., 2017). Prior work has focused on various code-mixed natural language processing (NLP) tasks, including part-ofspeech (POS) tagging (Soto and Hirschberg, 2017; Singh et al., 2018), sentiment analysis (Patwa et al., 2020), machine translation (Solorio et al., 2021) and speech recognition (Lee et al., 2017). Accurately modeling when and how to mix languages is critical to the above code-mixed NLP tasks. Research efforts on this front have either relied on established grammatical theories (Li and Fung, 2012; Bhat et al., 2016) or neural sequence-tosequence models (Winata et al., 2019) to generate realistic code-mixed text. Such models have found applications in speech recognition (Li and Fung, 2012; Lee et al., 2019), translation (Gupta et al., 2021) and other downstream NLP tasks (Pratapa et al., 2018b). These generation techniques have also been applied in human-machine dialogue (Ahn † work done at Microsoft Research. et al., 2020; Bawa et al., 2020) to generate seemingly natural code-mixed responses. However, current literature lacks a thorough comparison of different code-mixed text generation models. Such comparison is necessary to understand the relevance of these ge"
b-etal-2010-resource,I08-3009,0,\N,Missing
b-etal-2010-resource,W09-3501,0,\N,Missing
b-etal-2010-resource,I08-1058,0,\N,Missing
b-etal-2010-resource,J98-4003,0,\N,Missing
b-etal-2010-resource,I08-5014,0,\N,Missing
C08-1076,P07-1014,1,0.887929,"Missing"
C08-1076,P06-2017,1,0.70874,"ant inventories emerges due to an interaction of different forces acting upon them. In order to identify the nature of these interactions one has to understand the growth dynamics of these inventories. The theories of complex networks provide a number of growth models that have proved to be extremely successful in explaining the evolutionary dynamics of various social (Newman, 2001; Ramasco et al., 2004), biological (Jeong et al., 2000) and other natural systems. The basic framework for the current study develops around two such complex networks namely, the Phoneme-Language Network or PlaNet (Choudhury et al., 2006) and its onemode projection, the Phoneme-Phoneme Network or PhoNet (Mukherjee et al.2007a). We begin by analyzing some of the structural properties (Sec. 2) of the networks and observe that the consonant nodes in both PlaNet and PhoNet follow a powerlaw-like degree distribution. Moreover, PhoNet 601 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 601–608 Manchester, August 2008 is characterized by a high clustering coefficient, a property that has been found to be prevalent in many other social networks (Newman, 2001; Ramasco et al., 2004). We"
C10-2019,W02-0605,0,0.0146959,"is tempting to believe that a lot is known about word co-occurrences, in order to obtain a deeper insight into how these cooccurrence patterns emerged there are many other interesting properties that need to be investigated. One such property is the spectrum of the word co162 Coling 2010: Poster Volume, pages 162–170, Beijing, August 2010 occurrence network which can provide important information about its global organization. In fact, the application of this powerful mathematical machinery to infer global patterns in linguistic networks is rarely found in the literature (few exceptions are (Belkin and Goldsmith, 2002; Mukherjee et al, 2009)). However, note that spectral analysis has been quite successfully applied in the analysis of biological and social networks (Banerjee and Jost, 2007; Farkas et al, 2001). The aim of the present work is to investigate the spectral properties of a word co-occurrence network in order to understand its global structure. In particular, we study the properties of seven different languages namely Bangla (IndoEuropean family), English (Indo-European family), Estonian (Finno-Ugric family), French (IndoEuropean family), German (Indo-European family), Hindi (Indo-European family"
C10-2019,W07-0210,0,0.268177,"Missing"
C10-2019,W07-0213,0,0.0275605,"with time and new words are only attached to the periphery of the network. These properties are fundamental to the nature of word co-occurrence across languages. 1 Introduction In a natural language, words interact among themselves in different ways – some words co-occur In this paper, we present an in-depth study of the word co-occurrence patterns of a language in the framework of complex networks. The choice of this framework is strongly motivated by its success in explaining various properties of word co-occurrences previously (Ferrer-i-Cancho and Sol´e, 2001; Ferrer-i-Cancho et al, 2007; Kapustin and Jamsen, 2007). Local properties, such as the degree distribution and clustering coefficient of the word co-occurrence networks, have been thoroughly studied for a few languages (Ferreri-Cancho and Sol´e, 2001; Ferrer-i-Cancho et al, 2007; Kapustin and Jamsen, 2007) and many interesting conclusions have been drawn. For instance, it has been found that these networks are small-world in nature and are characterized by a two regime power-law degree distribution. Efforts have also been made to explain the emergence of such a two regime degree distribution through network growth models (Dorogovstev and Mendes, 2"
C10-2019,E09-1067,1,0.859323,"Missing"
C14-1098,1993.eamt-1.1,0,0.0582261,"Missing"
C14-1098,P07-1009,0,0.311619,"Missing"
C14-1098,P07-1107,0,0.0371361,"erb-Object ordering (Greenberg, 1963). Daum´e and Campbell (2007) present a statistical model for automatically discovering such implications from a large typological database and discuss many other typological implications involving adpositions. Motivation. Knowledge of the typological characteristics of languages is not only of interest to linguists, but also very useful in NLP for two main reasons. First, typological information, if appropriately exploited while designing computational methods, can lead to very promising results in tasks like coreference resolution and machine translation (Haghighi and Klein, 2007; Moore and Quirk, 2007). Second, as Bender and Langendoen (2010) have pointed out, in order to claim that a computational technique ∗ This work was done during the author’s internship at Microsoft Research India. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://wals.info/ 1037 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1037–1046, Dublin, Ireland, Augus"
C14-1098,hammarstrom-etal-2008-bootstrapping,0,0.0734818,"Missing"
C14-1098,I08-2093,0,0.336661,"1037–1046, Dublin, Ireland, August 23-29 2014. is truly language independent, one must show its usefulness for languages having diverse typological features. However, there is very little work on the automatic discovery of typological characteristics, primarily because it is assumed that such information is readily available. However, Hammarstr¨om et al. (2008) argue that documenting a language and its typological features is a time consuming process for the linguists and therefore, automatic methods for bootstrapping language description is a worthwhile effort towards language preservation. Lewis and Xia (2008) mine inter-linearized data from the Web and infer typological features for “low-density” languages, i.e. languages represented in scarce quantities on the Web. We argue that apart from documenting and understanding the typology of “low-density” languages, unsupervised discovery of adposition typology is also useful for analyzing undeciphered languages and scripts, such as the Indus valley script (Rao et al., 2009) and the Cypro-Minoan syllabary (Palaima, 1989), as well as newly emerging languages, such as the language of Web search queries (Saha Roy et al., 2012) or the Nicaraguan sign langua"
C14-1098,W07-0715,0,0.0317627,"berg, 1963). Daum´e and Campbell (2007) present a statistical model for automatically discovering such implications from a large typological database and discuss many other typological implications involving adpositions. Motivation. Knowledge of the typological characteristics of languages is not only of interest to linguists, but also very useful in NLP for two main reasons. First, typological information, if appropriately exploited while designing computational methods, can lead to very promising results in tasks like coreference resolution and machine translation (Haghighi and Klein, 2007; Moore and Quirk, 2007). Second, as Bender and Langendoen (2010) have pointed out, in order to claim that a computational technique ∗ This work was done during the author’s internship at Microsoft Research India. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://wals.info/ 1037 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1037–1046, Dublin, Ireland, August 23-29 2014. is truly l"
C14-1098,1995.mtsummit-1.1,0,0.356774,"Missing"
C14-1098,J01-2001,0,\N,Missing
D14-1105,N13-1131,0,0.0135086,"the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provid"
D14-1105,W14-3914,1,0.782056,"Missing"
D14-1105,J93-2004,0,0.0656776,"Missing"
D14-1105,D13-1084,0,0.0158619,"h is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the t"
D14-1105,N13-1039,0,0.112811,"Missing"
D14-1105,petrov-etal-2012-universal,0,0.106553,"Missing"
D14-1105,P11-2008,0,0.0903219,"Missing"
D14-1105,sankaran-etal-2008-common,1,0.435317,"Missing"
D14-1105,D08-1102,0,0.0877209,"s relatively less common. In languages with richer morphology and agglutination, like Bangla and most Dravidian languages, more frequent sublexical mixing may be observed. Also note that words are borrowed extensively between Hi and En such that certain English words (e.g., bus, party, vote etc) are no longer perceived as English words by the Hindi speakers. However, here we will not distinguish between CM and borrowing, and such borrowed English words have also been labeled as En words. have been works on POS tagging of social media data (Gimpel et al., 2011; Owoputi et al., 2013) and of CM (Solorio and Liu, 2008b), but we do not know of any work on POS tagging of CM text from social media that involves transliteration. The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data. 2 Corpus Creation For this study, we collected data from Facebook public pages of three celebrities: Amitabh Bachchan, Shahrukh Khan, Narendra Modi, and the BBC Hindi news page. All these pages are very"
D14-1105,D08-1110,0,0.745363,"s relatively less common. In languages with richer morphology and agglutination, like Bangla and most Dravidian languages, more frequent sublexical mixing may be observed. Also note that words are borrowed extensively between Hi and En such that certain English words (e.g., bus, party, vote etc) are no longer perceived as English words by the Hindi speakers. However, here we will not distinguish between CM and borrowing, and such borrowed English words have also been labeled as En words. have been works on POS tagging of social media data (Gimpel et al., 2011; Owoputi et al., 2013) and of CM (Solorio and Liu, 2008b), but we do not know of any work on POS tagging of CM text from social media that involves transliteration. The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data. 2 Corpus Creation For this study, we collected data from Facebook public pages of three celebrities: Amitabh Bachchan, Shahrukh Khan, Narendra Modi, and the BBC Hindi news page. All these pages are very"
D14-1105,b-etal-2010-resource,1,0.336504,"important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data. Nevertheless, CM on social media has not been studied from a computational aspect. Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions. Furthermore, for languages written in scripts other than Roman, like Hindi, Bangla, Japanese, Chinese and Arabic, Roman transliterations are typically used for representing the words (Sowmya et al., 2010). This can prove a challenge for language identification and segregation of the two languages. In this paper, we describe our initial efforts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data. POS tagging is one of the fundamental pre-processing steps for NLP, and while there Code-mixing is frequently observed in user generated content on social media, especially from multilingual users. The linguistic complexity of such content is compounded"
D14-1105,gupta-etal-2012-mining,1,0.481013,"ght into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 3.2 Normalization In our dataset, if a word is identified as Hi, then it must be back-transliterated to Devanagari script so that any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen"
D14-1105,N03-1033,0,0.052712,"Missing"
D16-1121,bakliwal-etal-2012-hindi,0,0.0425008,"cording to its sentiment (+, − or 0). Fig. 2 shows the architecture of the proposed model. Two-step classification was empirically found to be better than a single four-class classifier. We develop individual classifiers for each language class (er, hr, hd, mr) using an SVM with RBF kernel from Scikit-learn (Pedregosa et al., 1136 Classifier Features For opinion classification (opinion or ⊗), we propose a set of event-independent lexical features and Twitter-specific features. (i) Subjective words: Expected to be present in opinion tweets. We use lexicons from Volkova et al. (2013) for er and Bakliwal et al. (2012) for hd. We Romanize the hd lexicon for the hr classifiers (ii) Elongated words: Words with one character repeated more than two times, e.g. sooo, naaahhhhi (iii) Exclamations: Presence of contiguous exclamation marks (iv) Emoticons4 (v) Question marks: Queries are generally nonopinionated. (vi) Wh-words: These are used to form questions (vii) Modal verbs: e.g. should, could, would, cud, shud (viii) Excess hashtags: Presence of more than two hashtags (ix) Intensifiers: Generally used to emphasize sentiment, e.g., we shouldn’t get too comfortable (x) Swear words5 : Prevalent in opinionated twee"
D16-1121,W14-3914,1,0.300933,"is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection. 1 Rafiya Begum Microsoft Research Labs, Bangalore, India t-rafbeg@microsoft.com Conversational phenomena such as CS were observed only in speech and therefore, all previous studies are based on data collected from a small set of speakers or from interviews. With the growing popularity of social media, we now have an abundance of conversation-like data that exhibit CS and other speech phenomena, hitherto unseen in text (Bali et al., 2014). Leveraging such data from Twitter, we conduct a large-scale study on language preference, if any, for the expression of opinion and sentiment by Hindi-English (Hi-En) bilinguals. Introduction The pattern of language use in a multilingual society is a complex interplay of socio-linguistic, discursive and pragmatic factors. Sometimes speakers have a preference for a particular language for certain conversational and discourse settings; on other occasions, there is fluid alteration between two or more languages in a single conversation, also known as Code-switching (CS) or Code-mixing1 . Under∗"
D16-1121,W14-3902,0,0.11143,"Missing"
D16-1121,C12-2017,0,0.0222765,"of them could also be md. To classify Roman script tweets as er, hr or mr, we use the system that performed best in the FIRE 2013 shared task for word-level language detection of Hi-En text (Gella et al., 2013). This system uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a two-step class"
D16-1121,P13-1025,0,0.0181801,"is often found to be used as a signaling device to imply certain pragmatic functions (Barredo, 1997; Sanchez, 1983; Nishimura, 1995; Maschler, 1132 2.2 Hindi-English Bilingualism et al., 2014), to the best of our knowledge, there has been no study on automatic identification of functional aspects of CS or any large-scale, data-driven study of language preference. The current study adds to the growing repertoire of work on quantitative analysis of social media data for understanding socio-linguistic and pragmatic issues, such as detection of depression (De Choudhury et al., 2013), politeness (Danescu-Niculescu-Mizil et al., 2013), speech acts (Vosoughi and Roy, 2016), and social status (Tchokni et al., 2014). 3 number of socio-linguistic parameters beyond sentiment. For instance, on social media, English is overwhelmingly more common than any Indic language (Bali et al., 2014). This is because (a) English tweets come from a large number of users apart from Hi-En bilinguals and (b) English is the preferred language for tweeting even for Hi-En bilinguals because it expands the target audience of the tweet by manifolds. The preference of λσ for expressing , therefore, can be quantified as: pr(|λσ; T ) = Problem Formula"
D16-1121,W13-4306,0,0.0131859,"am features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a two-step classification model. We first identify whether a tweet is opinionated or nonopinionated (⊗). If the tweet is opinionated, we further classify it according to its sentiment (+, − or 0). Fig. 2 shows the architecture of the proposed model. Two-step classifica"
D16-1121,S13-2053,0,0.0457426,"tem uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a two-step classification model. We first identify whether a tweet is opinionated or nonopinionated (⊗). If the tweet is opinionated, we further classify it according to its sentiment (+, − or 0). Fig. 2 shows the architecture of the proposed model"
D16-1121,S12-1033,0,0.0104605,"2013). This system uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a two-step classification model. We first identify whether a tweet is opinionated or nonopinionated (⊗). If the tweet is opinionated, we further classify it according to its sentiment (+, − or 0). Fig. 2 shows the architectur"
D16-1121,W02-1011,0,0.0546907,"Missing"
D16-1121,P14-2110,0,0.0614149,"Missing"
D16-1121,W09-4306,0,0.0219676,"mall fraction of them could also be md. To classify Roman script tweets as er, hr or mr, we use the system that performed best in the FIRE 2013 shared task for word-level language detection of Hi-En text (Gella et al., 2013). This system uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a tw"
D16-1121,W14-3704,1,0.887974,"Missing"
D16-1121,S15-2078,0,0.0165752,"ximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space. 5.2 Opinion and Sentiment Detection Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al., 2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3. 5.3 Figure 2: Overview of the experimental method. 2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets. We propose a two-step classification model. We first identify whether a tweet is opinionated or nonopinionated (⊗). If the tweet is opinionated, we further classify it according to its sentiment (+, − or 0). Fig. 2 shows the architecture of the proposed model. Two-step classification was empirically foun"
D16-1121,W14-5124,0,0.0521816,"Missing"
D16-1121,D08-1110,0,0.0407251,"Missing"
D16-1121,W15-2902,0,0.0662922,"Missing"
D16-1121,P13-2090,0,0.0713365,"onated, we further classify it according to its sentiment (+, − or 0). Fig. 2 shows the architecture of the proposed model. Two-step classification was empirically found to be better than a single four-class classifier. We develop individual classifiers for each language class (er, hr, hd, mr) using an SVM with RBF kernel from Scikit-learn (Pedregosa et al., 1136 Classifier Features For opinion classification (opinion or ⊗), we propose a set of event-independent lexical features and Twitter-specific features. (i) Subjective words: Expected to be present in opinion tweets. We use lexicons from Volkova et al. (2013) for er and Bakliwal et al. (2012) for hd. We Romanize the hd lexicon for the hr classifiers (ii) Elongated words: Words with one character repeated more than two times, e.g. sooo, naaahhhhi (iii) Exclamations: Presence of contiguous exclamation marks (iv) Emoticons4 (v) Question marks: Queries are generally nonopinionated. (vi) Wh-words: These are used to form questions (vii) Modal verbs: e.g. should, could, would, cud, shud (viii) Excess hashtags: Presence of more than two hashtags (ix) Intensifiers: Generally used to emphasize sentiment, e.g., we shouldn’t get too comfortable (x) Swear word"
D16-1121,D14-1105,1,0.462117,"Missing"
D16-1121,D08-1102,0,\N,Missing
D17-1240,W14-3902,0,0.144803,"Missing"
D17-1240,W14-5152,0,0.32363,"Missing"
D17-1240,Q16-1003,0,0.0241441,"Missing"
D17-1240,P17-2009,0,0.0750903,"Missing"
D17-1240,P14-1096,1,0.797846,"Missing"
D17-1240,P17-1180,1,0.832551,"Missing"
D17-1240,D16-1121,1,0.831174,"Missing"
D17-1240,W09-0214,0,0.0410546,"Missing"
D17-1240,W14-3907,0,0.159317,"Missing"
D17-1240,D08-1110,0,0.180756,"Missing"
D17-1240,D14-1105,1,0.906574,"Missing"
D18-1344,E17-1088,0,0.0282016,"ks, as they allow to generalize the system on much larger corpora than the annotated dataset for the task. In recent times, there has been some interest in bilingual word embeddings, where words from two languages are embedded into the same space. The primary advantage of bilingual embeddings is in solving tasks involving reasoning across two languages, such as Machine Translation (Zou et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016) and cross-lingual IR (Vuli´c and Moens, 2015), as well as allowing transfer of models learnt on a resource-rich language on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio"
D18-1344,W16-5812,0,0.260938,"approx. 17M sentences. We train a skip-gram model for 10 iterations, with a window size of 5 and 5 negative samples, resulting in χ-gCM-Skip and ρ-gCM-Skip embeddings. To quantitatively compare the embedding models, we chose two CM tasks, one semantic (Sentiment Analysis) and one syntactic task (POS tagging). Our choice of tasks is primarily motivated by the availability of annotated CM data. There has been prior work on CM sentiment identification (Vilares and Alonso, 2016; Joshi et al., 2016; Rudra et al., 2016; Prabhu et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; AlGhamdi et al., 2016; Ghosh et al., 2016). But we are not aware of any work that utilizes pre-trained bilingual embeddings for these tasks. 4.1 Sentiment Analysis Vilares and Alonso (2016) provide 2103 sentiment annotated CM tweets. The data contains 650 positive, 529 negative and 924 neutral tweets and we split the data in 8:1:1 ratio (train:validation:test) 3069 Embedding None BiCCA BiCVM BiSkip χ-gCM-Skip ρ-gCM-Skip CM Overall 54.4 (1.3) 57.6 (3.0) 64.3 (1.3) 61.5 (1.7) 62.0 (1.9) 64.6 (2.0) Sentiment SemEval 2014 64.5 (0.6) 64.6 (1.0) 66.8 (1.0)) 66.6 (0.9) 67.4 (1.3) 67.7 (1.4) TASS 2016 61.4 (1.0) 59.5 (1.8"
D18-1344,W14-3914,1,0.762206,"s that synthetic CM data is a reasonably good proxy for real data. While our experiments show a promising direction towards obtaining bilingual embeddings for CM tasks, there are several interesting ideas that are worth exploring. In particular, the linguistic model used for generating artificial CM data only addresses the syntactic constraints of CM, but not other kinds of constraints such as lexical choice which in a particular CM context might be overly skewed towards one language (like the English words ‘school’ and ‘vote’ are more common than their Hindi translations in English-Hindi CM (Bali et al., 2014)), and semantic/pragmatic constraints that make the choice of a particular language more common in some contexts (e.g., Hindi used more commonly for negative sentiment during EnglishHindi CM (Rudra et al., 2016)). Similarly, the sense distribution of polysemous words can vary widely between a monolingual and CM corpus. For instance, the word ‘school’ in English has several meanings such as (a) an institute of education, (b) group of artists, writers etc., and (c) a large group of fish. However, in a Spanish dominant sentence or corpus, school is primarily, if not only, used in sense (a). As fu"
D18-1344,P17-2093,0,0.0308478,"generalize the system on much larger corpora than the annotated dataset for the task. In recent times, there has been some interest in bilingual word embeddings, where words from two languages are embedded into the same space. The primary advantage of bilingual embeddings is in solving tasks involving reasoning across two languages, such as Machine Translation (Zou et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016) and cross-lingual IR (Vuli´c and Moens, 2015), as well as allowing transfer of models learnt on a resource-rich language on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma e"
D18-1344,E14-1049,0,0.564879,"since all the standard bilingual word embedding techniques are designed to work on or across monolingual texts rather than on a mixture of the two languages, these techniques may not be ideal for learning embeddings for CM tasks. There are emergent syntactic structures and cross-lingual semantic associations in CM text, that do not exist in the individual monolingual corpora (Sec 3). Hence, ideally, word embeddings for CM tasks should be trained on real CM data. In this paper, we compare three popular bilingual word embedding techniques (Sec 2): Bilingual correlation based embeddings (BiCCA) (Faruqui and Dyer, 2014), Bilingual compositional model (BiCVM) (Hermann and Blunsom, 2014) and Bilingual Skip-gram (BiSkip) (Luong et al., 2015) on two tasks for CM text - sentiment analysis, a semantic task, and POS tagging, a syntactic task. On the same tasks, we also compare word embeddings learnt from synthetic CM data (generated using linguistic models as proposed in a recent work (Pratapa et al., 2018)) (Sec 3). Note that Wick et al. (2016) use artificial code mixed data to learn 3067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3067–3072 c Brussels, Belgium, Oc"
D18-1344,W16-5811,0,0.0257922,"We train a skip-gram model for 10 iterations, with a window size of 5 and 5 negative samples, resulting in χ-gCM-Skip and ρ-gCM-Skip embeddings. To quantitatively compare the embedding models, we chose two CM tasks, one semantic (Sentiment Analysis) and one syntactic task (POS tagging). Our choice of tasks is primarily motivated by the availability of annotated CM data. There has been prior work on CM sentiment identification (Vilares and Alonso, 2016; Joshi et al., 2016; Rudra et al., 2016; Prabhu et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; AlGhamdi et al., 2016; Ghosh et al., 2016). But we are not aware of any work that utilizes pre-trained bilingual embeddings for these tasks. 4.1 Sentiment Analysis Vilares and Alonso (2016) provide 2103 sentiment annotated CM tweets. The data contains 650 positive, 529 negative and 924 neutral tweets and we split the data in 8:1:1 ratio (train:validation:test) 3069 Embedding None BiCCA BiCVM BiSkip χ-gCM-Skip ρ-gCM-Skip CM Overall 54.4 (1.3) 57.6 (3.0) 64.3 (1.3) 61.5 (1.7) 62.0 (1.9) 64.6 (2.0) Sentiment SemEval 2014 64.5 (0.6) 64.6 (1.0) 66.8 (1.0)) 66.6 (0.9) 67.4 (1.3) 67.7 (1.4) TASS 2016 61.4 (1.0) 59.5 (1.8) 61.9 (1.0) 63.9 (1."
D18-1344,C16-1234,0,0.542335,"k=2 result in a gCM corpus of approx. 8M sentences. We also combine monolingual data with these gCM corpus resulting in approx. 17M sentences. We train a skip-gram model for 10 iterations, with a window size of 5 and 5 negative samples, resulting in χ-gCM-Skip and ρ-gCM-Skip embeddings. To quantitatively compare the embedding models, we chose two CM tasks, one semantic (Sentiment Analysis) and one syntactic task (POS tagging). Our choice of tasks is primarily motivated by the availability of annotated CM data. There has been prior work on CM sentiment identification (Vilares and Alonso, 2016; Joshi et al., 2016; Rudra et al., 2016; Prabhu et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; AlGhamdi et al., 2016; Ghosh et al., 2016). But we are not aware of any work that utilizes pre-trained bilingual embeddings for these tasks. 4.1 Sentiment Analysis Vilares and Alonso (2016) provide 2103 sentiment annotated CM tweets. The data contains 650 positive, 529 negative and 924 neutral tweets and we split the data in 8:1:1 ratio (train:validation:test) 3069 Embedding None BiCCA BiCVM BiSkip χ-gCM-Skip ρ-gCM-Skip CM Overall 54.4 (1.3) 57.6 (3.0) 64.3 (1.3) 61.5 (1.7) 62.0 (1.9) 64.6 (2."
D18-1344,D14-1098,0,0.0209941,"anguage (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some work that uses embeddings for CM text (Prabhu et al., 2016) (at sub-word level), we do not know of any study that systematically explores the usefulness of bilingual word embedding techniques in CM text processing. Further, we argue that since all the standard bilingual word embedding techniques are designed t"
D18-1344,W15-1521,0,0.183075,"Missing"
D18-1344,W93-0231,0,0.459063,"ngual embeddings is in solving tasks involving reasoning across two languages, such as Machine Translation (Zou et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016) and cross-lingual IR (Vuli´c and Moens, 2015), as well as allowing transfer of models learnt on a resource-rich language on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some"
D18-1344,P18-1143,1,0.875032,"ce, ideally, word embeddings for CM tasks should be trained on real CM data. In this paper, we compare three popular bilingual word embedding techniques (Sec 2): Bilingual correlation based embeddings (BiCCA) (Faruqui and Dyer, 2014), Bilingual compositional model (BiCVM) (Hermann and Blunsom, 2014) and Bilingual Skip-gram (BiSkip) (Luong et al., 2015) on two tasks for CM text - sentiment analysis, a semantic task, and POS tagging, a syntactic task. On the same tasks, we also compare word embeddings learnt from synthetic CM data (generated using linguistic models as proposed in a recent work (Pratapa et al., 2018)) (Sec 3). Note that Wick et al. (2016) use artificial code mixed data to learn 3067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3067–3072 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics multilingual embeddings for cross-lingual tasks, but their aim is to generate bilingual embeddings for monolingual or cross-lingual tasks. Our study shows that even though in certain NLP tasks specific embeddings might perform well, in general bilingual embedding techniques like BiCCA, BiCVM and BiSkip are not"
D18-1344,P16-1112,0,0.0256659,"Missing"
D18-1344,P17-1180,1,0.81192,"., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016) and cross-lingual IR (Vuli´c and Moens, 2015), as well as allowing transfer of models learnt on a resource-rich language on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some work that uses embeddings for CM text (Prabhu et al., 2016) (at sub-word level), we do not know of any study that syste"
D18-1344,D16-1121,1,0.917226,"potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some work that uses embeddings for CM text (Prabhu et al., 2016) (at sub-word level), we do not know of any study that systematically explores the usefulness of bilingual word embedding techniques in CM text processing. Further, we argue that since all the standard bilingual word embedding techniques are designed to work on or across monolingual texts rather than on a mixture o"
D18-1344,N16-1159,0,0.0204395,"n, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some work that uses embeddings for CM text (Prabhu et al., 2016) (at sub-word level), we do not know of any study that systematically explores the usefulness of bilingual word embedding techniques in CM text processing. Further, we argue that since all the standard bilingual word embedding techniques are designed to work on or across monolingual texts rathe"
D18-1344,D08-1110,0,0.119429,"on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwani et al., 2017). Consequently, in recent times, processing of CM text and speech has been receiving a growing amount of interest and attention from the NLP community (Solorio and Liu, 2008; Li and Fung, 2014; Solorio et al., 2014; Sharma et al., 2016; Rudra et al., 2016). Since CM text draws words and linguistic structures from multiple languages, use of bilingual word embeddings for processing of such text could not only be useful, but also necessary. On the other hand, while there is some work that uses embeddings for CM text (Prabhu et al., 2016) (at sub-word level), we do not know of any study that systematically explores the usefulness of bilingual word embedding techniques in CM text processing. Further, we argue that since all the standard bilingual word embedding techni"
D18-1344,W14-3907,0,0.069243,"Missing"
D18-1344,P16-1157,0,0.381282,"generate bilingual embeddings for monolingual or cross-lingual tasks. Our study shows that even though in certain NLP tasks specific embeddings might perform well, in general bilingual embedding techniques like BiCCA, BiCVM and BiSkip are not ideal for processing CM language. Embeddings learnt from CM data, even if artificially generated, performs consistently better across tasks. Our initial results are promising and provide several interesting directions for further exploration. 2 Bilingual Embeddings In the past few years, there has been a growing interest in learning bilingual embeddings (Upadhyay et al., 2016; Ruder et al., 2017) with a focus on cross-lingual transfer, which helps in building NLP models for low-resource languages. Upadhyay et al. (2016) provide an empirical comparison of four cross-lingual word embedding models varying in terms of the amount of supervision. Ruder et al. (2017) establishes the similarities among numerous cross-lingual word embedding models and shows that many models optimize for similar objectives. Along similar lines as Upadhyay et al. (2016), in this work, we chose the following three representative bilingual word embedding models for CM tasks. Training data is d"
D18-1344,L16-1655,0,0.152299,"sampling techniques, with k=2 result in a gCM corpus of approx. 8M sentences. We also combine monolingual data with these gCM corpus resulting in approx. 17M sentences. We train a skip-gram model for 10 iterations, with a window size of 5 and 5 negative samples, resulting in χ-gCM-Skip and ρ-gCM-Skip embeddings. To quantitatively compare the embedding models, we chose two CM tasks, one semantic (Sentiment Analysis) and one syntactic task (POS tagging). Our choice of tasks is primarily motivated by the availability of annotated CM data. There has been prior work on CM sentiment identification (Vilares and Alonso, 2016; Joshi et al., 2016; Rudra et al., 2016; Prabhu et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; AlGhamdi et al., 2016; Ghosh et al., 2016). But we are not aware of any work that utilizes pre-trained bilingual embeddings for these tasks. 4.1 Sentiment Analysis Vilares and Alonso (2016) provide 2103 sentiment annotated CM tweets. The data contains 650 positive, 529 negative and 924 neutral tweets and we split the data in 8:1:1 ratio (train:validation:test) 3069 Embedding None BiCCA BiCVM BiSkip χ-gCM-Skip ρ-gCM-Skip CM Overall 54.4 (1.3) 57.6 (3.0) 64.3 (1.3) 61.5 (1.7)"
D18-1344,D14-1105,1,0.885397,"orpus resulting in approx. 17M sentences. We train a skip-gram model for 10 iterations, with a window size of 5 and 5 negative samples, resulting in χ-gCM-Skip and ρ-gCM-Skip embeddings. To quantitatively compare the embedding models, we chose two CM tasks, one semantic (Sentiment Analysis) and one syntactic task (POS tagging). Our choice of tasks is primarily motivated by the availability of annotated CM data. There has been prior work on CM sentiment identification (Vilares and Alonso, 2016; Joshi et al., 2016; Rudra et al., 2016; Prabhu et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; AlGhamdi et al., 2016; Ghosh et al., 2016). But we are not aware of any work that utilizes pre-trained bilingual embeddings for these tasks. 4.1 Sentiment Analysis Vilares and Alonso (2016) provide 2103 sentiment annotated CM tweets. The data contains 650 positive, 529 negative and 924 neutral tweets and we split the data in 8:1:1 ratio (train:validation:test) 3069 Embedding None BiCCA BiCVM BiSkip χ-gCM-Skip ρ-gCM-Skip CM Overall 54.4 (1.3) 57.6 (3.0) 64.3 (1.3) 61.5 (1.7) 62.0 (1.9) 64.6 (2.0) Sentiment SemEval 2014 64.5 (0.6) 64.6 (1.0) 66.8 (1.0)) 66.6 (0.9) 67.4 (1.3) 67.7 (1.4) TASS 20"
D18-1344,D13-1141,0,0.0223443,"dding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text. 1 Introduction Word embeddings are useful for a variety of NLP tasks, as they allow to generalize the system on much larger corpora than the annotated dataset for the task. In recent times, there has been some interest in bilingual word embeddings, where words from two languages are embedded into the same space. The primary advantage of bilingual embeddings is in solving tasks involving reasoning across two languages, such as Machine Translation (Zou et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016) and cross-lingual IR (Vuli´c and Moens, 2015), as well as allowing transfer of models learnt on a resource-rich language on to a resource poor language (Adams et al., 2017; Fang and Cohn, 2017). One of the potential, yet unexplored, applications of bilingual word embeddings is in the processing of code-mixed language. Code-mixing (CM) refers to fluid alternation between two or more languages in a single conversation/sentence (Myers-Scotton, 1993). CM is a common phenomenon observed in almost all multilingual societies (Parshad et al., 2016; Rijhwan"
D19-3018,D18-1512,0,0.0801759,"Missing"
D19-3018,N19-1388,0,0.0247397,"build our NMT model for 5 different Indic Languages and English (en). These 5 Indic Languages include Bengali (bn), Hindi (hi), Malayalam (ml), Tamil (ta) and Telugu (te). The training data from OPUS contain 100-1500 thousand parallel sentences each for the mentioned Indic languages. All our models have been tested on the Indic Language Dataset (Post et al., 2012)7 which contains around 1000 parallel sentences with each sentence having 4 English references. This helps us measure multi-reference BLEU score which is often useful for relatively free word order languages. We take inspiration from Aharoni et al. (2019) to build a multilingual many-to-one and one-tomany indic neural MT system based on the state of the art transformer architecture (Vaswani et al., 6.2 Keystroke Reduction Keystroke Reduction algorithmically compares the minimum number of keystrokes required when typing interactively versus the same when manually typing. Interactive typing accounts for the keystrokes made when navigating through like ↑ , ↓ , Tab , Enter ←- each of which is one keystroke, whereas for manual typing, number of keystrokes is determined by the number of characters in the sentence. This allows us to get an approximat"
D19-3018,E14-2007,0,0.429925,"Missing"
D19-3018,P02-1040,0,0.104153,"Missing"
D19-3018,W12-3152,0,0.0265346,"Missing"
D19-3018,D13-1111,0,0.0272705,"g(yt−1 , st , ct ) 3.1 (2) For producing multiple suggestions based on the partial input, we rely on beam search decoding which features in all current NMT architectures. It selects the most probable full translation for a given input sentence. If and when the translator diverges from this full translation, a new beam search is conducted from the partial input prefix till end of sentence is encountered. A full beam search (till end of sentence) is done only for gisting. In case of suggestion, we do beam search of maximum length of 2. Beam search has been criticized for its lack of diversity (Gimpel et al., 2013), which is why, showing full sentence suggestions from beam search will present the translator with very similar suggestions. Beam search with length of 2 will produce bigrams with reasonable diversity. • Opens translation tasks to non-expert translators Expert human translators are often scarce for a large number of language pairs and are expensive to hire. In some countries (such as in the Indian subcontinent) it is easy to find multilingual speakers with native and near-native proficiency in multiple languages, and interactive MT systems enable non-expert translators to perform translation"
D19-3018,tiedemann-2012-parallel,0,0.176326,"Missing"
D19-3018,P16-1007,0,0.0532328,"nslation started off with TransType (Langlais et al., 2000), which uses a rule-based translation system. With the introduction of Statistical Machine Translation, it became easier to provide richer phrase based suggestions, which led to creation of tools such as CASMACAT (Alabau et al., 2014)3 and LILT4 . Green et al. (2014) extensively researched the user experience of such systems and compared between manual and assisted translation using various metrics. The current method of constrained decoding, in particular coupled with the advent of Neural Machine Translation (NMT), was put forward by Wuebker et al. (2016) and Knowles and Koehn (2016). All these studies have shown that ITP provides improved translation quality compared to PE, and also suggest that human translators prefer ITP over PE. However, due to heavy resource (parallel data) requirements, the available ITP systems work only for a handful of resourcerich languages such as Spanish, Chinese, French and German. In this paper, we present a proof-of-concept interactive translation system between English and five Indic languages (Bengali, Hindi, Malayalam, Tamil and Telugu) using state-of-the-art NMT models. As Indian languages are resource poor"
D19-3018,P17-4012,0,0.0425488,"the translator. This helps in getting top-k words which start with the intended prefix. • Closest to Partial Word Prefix We also devise a edit distance-based (Yujian and Bo, 2007) algorithm to rerank the beams which not only helps in suggesting sentences with partial word inputs but also helps in cases where the translator makes spelling mistakes for complex words. The latter algorithm already includes tokens which are limited by the former, but the former is faster due to lesser search complexity. We provide an option for the translator to select either one. 4 System Overview We use OpenNMT (Klein et al., 2017), an open source neural machine translation toolkit to build the MT system.5 We write a new interactive translation mechanism to accept the user input and do constrained decoding, which is plugged on top of this toolkit. This helps in keeping up with the 5 5 Interface Overview The interface is designed similar to MateCat (Federico et al., 2014) which is a open-source 6 http://opennmt.net/ 105 http://www.quillpad.in Word Coverage and Translation Gisting उसी परकार मान सक सवासथय के लए जञान क पराि Similarly , knowledge for mental health is necessary . आवशयक है उसी परकार मान सक सवासथय के"
D19-3018,2016.amta-researchers.9,0,0.0468114,"TransType (Langlais et al., 2000), which uses a rule-based translation system. With the introduction of Statistical Machine Translation, it became easier to provide richer phrase based suggestions, which led to creation of tools such as CASMACAT (Alabau et al., 2014)3 and LILT4 . Green et al. (2014) extensively researched the user experience of such systems and compared between manual and assisted translation using various metrics. The current method of constrained decoding, in particular coupled with the advent of Neural Machine Translation (NMT), was put forward by Wuebker et al. (2016) and Knowles and Koehn (2016). All these studies have shown that ITP provides improved translation quality compared to PE, and also suggest that human translators prefer ITP over PE. However, due to heavy resource (parallel data) requirements, the available ITP systems work only for a handful of resourcerich languages such as Spanish, Chinese, French and German. In this paper, we present a proof-of-concept interactive translation system between English and five Indic languages (Bengali, Hindi, Malayalam, Tamil and Telugu) using state-of-the-art NMT models. As Indian languages are resource poor, we could use only 100-1500"
E09-1015,clement-etal-2004-morphology,0,0.353154,"Missing"
E09-1015,I08-3012,0,0.191532,"aining data for supervised algorithms. The second gold standard contained 1906 word and POS category combinations. Only wordforms that did not appear in the first gold standard were included in the second one. Longest Suffix Match (LSM) In the LSM heuristic, when multiple suffixes can be applied to a word-form to stem it, we choose the longest one. Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it. It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008). This heuristic was used as the baseline for our experiments. 3.5.2 Highest Suffix Evidence (HSE) In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Siddiqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence. In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evid"
E09-1015,N07-1020,0,0.0695314,"Missing"
E09-1015,N03-1033,0,0.00337022,"to the Lightweight Stemmer. However, since our gold standard is different from that used to evaluate the Lightweight Stemmer, the comparison is not necessarily very meaningful. As shown in Table 9, in F-score comparisons, HSE seems to outperform LSM and HSE+Sup seems to outperform HSE, but the improvement in performance is not very large in the case of the second gold standard. In terms of accuracy scores, LSM outperforms HSE and HSE+Sup when evaluated against the second gold standard. 2 The Part-of-Speech tagger used was an implementation of a Cyclic Dependency Network Part-of-Speech tagger (Toutanova et al., 2003). The following feature set was used in the tagger: tag of previous word, tag of next word, word prefixes and suffixes of length exactly four, bigrams and the presence of numbers or symbols. 126 3. From the choices, select the root word-form with the highest frequency in the corpus. time and effort to create, can produce significant improvements in stemming performance. In order to assign tags to the words of the gold standard, sentences from the raw text corpus containing word-forms present in the gold standard were tagged using a POS tagger. The POS categories assigned to each word-form were"
E09-1015,W98-1232,0,0.0491463,"Missing"
E09-1015,oliver-tadic-2004-enlarging,0,0.0550367,"Missing"
E09-1015,sankaran-etal-2008-common,1,0.870478,"Missing"
E09-1015,sagot-etal-2006-lefff,0,\N,Missing
E09-1067,W02-0605,0,0.221544,"e degree and clustering coefficient of the nodes, and shortest paths between pairs of nodes. On the other hand, although it is a well known fact that the spectrum of a network can provide important information about its global structure, the use of this powerful mathematical machinery to infer global patterns in linguistic networks is rarely found in the literature. Note that spectral analysis, however, has been successfully employed in the domains of biological and social networks (Farkas et al., 2001; Gkantsidis et al., 2003; Banerjee and Jost, 2007). In the context of linguistic networks, (Belkin and Goldsmith, 2002) is the only work we are aware of that analyzes the eigenvectors to obtain a two dimensional visualize of the network. Nevertheless, the work does not study the spectrum of the graph. Recent research has shown that language and the socio-cognitive phenomena associated with it can be aptly modeled and visualized through networks of linguistic entities. However, most of the existing works on linguistic networks focus only on the local properties of the networks. This study is an attempt to analyze the structure of languages via a purely structural technique, namely spectral analysis, which is id"
E09-1067,C08-1076,1,0.892228,"ognitive phenomena can be modeled as networks, where the nodes correspond to linguistic entities and the edges denote the pairwise interaction or relationship between these entities. The study of linguistic networks has been quite popular in the recent times and has provided us with several interesting insights into the nature of language (see Choudhury and Mukherjee (to appear) for an extensive survey). Examples include study of the WordNet (Sigman and Cecchi, 2002), syntactic dependency network of words (Ferrer-i-Cancho, 2005) and network of co-occurrence of consonants in sound inventories (Mukherjee et al., 2008; Mukherjee et al., 2007). ∗ This research has been conducted during the author’s internship at Microsoft Research India. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 585–593, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 585 unweighted graph. λ is an eigenvalue of A if there is an n-dimensional vector x such that principles. It is worth mentioning here that earlier researchers have also noted the importance of the aforementioned principles. However, what was not known was how much importance one should associate with"
E09-1067,P06-2017,1,0.765799,"language are not chosen arbitrarily even though the speakers are capable of producing and perceiving a plethora of them. In contrast, these inventories show exceptionally regular patterns across the languages of the world, which is in fact, a common point of consensus in phonology. Right from the beginning of the 20th century, there have been a large number of linguistically motivated attempts (Trubetzkoy, 1969; Lindblom and Maddieson, 1988; Boersma, 1998; Clements, 2008) to explain the formation of these patterns across the consonant inventories. More recently, Mukherjee and his colleagues (Choudhury et al., 2006; Mukherjee et al., 2007; Mukherjee et al., 2008) studied this problem in the framework of complex networks. Since here we shall conduct a spectral analysis of the network defined in Mukherjee et al. (2007), we briefly survey the models and the important results of their work. Choudhury et al. (2006) introduced a bipartite network model for the consonant inventories. Formally, a set of consonant inventories is represented as a graph G = hVL , VC , Elc i, where the nodes in one partition correspond to the languages (VL ) and that in the other partition correspond to the consonants (VC ). There"
gupta-etal-2012-mining,b-etal-2010-resource,1,\N,Missing
gupta-etal-2012-mining,P06-1103,0,\N,Missing
gupta-etal-2012-mining,W07-0711,0,\N,Missing
gupta-etal-2012-mining,I08-1058,0,\N,Missing
gupta-etal-2012-mining,J98-4003,0,\N,Missing
gupta-etal-2012-mining,I08-6004,0,\N,Missing
gupta-etal-2012-mining,W09-3518,0,\N,Missing
gupta-etal-2012-mining,W09-3500,0,\N,Missing
L16-1260,W14-3914,1,0.852997,"Hindi-English (Hi-En) code-switched tweets based on a linguistic analysis and some initial experiments. Keywords: Code-Switching, Corpus (Creation, Annotation, etc.), Multilinguality, Hindi-English, Social-Media Processing, Twitter, Pragmatic Functions 1. Introduction Code-Switching (CS) or switch between two or more languages in the context of a single conversation is a wellstudied phenomenon in multilingual communities. The rise in social-media and other forms of Computer Mediated Communication (CMC) has seen CS, earlier associated more with spoken language, being used in the written form (Bali et al., 2014). The nature and extent of CS depends on a number of factors, including structural, pragmatic (functional), and socio-cultural aspects. Several studies (Labov, 1971; Joshi, 1985; Poplack, 1980) have indicated that CS is controlled by certain linguistic constraints at the structural level. At the functional level, it is generally considered as a conversational strategy to convey various distinct functions within a conversation (Barredo, 1997; Sanchez, 1983; Maschler, 1991; Blom and Gumprez, 1972). Other studies such as Annamalai (2001), Malhotra (1980), etc., investigate the social factors (e.g"
L16-1260,dey-fung-2014-hindi,0,0.0323705,"Missing"
L16-1260,D08-1110,0,0.130621,"Missing"
L16-1260,W14-3907,0,0.0387461,"c., investigate the social factors (e.g. age and socioeconomic status) effecting the nature of CS. Previous linguistic studies have looked at the structural and functional aspects of spoken and hence, small scale code-switched data. However, with the huge amount of text available on social-media there is now an opportunity to study different aspects of this phenomenon on a large scale. With the advent of speech-like conversational interaction on social-media, there has been a recent surge of interest in processing CS data. These studies are mostly in the areas of: (a) Language identification (Solorio et al., 2014), and (b) POS tagging (Solorio et al., 2008; Vyas et al., 2014). The computational processing of code-switched data is a 1644 challenging task from the perspective of linguistic understanding vis-à-vis discourse and conversational analysis, as well as computational modelling and applications to Machine Translation, Information Retrieval, and Natural Interfaces. For an in-depth understanding of why (pragmatic aspects) and how (structural aspects) people code-switch, we need data annotated at different levels. We present here a scheme and some initial experiments on annotating the functions of C"
L16-1260,D14-1105,1,0.884978,"tus) effecting the nature of CS. Previous linguistic studies have looked at the structural and functional aspects of spoken and hence, small scale code-switched data. However, with the huge amount of text available on social-media there is now an opportunity to study different aspects of this phenomenon on a large scale. With the advent of speech-like conversational interaction on social-media, there has been a recent surge of interest in processing CS data. These studies are mostly in the areas of: (a) Language identification (Solorio et al., 2014), and (b) POS tagging (Solorio et al., 2008; Vyas et al., 2014). The computational processing of code-switched data is a 1644 challenging task from the perspective of linguistic understanding vis-à-vis discourse and conversational analysis, as well as computational modelling and applications to Machine Translation, Information Retrieval, and Natural Interfaces. For an in-depth understanding of why (pragmatic aspects) and how (structural aspects) people code-switch, we need data annotated at different levels. We present here a scheme and some initial experiments on annotating the functions of CS in Hindi-English (Hi-En) CS tweets. This work goes beyond the"
L16-1260,C82-1023,0,\N,Missing
L18-1256,L16-1260,1,0.933416,"heoretical studies as well as the automatic detection of CS functions. Keywords: Code-Switching, Multilingual, Bilingual, Pragmatics, Discourse 1. Introduction Code-Switching (CS), or alternating between two or more languages in a single conversation, is a marked feature of multilingual communities. Linguists have studied this phenomenon in great detail and recently, with the rise of social media, the processing and generating of CS content has gained attention in the NLP community as well. The amount and type of code-switching depends on a number of structural, functional and social factors (Begum et al., 2016). Linguistic studies in the past have focused on two aspects that may be summarized as the ”how” and the ”why” of code-switching (Poplack, 2015). The ”how” aims to explain the grammatical principles that underlie code-switching performance; the ”why” aims to explain the function of codeswitching in discourse, defining social, pragmatic, and discourse functions of code-switching, for instance addressee specification, emphasis, or marking quotations. These studies are mostly based on limited recordings of conversations, concentrating on a small subset of functions, either linguistic, based on lo"
L18-1256,W17-0804,0,0.0405775,"Missing"
L18-1256,L16-1292,0,0.053377,"Missing"
L18-1256,S13-2052,0,0.0309721,"tity • sentiment-pair(si , si+1 ) = hx, yi with 42 values for hx, yi ∈ SENT×SENT Semantic properties • speechact-pair(si , si+1 ) = hx, yi with |SACT |2 values for x, e.g., hx, yi ∈ SACT×SACT • topic(si ) = x in a set of topics or in a distribution over topics • discourse-rel(si , si+1 ) = x ∈ a set of discourse relations SREL that connect si and si+1 • content(si ) = x where x is a semantic representation of si (e.g., vector representation, logic form) Sentiment properties • sentiment(si ) = x ∈ SENT = {-1,0,+1}, where -1 stands for negative, 0 for neutral, and +1 for positive sentiment, cf. Nakov et al. (2013). Pragmatic properties • speechact(si ) = x ∈ SACS = {question, request, command,. . . }, cf. Searle (1969). • dm(si ) = x ∈ {0, 1}, where x = 1 if si contains a discourse marker, cf. Prasad et al. (2014). Compound properties (on the CS pair level) Just like si and si+1 are combined to build a CS pair p(si , si+1 ), properties of si and si+1 can be combined to build compound properties. Some of the possible combinations are relevant with respect to the analysis of CS functions, for instance discovering the function of sentiment change. In general, we create compound properties by a) pairing ar"
L18-1256,W17-7510,1,0.836674,"he detailed analysis of language distribution in Table 1, that shows similar Span Entropy, but different CMI, M-Metric, Burstiness, and Language Entropy for the two corpora D1 and M1. Relevant for studying CS functions is the more even distribution of languages in M1 (indicating more instances of switching and less borrowing) and its larger variety of social properties (e.g., speakers with different relationship and hierarchy levels). To contrast CS for different social aspects, statistics on interactions between specific pairs of speakers can be analyzed as sub-corpora of M1, as proposed in (Pratapa and Choudhury, 2017). The local properties also show a larger proportion of positive and negative sentiment, sentiment change, and discourse relations in M1 compared to D1, which indicates that it is a more promising source for studying CS functions related to these properties. One exception are Hindi discourse relations that seem to be more prevalent in D1 compared to the movie datasets. The other movie datasets, M2 to M6 are fairly similar to M1. The proportion of word-level switches is lower in all movie datasets compared to D1. In contrast to previous work on Twitter datasets (Rudra et al., 2016), there is no"
L18-1256,D13-1066,0,0.0243069,"Missing"
L18-1256,D16-1121,1,0.345709,"ordings of conversations, concentrating on a small subset of functions, either linguistic, based on local properties of the discourse, or social and other global aspects. There are no large empirical studies on the interaction of linguistic and social aspects of CS functions. Further, there is no unified theory on the different CS functions and how they are expressed in discourse. Previous theoretical work identifies lists of functions (see e.g., Abdul-Zahra (2010)), and recent work in NLP picks specific functions and aims to identify them in large collections of CS texts (Begum et al., 2016; Rudra et al., 2016). Thus, such studies use different levels of analysis, and often confuse observations (such as observing code-switched reiteration or translation) with CS functions (reiteration is often used for emphasis), as in the following example from Begum et al. (2016) that uses the translation of an utterance for emphasis: dimaag mein bhoosa bhara hai [gloss:up in their heads with fodder]. up in their heads with fodder. The task of defining a comprehensive set of functions is very difficult, if not elusive. The same switching phenomenon, in our example translation, is used for emphasis in some communit"
nath-etal-2008-unsupervised,W06-3812,0,\N,Missing
nath-etal-2008-unsupervised,D07-1023,0,\N,Missing
nath-etal-2008-unsupervised,C04-1052,0,\N,Missing
nath-etal-2008-unsupervised,E03-1009,0,\N,Missing
nath-etal-2008-unsupervised,N06-1041,0,\N,Missing
nath-etal-2008-unsupervised,P05-3020,0,\N,Missing
nath-etal-2008-unsupervised,W00-0717,0,\N,Missing
nath-etal-2008-unsupervised,P06-3002,0,\N,Missing
nath-etal-2008-unsupervised,P93-1034,0,\N,Missing
nath-etal-2008-unsupervised,E95-1020,0,\N,Missing
nath-etal-2008-unsupervised,P07-1094,0,\N,Missing
P09-2062,P06-3002,1,0.848394,"on here, however, lies in the comparison of the topology of the syntactic and semantic DSNs, which, to the best of our knowledge, has not been explored previously. 245 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245–248, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2 Network Construction The syntactic and semantic DSNs are constructed from a raw text corpus. This work is restricted to the study of English DSNs only1 . Syntactic DSN: We define our syntactic network in a similar way as previous works in unsupervised parts-of-speech induction (cf. (Sch¨utze, 1995; Biemann, 2006)): The most frequent 200 words in the corpus (July 2008 dump of English Wikipedia) are used as features in a word window of ±2 around the target words. Thus, each target word is described by an 800-dimensional feature vector, containing the number of times we observe one of the most frequent 200 words in the respective positions relative to the target word. In our experiments, we collect data for the most frequent 1000 and 5000 target words, arguing that all syntactic classes should be represented in those. A similarity measure between target words is defined by the cosine between the feature"
P09-2062,J93-1003,0,0.0424733,"e construction of this network is inspired by (Lin, 1998). Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al., 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Nouncoordination, NN-compound, Adj-Mod. These lexicalized relations act as features for the nouns. Verbs are recorded together with their subcategorization frame, i.e. the same verb lemmas in different subcat frames would be treated as if they were different verbs. We compute log-likelihood significance between features and target nouns (as in (Dunning, 1993)) and keep only the most significant 200 features per target word. Each feature f gets a feature weight that is inversely proportional to the logarithm of the number of target words it applies on. The similarity of two target nouns is then computed as the sum of the feature weights they share. For our analysis, we restrict the graph to the most frequent 5000 target common nouns and keep only the 200 highest weighted edges per target noun. Note that the degree of a node can Figure 1: The spectrum of the syntactic and semantic DSNs of 1000 nodes. still be larger than 200 if this node is containe"
P09-2062,E09-1067,1,0.803279,"actic and semantic distributional patterns of the words of a language? This study is an initial attempt to answer this fundamental and Spectral analysis is the backbone of several techniques, such as multi-dimensional scaling, principle component analysis and latent semantic analysis, that are commonly used in NLP. In recent times, there have been some work on spectral analysis of linguistic networks as well. Belkin and Goldsmith (2002) applied spectral analysis to understand the struture of morpho-syntactic networks of English words. The current work, on the other hand, is along the lines of Mukherjee et al. (2009), where the aim is to understand not only the principles of organization, but also the global topology of the network through the study of the spectrum. The most important contribution here, however, lies in the comparison of the topology of the syntactic and semantic DSNs, which, to the best of our knowledge, has not been explored previously. 245 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245–248, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2 Network Construction The syntactic and semantic DSNs are constructed from a raw text corpus. This work is restricted t"
P09-2062,P02-1035,0,0.012507,"In our experiments, we collect data for the most frequent 1000 and 5000 target words, arguing that all syntactic classes should be represented in those. A similarity measure between target words is defined by the cosine between the feature vectors. The syntactic graph is formed by inserting the target words as nodes and connecting nodes with edge weights equal to their cosine similarity if this similarity exceeds a threshold t = 0.66. Semantic DSN: The construction of this network is inspired by (Lin, 1998). Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al., 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Nouncoordination, NN-compound, Adj-Mod. These lexicalized relations act as features for the nouns. Verbs are recorded together with their subcategorization frame, i.e. the same verb lemmas in different subcat frames would be treated as if they were different verbs. We compute log-likelihood significance between features and target nouns (as in (Dunning, 1993)) and keep only the most significant 200 features per target word. Each feature f gets a feature weight that is inversely proportional to the logarithm"
P09-2062,E95-1020,0,0.208277,"Missing"
P09-2062,nath-etal-2008-unsupervised,1,\N,Missing
P09-2062,W02-0605,0,\N,Missing
P09-2062,P98-2127,0,\N,Missing
P09-2062,C98-2122,0,\N,Missing
P13-1168,H92-1086,0,0.720756,"(2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the authors recursively split a query (or its fragment) into exactly two parts and evaluate the 3 http://www.webis.de/research/corpora final output against human annota"
P13-1168,J08-4004,0,0.273951,"sign the height of the node to the corresponding boundaries. The number of unique nested segmentations of a query of length |q |is its corresponding Catalan number7 . Boundary variables for flat and nested segmentation are illustrated with an example of each kind in Tables 1 and 2 (last column). 4.2 Krippendorff ’s α for Segmentation Krippendorff ’s α (Krippendorff, 2004) is an extremely versatile agreement coefficient, which is based on the assumption that the expected agreement is calculated by looking at the overall distribution of judgments without regard to which annotator produced them (Artstein and Poesio, 2008). 1717 7 http://goo.gl/vKQvK Hence, it is appropriate for crowdsourced annotation, where the judgments come from a large number of unrelated annotators. Moreover, it allows for different magnitudes of disagreement, which is a useful feature as we might want to differentially penalize disagreements at various levels of the tree for nested segmentation. α is defined as α=1− s2 Do = 1 − within De s2total (1) where Do and De are, respectively, the observed and expected disagreements that are measured by s2within – the variance within the annotation of an item and s2total – variance across annotati"
P13-1168,D07-1086,0,0.0852032,"and evaluation considers query segmentation as a process analogous to identification of phrases within a query which when put within double-quotes (implying exact matching of the quoted phrase in the document) leads to better IR performance. However, this is a very restricted view of the process and does not take into account the full potential of query segmentation. A more generic notion of segments leads to diverse and ambiguous definitions, making its evaluation a hard problem (see Saha Roy et. al. (2012) for a discussion on issues with evaluation). Most automatic segmentation techniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2 Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the problem of data scarcity, Hagen et. al."
P13-1168,J95-3006,0,0.717403,"d, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the"
P13-1168,D09-1030,0,0.0157825,"n generate the correct segmentation of a query within top few options. It is far from obvious how to generate these initial segmentations in a reliable manner. This may also result in an over-optimistic IAA. An ideal segmentation should be based on the annotators’ own interpretation of the query. Nevertheless, if large scale data has to be procured, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on m"
P13-1168,D08-1027,0,0.179677,"Missing"
P13-1168,P09-2047,0,0.0391687,"Missing"
P17-1180,W14-3914,1,0.682035,"m is then used with the HMM parameters to perform word-level LD. When an unknown n-gram, is encountered, its emission probability is estimated by recursively backing off to (n − 1)-gram, until we find a known n-gram. If the unigram, i.e., the token, is also unknown, then the observation of the symbol unk is used instead. 5 Dataset Creation The data for both training and testing comes primarily from Twitter because of its public API, and studies have shown the presence of codeswitching in social media (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009; Bali et al., 2014). Our experiments use monolingual and codeswitched tweets in seven languages – Dutch (nl), English (en), French (fr), German (de), Portuguese (pt), Spanish (es) and Turkish (tr). These form the set L. The choice of languages is motivated by several factors. First, LD is non-trivial as all these languages use the Latin script. Second, a large volume of tweets are generated in these languages. 1974 L1-L2 nl fr pt de tr es en nl-en fr-en pt-en de-en tr-en es-en nl-tr Third, there is annotated code-switched data available in nl-tr and en-es, which can be used for validation and testing. Lastly, we"
P17-1180,W14-3902,0,0.077652,"chniques to analyze mixed language, which can help not only in developing end-user applications, but also in conducting fundamental sociolinguistic studies. Language detection (LD) is a prerequisite to several NLP techniques. Most state-of-the-art LD systems detect a single language for an entire document or sentence. Such methods often fail to detect code-switching, which can occur within a sentence. In recent times, there has been some effort to build word-level LD for code-switching between a specific pair of languages (Nguyen and Dogru¨oz, 2013; Elfardy et al., 2013; Solorio et al., 2014; Barman et al., 2014). However, usually user-generated text (e.g., on social media) has no prior information of the languages being used. Further, as several previous social-media based studies on multilingualism have pointed out (Kim et al., 2014; Manley, 2012), lack of general wordlevel LD has been a bottleneck in studying codeswitching patterns in multilingual societies. This paper proposes a novel technique for wordlevel LD that generalizes to an arbitrarily large set of languages. The method does not require a priori information on the specific languages (potentially more than two) being mixed in an input tex"
P17-1180,L16-1260,1,0.866997,"Missing"
P17-1180,W12-2108,0,0.136935,"Missing"
P17-1180,W14-3908,1,0.800555,"Collection and Preprocessing Using the Twitter API (Twitter, 2013), we collected tweets over May-July 2015. We selected tweets identified by Twitter LD API (Twitter, 2015) as one of the languages in L. We also removed non-Latin script tweets. As preprocessing, each tweet is first tokenized using ark-twitter (Gimpel et al., 2011) and URLs, hashtags and user mentions are identified using regular expressions. We also identify emoticons, punctuation, digits, special characters, and some universal interjections and abbreviations (such as RT, aww) as universal tokens. We use an existing dictionary (Chittaranjan et al., 2014) for the latter. Let the set of tweets after preprocessing be T . 5.2 Sets W and U We use the C OVER S ET algorithm (Gella et al., 2014) on each tweet in T . It obtains a confidence score for a word wi belonging to a language lj using a Naive Bayes classifier trained on Wikipedia. These scores are used to find the minimal set of languages are required to label all the input words. If C OVER S ET detects the tweet as monolingual (i.e., one language can label all words) and the identified language is the same as the Twitter LD label, the tweet is added to the weakly-labeled set W. These tweets a"
P17-1180,W14-5152,0,0.438379,"Missing"
P17-1180,W14-5151,1,0.883718,"itter LD API (Twitter, 2015) as one of the languages in L. We also removed non-Latin script tweets. As preprocessing, each tweet is first tokenized using ark-twitter (Gimpel et al., 2011) and URLs, hashtags and user mentions are identified using regular expressions. We also identify emoticons, punctuation, digits, special characters, and some universal interjections and abbreviations (such as RT, aww) as universal tokens. We use an existing dictionary (Chittaranjan et al., 2014) for the latter. Let the set of tweets after preprocessing be T . 5.2 Sets W and U We use the C OVER S ET algorithm (Gella et al., 2014) on each tweet in T . It obtains a confidence score for a word wi belonging to a language lj using a Naive Bayes classifier trained on Wikipedia. These scores are used to find the minimal set of languages are required to label all the input words. If C OVER S ET detects the tweet as monolingual (i.e., one language can label all words) and the identified language is the same as the Twitter LD label, the tweet is added to the weakly-labeled set W. These tweets are almost certainly monolingual, as C OVER S ET has very high recall (and low precision) for detecting code-switching. As these are not"
P17-1180,P11-2008,0,0.0478243,"Missing"
P17-1180,hughes-etal-2006-reconsidering,0,0.046516,"Missing"
P17-1180,P17-2009,0,0.0203221,"Missing"
P17-1180,N13-1131,0,0.362255,"ge mixing. Also, these models do not fragment the document based on language, making language-specific analysis impossible. Document-level or sentence-level LD does not identify code-switching accurately, which can occur within a sentence. Word-level LD systems attempt to remedy this problem. Most work has been restricted to cases where two languages, known a priori, is to be detected in the input i.e, binary LD at the word-level. There has been work on Dutch-Turkish (Nguyen and Dogru¨oz, 2013), English-Bengali (Das and Gamb¨ack, 2014) and Standard and dialectal Arabic (Elfardy et al., 2013). King and Abney (2013) address wordlevel LD for bilingual documents in 30 language pairs, where the language pair is known a priori. The features for word-level LD proposed by Al-Badrashiny and Diab (2016) are languageindependent, however, at any given time, the model is only trained to tag a specific language pair. There have also been two shared task series on word-level LD: FIRE (Roy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015) focused on Indian languages and the EMNLP CodeSwitching Workshop (Solorio et al., 2014; Molina et al., 2016). These pairwise LD methods vary from dictionary-based to compl"
P17-1180,P12-3005,0,0.0345021,"521 G WLD: The Proposed Method Initial 0.838 0.825 Reestimated 0.963 0.914 IsMix 0.600 0.733 0.783 0.783 0.692 0.837 0.88 Table 2: Performance of LD Systems on Test Set languages, which is rare in general. We distributed 3000 tweets between the four annotators (monolingual and code-switched tweets from C OVER S ET). Disagreements were settled between the annotators and a linguist. A subset of the annotated tweets form the validation and test sets (Table 1), and were removed from W and U. 6 Experiments and Results We compare G WLD with three existing systems: L INGUINI (Prager, 1999), L ANGID (Lui and Baldwin, 2012), and P OLYGLOT (Lui et al., 2014). None of these perform word-level LD, however, L ANGID and P OLYGLOT return a list of languages with confidence scores for the input. Since codeswitching with more than two languages is absent in our dataset, we consider up to two language labels. We define the tweet to be monolingual if the difference between the confidence values for the top two languages is greater than a parameter δ. Otherwise, it is assumed to be code-switched with the top two languages. δ is tuned independently for the two LD systems on the validation set by maximizing the metric L1 L2"
P17-1180,Q14-1003,0,0.0390723,"Namee, 2005). However, there has been renewed interest with the amount of user-generated content on the web. Such text poses unique challenges such as short length, misspelling, idiomatic expressions and acronyms (Carter et al., 2013; Goldszmidt et al., 2013). Xia et al. (2009), Tromp and Pechenizkiy (2011) and Lui and Baldwin (2012) created LD systems for monolingual sentences, web pages and tweets. Zhang et al. (2016) built an unsupervised model to detect the majority language in a document. There has also been document-level LD that assigns multiple language to each document (Prager, 1999; Lui et al., 2014). However, documents were synthetically generated, restricted to inter-sentential language mixing. Also, these models do not fragment the document based on language, making language-specific analysis impossible. Document-level or sentence-level LD does not identify code-switching accurately, which can occur within a sentence. Word-level LD systems attempt to remedy this problem. Most work has been restricted to cases where two languages, known a priori, is to be detected in the input i.e, binary LD at the word-level. There has been work on Dutch-Turkish (Nguyen and Dogru¨oz, 2013), English-Ben"
P17-1180,W16-5805,0,0.321269,"Missing"
P17-1180,D13-1084,0,0.128911,"Missing"
P17-1180,D16-1121,1,0.427584,"Missing"
P17-1180,E09-1099,0,0.0607879,"Missing"
P18-1143,P13-2037,0,0.252693,"Missing"
P18-1143,W12-2703,0,0.0287349,". We would also like to point out that the choice of experimenting with a much smaller set of tweets, only 50K per language, was made because the number of generated tweets even from this small set of monolingual tweet pairs is almost prohibitively large to allow experimentation with several models and their respective configurations. 4 Approach Language modeling is a very widely researched topic (Rosenfeld, 2000; Bengio et al., 2003; Sundermeyer et al., 2015). In recent times, deep learning has been successfully employed to build efficient LMs (Mikolov et al., 2010; Sundermeyer et al., 2012; Arisoy et al., 2012; Che et al., 2017). 3 https://github.com/clab/fast align Baheti et al. (2017) recently showed that there is significant effect of the training curriculum, that is the order in which data is presented to an RNNbased LM, on the perplexity of the learnt EnglishSpanish CM language model on tweets. Along similar lines, in this study we focus our experiments on training curriculum, especially regarding the use of gCM data during training, which is the primary contribution of this paper. We do not attempt to innovate in terms of the architecture or computational structure of the LM, and use a standa"
P18-1143,W17-7509,1,0.929791,"h smaller set of tweets, only 50K per language, was made because the number of generated tweets even from this small set of monolingual tweet pairs is almost prohibitively large to allow experimentation with several models and their respective configurations. 4 Approach Language modeling is a very widely researched topic (Rosenfeld, 2000; Bengio et al., 2003; Sundermeyer et al., 2015). In recent times, deep learning has been successfully employed to build efficient LMs (Mikolov et al., 2010; Sundermeyer et al., 2012; Arisoy et al., 2012; Che et al., 2017). 3 https://github.com/clab/fast align Baheti et al. (2017) recently showed that there is significant effect of the training curriculum, that is the order in which data is presented to an RNNbased LM, on the perplexity of the learnt EnglishSpanish CM language model on tweets. Along similar lines, in this study we focus our experiments on training curriculum, especially regarding the use of gCM data during training, which is the primary contribution of this paper. We do not attempt to innovate in terms of the architecture or computational structure of the LM, and use a standard LSTM-based RNN LM (Sundermeyer et al., 2012) for all our experiments. Indee"
P18-1143,W14-3902,0,0.169444,"one during author’s internship at Microsoft Research 1 According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely impor"
P18-1143,N13-1073,0,0.189909,"Missing"
P18-1143,L16-1292,0,0.0811569,"Missing"
P18-1143,C10-2043,0,0.0190504,"Missing"
P18-1143,P03-1054,0,0.369962,"dicates that the two grammars are ‘equivalent’ at the code-switch point. More importantly, it shows that switching languages at this point does not require another switch later in the sentence. If every switch-point in the generated sentence abides by the EC, the generated sentence is allowed by the EC theory. 2.3 System Description We assume that the input to the generation model is a pair of parallel sentences in L1 and L2 , along with word level alignments. For our experiments, L1 and L2 are English and Spanish, and Sec 3.2 describes how we create the input set. We use the Stanford Parser (Klein and Manning, 2003) to parse the English sentence. Projecting parses. We use the alignments to project the English parse tree onto the Spanish sentence in two steps: (1) We first replace every word in the English parse tree with its Spanish equivalent (2) We re-order the child nodes of each internal node in the tree such that their left-to-right order is as in the Spanish sentence. For instance, after replacing every English word in Fig.1(a) with its corresponding Spanish word, we interchange the positions of casa and blanca to arrive Fig.1(b). For a pair of parallel sentences that follow all the assumptions of"
P18-1143,D14-1098,0,0.223803,"bine data from monolingual data sources in both languages (Weng et al., 1997). Factored models: Gebhardt (2011) uses Factored Language Models for rescoring n-best lists during ASR decoding. The factors used include POS tags, CS point probability and LID. In Adel et al.(2014b; 2014a; 2013) RNNLMs are combined with n-gram based models, or converted to backoff models, giving improvements in perplexity and mixed error rate. Models that incorporate linguistic constraints: Li and Fung (2013) use inversion constraints to predict CS points and integrates this prediction into the ASR decoding process. Li and Fung (2014) integrates Functional Head constraints (FHC) for code-switching into the Language Model for Mandarin-English speech recognition. This work uses parsing techniques to restrict the lattice paths during decoding of speech to those permissible under the FHC theory. Our method instead imposes grammatical constraints (EC theory) to generate synthetic data, which can potentially be used to augment real CM data. This allows flexibility to deploy any sophisticated LM architecture and the synthetic data generated can also be used for CM tasks other than speech recognition. Training curricula for CM: Ba"
P18-1143,W93-0231,0,0.281876,"e linguistic theory used to generate the data, and on the other hand an indication that the approach in general is promising and can help solve the issue of data scarcity for a variety of NLP tasks for CM text and speech. 2 Generating Synthetic Code-mixed Data There is a large and growing body of linguistic research regarding the occurrence, syntactic structure and pragmatic functions of codemixing in multilingual communities across the world. This includes many attempts to explain the grammatical constraints on CM, with three of the most widely-accepted being the EmbeddedMatrix (Joshi, 1985; Myers-Scotton, 1993, 1995), the Equivalence Constraint (EC) (Poplack, 1980; Sankoff, 1998) and the Functional Head Constraint (DiSciullo et al., 1986; Belazi et al., 1994) theories. For our experiments, we generate CM sentences as per the EC theory, since it explains a range of interesting CM patterns beyond lexical substitution and is also suitable for computational modeling. Further, in a brief human-evaluation we conducted, we found that it is representative of real CM usage. In this section, we list the assumptions made by the EC theory, briefly explain the theory, and then describe how we generate CM senten"
P18-1143,C16-1234,0,0.0560473,"g of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM from data. This implies that the amount of data required would not just be twice, but probably 10 or"
P18-1143,P17-1180,1,0.637233,"-switching or code-mixing (CM) refers to the juxtaposition of linguistic units from two or more languages in a single conversation or sometimes even a single utterance.1 It is quite commonly observed in speech conversations of multilingual societies across the world. Although, traditionally, CM has been associated with informal or casual speech, there is evidence that in several societies, such as urban India and Mexico, CM has become the default code of communication (Parshad et al., 2016), and it has also pervaded written text, especially in computer-mediated communication and social media (Rijhwani et al., 2017). ∗ Work done during author’s internship at Microsoft Research 1 According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et a"
P18-1143,D16-1121,1,0.762587,"as code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM from data. This implies that the amount of data required would not just be twice, but probably 10 or 100 times more than t"
P18-1143,N16-1159,0,0.045539,"nter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM from data. This implies that the amount of data required would not just be twice"
P18-1143,D08-1110,0,0.121512,"According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM from data. This implies tha"
P18-1143,W14-3907,0,0.130017,"Missing"
P18-1143,D14-1105,1,0.883765,"icrosoft Research 1 According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more general, we will use code-mixing in this paper to mean both. It is, therefore, imperative to build NLP technology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech (Li and Fung, 2013, 2014; Gebhardt, 2011; Sitaram et al., 2016), and tasks like language identification (Solorio et al., 2014; Barman et al., 2014), POS tagging (Vyas et al., 2014; Solorio and Liu, 2008), parsing and sentiment analysis (Sharma et al., 2016; Prabhu et al., 2016; Rudra et al., 2016) for CM text. Nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM system. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of CM fro"
sankaran-etal-2008-common,I08-7013,1,\N,Missing
saravanan-etal-2012-empirical,A97-1028,0,\N,Missing
saravanan-etal-2012-empirical,P04-1053,0,\N,Missing
saravanan-etal-2012-empirical,C10-2019,1,\N,Missing
saravanan-etal-2012-empirical,C96-1079,0,\N,Missing
saravanan-etal-2012-empirical,P05-1045,0,\N,Missing
W07-0212,P06-3002,0,0.0310171,"ic and English the least phonemic orthography. This correlation calls for further investigation. 87 Throughout the present discussion, we have focussed on spell-checkers that ignore the context; consequently, many of the aforementioned results, especially those involving spelling correction, are valid only for context-insensitive spell-checkers. Nevertheless, many of the practically useful spellcheckers incorporate context information and the current analysis on SpellNet can be extended for such spell-checkers by conceptualizing a network of words that capture the word co-occurrence patterns (Biemann, 2006). The word co-occurrence network can be superimposed on SpellNet and the properties of the resulting structure can be appropriately analyzed to obtain similar bounds on hardness of context-sensitive spell-checkers. We deem this to be a part of our future work. Another way to improve the study could be to incorporate a more realistic measure for the orthographic similarity between the words. Nevertheless, such a modification will have no effect on the analysis technique, though the results of the analysis may be different from the ones reported here. Appendix A: Derivation of the Probability of"
W07-0212,J93-2003,0,0.00673855,"he resulting structure can be appropriately analyzed to obtain similar bounds on hardness of context-sensitive spell-checkers. We deem this to be a part of our future work. Another way to improve the study could be to incorporate a more realistic measure for the orthographic similarity between the words. Nevertheless, such a modification will have no effect on the analysis technique, though the results of the analysis may be different from the ones reported here. Appendix A: Derivation of the Probability of RWE We take a noisy channel approach, which is a common technique in NLP (for example (Brown et al., 1993)), including spellchecking (Kernighan et al., 1990). Depending on the situation. the channel may model typing or OCR errors. Suppose that a word w, while passing through the channel, gets transformed to a word w0 . Therefore, the aim of spelling correction is to find the w∗ ∈ Λ (the lexicon), which maximizes p(w∗ |w0 ), that is argmax p(w|w0 ) = argmax p(w0 |w)p(w) w∈Λ w∈Λ (9) The likelihood p(w0 |w) models the noisy channel, whereas the term p(w) is traditionally referred to as the language model (see (Jurafsky and Martin, 2000) for an introduction). In this equation, as well as throughout th"
W07-0212,P06-2017,1,0.875536,"highest in Hindi followed by Bengali and English. A similar observation has been previously reported in (Bhatt et al., 2005) for RWEs in Bengali and English. Apart from providing insight into spell-checking, the complex structure of SpellNet also reveals the self-organization and evolutionary dynamics underlying the orthographic properties of natural languages. In recent times, complex networks have been successfully employed to model and explain the structure and organization of several natural and social phenomena, such as the foodweb, protien interaction, formation of language inventories (Choudhury et al., 2006), syntactic structure of languages (i Cancho and Sol´e, 2004), WWW, social collaboration, scientific citations and many more (see (Albert and Barab´asi, 2002; Newman, 2003) and references therein). This work is inspired by the aforementioned models, and more specifically a couple of similar works on phonological neighbors’ network of words (Kapatsinski, 2006; Vitevitch, 2005), which try to explain the human perceptual and cognitive processes in terms of the organization of the mental lexicon. The rest of the paper is organized as follows. Section 2 defines the structure and construction proced"
W07-0212,J00-4006,0,0.00350553,"oisy channel approach, which is a common technique in NLP (for example (Brown et al., 1993)), including spellchecking (Kernighan et al., 1990). Depending on the situation. the channel may model typing or OCR errors. Suppose that a word w, while passing through the channel, gets transformed to a word w0 . Therefore, the aim of spelling correction is to find the w∗ ∈ Λ (the lexicon), which maximizes p(w∗ |w0 ), that is argmax p(w|w0 ) = argmax p(w0 |w)p(w) w∈Λ w∈Λ (9) The likelihood p(w0 |w) models the noisy channel, whereas the term p(w) is traditionally referred to as the language model (see (Jurafsky and Martin, 2000) for an introduction). In this equation, as well as throughout this discussion, we shall assume a unigram language model, where p(w) is the normalized frequency of occurrence of w in a standard corpus. We define the probability of RWE for a word w, prwe (w), as follows X prwe (w) = p(w0 |w) (10) w0 ∈Λ w6=w0 Stated differently, prwe (w) is a measure of the probability that while passing through the channel, w gets transformed into a form w0 , such that w0 ∈ Λ and w0 6= w. The probability of RWE in the language, denoted by prwe (Λ), can then be defined in terms of the probability prwe (w) as fol"
W07-0212,C90-2036,0,0.156032,"lyzed to obtain similar bounds on hardness of context-sensitive spell-checkers. We deem this to be a part of our future work. Another way to improve the study could be to incorporate a more realistic measure for the orthographic similarity between the words. Nevertheless, such a modification will have no effect on the analysis technique, though the results of the analysis may be different from the ones reported here. Appendix A: Derivation of the Probability of RWE We take a noisy channel approach, which is a common technique in NLP (for example (Brown et al., 1993)), including spellchecking (Kernighan et al., 1990). Depending on the situation. the channel may model typing or OCR errors. Suppose that a word w, while passing through the channel, gets transformed to a word w0 . Therefore, the aim of spelling correction is to find the w∗ ∈ Λ (the lexicon), which maximizes p(w∗ |w0 ), that is argmax p(w|w0 ) = argmax p(w0 |w)p(w) w∈Λ w∈Λ (9) The likelihood p(w0 |w) models the noisy channel, whereas the term p(w) is traditionally referred to as the language model (see (Jurafsky and Martin, 2000) for an introduction). In this equation, as well as throughout this discussion, we shall assume a unigram language m"
W07-1309,W04-0103,1,0.916152,"er epenthesis or insertion as an APO, because epenthesis is not observed for the case of the change affecting BVI. The motivation behind defining APOs rather than representing the change in terms of rewrite rules is as follows. Rewrite rules are quite expressive and therefore, it is possible to represent complex phonological changes using a single rewrite rule. On the other hand, APOs are simple phonological changes that can be explained independently in terms of phonetic factors (Ohala, 1993). In fact, there are also computational models satisfactorily accounting for cases of vowel deletion (Choudhury et al., 2004; Choudhury et al., 2006b) and assimilation (Dras et al., 2003). Table 3 shows the derivation of the SCB verb forms from classical Bengali in terms of APOs. The derivations are constructed based on the data provided in (Chatterji, 1926). 2.3 Functional Explanation for Change of BVI Let Λ0 be the lexicon of classical Bengali verb forms. Let Θ : θ1 , θ2 , · · · θr be a sequence of r APOs. Application of an APO on a lexicon implies the application of the operator on every word of the lexicon. The sequence of operators Θ, thus, represent a dialect obtained through the process of change from Λ0 , w"
W07-1309,J03-1001,0,\N,Missing
W07-1313,P06-2017,1,0.894895,"el communities the constituent nodes (read vowels) are largely uncorrelated in terms of their features indicating that they are formed based on the principle of maximal perceptual contrast. However, in the rest of the communities, strong correlations are reflected among the constituent vowels with respect to their features indicating that it is the principle of feature economy that binds them together. 1 Introduction Linguistic research has documented a wide range of regularities across the sound systems of the world’s languages (Liljencrants and Lindblom, 1972; Lindblom, 1986; de Boer, 2000; Choudhury et al., 2006; Mukherjee et al., 2006a; Mukherjee et al., 2006b). Functional phonologists argue that such regularities are the consequences of certain general principles like maximal perceptual contrast (Liljencrants and Lindblom, 1972), which is desirable between the phonemes of a language for proper perception of each individual phoneme in a noisy environment, ease of articulation (Lindblom and Maddieson, 1988; de Boer, 2000), which requires that the sound systems of all languages are formed of certain universal (and highly frequent) sounds, and ease of learnability (de Boer, 2000), which is required so"
W07-1313,J03-1001,0,0.0263177,"nds, and ease of learnability (de Boer, 2000), which is required so that a speaker can learn the sounds of a language with minimum effort. In the study of vowel systems the optimizing principle, which has a long tradition (Jakobson, 1941; Wang, 1968) in linguistics, is maximal perceptual contrast. A number of numerical studies based on this principle have been reported in literature (Liljencrants and Lindblom, 1972; Lindblom, 1986; Schwartz et al., 1997). Of late, there have been some attempts to explain the vowel systems through multi agent simulations (de Boer, 2000) and genetic algorithms (Ke et al., 2003); all of these experiments also use the principle of perceptual contrast for optimization purposes. An exception to the above trend is a school of linguists (Boersma, 1998; Clements, 2004) who argue that perceptual contrast-based theories fail to account for certain fundamental aspects such as the patterns of co-occurrence of vowels based on similar acoustic/articulatory features1 observed across 1 In linguistics, features are the elements, which distinguish one phoneme from another. The features that describe the vowles can be broadly categorized into three different classes namely the height"
W09-0907,P06-2017,1,0.762858,"rms of its evolution, acquisition and change. In this work, we attempt to investigate the diversity that exists across the consonant inventories of the world’s languages through an evolutionary framework based on network growth. The use of a network based model is motivated from the fact that in the recent years, complex networks have proved to be an extremely suitable framework for modeling and studying the structure and dynamics of linguistic systems (Cancho and Sol´e, 2001; Dorogovtsev and Mendes, 2001; Cancho and Sol´e, 2004; Sol´e et al., 2005). Along the lines of the study presented in (Choudhury et al., 2006), we model the structure of the inventories through a bipartite network, which has two different sets of nodes, one labeled by the languages and the other by the consonants. Edges run in between these two sets depending on whether a particular consonant is found in a particular language. This network is termed the Phoneme–Language Network or PlaNet in (Choudhury et al., 2006). We construct five such networks that respectively represent the consonant inventories belonging to the five maIn this paper, we attempt to explain the emergence of the linguistic diversity that exists across the consonan"
W09-3002,sankaran-etal-2008-common,1,0.820209,"the specific language and task. The complete framework supports 11 categories at the top level with 32 types at the second level to represent the main POS categories and their sub-types. Further, 18 morphological attributes or features are associated with the types. The framework can thus, be used to derive a flat tagset of only 11 categories or a complex three level tagset of several thousand tags depending on the language and/or application. Figure 1 shows a schematic of the ILPOST framework. The current framework has been used to derive maximally specified tagsets for Bangla and Hindi (see Baskaran et al. (2008) for the descriptions of the tagsets), which have been used to design the experiments presented in this paper. 3 Annotation Tool Though a number of POS annotation tools are available none are readily suitable for hierarchical tagging. The tools from other domains (like discourse annotation, for example) that use hierarchical tagsets require considerable customization for the task described here. Thus, in order to facilitate the task of word-level linguistic annotation for complex tagsets we developed a generic annotation tool. The annotation tool can be customized to work for any tagset that h"
W09-3002,W07-1519,0,0.0614203,"and the level of expertise required, the productivity or the benefit can be measured in terms of the reliability and usability of the end-product, i.e., the annotated dataset. It is thus no surprise that considerable effort has gone into developing techniques and tools that can effectively boost the benefit-to-cost ratio of the annotation process. These include, but are not limited to: (a) exploiting the reach of the web to reduce the effort required for annotation (see, e.g., Snow et al. (2008) and references therein) (b) smartly designed User Interfaces for aiding the annotators (see, e.g., Eryigit (2007); Koutsis et al. (2007); Reidsma et al. (2004)) (c) using supervised learning to bootstrap a small annotated dataset to automatically label a larger corpus and getting it corrected by human annotators (see, e.g., Tomanek et al. (2007); Wu et al. (2007)) (d) Active Learning (Ringger et al. 2007) where only those data-points which are directly relevant for training are presented for manual annotation. Methods exploiting the web-users for linguistic annotation are particularly popular these days, presumably because of the success of the ESPGame (von Ahn and Dabbish, 2004) and its successors in im"
W09-3002,W07-1516,0,0.0542637,"Missing"
W09-3002,D08-1027,0,0.151323,"Missing"
W09-3002,W07-1521,0,0.016278,"echniques and tools that can effectively boost the benefit-to-cost ratio of the annotation process. These include, but are not limited to: (a) exploiting the reach of the web to reduce the effort required for annotation (see, e.g., Snow et al. (2008) and references therein) (b) smartly designed User Interfaces for aiding the annotators (see, e.g., Eryigit (2007); Koutsis et al. (2007); Reidsma et al. (2004)) (c) using supervised learning to bootstrap a small annotated dataset to automatically label a larger corpus and getting it corrected by human annotators (see, e.g., Tomanek et al. (2007); Wu et al. (2007)) (d) Active Learning (Ringger et al. 2007) where only those data-points which are directly relevant for training are presented for manual annotation. Methods exploiting the web-users for linguistic annotation are particularly popular these days, presumably because of the success of the ESPGame (von Ahn and Dabbish, 2004) and its successors in image annotation. A more recent study by (Snow et al., 2008) shows that annotated data obtained from non-expert anonymous web-users is as good as those obtained from experts. However, unlike the game model, here the task is distributed among non-experts"
W09-3002,W07-1502,0,\N,Missing
W11-3501,I08-5014,0,0.044485,"Missing"
W11-3501,W09-3518,0,0.018072,"ique for identifying abbreviations, the Indic IMEs presently list-lookup based approach where very common abbreviawhile the correct transliterations should have been either िेंडस [freɳɖs] and टरे न [ʈren], or their original Roman spellings (which might look a little informal style of writing, but not unsual). It should be possible to handle foreign origin words through a two step process: first, identification of the origin of the word using a classifier, and second, transliteration of nonnative words using a different statistical transliterator or by using different sets of rules. In fact, Khapra and Bhattacharyya (2009) and 6 Chinnakotla et al (2010), have both reported improved accuracies in transliteration tasks through origin detection. There are also instances of an English word with Hindi inflections, as in “computeron” [kəmpjuʈərõ] (computer + Hindi plural marker on), and Hindi words with English inflections – “sadaks” [səɽəkõ] (sadak meaning road in Hindi + English plural marker s). Such cases are rare in the dataset, and it might be much harder a challenge to automatically identify morpheme level code-mixing and subsequent transliteration. We do not know of any previous studies addressing this issue,"
W11-3501,J98-4003,0,0.0826804,"s or a combination of both. These Machine Transliteration systems may be used as Input Method Editors (IMEs) for desktop application, e.g., Baraha1 or as web applications, e.g., Google Transliterate 2 and Quillpad3. Microsoft Indic Language Input Tool (MSILIT)4 supports both a desktop as well as a web-based version. While all the above systems are popular and seem to serve their purpose adequately, there has not been any systematic evaluation to identify and address common problems that they may face, either specific to the languages concerned or due to the process of back-transliteration. As Knight and Graehl (1998) point out back-transliteration is “less forgiving” than forward transliteration for there may be many ways to transliterate a word in another script but there is only one way in which a transliterated word can be rendered back in its native form. For example, “London” Abstract Back-transliteration based Input Method Editors are very popular for Indian Languages. In this paper we evaluate two such Indic language systems to help understand the challenge of designing a back-transliteration based IME. Through a detailed error-analysis of Hindi, Bangla and Telugu data, we study the role of phonolo"
W11-3501,I08-3009,0,0.0266751,"d ambiguity in the transliteration. The impact of word-origin on back-transliteration is discussed in the context of codeswitching. We also explore the role of word-level context to help overcome some of these challenges. 1 Introduction Automatic Machine Transliteration finds practical use in various Natural Language Processing applications like Machine Translation, Mono lingual and Cross lingual information retrieval. Backward transliteration – the reverse process of converting a transliterated word into its native script, has been employed as a popular mechanism for multilingual text-input (Sandeva et al, 2008; Ehara and Tanaka-Ishii, 2008). This has given rise to many Input Method Editors (IME)s that allow the use of a normal QWERTY keyboard to input text in non-Roman scripts like Japanese, Chinese, Arabic and several Indic languages Roman transliteration is widely used for inputting Indian languages in a number of domains. A lack of standard keyboards, a large number of scripts, as well as familiarity with English and QWERTY keyboards has given rise 1 http://www.baraha.com/ 2 http://www.google.com/transliterate/ 3 http://quillpad.in/hindi/ 4 http://specials.msn.co.in/ilit/Hindi.aspx 1 Proceedings"
W11-3501,b-etal-2010-resource,1,\N,Missing
W11-3501,I08-1058,0,\N,Missing
W13-2306,H92-1086,0,0.047327,"agen et al., 2011; Saha Roy et al., 2012). The issue of granularity is effectively addressed in nested annotation, because the annotator is expected to mark the most atomic segments (such as named entities and multiword expressions) and then recursively combine them to obtain larger segments. Certain amount of ambiguity, that may arise because of lack of specific guidelines on the number of valid segments at the last level (i.e., topmost level of the nested segmentation tree), can also be resolved by forcing the annotator to recursively divide the sentence/query always into exactly two parts (Abney, 1992; Bali et al., 2009). The present study is an extension of our recent work (Ramanath et al., 2013) on analysis of the effectiveness of crowdsourcing for query and sentence segmentation. We introduced a novel IAA metric based on Kripendorff’s α, and showed that while the apparent agreement between the annota2 tors in a crowdsourced experiment might be high, the chance corrected agreement is actually low for both flat and nested segmentations (as compared to gold annotations obtained from three experts). The reason for the apparently high agreement is due to an inherent bias of the crowd to divi"
W13-2306,J11-4006,0,0.0186199,"gmentation(s) from a set of noisy annotations. The study uses the same experimental setup and annotated datasets as described in (Ramanath et al., 2013). Nevertheless, for the sake of readability and self-containedness, the relevant details will be mentioned here again. We do not know of any previous work that compares flat and nested schemes of annotation. In fact, Artstein and Poesio (2008), in a detailed survey of IAA metrics and their usage in NLP, mention that defining IAA metrics for trees (hierarchical annotations) is a difficult problem due to the existence of overlapping annotations. Vadas and Curran (2011) and Brants (2000) discuss measuring IAA of nested segmentations employing the concepts of precision, recall, and f-score. However, neither of these studies apply statistical correction for chance agreement. 3 Entailment: Definition and Modeling In this section, we shall introduce certain notations and use them to formalize the notion of entailment, which in turn, is used for the computation of agreement between flat and nested segmentations. Although we shall develop the whole framework in the context of queries, it is applicable to sentence segmentation and, in fact, more generally to any fl"
W13-2306,J08-4004,0,0.150517,"ends this work by introducing a metric to compare across flat and nested segmentations that enables us to further analyze the reliability of the crowdsourced annotations. This metric is then employed to identify the optimal flat segmentation(s) from a set of noisy annotations. The study uses the same experimental setup and annotated datasets as described in (Ramanath et al., 2013). Nevertheless, for the sake of readability and self-containedness, the relevant details will be mentioned here again. We do not know of any previous work that compares flat and nested schemes of annotation. In fact, Artstein and Poesio (2008), in a detailed survey of IAA metrics and their usage in NLP, mention that defining IAA metrics for trees (hierarchical annotations) is a difficult problem due to the existence of overlapping annotations. Vadas and Curran (2011) and Brants (2000) discuss measuring IAA of nested segmentations employing the concepts of precision, recall, and f-score. However, neither of these studies apply statistical correction for chance agreement. 3 Entailment: Definition and Modeling In this section, we shall introduce certain notations and use them to formalize the notion of entailment, which in turn, is us"
W13-2306,D07-1086,0,0.0305135,"? • How to compare the agreement between the flat and the nested annotations? • How can we identify or construct the optimal or error-free flat annotations from a noisy mixture of nested and flat annotations? In this paper, we introduce the concept of “entailment of a flat annotation by a nested annotation”. For a given linguistic unit (a query or a sentence, for example), a nested annotation is said to 1 43 https://www.mturk.com/mturk/welcome (see Hagen et al. (2011) for a survey). Automatic query segmentation algorithms are typically evaluated against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the issue of data scarcity, Hagen et al. (2011) created a large set of manually segmented queries through crowdsourcing2 . However, their approach has certain limitations because the crowd is already provided with a few possible segmentations of a query to choose from. Nevertheless, if large scale data has to be procured crowdsourcing seems to be the only efficient and effective model for t"
W13-2306,J95-3006,0,0.25218,"d problems of flat and nested annotations using the examples of sentence and query segmentation, these issues are generic and typical of any flat annotation scheme which tries to flatten or approximate an underlying hierarchical structure. There are three important research questions pertaining to the linguistic annotations of this kind: 2 Background Segmentation or chunking of NL text is a wellstudied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) define it as Noun Group and Verb Group based only on local surface information. Chunking is an important preprocessing step towards parsing. Like chunking, query segmentation is an important step towards query understanding and is generally believed to be useful for Web search • How to measure the true IAA and the quality of the flat annotations? • How to compare the agreement between the flat and the nested annotations? • How can we identify or construct the optimal or error-free flat annotations from a noisy mixture of nested and flat annotations? In this paper, we introduce the concept of"
W13-2306,brants-2000-inter,0,0.0681825,"noisy annotations. The study uses the same experimental setup and annotated datasets as described in (Ramanath et al., 2013). Nevertheless, for the sake of readability and self-containedness, the relevant details will be mentioned here again. We do not know of any previous work that compares flat and nested schemes of annotation. In fact, Artstein and Poesio (2008), in a detailed survey of IAA metrics and their usage in NLP, mention that defining IAA metrics for trees (hierarchical annotations) is a difficult problem due to the existence of overlapping annotations. Vadas and Curran (2011) and Brants (2000) discuss measuring IAA of nested segmentations employing the concepts of precision, recall, and f-score. However, neither of these studies apply statistical correction for chance agreement. 3 Entailment: Definition and Modeling In this section, we shall introduce certain notations and use them to formalize the notion of entailment, which in turn, is used for the computation of agreement between flat and nested segmentations. Although we shall develop the whole framework in the context of queries, it is applicable to sentence segmentation and, in fact, more generally to any flat and nested anno"
W13-2306,P13-1168,1,0.75742,"essed in nested annotation, because the annotator is expected to mark the most atomic segments (such as named entities and multiword expressions) and then recursively combine them to obtain larger segments. Certain amount of ambiguity, that may arise because of lack of specific guidelines on the number of valid segments at the last level (i.e., topmost level of the nested segmentation tree), can also be resolved by forcing the annotator to recursively divide the sentence/query always into exactly two parts (Abney, 1992; Bali et al., 2009). The present study is an extension of our recent work (Ramanath et al., 2013) on analysis of the effectiveness of crowdsourcing for query and sentence segmentation. We introduced a novel IAA metric based on Kripendorff’s α, and showed that while the apparent agreement between the annota2 tors in a crowdsourced experiment might be high, the chance corrected agreement is actually low for both flat and nested segmentations (as compared to gold annotations obtained from three experts). The reason for the apparently high agreement is due to an inherent bias of the crowd to divide a piece of text in roughly two equal parts. The present study extends this work by introducing"
W14-3908,N13-1131,0,0.058004,"sitive and depend on various structural restrictions (Muysken, 2001; Poplack, 1980). Special Character Features: They capture the existence of special characters and numbers in the token. Tweets contain various entities like hashtags, mentions, links, smileys, etc., which are signaled by #, @ and other special characters. Lexicon Features: These features indicate the existence of a token in lexicons. Common words in a language and named entities can be curated into finite, manageable lexicons and were therefore used for cases where such data was available. Character n-gram features: Following King and Abney (2013), we also used charagter n-grams for n=1 to 5. However, instead of directly using the n-grams as features in the CRF, we trained two binary maximum entropy classifiers to identify words of lang1 and lang2. The classifiers returned the probability that a word is of lang1 (or lang2), which were then binned into 10 equal buckets and used as features. The features are listed in Table 1. 3.1 English Spanish English Spanish Table 3: External resources used in the task. Table 2: Number of tweets retrieved for the various datasets. 3 Language For 1 3.2 Feature extraction and labeling Named entities fo"
W14-3908,D13-1084,0,0.0659137,"Missing"
W14-3908,quasthoff-etal-2006-corpus,0,0.026609,"of lang1 (or lang2), which were then binned into 10 equal buckets and used as features. The features are listed in Table 1. 3.1 English Spanish English Spanish Table 3: External resources used in the task. Table 2: Number of tweets retrieved for the various datasets. 3 Language For 1 3.2 Feature extraction and labeling Named entities for English and Spanish were obtained from DBPedia instance types, namely, Agent, Award, Device, Holiday, Language, MeansOfTransportation, Name, PersonFunction, Place, and Work. Frequency lists for these languages were obtained from the Leipzig Copora Collection(Quasthoff et al., 2006); words containing special characters and numbers were removed from the list. The files used are listed in table 3. The character n-gram classifiers were implemented using the MaxEnt classifier provided in MALLET (McCallum, 2002). The classifiers were trained on 6,000 positive examples randomly sampled from the training set and negative examples sampled from both, the training set and from word lists of multiple languages from (Quasthoff et al., 2006); the number of examples used for each of these classifiers is given in Table 4. We used CRF++ (Kudo, 2014) for labeling the tweets. For all lang"
W14-3908,D08-1102,0,0.0600352,"ives (Muysken, 2001; Poplack, 2004; Senaratne, 2009; Boztepe, 2005). Although bilingualism is very common in many countries, it has seldom been studied in detail in computer-mediatedcommunication, and more particularly in social media. A large portion of related work (Androutsopoulos, 2013; Paolillo, 2011; Dabrowska, 2013; Halim and Maros, 2014), does not explicitly deal with computational modeling of this phenomena. Therefore, identifying code-mixing in social media conversations and the web is a very relevant topic today. It has garnered interest recently, in the context of basic NLP tasks (Solorio and Liu, 2008b; Solorio and Liu, 2008a), IR (Roy et al., 2013) and social media analysis (Lignos and Marcus, 2013). It should also be noted that the identi2 System overview The task can be viewed as a sequence labeling problem, where, like POS tagging, each token in a sentence needs to be labeled with one of the 6 tags. Conditional Random Fields (CRF) are a reasonable choice for such sequence labeling tasks (Lafferty et al., 2001); previous work (King and Abney, 2013) has shown that it provides good performance for the language identification task as well. Therefore, in our work, we explored various token"
W14-3908,D08-1110,0,0.028956,"ives (Muysken, 2001; Poplack, 2004; Senaratne, 2009; Boztepe, 2005). Although bilingualism is very common in many countries, it has seldom been studied in detail in computer-mediatedcommunication, and more particularly in social media. A large portion of related work (Androutsopoulos, 2013; Paolillo, 2011; Dabrowska, 2013; Halim and Maros, 2014), does not explicitly deal with computational modeling of this phenomena. Therefore, identifying code-mixing in social media conversations and the web is a very relevant topic today. It has garnered interest recently, in the context of basic NLP tasks (Solorio and Liu, 2008b; Solorio and Liu, 2008a), IR (Roy et al., 2013) and social media analysis (Lignos and Marcus, 2013). It should also be noted that the identi2 System overview The task can be viewed as a sequence labeling problem, where, like POS tagging, each token in a sentence needs to be labeled with one of the 6 tags. Conditional Random Fields (CRF) are a reasonable choice for such sequence labeling tasks (Lafferty et al., 2001); previous work (King and Abney, 2013) has shown that it provides good performance for the language identification task as well. Therefore, in our work, we explored various token"
W14-3908,W14-3907,0,0.056025,"rshi@cs.umd.edu Gokul Chittaranjan Microsoft Research India t-gochit@microsoft.com Kalika Bali Monojit Choudhury Microsoft Research India {kalikab, monojitc}@microsoft.com Abstract fication of languages due to code-switching is different from identifying multiple languages in documents (Nguyen and Dogruz, 2013), as the different languages contained in a single document might not necessarily be due to instances of code switching. In this paper, we present a system built with off-the-shelf tools that utilize several character and word-level features to solve the EMNLP CodeSwitching shared task (Solorio et al., 2014) of labeling a sequence of words with six tags viz. lang1, lang2, mixed, ne, ambiguous, and others. Here, lang1 and lang2 refer to the two languages that are mixed in the text, which could be EnglishSpanish, English-Nepali, English-Mandarin or Standard Arabic-dialectal Arabic. mixed refers to tokens with morphemes from both, lang1 and lang2, ne are named entities, a word whose label cannot be determined with certainty in the given context is labeled ambiguous, and everything else is tagged other (Smileys, punctuations, etc.). The report is organized as follows. In Sec. 2, we present an overvie"
W14-3914,b-etal-2010-resource,1,0.28125,"nguage data is challenging from the perspective of linguistic understanding vis-à-vis discourse and conversational analysis, as well as computational modelling and applications to Machine Translation, Information Retrieval and Natural Interfaces. Especially, in the case of social-media content where there are added complications due to contractions, non-standard spellings, and ungrammatical constructions as well as mixing of scripts. Many languages that use nonRoman scripts, like Hindi, Bangla, Chinese, Arabic etc., are often represented using Roman transliterations (Virga and Khudanpur 2003, Sowmya et al 2010). This poses additional challenges of accurately identifying and separating the two languages. Further, it is often difficult to disambiguate a borrowing as a valid native vocabulary from a mixing of a second language when dealing with single words. An understanding of the nature of mixing in such data is one of the first steps towards processing this data and hence, making a more natural interaction in CMC a real possibility. 2 Nonce-borrowings are typically borrowings that do not necessarily follow any phonological, morpho-syntactic or sociolinguistic constraints on their assimilation into t"
W14-3914,W03-1508,0,0.0147522,"Missing"
W14-3914,petrov-etal-2012-universal,0,\N,Missing
W14-5149,sankaran-etal-2008-common,1,0.813483,"cedure: Preheat oven to 350F. Toss bread and 3 tablespoons oil on a large rimmed baking sheet, squeezing bread so it absorbs oil evenly; season with salt and pepper. Spread out bread pieces in an even layer and bake, tossing occasionally, until crisp on the outside but still chewy in the center, 10 to 15 minutes. Let croutons cool...... 4 In this section we discuss the principles behind our tagset framework. The tagset proposed is hierarchical and recursive. Flat tagsets just list down the categories applicable for each unit of text without any provision for modularity or feature reusability (Baskaran et al., 2008). Hierarchical tagsets on the other hand are structured relative to one another and offer a well-defined mechanism for finding semantic relations between ingredients, utensils/devices, cooking actions and time. 4.1 Recursive and Hierarchical The framework is recursive and forms a treestructure. This ensures that instead of having a large number of independent categories, a recursive tagset contains a small number of broad categories at the top level, tagging larger units of text. Each broad category text has a number of subcategories in a tree-structure. The finer details of recipe text are ca"
W14-5149,J13-4004,0,0.0180374,"an adjective as its head. An AdjP can function as a modifier within a noun phrase (for example, the brown flaxseeds) in which case it is annotated under property. The following are some issues which are taken care while automatically tagging the input recipe. 1. Co-reference: In linguistics, co-reference occurs when two or more expressions in a text refer to the same person or thing. Ingredients or intermediate outputs and utensils from the previous steps are co-referenced in the current step. Usually pronouns refer to nouns. We are using Stanford Deterministic Coreference Resolution System7 (Lee et al., 2013) to resolve the coreferences. 2. Distance references: The tags corresponding to a particular subtree may not be at consecutive positions in the input recipe. So for an individual tag, if it is not inside its subtree, we have to keep a reference to its parent using an attribute called ref. In the example below, action, how-adv have same parent ca. But they are not present in consecutive positions, so how-adv makes a reference to ca using ref attribute. Cut potatoes smoothly using knife hca id = ‘‘1’’i hactioniCuth/actioni h/cai hicai http://nlp.stanford.edu/software/ dcoref.shtml h/icai hhow-ad"
W14-5149,P13-1045,0,0.0278531,"e The procedure section consists of natural language text. We used tools like natural language parser and WordNet (2010) in the process of annotating the recipe. A natural language parser is a program that works out the grammatical structure of sentences, for instance, which groups of words go together (as “phrases”) and which words are the subject or object of a verb. Parse tree gives a complete picture of relations between various phrases in the sentence. These relations can be used to form semantic relations in our analysis, which will be helpful in annotating the recipe. Stanford Parser6 (Socher et al., 2013), a statistical parser is used for our purpose. WordNet gives the sense of various words in the recipe text. For example, if we query WordNet with the word avocado, the output contains noun.food. This information can be used for classifying words into utensils, ingredients and cooking actions etc. The first step involves the tokenization of procedure into many individual sentences. Each sentence is annotated as step. Each step is then parsed and the parse tree is analyzed to annotate various details inside that step. 7.2.1 Handling Various Phrases The most common phrases to be handled includes"
W14-5149,N03-1033,0,0.0420405,"ocation measure, salience score (Pecina, 2010), as the lexical association measure for bigram modeling. The process of combining words into phrases and sentences of natural language is governed by a complex system of rules and constraints. In general, basic rules 4 http://www.epicurious.com 357 p (xy)2 . log f (xy) p (x∗) p (∗y) where p(xy) is the probability of xy occuring together f(xy) is the frequency of bigram xy Here * means any token. 7.1 Ingredients Description Usually ingredient section has a semi structured data. First, annotate the ingredient description using Stanford POS tagger5 (Toutanova et al., 2003). Then scan through all the tokens formed from the input line in a linear fashion and annotate based on the heuristics. We describe some heuristics below. The heuristics given here are in a broad sense. Exact implementation details are omitted. H1: Let k be the current token in the linear scan. If tag(k) is a number then annotate token k and k + 1 as quantity. Inside this quantity tag, annotate k as number and k + 1 as units. H2: Let k be the current token in the linear scan. If tag(k) is a adverb and if it is followed by verb or noun or adjective, then annotate k and k + 1 under ca tag. Insid"
W14-5149,zhang-etal-2012-automatically,0,0.0388141,"Missing"
W14-5149,mori-etal-2014-flow,0,\N,Missing
W14-5151,W11-3501,1,0.859055,"Table 4 is due to the fact that most of the Gujarati words were 376 also identified as Hindi words. Hence,these are likely to be confused by the LI systems. It therefore seems that different techniques and features would be required to disambiguate between these languages. The fall in accuracies for all the proposed extensions when tested on FIRE-data could be attributed to the fact that this data is not synthetic and shows all the variations we might expect in real code-mixed data, but more importantly the data is in Roman transliteration. As previous studies have shown (Sowmya et al., 2010; Ahmed et al., 2011) there is a lot of variation in transliterations of Indian languages due to one-to-many mappings at character level, regional variations and lack of any universally applicable standard conventions. The difficulty for LI systems to disambiguate transliterated Indian languages, thus, increases manifold. It should be noted, however, that the overall trend followed by the proposed extensions remains the same as that on the synthetic data for other languages. 7 Conclusion In this paper we considered language identification task for short code-mixed documents containing one or two languages. We anal"
W14-5151,W12-2108,0,0.0827717,"ally related (e.g., Hindi and Gujarati, Czech and Slovak), for which special disambiguation techniques might be required. 1 Introduction As the World Wide Web is constantly being inundated by user-generated content in many languages, automatic Language Identification (LI) of text fragments has become an extremely important problem. Since mid 90s, this problem has attracted the attention of researchers (Prager, ), and by 2005 it was thought to be a solved problem (McNamee, 2005). However, there is a renewed interest in LI over last few years (Lui and Baldwin, 2011; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Lui and Baldwin, 2012; Goldszmidt et al., 2013; King and Abney, 2013; Carter et al., 2013; Lui et al., 2014) primarily due to a surge in usergenerated content in social media, given the rising popularity of Twitter, Facebook and several other online social networking platforms. Short length, presence of non-standard spelling variations, idiomatic expressions and acronyms are the most commonly cited reasons that make LI for social media content, such as tweets, a challenging problem (Carter et al., 2013; Goldszmidt et al., 2013). However, there are two other frequently observed phenomena in s"
W14-5151,N13-1131,0,0.135002,"pecial disambiguation techniques might be required. 1 Introduction As the World Wide Web is constantly being inundated by user-generated content in many languages, automatic Language Identification (LI) of text fragments has become an extremely important problem. Since mid 90s, this problem has attracted the attention of researchers (Prager, ), and by 2005 it was thought to be a solved problem (McNamee, 2005). However, there is a renewed interest in LI over last few years (Lui and Baldwin, 2011; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Lui and Baldwin, 2012; Goldszmidt et al., 2013; King and Abney, 2013; Carter et al., 2013; Lui et al., 2014) primarily due to a surge in usergenerated content in social media, given the rising popularity of Twitter, Facebook and several other online social networking platforms. Short length, presence of non-standard spelling variations, idiomatic expressions and acronyms are the most commonly cited reasons that make LI for social media content, such as tweets, a challenging problem (Carter et al., 2013; Goldszmidt et al., 2013). However, there are two other frequently observed phenomena in social media (and also in other forms of user-generated content such as"
W14-5151,I11-1062,0,0.145847,"ly to confuse between languages which are linguistically related (e.g., Hindi and Gujarati, Czech and Slovak), for which special disambiguation techniques might be required. 1 Introduction As the World Wide Web is constantly being inundated by user-generated content in many languages, automatic Language Identification (LI) of text fragments has become an extremely important problem. Since mid 90s, this problem has attracted the attention of researchers (Prager, ), and by 2005 it was thought to be a solved problem (McNamee, 2005). However, there is a renewed interest in LI over last few years (Lui and Baldwin, 2011; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Lui and Baldwin, 2012; Goldszmidt et al., 2013; King and Abney, 2013; Carter et al., 2013; Lui et al., 2014) primarily due to a surge in usergenerated content in social media, given the rising popularity of Twitter, Facebook and several other online social networking platforms. Short length, presence of non-standard spelling variations, idiomatic expressions and acronyms are the most commonly cited reasons that make LI for social media content, such as tweets, a challenging problem (Carter et al., 2013; Goldszmidt et al., 2013). However, the"
W14-5151,P12-3005,0,0.213459,"ndi and Gujarati, Czech and Slovak), for which special disambiguation techniques might be required. 1 Introduction As the World Wide Web is constantly being inundated by user-generated content in many languages, automatic Language Identification (LI) of text fragments has become an extremely important problem. Since mid 90s, this problem has attracted the attention of researchers (Prager, ), and by 2005 it was thought to be a solved problem (McNamee, 2005). However, there is a renewed interest in LI over last few years (Lui and Baldwin, 2011; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Lui and Baldwin, 2012; Goldszmidt et al., 2013; King and Abney, 2013; Carter et al., 2013; Lui et al., 2014) primarily due to a surge in usergenerated content in social media, given the rising popularity of Twitter, Facebook and several other online social networking platforms. Short length, presence of non-standard spelling variations, idiomatic expressions and acronyms are the most commonly cited reasons that make LI for social media content, such as tweets, a challenging problem (Carter et al., 2013; Goldszmidt et al., 2013). However, there are two other frequently observed phenomena in social media (and also i"
W14-5151,D13-1084,0,0.310188,"abeling at word level. In the absence of sufficient data across many languages annotated with language labels for words, we created a synthetic dataset by mixing natural language text fragments in 28 languages. Our dataset included three cases of transliterated text, for Hindi, Bengali and Gujarati. The proposed extensions outperform all the existing LI systems evaluated by a significant margin for all 28 languages. Previous work on LI at word level presumes an a priori knowledge of the two languages mixed and the task mostly reduces to a binary classification problem (Solorio and Liu, 2008a; Nguyen and Dogruoz, 2013; Gella et al., 2013). However, this is not the case in practice. The languages to be identified are usually not known in advance and the set of potential language labels can span all the languages represented on the World Wide Web. The primary contribution of the current work is to highlight the challenges in such a scenario and present a few techniques to deal with these. The rest of the paper is organized as follows. In Sec. 2 we review related work with special reference to the existing LI systems evaluated in this work. Sec. 3 describes the synthetic dataset creation. In Sec. 4 we propose"
W14-5151,D08-1102,0,0.0466097,"technique for language labeling at word level. In the absence of sufficient data across many languages annotated with language labels for words, we created a synthetic dataset by mixing natural language text fragments in 28 languages. Our dataset included three cases of transliterated text, for Hindi, Bengali and Gujarati. The proposed extensions outperform all the existing LI systems evaluated by a significant margin for all 28 languages. Previous work on LI at word level presumes an a priori knowledge of the two languages mixed and the task mostly reduces to a binary classification problem (Solorio and Liu, 2008a; Nguyen and Dogruoz, 2013; Gella et al., 2013). However, this is not the case in practice. The languages to be identified are usually not known in advance and the set of potential language labels can span all the languages represented on the World Wide Web. The primary contribution of the current work is to highlight the challenges in such a scenario and present a few techniques to deal with these. The rest of the paper is organized as follows. In Sec. 2 we review related work with special reference to the existing LI systems evaluated in this work. Sec. 3 describes the synthetic dataset cre"
W14-5151,D08-1110,0,0.0591438,"technique for language labeling at word level. In the absence of sufficient data across many languages annotated with language labels for words, we created a synthetic dataset by mixing natural language text fragments in 28 languages. Our dataset included three cases of transliterated text, for Hindi, Bengali and Gujarati. The proposed extensions outperform all the existing LI systems evaluated by a significant margin for all 28 languages. Previous work on LI at word level presumes an a priori knowledge of the two languages mixed and the task mostly reduces to a binary classification problem (Solorio and Liu, 2008a; Nguyen and Dogruoz, 2013; Gella et al., 2013). However, this is not the case in practice. The languages to be identified are usually not known in advance and the set of potential language labels can span all the languages represented on the World Wide Web. The primary contribution of the current work is to highlight the challenges in such a scenario and present a few techniques to deal with these. The rest of the paper is organized as follows. In Sec. 2 we review related work with special reference to the existing LI systems evaluated in this work. Sec. 3 describes the synthetic dataset cre"
W14-5151,b-etal-2010-resource,1,0.94126,"mediated communication like email, chat, and more recently, on social media like Facebook and Twitter (Herring, 2003; Cardenas-Claros and Isharyanti, 2009; Paolillo, 2011) has ensured that code-mixed data is fairly prevalent on the web. In the case of social-media content where there are additional complications due to contractions, non-standard spellings, and non-standard constructions as well as mixing of scripts, processing of the data poses major challenges. Further, many languages that use nonRoman scripts, like Hindi, Bangla, Chinese, Arabic etc. are often present in a Romanized form. (Sowmya et al., 2010). Not a lot of work has been done on computational models of code-mixing (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013) , primarily due to the paucity of CM data in conventional text corpora which makes data-intensive methods hard to apply. (Solorio and Liu, 2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monlingual corpora. While natural language processing like POS tagging, normalization, etc remain hard problems to solve, any processing of code-mixed text, first"
W14-5151,D14-1105,1,0.846506,"man script because of its popularity. If in a code-mixed text, each language is written in their native script (say Hindi words in Devanagari and English words in Roman script), LI becomes a trivial problem. Therefore, we choose languages which either use Roman script (like most of the West European languages) or languages which are quite commonly Romanized on the Web such as the Indian languages. 3. Whenever code-mixing happens, one of the languages is always English. This principle is also based on the empirical observation that codemixing happens mostly between English and other languages (Vyas et al., 2014). However, this is not necessary; there are examples of code-mixing between Turkish and Dutch, Arabic and French, Chinese and Malay, and so on. Since selecting a representative set of language pairs is difficult, and mixing between all pairs would not only lead to impractical cases, but also make it cumbersome to analyze and represent the experimental data, we decided to only experiment with mixing between English and other languages. Note that this choice does not limit the generality of the conclusions of this study as none of the algorithms exploit the fact that one of the languages is Engl"
W14-5151,E09-1099,0,0.0402508,"built on smaller datasets to predict valid switching points to synthetically generate data from monlingual corpora. While natural language processing like POS tagging, normalization, etc remain hard problems to solve, any processing of code-mixed text, first needs to deal with the identification and labeling of the parts of text which are in different languages. Monolingual language identification has been worked on intensively in NLP where the task is to assign a language to every document according to the language it contains. There are existing methods which show high accuracies on large (Xia et al., 2009) and short (Tromp and Pechenizkiy, 2011; Lui and Baldwin, 2012) documents when tested against a small set of languages. However, for Code-mixed text, especially those involving transliterations and orthographic variation, this is far from a solved problem. In their work, (King and Abney, 2013) use a weakly supervised n-gram based model trained on monolingual data for labeling languages in a mixed-language document. In their experiments with language-labels of words in multilingual online discussions using language models and dictionaries, (Nguyen and Dogruoz, 2013) show that spelling variation"
W14-5151,Q14-1003,0,\N,Missing
W15-5936,W14-3914,1,0.903718,"Missing"
W15-5936,W14-3908,1,0.839908,"0 Accuracy 0.798 0.812 0.827 0.837 Table 7: Effect of context on learning Scheme Accuracy the SL tagger does not utilize the contextual features, normalization features, sub-word features such as whether the first letter in a word is capitalized and so on. In this section, we propose additional features to the existing feature set used by the SL model and also propose extensions to the existing models. Without Normalization (SL) 0.831 With Automated Normalization 0.834 6.1 With Gold Normalization 0.840 In addition to the feature set proposed by the SL model, we use the feature set proposed by Chittaranjan et al. (2014). The final augmented feature set is shown below: Monolingual POS tagger Features: The output generated by the individual POS taggers mapped to the universal POS tag set is used as features. The confidence of the taggers is also used as a feature but as ILPOST does not generate a confidence score per tag, the confidence score generated by the Twitter tagger alone is used. Normalization Feature: The words are normalized to their native script. That is, if the word is an En word, its standard form is used. Similarly, if the word is an Hi word, its gold transliteration is used. The rationale behi"
W15-5936,P11-2008,0,0.0863049,"Missing"
W15-5936,R15-1033,0,0.46201,"Missing"
W15-5936,N13-1131,0,0.0387483,"are tagged by the Hi POS tagger and En chunks are tagged by En POS tagger. The model presents three different experiments: The first experiment uses gold language labels(LL) and gold normalization (HN) of the token. On the other hand, the second experiment uses gold language labels but automated normalization of the tokens which helps one to individually study the effect of gold standard normalization. Finally, machine generated language labels and transliteration is used to establish the their combined role. We implement an n-gram based language identifier as proposed by (Gella et al., 2013; King and Abney, 2013a). For generating back-transliterations, we use a transliteration system inspired by (Gella et al., 2013). For Hi, a Figure 1: An annotation example M atrix : Hindi M atrix : English Overall HiM ono HiCM Overall EnM ono EnCM Overall Gold Std LL, Gold Std HN 0.794 0.764 0.769 0.827 0.817 0.825 0.782 Gold Std LL, Machine HN 0.775 0.759 0.762 0.754 0.722 0.749 0.759 Machine LL, Machine HN 0.759 0.741 0.744 0.544 0.556 0.546 0.698 Table 4: VGSBC results on the data set CRF++ based Hindi POS Tagger is used which can be downloaded from http://nltr.org/ snltr-software/ and for En, we use the Twitter"
W15-5936,J93-2004,0,0.055169,"Missing"
W15-5936,N13-1039,0,0.0403659,"Missing"
W15-5936,petrov-etal-2012-universal,0,0.10907,"Missing"
W15-5936,sankaran-etal-2008-common,1,0.867684,"Missing"
W15-5936,D08-1102,0,0.476612,"ations. CM is predominately a speech-level phenomenon, though with the prevalence of social media and user-generated content that are more speech-like, we now observe CM quite commonly in text as well (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Therefore, it is imperative that we develop NLP techniques for processing of CM text to analyze the user-generated content from and cater to the needs of multilingual societies. In the recent past, there has been some work on CM data most of which has been focused on word level language identification (Solorio and Liu, 2008a; Saha Roy et al., 2013; Gella et al., 2013) and POS tagging of CM text which is one of the first steps towards processing of CM text. Parts237 of-Speech tagging is another task which has been explored to a little extent for CM text (Solorio and Liu, 2008b; Vyas et al., 2014). POS tagging of CM data is an interesting problem to study both from a practical and a theoretical perspective because it requires modeling of the grammatical structures of both the languages as well as the syntactic constraints applicable on CM. In this paper, we explore machine learning approaches for POS tagging of Hi"
W15-5936,D08-1110,0,0.59032,"ations. CM is predominately a speech-level phenomenon, though with the prevalence of social media and user-generated content that are more speech-like, we now observe CM quite commonly in text as well (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Therefore, it is imperative that we develop NLP techniques for processing of CM text to analyze the user-generated content from and cater to the needs of multilingual societies. In the recent past, there has been some work on CM data most of which has been focused on word level language identification (Solorio and Liu, 2008a; Saha Roy et al., 2013; Gella et al., 2013) and POS tagging of CM text which is one of the first steps towards processing of CM text. Parts237 of-Speech tagging is another task which has been explored to a little extent for CM text (Solorio and Liu, 2008b; Vyas et al., 2014). POS tagging of CM data is an interesting problem to study both from a practical and a theoretical perspective because it requires modeling of the grammatical structures of both the languages as well as the syntactic constraints applicable on CM. In this paper, we explore machine learning approaches for POS tagging of Hi"
W15-5936,W14-3907,0,0.153676,"Missing"
W15-5936,b-etal-2010-resource,1,0.894781,"Missing"
W15-5936,D14-1105,1,0.837255,"aryanti, 2009). Therefore, it is imperative that we develop NLP techniques for processing of CM text to analyze the user-generated content from and cater to the needs of multilingual societies. In the recent past, there has been some work on CM data most of which has been focused on word level language identification (Solorio and Liu, 2008a; Saha Roy et al., 2013; Gella et al., 2013) and POS tagging of CM text which is one of the first steps towards processing of CM text. Parts237 of-Speech tagging is another task which has been explored to a little extent for CM text (Solorio and Liu, 2008b; Vyas et al., 2014). POS tagging of CM data is an interesting problem to study both from a practical and a theoretical perspective because it requires modeling of the grammatical structures of both the languages as well as the syntactic constraints applicable on CM. In this paper, we explore machine learning approaches for POS tagging of Hindi (Hi)-English (En) CM text from social media. We start with replication of the experiments presented in (Vyas et al., 2014) and (Solorio and Liu, 2008b), and reconfirm their results on our dataset. Then we extend the set of features used by (Solorio and Liu, 2008b) and do s"
W17-7509,P13-2037,0,0.578026,"Missing"
W17-7509,W14-3914,1,0.897653,"Missing"
W17-7509,W16-5807,0,0.0226203,"Missing"
W17-7509,R15-1033,0,0.0147654,"supervised model can be learnt simply from the monolingual and CS data. Thus, y = g1∪2∪12 (x) (1) These models often use features or extra information specific to CS, but do not particularly modify the training process or system architecture for handling CS. This approach has been applied to language identification, e.g., most submissions in the LID shared task in the Computational Approaches to Code-Switching Workshops (Solorio et al., 2014; Molina et al., 2016); to POS tagging, e.g., most submissions in the ICON 2016 shared task on CS POS tagging (Das, 2016) and also (Jamatia and Das, 2014; Jamatia et al., 2015); and to ASR (Gebhardt, 2011). Combining Monolingual Models: In this approach, the output of two monolingual systems on x is used as features for a third model (f12 in Eq. 2). This third model f is trained on a small amount of CS data, and can use other features which often includes LID output. y = f12 (g1 (x), g2 (x), lid(x)) (2) Solorio and Liu (2008) proposed this architecture for POS tagging of English-Spanish CS data, and Lyu et al. (2006) proposed a similar model for ASR. Both reported significant gain over the monolingual models by using very little CS data. Later works, such as (Sequie"
W17-7509,O09-5003,0,0.111309,"n the question: whether for a particular NLP task (say ASR, MT or POS Tagging), it is possible to build CS models only from pretrained monolingual models or monolingual training data? Indeed, several studies in the past (Solorio and Liu, 2008; Vyas et al., 2014; Gadre et al., 2016; Gonzalez-Dominguez et al., 2015) have proposed techniques for combining monolingual models or training data coupled with a little amount of CS data to build models of CS text or speech. These techniques have reported promising results. However, all these studies, except (Johnson et al., 2016; Rijhwani et al., 2017; Chan et al., 2009), have tried to combine the outputs of pre-trained monolingual models in intelligent ways. On the other hand, one might ask whether a single system trained on monolingual data from both the languages would be able to handle CS between these languages? And, if we also had a little amount of CS data, how best to use it during the training process? In this paper, we explore various training strategies, also known as Curriculum (Bengio et al., 2009) for DNN-based architectures for codeswitching. In particular, we design a set of strategies or curricula involving various ordering of the monolingual"
W17-7509,D14-1098,0,0.164845,"to account strong indicators of CS like POS and LID, and (c) Models that incorporate linguistic constraints for CS. Bilingual language models are typically trained using pooled text data (Weng et al., 1997). Gebhardt (2011) describes a framework to use Factored Language Models for rescoring n-best lists during decoding. The factors used include POS tags, CS point probability and LID. In Adel et al.(2014b; 2014a; 2013) recurrent language models built on the same corpus are combined with n-gram based models, or converted to backoff models, giving improvements in perplexity and mixed error rate. Li and Fung (2014) integrates Functional Head constraints for codeswitching into the Language Model for decoding a Mandarin-English corpus. Li and Fung (2013) use inversion constraints to predict CS points and integrates this prediction into the decoding process. Our work is similar to the bilingual model approach in that we pool data from both languages. 72 However, we also add a very small amount of CS data to our models in some of the experiments. In addition, we also focus on the ordering of the monolingual and CS data, which to our knowledge, none of the previous approaches do. 6 Discussion and Conclusion"
W17-7509,W16-5805,0,0.447523,"tence boundary detection. However, intra-sentential CS is more challenging to handle, and will be our primary focus. In this paper, the terms monolingual model and monolingual data will be used for cases where the data was collected and the model was built assuming that the input will be only in a single language. Such datasets might also contain some borrowed words and text in other language(s). On the other hand, we will use the term CS data to imply datasets where all instances contain intra-sentential CS, even though most of the datasets released in the past for training CS models, e.g., (Molina et al., 2016; Das, 2016; Sequiera et al., 2015b), do contain fair amounts of monolingual and inter-sentential CS. The term CS model will be used for systems that can handle monolingual, inter-sentential as well as intra-sentential CS. 2.2 A Taxonomy of CS Models In order to succinctly represent the various types of CS models proposed in the literature, we will use the following notation. Let l1 and l2 be two 66 languages. Let x denote the input string, usually a sentence, i.e., string of tokens, in l1 , l2 or l12 , i.e., l1 ↔ l2 code-switched. Let y be the output string of tokens in a target language (as"
W17-7509,P17-1180,1,0.112215,"id2 (x)) (3) y = g1∪2 (x) (4) This approach does not require any CS training data, but it does not work well for intra-sentential CS because splitting by language can lead to loss of context especially at the code-switch points. However, some benefits of this approach have been shown for POS tagging (Vyas et al., 2014), MT (Gadre et al., 2016) and ASR (Lyudovyk and Pylypenko, 2014) respectively. Zero Shot Learning: This is an extreme case, where only monolingual data from two or more languages is used to train a single system with the hope that it will work for CS data as well. A recent work (Rijhwani et al., 2017) uses this technique very effectively for developing an LID system for 7 languages. While no annotated CS data is used for training, the system uses unlabeled data that is expected to contain CS data, for unsupervised training. Johnson et al. (2016) trains a neural MT system with data from two pairs of languages, l1 ⇔ l2 and l1 ⇔ l3 and show that the resultant model not only works for l2 ⇔ l3 (the so called “zero shot learning” but also for CS input in these languages, albeit to a limited extent. Factors such as lack of large-scale CS datasets, possibility of CS between any pair (or even tripl"
W17-7509,D16-1121,1,0.775343,"Missing"
W17-7509,W16-5806,0,0.0138063,"Missing"
W17-7509,W15-5936,1,0.908192,"r, intra-sentential CS is more challenging to handle, and will be our primary focus. In this paper, the terms monolingual model and monolingual data will be used for cases where the data was collected and the model was built assuming that the input will be only in a single language. Such datasets might also contain some borrowed words and text in other language(s). On the other hand, we will use the term CS data to imply datasets where all instances contain intra-sentential CS, even though most of the datasets released in the past for training CS models, e.g., (Molina et al., 2016; Das, 2016; Sequiera et al., 2015b), do contain fair amounts of monolingual and inter-sentential CS. The term CS model will be used for systems that can handle monolingual, inter-sentential as well as intra-sentential CS. 2.2 A Taxonomy of CS Models In order to succinctly represent the various types of CS models proposed in the literature, we will use the following notation. Let l1 and l2 be two 66 languages. Let x denote the input string, usually a sentence, i.e., string of tokens, in l1 , l2 or l12 , i.e., l1 ↔ l2 code-switched. Let y be the output string of tokens in a target language (as in MT, ASR or POS tagging). Let gi"
W17-7509,D08-1110,0,0.484615,"of the tweets from the cities around the world are code-switched. It is therefore imperative to build speech and text processing technologies that can handle CS. Indeed, quite some amount of effort is being invested towards technology for CS (see Diab et al. (2014; 2016), Sharma et al. (2015), and 65 references therein). It is of theoretical and practical interest to ponder on the question: whether for a particular NLP task (say ASR, MT or POS Tagging), it is possible to build CS models only from pretrained monolingual models or monolingual training data? Indeed, several studies in the past (Solorio and Liu, 2008; Vyas et al., 2014; Gadre et al., 2016; Gonzalez-Dominguez et al., 2015) have proposed techniques for combining monolingual models or training data coupled with a little amount of CS data to build models of CS text or speech. These techniques have reported promising results. However, all these studies, except (Johnson et al., 2016; Rijhwani et al., 2017; Chan et al., 2009), have tried to combine the outputs of pre-trained monolingual models in intelligent ways. On the other hand, one might ask whether a single system trained on monolingual data from both the languages would be able to handle"
W17-7509,W14-3907,0,0.126892,"Missing"
W17-7509,D14-1105,1,0.555847,"cities around the world are code-switched. It is therefore imperative to build speech and text processing technologies that can handle CS. Indeed, quite some amount of effort is being invested towards technology for CS (see Diab et al. (2014; 2016), Sharma et al. (2015), and 65 references therein). It is of theoretical and practical interest to ponder on the question: whether for a particular NLP task (say ASR, MT or POS Tagging), it is possible to build CS models only from pretrained monolingual models or monolingual training data? Indeed, several studies in the past (Solorio and Liu, 2008; Vyas et al., 2014; Gadre et al., 2016; Gonzalez-Dominguez et al., 2015) have proposed techniques for combining monolingual models or training data coupled with a little amount of CS data to build models of CS text or speech. These techniques have reported promising results. However, all these studies, except (Johnson et al., 2016; Rijhwani et al., 2017; Chan et al., 2009), have tried to combine the outputs of pre-trained monolingual models in intelligent ways. On the other hand, one might ask whether a single system trained on monolingual data from both the languages would be able to handle CS between these la"
W17-7510,W14-3914,1,0.83071,"on (e.g., Barcelona) are different from those where English is the primary language (e.g., Houston). In an excellent survey on computational sociolinguistics, Nguyen et al. (2016) report a few other studies on socio-linguistic aspects of multilingual communities. 2.3 Code-switching in Indian Cinema Hindi-English CS, commonly called Hinglish, is extremely widespread in India. There is historical attestation, as well as recent studies on the growing use of Hinglish in general conversation, and in entertainment and media (see Parshad et al. (2016) and references therein). Several recent studies (Bali et al., 2014; Barman et al., 2014; Sequiera et al., 2015) also provide evidence of Hinglish and other instances of CS on online social media, such as Twitter and Facebook. Hindi movies provide a rich data source for studying CS in the Indian context. According to the Conversational Analysis approach to CS (Auer, 2013; Wei, 2002), in any given context a particular language is preferred or unmarked. Therefore, “speakers, and in turn script writers, choose marked or unmarked codes on the basis of which one will bring them the best outcomes” (Vaish, 2011). Myers-Scotton (2005) suggested that the matrix or unm"
W17-7510,W14-3902,0,0.0269823,") are different from those where English is the primary language (e.g., Houston). In an excellent survey on computational sociolinguistics, Nguyen et al. (2016) report a few other studies on socio-linguistic aspects of multilingual communities. 2.3 Code-switching in Indian Cinema Hindi-English CS, commonly called Hinglish, is extremely widespread in India. There is historical attestation, as well as recent studies on the growing use of Hinglish in general conversation, and in entertainment and media (see Parshad et al. (2016) and references therein). Several recent studies (Bali et al., 2014; Barman et al., 2014; Sequiera et al., 2015) also provide evidence of Hinglish and other instances of CS on online social media, such as Twitter and Facebook. Hindi movies provide a rich data source for studying CS in the Indian context. According to the Conversational Analysis approach to CS (Auer, 2013; Wei, 2002), in any given context a particular language is preferred or unmarked. Therefore, “speakers, and in turn script writers, choose marked or unmarked codes on the basis of which one will bring them the best outcomes” (Vaish, 2011). Myers-Scotton (2005) suggested that the matrix or unmarked code for Hindi"
W17-7510,P17-1180,1,0.901436,"Missing"
W17-7510,D16-1121,1,0.864225,"Missing"
W17-7510,L16-1292,0,0.183277,"On the other hand, functions of CS are most relevant and discernible in relatively long multi-party conversations embedded in a social context. For instance, it is well documented (Auer, 2013) that CS is motivated by complex social functions, such as identity, social power and style accommodation, which are difficult to elicit and establish from short social media texts. In this work, we propose a set of techniques for analyzing CS styles and functions in conversations grounded over social networks. Our approach develops on two previously proposed metrics of CS – the Code-mixing Index (CMI) (Gamback and Das, 2016) and corpus level metrics proposed in (Guzman et al., 2017), applied to conversations at the level of dyads, participants, conversation scenes and the entire social network of the participants. We apply this new approach to analyze scripts of 18 recent Hindi movies with various degrees and styles of Hindi-English CS. Through this analysis technique, we are able to bring out the social functions of CS at different levels. The primary contributions of this work are: (a) development of a set of quantitative conversation analysis techniques for CS; (b) some visualization techniques for CS patterns"
W17-7510,P14-2110,0,0.053658,"Missing"
W17-7510,N16-1159,0,0.0219452,"Missing"
W17-7510,D08-1110,0,0.057127,"Missing"
W17-7510,D14-1105,1,0.906666,"Missing"
W18-3202,O09-5003,0,0.439155,"tion Multilingual speakers tend to alternate between several languages within a conversation. This phenomenon is referred to as code-switching (CS). Automatic Speech Recognition for CS speech is challenging. Code-switched speech recognition present challenges in acoustic, language and pronunciation modeling of speech. Acoustic Mod∗ This work was done while interning at Microsoft Research-India 11 Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 11–19 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics nunciation lexicon. (Chan et al., 2009) describe a two pass approach for Cantonese-English mixed speech recognition, in which they develop a cross-lingual phonetic AM, with the phone set designed based on linguistic knowledge. (Yu et al., 2004) present three approaches for bilingual phone modeling for Mandarin-English speech, namely combining phone inventories, use IPA mappings to construct a bilingual phone set and clustering phones with hierarchical clustering by using the Bhattacharyya distance and the acoustic likelihood. The third approach outperforms the IPA-based mapping and is comparable to the combination of the phone inve"
W18-3202,L16-1546,1,0.829163,"plot of the phone accuracy with respect to the log of the data available per phone in the test dataset. Evidently, we observe higher accuracies for phones with larger count with a few exceptions such as, /nN HP/, /zh/ and /dR HP/. 4.2 Manually merging similar sounding phones To increase the data availability per phone, we merged similar sounds in both languages even if they are not exactly the same linguistically (in terms of their articulation). The mapping between Hindi phones and CMUdict phones in the Festvox Indic frontend, built for enabling a Hindi TTS system to pronounce English words (Sitaram and Black, 2016), was used for this purpose. All the merged phones (a total of 31) were prefixed with “-HP-M”. To analyze the impact of merging, we started by merging a pair of phones - the English phone eh (example “academic ae k ah d eh m ih k”) with a similar sounding Hindi phone E-HP (example in Roman script: “kehana k-HP E-HP hv-HP nBHP Aa-HP”). This resulted in 38 English phones, 49 Hindi phones and 1 merged phone resulting in 88 unique phones. We obtained a WER of 40.21 using a GMM-HMM acoustic model, which 13 is a negligible improvement over the system with no merging. The bar plot in Figure 2 shows t"
W18-3210,W11-0609,0,0.0386398,"n/category/fc-pro/scripts/ amount of noise, we corrected frequently observed errors with manual supervision. Each dialog is assumed to be in response to the immediately preceding dialog within a scene. We restrict our analysis to dialogs that are between no more than two speakers, to avoid confounding effects of multi-party conversations on accommodation. This also filters out most dialogs in the scripts which are not conversational in nature. Movie conversations, even though imagined, are designed to sound natural, and therefore, are suitable for studying style accommodation, as is argued in Danescu-Niculescu-Mizil and Lee (2011), and also multilingualism (Bleichenbacher, 2008) and code-choice (Vaish, 2011). It is true that movie dialogs promote stereotypes that may affect characters’ expression of code-choice, however accommodative effects can still be expected to play out largely independent of such stereotypes. There have been several linguistic and quantitative studies on Hindi-English CS in Hindi movies (Parshad et al., 2016; L¨osch, 2007; Pratapa and Choudhury, 2017). Figure 1: Fraction of Spanish over time in a conversation. The x-axis denotes consecutive dialog pairs, with dialog i above aligned with dialog i"
W18-3210,W17-7510,1,0.825312,"tions, even though imagined, are designed to sound natural, and therefore, are suitable for studying style accommodation, as is argued in Danescu-Niculescu-Mizil and Lee (2011), and also multilingualism (Bleichenbacher, 2008) and code-choice (Vaish, 2011). It is true that movie dialogs promote stereotypes that may affect characters’ expression of code-choice, however accommodative effects can still be expected to play out largely independent of such stereotypes. There have been several linguistic and quantitative studies on Hindi-English CS in Hindi movies (Parshad et al., 2016; L¨osch, 2007; Pratapa and Choudhury, 2017). Figure 1: Fraction of Spanish over time in a conversation. The x-axis denotes consecutive dialog pairs, with dialog i above aligned with dialog i + 1 below, so two aligned bars denote two consecutive dialogs. Dataset Bangor Movies Acm∗ (FL ) 0.04 0.09 0.06 -0.02† plot in Figure 2, the rate of accommodation by s, Acms (F ), against the respective base rate P (dFs ), for F ∈ {FEn , FHi }. Clearly, we see that a high base rate of expression corresponds to far less accommodation. In other words, the instances of code-choice that are uncommon and therefore unexpected within the conversational con"
W18-3210,P17-1180,1,0.82948,"rns in CS as well as the choice to switch between languages have been the focus of many linguistic studies(Poplack, 1988)(Auer, 1995). As CS is typically used as a conversation strategy by bilinguals who are proficient in both languages (Auer, 2013), it is not surprising that certain pragmatic and socio-linguistic factors, such as formality of context (Fishman, 1970), age (Ervin-Tripp and Reyes, 2005), expression of emotion (Dewaele, 2010) and sentiment (Rudra et al., 2016), are found to signal language preference in CS conversations. A Twitter study of CS patterns across several geographies (Rijhwani et al., 2017), also suggests that there might be complex sociolinguistic reasons for code-choice. Thus, CS, and the choice of language or code in which one communicates during a multilingual conversation, could be considered a marker of linguistic style. Communication accommodation theory (Giles et al., 1973; Giles, 2007) states that speakers shift The first computational study of linguistic style accommodation (Danescu-Niculescu-Mizil et al., 2011) shows that it is highly prevalent in Twitter conversations. They use binary features for the presence of various psychologically meaningful word categories as"
W18-3210,D16-1121,1,0.931494,"). As a marker of informality, it has been shown to lower interpersonal distance (MyersScotton, 1995; Genesee, 1982). Common structural patterns in CS as well as the choice to switch between languages have been the focus of many linguistic studies(Poplack, 1988)(Auer, 1995). As CS is typically used as a conversation strategy by bilinguals who are proficient in both languages (Auer, 2013), it is not surprising that certain pragmatic and socio-linguistic factors, such as formality of context (Fishman, 1970), age (Ervin-Tripp and Reyes, 2005), expression of emotion (Dewaele, 2010) and sentiment (Rudra et al., 2016), are found to signal language preference in CS conversations. A Twitter study of CS patterns across several geographies (Rijhwani et al., 2017), also suggests that there might be complex sociolinguistic reasons for code-choice. Thus, CS, and the choice of language or code in which one communicates during a multilingual conversation, could be considered a marker of linguistic style. Communication accommodation theory (Giles et al., 1973; Giles, 2007) states that speakers shift The first computational study of linguistic style accommodation (Danescu-Niculescu-Mizil et al., 2011) shows that it i"
