2021.wnut-1.9,"A Text Editing Approach to Joint {J}apanese Word Segmentation, {POS} Tagging, and Lexical Normalization",2021,-1,-1,2,1,126,shohei higashiyama,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Lexical normalization, in addition to word segmentation and part-of-speech tagging, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed model achieved better normalization performance when trained on more diverse pseudo-labeled data."
2021.wat-1.4,{NICT}{'}s Neural Machine Translation Systems for the {WAT}21 Restricted Translation Task,2021,-1,-1,2,1,304,zuchao li,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper describes our system (Team ID: nictrb) for participating in the WAT{'}21 restricted machine translation task. In our submitted system, we designed a new training approach for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance."
2021.naacl-main.311,Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios,2021,-1,-1,4,1,4177,haipeng sun,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems."
2021.naacl-main.438,User-Generated Text Corpus for Evaluating {J}apanese Morphological Analysis and Lexical Normalization,2021,-1,-1,2,1,126,shohei higashiyama,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT."
2021.emnlp-main.261,Unsupervised Neural Machine Translation with Universal Grammar,2021,-1,-1,2,1,304,zuchao li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky{'}s Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches."
2021.emnlp-main.299,Smoothing Dialogue States for Open Conversational Machine Reading,2021,-1,-1,4,0.45977,6857,zhuosheng zhang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial by using hard-label decisions to activate question generation, which eventually hinders the model performance. In this work, we propose an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference. Experiments on the OR-ShARC dataset show the effectiveness of our method, which achieves new state-of-the-art results."
2021.emnlp-demo.1,{M}i{SS}: An Assistant for Multi-Style Simultaneous Translation,2021,-1,-1,3,1,304,zuchao li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"In this paper, we present \textbf{MiSS}, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation, flexibility, and measurable translation quality. Compared with the free commercial translation systems commonly used, our translation assistance system regards the machine translation application as a more complete and fully-featured tool for users. By incorporating additional features and giving the user better control over their experience, we improve translation efficiency and performance. Additionally, our assistant system combines machine translation, grammatical error correction, and interactive edits, and uses a crowdsourcing mode to collect more data for further training to improve both the machine translation and grammatical error correction models. A short video demonstrating our system is available at \url{https://www.youtube.com/watch?v=ZGCo7KtRKd8}."
2020.wmt-1.22,{SJTU}-{NICT}{'}s Supervised and Unsupervised Neural Machine Translation Systems for the {WMT}20 News Translation Task,2020,-1,-1,5,1,304,zuchao li,Proceedings of the Fifth Conference on Machine Translation,0,"In this paper, we introduced our joint team SJTU-NICT {`}s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,28,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.364,A {M}yanmar ({B}urmese)-{E}nglish Named Entity Transliteration Dictionary,2020,-1,-1,5,0,17404,aye mon,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Transliteration is generally a phonetically based transcription across different writing systems. It is a crucial task for various downstream natural language processing applications. For the Myanmar (Burmese) language, robust automatic transliteration for borrowed English words is a challenging task because of the complex Myanmar writing system and the lack of data. In this study, we constructed a Myanmar-English named entity dictionary containing more than eighty thousand transliteration instances. The data have been released under a CC BY-NC-SA license. We evaluated the automatic transliteration performance using statistical and neural network-based approaches based on the prepared data. The neural network model outperformed the statistical model significantly in terms of the BLEU score on the character level. Different units used in the Myanmar script for processing were also compared and discussed."
2020.findings-emnlp.371,Reference Language based Unsupervised Neural Machine Translation,2020,28,0,4,1,304,zuchao li,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel corpus. The rise of unsupervised neural machine translation (UNMT) almost completely relieves the parallel corpus curse, though UNMT is still subject to unsatisfactory performance due to the vagueness of the clues available for its core back-translation training. Further enriching the idea of pivot translation by extending the use of parallel corpora beyond the source-target paradigm, we propose a new reference language-based framework for UNMT, RUNMT, in which the reference language only shares a parallel corpus with the source, but this corpus still indicates a signal clear enough to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline that uses only one auxiliary language, demonstrating the usefulness of the proposed reference language-based UNMT and establishing a good start for the community."
2020.coling-main.374,Robust Unsupervised Neural Machine Translation with Adversarial Denoising Training,2020,27,1,5,1,4177,haipeng sun,Proceedings of the 28th International Conference on Computational Linguistics,0,"Unsupervised neural machine translation (UNMT) has recently attracted great interest in the machine translation community. The main advantage of the UNMT lies in its easy collection of required large training text sentences while with only a slightly worse performance than supervised neural machine translation which requires expensive annotated translation pairs on some translation tasks. In most studies, the UMNT is trained with clean data without considering its robustness to the noisy data. However, in real-world scenarios, there usually exists noise in the collected input sentences which degrades the performance of the translation system since the UNMT is sensitive to the small perturbations of the input sentences. In this paper, we first time explicitly take the noisy data into consideration to improve the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios."
2020.coling-main.376,Improving Low-Resource {NMT} through Relevance Based Linguistic Features Incorporation,2020,-1,-1,4,0,373,abhisek chakrabarty,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation."
2020.coling-main.378,Bilingual Subword Segmentation for Neural Machine Translation,2020,-1,-1,2,0,12486,hiroyuki deguchi,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper proposed a new subword segmentation method for neural machine translation, {``}Bilingual Subword Segmentation,{''} which tokenizes sentences to minimize the difference between the number of subword units in a sentence and that of its translation. While existing subword segmentation methods tokenize a sentence without considering its translation, the proposed method tokenizes a sentence by using subword units induced from bilingual sentences; this method could be more favorable to machine translation. Evaluations on WAT Asian Scientific Paper Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU)."
2020.acl-main.34,Content Word Aware Neural Machine Translation,2020,-1,-1,3,1,4178,kehai chen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT."
2020.acl-main.44,A Three-Parameter Rank-Frequency Relation in Natural Languages,2020,-1,-1,2,1,285,chenchen ding,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present that, the rank-frequency relation in textual data follows $f \propto r^{-\alpha}(r+\gamma)^{-\beta}$, where $f$ is the token frequency and $r$ is the rank by frequency, with ($\alpha$, $\beta$, $\gamma$) as parameters. The formulation is derived based on the empirical observation that $d^2 (x+y)/dx^2$ is a typical impulse function, where $(x,y)=(\log r, \log f)$. The formulation is the power law when $\beta=0$ and the Zipf{--}Mandelbrot law when $\alpha=0$. We illustrate that $\alpha$ is related to the analytic features of syntax and $\beta+\gamma$ to those of morphology in natural languages from an investigation of multilingual corpora."
2020.acl-main.324,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,2020,37,1,4,1,4177,haipeng sun,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs."
W19-7201,Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation,2019,23,0,2,0,1478,junya ono,Proceedings of The 8th Workshop on Patent and Scientific Literature Translation,0,"Reduction of training time is an important issue in many tasks like patent translation involving neural networks. Data parallelism and model parallelism are two common approaches for reducing training time using multiple graphics processing units (GPUs) on one machine. In this paper, we propose a hybrid data-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent neural network (RNN) machine translation. We apply a model parallel approach to the RNN encoder-decoder part of the Seq2Seq model and a data parallel approach to the attention-softmax part of the model. We achieved a speed-up of 4.13 to 4.20 times when using 4 GPUs compared with the training speed when using 1 GPU without affecting machine translation accuracy as measured in terms of BLEU scores."
W19-6601,Online Sentence Segmentation for Simultaneous Interpretation using Multi-Shifted Recurrent Neural Network,2019,0,0,2,1,23611,xiaolin wang,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5313,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task,2019,0,2,6,0.139312,286,raj dabre,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for KazakhâEnglish, GujaratiâEnglish, ChineseâEnglish, and EnglishâFinnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: KazakhâEnglish and GujaratiâEnglish translation. For the ChineseâEnglish translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of ChineseâEnglish. For EnglishâFinnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year{'}s task."
W19-5330,{NICT}{'}s Unsupervised Neural and Statistical Machine Translation Systems for the {WMT}19 News Translation Task,2019,0,4,6,0.573477,8610,benjamin marie,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper presents the NICT{'}s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers ({``}constraint{'}{''}), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions."
P19-1119,Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation,2019,0,7,4,1,4177,haipeng sun,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT."
P19-1174,Neural Machine Translation with Reordering Embeddings,2019,0,5,3,1,4178,kehai chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT{'}14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer."
P19-1296,Sentence-Level Agreement for Neural Machine Translation,2019,0,3,4,0,25711,mingming yang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance."
N19-1205,Improving Neural Machine Translation with Neural Syntactic Distance,2019,0,3,3,1,10519,chunpeng ma,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The explicit use of syntactic information has been proved useful for neural machine translation (NMT). However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence. NSD has been used for constituent parsing, but not in machine translation. We propose five strategies to improve NMT with NSD. Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En{--}Ja), +1.3 (Ja{--}En), +1.2 (En{--}Ch), and +1.0 (Ch{--}En) BLEU)."
N19-1276,Incorporating Word Attention into Character-Based Word Segmentation,2019,0,1,2,1,126,shohei higashiyama,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Neural network models have been actively applied to word segmentation, especially Chinese, because of the ability to minimize the effort in feature engineering. Typical segmentation models are categorized as character-based, for conducting exact inference, or word-based, for utilizing word-level information. We propose a character-based model utilizing word information to leverage the advantages of both types of models. Our model learns the importance of multiple candidate words for a character on the basis of an attention mechanism, and makes use of it for segmentation decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets."
K19-2004,{SJTU}-{NICT} at {MRP} 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing,2019,0,0,5,1,304,zuchao li,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"This paper describes our SJTU-NICT{'}s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). Our system uses a graph-based approach to model a variety of semantic graph parsing tasks. Our main contributions in the submitted system are summarized as follows: 1. Our model is fully end-to-end and is capable of being trained only on the given training set which does not rely on any other extra training source including the companion data provided by the organizer; 2. We extend our graph pruning algorithm to a variety of semantic graphs, solving the problem of excessive semantic graph search space; 3. We introduce multi-task learning for multiple objectives within the same framework. The evaluation results show that our system achieved second place in the overall $F_1$ score and achieved the best $F_1$ score on the DM framework."
D19-5206,Supervised and Unsupervised Machine Translation for {M}yanmar-{E}nglish and {K}hmer-{E}nglish,2019,0,0,6,0.573477,8610,benjamin marie,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the NICT{'}s supervised and unsupervised machine translation systems for the WAT2019 Myanmar-English and Khmer-English translation tasks. For all the translation directions, we built state-of-the-art supervised neural (NMT) and statistical (SMT) machine translation systems, using monolingual data cleaned and normalized. Our combination of NMT and SMT performed among the best systems for the four translation directions. We also investigated the feasibility of unsupervised machine translation for low-resource and distant language pairs and confirmed observations of previous work showing that unsupervised MT is still largely unable to deal with them."
D19-5209,{E}nglish-{M}yanmar Supervised and Unsupervised {NMT}: {NICT}{'}s Machine Translation Systems at {WAT}-2019,2019,0,0,5,0,3690,rui wang,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the NICT{'}s participation (team ID: NICT) in the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically Myanmar (Burmese) - English task in both translation directions. We built neural machine translation (NMT) systems for these tasks. Our NMT systems were trained with language model pretraining. Back-translation technology is adopted to NMT. Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score."
D19-3027,{MY}-{AKKHARA}: A {R}omanization-based {B}urmese ({M}yanmar) Input Method,2019,0,1,2,1,285,chenchen ding,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"MY-AKKHARA is a method used to input Burmese texts encoded in the Unicode standard, based on commonly accepted Latin transcription. By using this method, arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters. Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA to realize an efficient keystroke distribution on a QWERTY keyboard. Given that the Unicode standard has not been extensively used in digitization of Burmese, we hope that MY-AKKHARA can contribute to the widespread use of Unicode in Myanmar and can provide a platform for smart input methods for Burmese in the future. An implementation of MY-AKKHARA running in Windows is released at http://www2.nict.go.jp/astrec-att/member/ding/my-akkhara.html"
D19-1139,Recurrent Positional Embedding for Neural Machine Translation,2019,0,2,3,1,4178,kehai chen,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head self-attention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the state-of-the-art Transformer baseline in WMT{'}14 English-to-German and NIST Chinese-to-English translation tasks."
Y18-3006,{E}nglish-{M}yanmar {NMT} and {SMT} with Pre-ordering: {NICT}{'}s Machine Translation Systems at {WAT}-2018,2018,0,0,3,0,3690,rui wang,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
W18-6419,{NICT}{'}s Neural and Statistical Machine Translation Systems for the {WMT}18 News Translation Task,2018,0,0,4,0.573477,8610,benjamin marie,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper presents the NICT{'}s participation to the WMT18 shared news translation task. We participated in the eight translation directions of four language pairs: Estonian-English, Finnish-English, Turkish-English and Chinese-English. For each translation direction, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems were trained with the transformer architecture using the provided parallel data enlarged with a large quantity of back-translated monolingual data that we generated with a new incremental training framework. Our primary submissions to the task are the result of a simple combination of our SMT and NMT systems. Our systems are ranked first for the Estonian-English and Finnish-English language pairs (constraint) according to BLEU-cased."
W18-6489,{NICT}{'}s Corpus Filtering Systems for the {WMT}18 Parallel Corpus Filtering Task,2018,6,0,3,0,3690,rui wang,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper presents the NICT{'}s participation in the WMT18 shared parallel corpus filtering task. The organizers provided 1 billion words German-English corpus crawled from the web as part of the Paracrawl project. This corpus is too noisy to build an acceptable neural machine translation (NMT) system. Using the clean data of the WMT18 shared news translation task, we designed several features and trained a classifier to score each sentence pairs in the noisy data. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance."
P18-2048,Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation,2018,17,0,2,0,3690,rui wang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance."
P18-2078,Simplified Abugidas,2018,0,0,2,1,285,chenchen ding,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics. We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values. This is crucial for developing more efficient input methods by reducing the complexity in abugidas. Four abugidas in the southern Brahmic family, i.e., Thai, Burmese, Khmer, and Lao, were studied using a newswire 20,000-sentence dataset. We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network, finding that the abugida graphemes could be recovered with 94{\%} - 97{\%} accuracy at the top-1 level and 98{\%} - 99{\%} at the top-4 level, even after omitting most diacritics (10 - 30 types) and merging the remaining 30 - 50 characters into 21 graphemes."
P18-1116,Forest-Based Neural Machine Translation,2018,0,1,3,1,10519,chunpeng ma,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems."
N18-1120,Guiding Neural Machine Translation with Retrieved Translation Pieces,2018,15,4,2,1,12715,jingyi zhang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call {``}translation pieces{''}. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation."
D18-2023,{C}yton{MT}: an Efficient Neural Machine Translation Open-source Toolkit Implemented in {C}++,2018,18,0,2,1,23611,xiaolin wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"This paper presents an open-source neural machine translation toolkit named CytonMT. The toolkit is built from scratch only using C++ and NVIDIA{'}s GPU-accelerated libraries. The toolkit features training efficiency, code simplicity and translation quality. Benchmarks show that cytonMT accelerates the training speed by 64.5{\%} to 110.8{\%} on neural networks of various sizes, and achieves competitive translation quality."
D18-1511,Exploring Recombination for Efficient Decoding of Neural Machine Translation,2018,0,9,3,0,1041,zhisong zhang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the {``}equivalence{''} of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient."
W17-5712,A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task,2017,0,0,4,0,296,yusuke oda,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture."
W17-4753,{NICT}-{NAIST} System for {WMT}17 Multimodal Translation Task,2017,7,3,2,1,12715,jingyi zhang,Proceedings of the Second Conference on Machine Translation,0,None
P17-2089,Sentence Embedding for Neural Machine Translation Domain Adaptation,2017,9,26,3,0.207661,3690,rui wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT{'}s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points."
I17-2049,Key-value Attention Mechanism for Neural Machine Translation,2017,12,3,2,0,287,hideya mino,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two types of memory known as the key and the value. The key is used for calculating the attention distribution, and the value is used for encoding the context representation. Experiments on three different tasks indicate that our model outperforms an NMT model with a conventional attention mechanism. Furthermore, we perform experiments with a conventional NMT framework, in which a part of the initial value of a weight matrix is set to zero so that the matrix is as the same initial-state as the key-value attention mechanism. As a result, we obtain comparable results with the key-value attention mechanism without changing the network structure."
I17-1002,Context-Aware Smoothing for Neural Machine Translation,2017,12,2,3,1,4178,kehai chen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In Neural Machine Translation (NMT), each word is represented as a low-dimension, real-value vector for encoding its syntax and semantic information. This means that even if the word is in a different sentence context, it is represented as the fixed vector to learn source representation. Moreover, a large number of Out-Of-Vocabulary (OOV) words, which have different syntax and semantic information, are represented as the same vector representation of {``}unk{''}. To alleviate this problem, we propose a novel context-aware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems."
I17-1016,Improving Neural Machine Translation through Phrase-based Forced Decoding,2017,26,1,2,1,12715,jingyi zhang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Compared to traditional statistical machine translation (SMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs."
D17-1155,Instance Weighting for Neural Machine Translation Domain Adaptation,2017,7,38,2,0.207661,3690,rui wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points."
D17-1304,Neural Machine Translation with Source Dependency Representation,2017,14,17,3,1,4178,kehai chen,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of NMT, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system."
W16-4606,Global Pre-ordering for Improving Sublanguage Translation,2016,14,2,2,1,33573,masaru fuji,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage. The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations."
W16-4613,An Efficient and Effective Online Sentence Segmenter for Simultaneous Interpretation,2016,0,1,3,1,23611,xiaolin wang,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"Simultaneous interpretation is a very challenging application of machine translation in which the input is a stream of words from a speech recognition engine. The key problem is how to segment the stream in an online manner into units suitable for translation. The segmentation process proceeds by calculating a confidence score for each word that indicates the soundness of placing a sentence boundary after it, and then heuristics are employed to determine the position of the boundaries. Multiple variants of the confidence scoring method and segmentation heuristics were studied. Experimental results show that the best performing strategy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation."
W16-4614,Similar {S}outheast {A}sian Languages: Corpus-Based Case Study on {T}hai-{L}aotian and {M}alay-{I}ndonesian,2016,1,1,2,1,285,chenchen ding,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper illustrates the similarity between Thai and Laotian, and between Malay and Indonesian, based on an investigation on raw parallel data from Asian Language Treebank. The cross-lingual similarity is investigated and demonstrated on metrics of correspondence and order of tokens, based on several standard statistical machine translation techniques. The similarity shown in this study suggests a possibility on harmonious annotation and processing of the language pairs in future development."
P16-1130,A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation,2016,23,0,2,1,12715,jingyi zhang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1046,Agreement on Target-bidirectional Neural Machine Translation,2016,19,22,2,0,3591,lemao liu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1249,Introducing the {A}sian Language Treebank ({ALT}),2016,0,15,3,0.833333,312,ye thu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper introduces the ALT project initiated by the Advanced Speech Translation Research and Development Promotion Center (ASTREC), NICT, Kyoto, Japan. The aim of this project is to accelerate NLP research for Asian languages such as Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese. The original resource for this project was English articles that were randomly selected from Wikinews. The project has so far created a corpus for Myanmar and will extend in scope to include other languages in the near future. A 20000-sentence corpus of Myanmar that has been manually translated from an English corpus has been word segmented, word aligned, part-of-speech tagged and constituency parsed by human annotators. In this paper, we present the implementation steps for creating the treebank in detail, including a description of the ALT web-based treebanking tool. Moreover, we report statistics on the annotation quality of the Myanmar treebank created so far."
L16-1350,{ASPEC}: {A}sian Scientific Paper Excerpt Corpus,2016,0,24,4,0,283,toshiaki nakazawa,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we describe the details of the ASPEC (Asian Scientific Paper Excerpt Corpus), which is the first large-size parallel corpus of scientific paper domain. ASPEC was constructed in the Japanese-Chinese machine translation project conducted between 2006 and 2010 using the Special Coordination Funds for Promoting Science and Technology. It consists of a Japanese-English scientific paper abstract corpus of approximately 3 million parallel sentences (ASPEC-JE) and a Chinese-Japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (ASPEC-JC). ASPEC is used as the official dataset for the machine translation evaluation workshop WAT (Workshop on Asian Translation)."
C16-2007,A Prototype Automatic Simultaneous Interpretation System,2016,4,1,3,1,23611,xiaolin wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"Simultaneous interpretation allows people to communicate spontaneously across language boundaries, but such services are prohibitively expensive for the general public. This paper presents a fully automatic simultaneous interpretation system to address this problem. Though the development is still at an early stage, the system is capable of keeping up with the fastest of the TED speakers while at the same time delivering high-quality translations. We believe that the system will become an effective tool for facilitating cross-lingual communication in the future."
C16-2008,{M}u{TUAL}: A Controlled Authoring Support System Enabling Contextual Machine Translation,2016,3,0,5,0,10715,rei miyata,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"The paper introduces a web-based authoring support system, MuTUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system."
C16-1291,Neural Machine Translation with Supervised Attention,2016,28,24,2,0,3591,lemao liu,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The attention mechanism is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in alignment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT."
C16-1295,Connecting Phrase based Statistical Machine Translation Adaptation,2016,26,1,4,0.207661,3690,rui wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Although more additional corpora are now available for Statistical Machine Translation (SMT), only the ones which belong to the same or similar domains of the original corpus can indeed enhance SMT performance directly. A series of SMT adaptation methods have been proposed to select these similar-domain data, and most of them focus on sentence selection. In comparison, phrase is a smaller and more fine grained unit for data selection, therefore we propose a straightforward and efficient connecting phrase based adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram adaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods)."
Y15-1030,A Large-scale Study of Statistical Machine Translation Methods for {K}hmer Language,2015,18,0,4,1,312,ye thu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"This paper contributes the first published evaluation of the quality of automatic translation between Khmer (the official language of Cambodia) and twenty other languages, in both directions. The experiments were carried out using three different statistical machine translation approaches: phrase-based, hierarchical phrase-based, and the operation sequence model (OSM). In addition two different segmentation schemes for Khmer were studied, these were syllable segmentation and supervised word segmentation. The results show that the highest quality machine translation was attained with word segmentation in all of the experiments. Furthermore, with the exception of very distant language pairs the OSM approach gave the highest quality translations when measured in terms of both the BLEU and RIBES scores. For distant languages, our results showed a hierarchical phrase-based approach to be the most effective. An analysis of the experimental results indicated that Kendallxe2x80x99s tau may be directly used as a means of selecting an appropriate machine translation approach for a given language pair."
W15-5004,{NICT} at {WAT} 2015,2015,-1,-1,2,1,285,chenchen ding,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-4945,{MNH}-{TT}: A Platform to Support Collaborative Translator Training,2015,0,0,1,1,127,masao utiyama,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
P15-2089,Learning Word Reorderings for Hierarchical Phrase-based Statistical Machine Translation,2015,35,3,2,1,12715,jingyi zhang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Statistical models for reordering source words have been used to enhance hierarchical phrase-based statistical machine translation. There are existing word-reordering models that learn reorderings for any two source words in a sentence or only for two contiguous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distances less than a specific threshold are useful to improve translation quality. Compared with previous work, our method more effectively and efficiently exploits helpful word-reordering information; it improves a basic hierarchical phrase-based system by 2.4-3.1 BLEU points and keeps the average time of translating one sentence under 10 s."
D15-1119,Improving fast{\\_}align by Reordering,2015,18,2,2,1,285,chenchen ding,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"fast align is a simple, fast, and efficient approach for word alignment based on the IBM model 2. fast align performs well for language pairs with relatively similar word orders; however, it does not perform well for language pairs with drastically different word orders. We propose a segmenting-reversing reordering process to solve this problem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported."
D15-1128,Hierarchical Phrase-based Stream Decoding,2015,12,0,3,0,16459,andrew finch,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a method for hierarchical phrase-based stream decoding. A stream decoder is able to take a continuous stream of tokens as input, and segments this stream into word sequences that are translated and output as a stream of target word sequences. Phrase-based stream decoding techniques have been shown to be effective as a means of simultaneous interpretation. In this paper we transfer the essence of this idea into the framework of hierarchical machine translation. The hierarchical decoding framework organizes the decoding process into a chart; this structure is naturally suited to the process of stream decoding, leading to an efficient stream decoding algorithm that searches a restricted subspace containing only relevant hypotheses. Furthermore, the decoder allows more explicit access to the word re-ordering process that is of critical importance in decoding while interpreting. The decoder was evaluated on TED talk data for English-Spanish and English-Chinese. Our results show that like the phrase-based stream decoder, the hierarchical is capable of approaching the performance of the underlying hierarchical phrase-based machine translation decoder, at useful levels of latency. In addition the hierarchical approach appeared to be robust to the difficulties presented by the more challenging English-Chinese task."
D15-1209,Leave-one-out Word Alignment without Garbage Collector Effects,2015,29,4,2,1,23611,xiaolin wang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Expectation-maximization algorithms, such as those implemented in GIZA pervade the field of unsupervised word alignment. However, these algorithms have a problem of over-fitting, leading to xe2x80x9cgarbage collector effects,xe2x80x9d where rare words tend to be erroneously aligned to untranslated words. This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem. The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it. This prevents erroneous alignments within a sentence pair from supporting themselves. Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% xe2x80x90 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 xe2x80x90 1.30. The proposed method also outperformed l0-normalized GIZA and Kneser-Ney smoothed GIZA."
D15-1250,A Binarized Neural Network Joint Model for Machine Translation,2015,17,2,2,1,12715,jingyi zhang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks."
2015.mtsummit-papers.1,Patent claim translation based on sublanguage-specific sentence structure,2015,-1,-1,3,1,33573,masaru fuji,Proceedings of Machine Translation Summit XV: Papers,0,None
2015.iwslt-papers.17,Risk-aware distribution of {SMT} outputs for translation of documents targeting many anonymous readers,2015,-1,-1,2,0,260,yo ehara,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.eamt-1.46,{MNH}-{TT}: A Platform to Support Collaborative Translator Training,2015,0,0,1,1,127,masao utiyama,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-7011,Word Order Does {NOT} Differ Significantly Between {C}hinese and {J}apanese,2014,-1,-1,2,1,285,chenchen ding,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
P14-2026,Dependency-based Pre-ordering for {C}hinese-{E}nglish Machine Translation,2014,19,13,2,0,38092,jingsheng cai,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders. This paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT. We present a set of dependency-based preordering rules which improved the BLEU score by 1.61 on the NIST 2006 evaluation data. We also investigate the accuracy of the rule set by conducting human evaluations."
P14-2122,Empirical Study of Unsupervised {C}hinese Word Segmentation Methods for {SMT} on Large-scale Corpora,2014,25,7,2,1,23611,xiaolin wang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmentation for alignment. Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity. This paper proposes an efficient unified PYP-based monolingual and bilingual UWS method. Experimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain."
D14-1022,Learning Hierarchical Translation Spans,2014,23,10,2,1,12715,jingyi zhang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model. Our model evaluates if a source span should be covered by translation rules during decoding, which is integrated into the translation system as soft constraints. Compared to syntactic constraints, our model is directly acquired from an aligned parallel corpus and does not require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system."
D14-1023,Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation,2014,40,19,4,0.308177,3690,rui wang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus."
D14-1173,Refining Word Segmentation Using a Manually Aligned Corpus for Statistical Machine Translation,2014,26,4,2,1,23611,xiaolin wang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Languages that have no explicit word delimiters often have to be segmented for statistical machine translation (SMT). This is commonly performed by automated segmenters trained on manually annotated corpora. However, the word segmentation (WS) schemes of these annotated corpora are handcrafted for general usage, and may not be suitable for SMT. An analysis was performed to test this hypothesis using a manually annotated word alignment (WA) corpus for Chinese-English SMT. An analysis revealed that 74.60% of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank (CTB) will contain conflicts with the gold WA annotations. We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts. Experimental results show that the refined WS reduced word alignment error rate by 6.82% and achieved the highest BLEU improvement (0.63 on average) on the Chinese-English open machine translation (OpenMT) corpora compared to related work."
2014.iwslt-papers.5,"Empircal dependency-based head finalization for statistical {C}hinese-, {E}nglish-, and {F}rench-to-{M}yanmar ({B}urmese) machine translation",2014,23,5,3,1,285,chenchen ding,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"We conduct dependency-based head finalization for statistical machine translation (SMT) for Myanmar (Burmese). Although Myanmar is an understudied language, linguistically it is a head-final language with similar syntax to Japanese and Korean. So, applying the efficient techniques of Japanese and Korean processing to Myanmar is a natural idea. Our approach is a combination of two approaches. The first is a head-driven phrase structure grammar (HPSG) based head finalization for English-to-Japanese translation, the second is dependency-based pre-ordering originally designed for English-to-Korean translation. We experiment on Chinese-, English-, and French-to-Myanmar translation, using a statistical pre-ordering approach as a comparison method. Experimental results show the dependency-based head finalization was able to consistently improve a baseline SMT system, for different source languages and different segmentation schemes for the Myanmar language."
2014.iwslt-evaluation.20,The {NICT} translation system for {IWSLT} 2014,2014,-1,-1,3,1,23611,xiaolin wang,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes NICT{'}s participation in the IWSLT 2014 evaluation campaign for the TED Chinese-English translation shared-task. Our approach used a combination of phrase-based and hierarchical statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model."
2014.amta-researchers.9,Document-level re-ranking with soft lexical and semantic features for statistical machine translation,2014,28,0,2,1,285,chenchen ding,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,We introduce two document-level features to polish baseline sentence-level translations generated by a state-of-the-art statistical machine translation (SMT) system. One feature uses the word-embedding technique to model the relation between a sentence and its context on the target side; the other feature is a crisp document-level token-type ratio of target-side translations for source-side words to model the lexical consistency in translation. The weights of introduced features are tuned to optimize the sentence- and document-level metrics simultaneously on the basis of Pareto optimality. Experimental results on two different schemes with different corpora illustrate that the proposed approach can efficiently and stably integrate document-level information into a sentence-level SMT system. The best improvements were approximately 0.5 BLEU on test sets with statistical significance.
W13-4503,Rescue Activity for the Great {E}ast {J}apan Earthquake Based on a Website that Extracts Rescue Requests from the Net,2013,-1,-1,3,0,40662,shin aida,Proceedings of the Workshop on Language Processing and Crisis Information 2013,0,None
P13-1016,Distortion Model Considering Rich Context for Statistical Machine Translation,2013,26,6,2,1,288,isao goto,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models."
D13-1082,Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation,2013,24,24,2,0.308177,3690,rui wang,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking."
P12-2061,Post-ordering by Parsing for {J}apanese-{E}nglish Statistical Machine Translation,2012,25,23,2,1,288,isao goto,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Reordering is a difficult task in translating between widely different languages such as Japanese and English. We employ the post-ordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language."
2012.tc-1.1,{MNH}-{TT}: a collaborative platform for translator training,2012,-1,-1,5,0,12054,bogdan babych,Proceedings of Translating and the Computer 34,0,None
2012.iwslt-evaluation.15,Minimum {B}ayes-risk decoding extended with similar examples: {NAIST}-{NCT} at {IWSLT} 2012,2012,10,0,2,0,39435,hiroaki shimizu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes our methods used in the NAIST-NICT submission to the International Workshop on Spoken Language Translation (IWSLT) 2012 evaluation campaign. In particular, we propose two extensions to minimum bayes-risk decoding which reduces a expected loss."
P11-2076,Reordering Constraint Based on Document-Level Context,2011,17,2,2,1,41606,takashi onishi,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the document-level context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation."
2011.mtsummit-papers.37,Searching Translation Memories for Paraphrases,2011,-1,-1,1,1,127,masao utiyama,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.40,A Comparison of Unsupervised Bilingual Term Extraction Methods Using Phrase-Tables,2011,-1,-1,3,0,44906,masamichi ideue,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.51,A Comparison Study of Parsers for Patent Machine Translation,2011,-1,-1,2,1,288,isao goto,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.iwslt-papers.11,Annotating data selection for improving machine translation,2011,7,0,3,0,4955,keiji yasuda,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"In order to efficiently improve machine translation systems, we propose a method which selects data to be annotated (manually translated) from speech-to-speech translation field data. For the selection experiments, we used data from field experiments conducted during the 2009 fiscal year in five areas of Japan. For the selection experiments, we used data sets from two areas: one data set giving the lowest baseline speech translation performance for its test set, and another data set giving the highest. In the experiments, we compare two methods for selecting data to be manually translated from the field data. Both of them use source side language models for data selection, but in different manners. According to the experimental results, either or both of the methods show larger improvements compared to a random data selection."
W10-3508,"Helping Volunteer Translators, Fostering Language Resources",2010,6,0,1,1,127,masao utiyama,Proceedings of the 2nd Workshop on {T}he {P}eople{'}s {W}eb {M}eets {NLP}: {C}ollaboratively {C}onstructed {S}emantic {R}esources,0,"This paper introduces a website called Minna no Honxe2x80x99yaku(MNH, xe2x80x9cTranslation for Allxe2x80x9d), which hosts online volunteer translators. Its core features are (1) a set of translation aid tools, (2) high quality, comprehensive language resources, and (3) the legal sharing of translations. As of May 2010, there are about 1200 users and 4 groups registered to MNH. The groups using it include such major"
P10-2001,Paraphrase Lattice for Statistical Machine Translation,2010,12,25,2,1,41606,takashi onishi,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets."
abekawa-etal-2010-community,Community-based Construction of Draft and Final Translation Corpus Through a Translation Hosting Site Minna no Hon{'}yaku ({MNH}),2010,6,6,2,0,35650,takeshi abekawa,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper we report a way of constructing a translation corpus that contains not only source and target texts, but draft and final versions of target texts, through the translation hosting site Minna no Hon'yaku (MNH). We made MNH publicly available on April 2009. Since then, more than 1,000 users have registered and over 3,500 documents have been translated, as of February 2010, from English to Japanese and from Japanese to English. MNH provides an integrated translation-aid environment, QRedit, which enables translators to look up high-quality dictionaries and Wikipedia as well as to search Google seamlessly. As MNH keeps translation logs, a corpus consisting of source texts, draft translations in several versions, and final translations is constructed naturally through MNH. As of 7 February, 764 documents with multiple translation versions are accumulated, of which 110 are edited by more than one translators. This corpus can be used for self-learning by inexperienced translators on MNH, and potentially for improving machine translation."
2009.tc-1.4,"Minna no Hon{'}yaku: a website for hosting, archiving, and promoting translations",2009,-1,-1,1,1,127,masao utiyama,Proceedings of Translating and the Computer 31,0,None
2009.mtsummit-wpt.1,Exploiting Patent Information for the Evaluation of Machine Translation,2009,-1,-1,2,0,37781,atsushi fujii,Proceedings of the Third Workshop on Patent Translation,0,None
2009.mtsummit-wpt.2,Meta-evaluation of Automatic Evaluation Methods for Machine using Patent Translation Data in {NTCIR}-7,2009,-1,-1,5,0,26162,hiroshi echizenya,Proceedings of the Third Workshop on Patent Translation,0,None
2009.mtsummit-posters.10,Development of a {J}apanese-{E}nglish Software Manual Parallel Corpus,2009,-1,-1,2,0,45044,tatsuya ishisaka,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.mtsummit-posters.22,Hosting Volunteer Translators,2009,-1,-1,1,1,127,masao utiyama,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.mtsummit-papers.18,Mining Parallel Texts from Mixed-Language Web Pages,2009,-1,-1,1,1,127,masao utiyama,Proceedings of Machine Translation Summit XII: Papers,0,None
2009.iwslt-evaluation.12,Two methods for stabilizing {MERT},2009,5,6,1,1,127,masao utiyama,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes the NICT SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2009 evaluation campaign. We participated in the Challenge Task. Our system was based on a fairly common phrase-based machine translation system. We used two methods for stabilizing MERT.
isahara-etal-2008-development,Development of the {J}apanese {W}ord{N}et,2008,6,79,4,0,15925,hitoshi isahara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"After a long history of compilation of our own lexical resources, EDR Japanese/English Electronic Dictionary, and discussions with major players on development of various WordNets, Japanese National Institute of Information and Communications Technology started developing the Japanese WordNet in 2006 and will publicly release the first version, which includes both the synset in Japanese and the annotated Japanese corpus of SemCor, in June 2008. As the first step in compiling the Japanese WordNet, we added Japanese equivalents to synsets of the Princeton WordNet. Of course, we must also add some synsets which do not exist in the Princeton WordNet, and must modify synsets in the Princeton WordNet, in order to make the hierarchical structure of Princeton synsets represent thesaurus-like information found in the Japanese language, however, we will address these tasks in a future study. We then translated English sentences which are used in the SemCor annotation into Japanese and annotated them using our Japanese WordNet. This article describes the overview of our project to compile Japanese WordNet and other resources which relate to our Japanese WordNet."
fujii-etal-2008-producing,Producing a Test Collection for Patent Machine Translation in the Seventh {NTCIR} Workshop,2008,9,2,2,0,37781,atsushi fujii,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In aiming at research and development on machine translation, we produced a test collection for Japanese-English machine translation in the seventh NTCIR Workshop. This paper describes details of our test collection. From patent documents published in Japan and the United States, we extracted patent families as a parallel corpus. A patent family is a set of patent documents for the same or related invention and these documents are usually filed to more than one country in different languages. In the parallel corpus, we aligned Japanese sentences with their counterpart English sentences. Our test collection, which includes approximately 2,000,000 sentence pairs, can be used to train and test machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval and the contribution of machine translation to a patent retrieval task can also be evaluated. Our test collection will be available to the public for research purposes after the NTCIR final meeting."
isahara-etal-2008-application,Application of Resource-based Machine Translation to Real Business Scenes,2008,3,1,2,0,15925,hitoshi isahara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"As huge quantities of documents have become available, services using natural language processing technologies trained by huge corpora have emerged, such as information retrieval and information extraction. In this paper we verify the usefulness of resource-based, or corpus-based, translation in the aviation domain as a real business situation. This study is important from both a business perspective and an academic perspective. Intuitively, manuals for similar products, or manuals for different versions of the same product, are likely to resemble each other. Therefore, even with only a small training data, a corpus-based MT system can output useful translations. The corpus-based approach is powerful when the target is repetitive. Manuals for similar products, or manuals for different versions of the same product, are real-world documents that are repetitive. Our experiments on translation of manual documents are still in a beginning stage. However, the BLEU score from very small number of training sentences is already rather high. We believe corpus-based machine translation is a player full of promise in this kind of actual business scene."
2008.iwslt-evaluation.11,The {NICT}/{ATR} speech translation system for {IWSLT} 2008.,2008,13,14,1,1,127,masao utiyama,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the National Institute of Information and Communications Technology/Advanced Telecommunications Research Institute International (NICT/ATR) statistical machine translation (SMT) system used for the IWSLT 2008 evaluation campaign. We participated in the Chinese{--}English (Challenge Task), English{--}Chinese (Challenge Task), Chinese{--}English (BTEC Task), Chinese{--}Spanish (BTEC Task), and Chinese{--}English{--}Spanish (PIVOT Task) translation tasks. In the English{--}Chinese translation Challenge Task, we focused on exploring various factors for the English{--}Chinese translation because the research on the translation of English{--}Chinese is scarce compared to the opposite direction. In the Chinese{--}English translation Challenge Task, we employed a novel clustering method, where training sentences similar to the development data in terms of the word error rate formed a cluster. In the pivot translation task, we integrated two strategies for pivot translation by linear interpolation."
2008.amta-papers.8,Toward the Evaluation of Machine Translation Using Patent Information,2008,12,6,2,0,37781,atsushi fujii,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"To aid research and development in machine translation, we have produced a test collection for Japanese/English machine translation. To obtain a parallel corpus, we extracted patent documents for the same or related inventions published in Japan and the United States. Our test collection includes approximately 2000000 sentence pairs in Japanese and English, which were extracted automatically from our parallel corpus. These sentence pairs can be used to train and evaluate machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments."
N07-1061,A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation,2007,26,113,1,1,127,masao utiyama,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We compare two pivot strategies for phrase-based statistical machine translation (SMT), namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a phrase translation table (phrase-table) of the source and target language pair from two phrase-tables; one constructed from the source language and English and one constructed from English and the target language. We then use that phrase-table in a phrase-based SMT system. The sentence translation strategy means that we first translate a source language sentence into n English sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems."
2007.mtsummit-papers.63,A {J}apanese-{E}nglish patent parallel corpus,2007,-1,-1,1,1,127,masao utiyama,Proceedings of Machine Translation Summit XI: Papers,0,None
W06-1653,Relevance Feedback Models for Recommendation,2006,28,4,1,1,127,masao utiyama,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF). Our approach is based on the analogy between IR and CF, especially between CF and relevance feedback (RF). Both CF and RF exploit users' preference/relevance judgments to recommend items. We first introduce a multinomial model that combines CF and CBF in a language modeling framework. We then generalize the model to another multinomial model that approximates the Polya distribution. This generalized model outperforms the multinomial model by 3.4% for CBF and 17.4% for CF in recommending English Wikipedia articles. The performance of the generalized model for three different datasets was comparable to that of a state-of-the-art item-based CF method."
kuroda-etal-2006-getting,Getting Deeper Semantics than {B}erkeley {F}rame{N}et with {MSFA},2006,12,3,2,0,45029,kow kuroda,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper illustrates relevant details of an on-going semantic-role annotation work based on a framework called MULTILAYERED/DIMENSIONAL SEMANTIC FRAME ANALYSIS (MSFA for short) (Kuroda and Isahara, 2005b), which is inspired by, if not derived from, Frame Semantics/Berkeley FrameNet approach to semantic annotation (Lowe et al., 1997; Johnson and Fillmore, 2000)."
P05-3030,Organizing {E}nglish Reading Materials for Vocabulary Learning,2005,2,0,1,1,127,masao utiyama,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We propose a method of organizing reading materials for vocabulary learning. It enables us to select a concise set of reading texts (from a target corpus) that contains all the target vocabulary to be learned. We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus. The organized reading materials would enable learners not only to study the target vocabulary efficiently but also to gain a variety of knowledge through reading. The reading materials are available on our web site."
Y04-1017,Constructing {E}nglish Reading Courseware,2004,3,2,1,1,127,masao utiyama,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"There is a wide range of English reading materials for EFL (English as a foreign language) learners. However, it is difficult for teachers to select appropriate materials to construct courseware that can be used for an English course. We propose a method for constructing courseware from a target vocabulary and a corpus. We used the specialized vocabulary for the Test of English for International Communication (TOEIC) and articles from The Daily Yomiurinewspaper to construct effective courseware. The constructed courseware consisted of articles in which the target vocabulary frequently occurred. Evaluation of the constructed courseware is ongoing. However, students have accepted it as an effective tool for learning the TOEIC vocabulary from real texts."
W03-1607,Criterion for Judging Request Intention in Response Texts of Open-Ended Questionnaires,2003,6,4,2,0,52674,hiroko inui,Proceedings of the Second International Workshop on Paraphrasing,0,"Our general research aim is to extract the actual intentions of persons when they respond to open-ended questionnaires. These intentions include the desire to make requests, complaints, expressions of resignation and so forth, but here we focus on extracting the intention to make a request. To do so, we first have to judge whether their responses contain the intent to make a request. Therefore, as a first step, we have developed a criterion for judging the existence of request intentions in responses. This criterion, which is based on paraphrasing, is described in detail in this paper. Our assumption is that a response with request intentions can be paraphrased into a typical request expression, e.g., I would like to ..., while responses without request are not paraphrasable. The criterion is evaluated in terms of objectivity, reproducibility and effectiveness. Objectivity is demonstrated by showing that machine learning methods can learn the criterion from a set of intention-tagged data, while reproducibility, that the judgments of three annotators are reasonably consistent, and effectiveness, that judgments based not on the criterion but on intuition do not agree. This means the criterion is necessary to achieve reproducibility. These experiments indicate that the criterion can be used to judge the existence of request intentions in responses reliably."
P03-1010,Reliable Measures for Aligning {J}apanese-{E}nglish News Articles and Sentences,2003,7,144,1,1,127,masao utiyama,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public."
2002.tmi-papers.14,Correction of errors in a modality corpus used for machine translation using machine-learning,2002,-1,-1,2,0.444069,16788,masaki murata,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
S01-1033,{J}apanese Word Sense Disambiguation using the Simple {B}ayes and Support Vector Machine Methods,2001,2,16,2,0.444069,16788,masaki murata,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"We submitted four systems to the Japanese dictionary-based lexical-sample task of Senseval-2. They were i) the support vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method, and it obtained higher precision. An explanation of these systems used in Japanese word sense disambiguation was provided."
P01-1064,A Statistical Model for Domain-Independent Text Segmentation,2001,22,216,1,1,127,masao utiyama,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system."
C00-2128,A Statistical Approach to the Processing of Metonymy,2000,16,17,1,1,127,masao utiyama,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"This paper describes a statistical approach to the interpretation of metonymy. A metonymy is received as an input, then its possible interpretations are ranked by applying a statistical measure. The method has been tested experimentally. It correctly interpreted 53 out of 75 metonymies in Japanese."
C00-2129,Multi-Topic Multi-Document Summarization,2000,11,3,1,1,127,masao utiyama,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Summarization of multiple documents featuring multiple topics is discussed. The example treated here consists of fifty articles about the Peru hostage incident for December 1996 through April 1997. They include a lot of topics such as opening, negotiation, ending, and so on. The method proposed in this paper is based on spreading activation over documents syntactically and scmantically annotated with GDA (Global Document Annotation) tags. The method extracts important documents and important parts therein, and creates a network consisting of important entities and relations among them. It also identifies cross-document coreferences to replace expressions with more concrete ones. The method is essentially multilingual due to the language-independence of the GDA tagset. This tagset can provide a standard format for the study on the transformation and/or generation stage of summarization process, among other natural language processing tasks."
W99-0204,Automatic Slide Presentation from Semantically Annotated Documents,1999,5,14,1,1,127,masao utiyama,Coreference and Its Applications,0,"This paper discusses how to automatically generate slide shows. The reported presentation system inputs documents annotated with the GDA tagset, an XML tagset which allows machines to automatically infer the semantic structure underlying the raw documents. The system picks up important topics in the input document on the basis of the semantic dependencies and coreferences identified from the tags. This topic selection depends also on interactions with the audience, leading to dynamic adaptation of the presentation. A slide is composed for each topic by extracting relevant sentences and paraphrasing them to an itemized summary. Some heuristics are employed here for paraphrasing and layout. Since the GDA tagset is independent of the domain and style of documents and applicable to diverse natural languages, the reported system is also domain/style independent and easy to adapt to different languages."
