D18-1374,Content Explorer: Recommending Novel Entities for a Document Writer,2018,0,1,2,0,20418,michal lukasik,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However, the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper, we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a user study to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the deep learning models."
P12-2006,Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation,2012,13,4,3,0,7150,joern wuebker,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% Bleu, whereas our approach yields 20.0% with identical models."
D12-1089,A Systematic Comparison of Phrase Table Pruning Techniques,2012,28,35,1,1,30618,richard zens,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods."
2008.iwslt-papers.8,Improvements in dynamic programming beam search for phrase-based statistical machine translation.,2008,16,35,1,1,30618,richard zens,Proceedings of the 5th International Workshop on Spoken Language Translation: Papers,0,"Search is a central component of any statistical machine translation system. We describe the search for phrase-based SMT in detail and show its importance for achieving good translation quality. We introduce an explicit distinction between reordering and lexical hypotheses and organize the pruning accordingly. We show that for the large Chinese-English NIST task already a small number of lexical alternatives is sufficient, whereas a large number of reordering hypotheses is required to achieve good translation quality. The resulting system compares favorably with the current stateof-the-art, in particular we perform a comparison with cube pruning as well as with Moses."
W07-0401,Chunk-Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation,2007,29,69,2,0,7420,yuqi zhang,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"In this paper, we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation. First, we shallow parse the source language sentences. Then, reordering rules are automatically learned from source-side chunks and word alignments. During translation, the rules are used to generate a reordering lattice for each sentence. Experimental results are reported for a Chinese-to-English task, showing an improvement of 0.5%--1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level."
P07-2026,Minimum {B}ayes Risk Decoding for {BLEU},2007,9,20,2,0,49154,nicola ehling,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"We present a Minimum Bayes Risk (MBR) decoder for statistical machine translation. The approach aims to minimize the expected loss of translation errors with regard to the BLEU score. We show that MBR decoding on N-best lists leads to an improvement of translation quality.n n We report the performance of the MBR decoder on four different tasks: the TC-STAR EPPS Spanish-English task 2006, the NIST Chinese-English task 2005 and the GALE Arabic-English and Chinese-English task 2006. The absolute improvement of the BLEU score is between 0.2% for the TC-STAR task and 1.1% for the GALE Chinese-English task."
P07-2045,{M}oses: Open Source Toolkit for Statistical Machine Translation,2007,13,3819,10,0,4417,philipp koehn,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
N07-2015,Are Very Large {N}-Best Lists Useful for {SMT}?,2007,7,18,2,0.740741,34439,savsa hasan,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"This paper describes an efficient method to extract large n-best lists from a word graph produced by a statistical machine translation system. The extraction is based on the k shortest paths algorithm which is efficient even for very large k. We show that, although we can generate large amounts of distinct translation hypotheses, these numerous candidates are not able to significantly improve overall system performance. We conclude that large n-best lists would benefit from better discriminating models."
N07-1062,Efficient Phrase-Table Representation for Machine Translation with Applications to Online {MT} and Speech Translation,2007,10,46,1,1,30618,richard zens,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"In phrase-based statistical machine translation, the phrase-table requires a large amount of memory. We will present an efficient representation with two key properties: on-demand loading and a prefix tree structure for the source phrases. We will show that this representation scales well to large data tasks and that we are able to store hundreds of millions of phrase pairs in the phrase-table. For the large Chinesexe2x80x90 English NIST task, the memory requirements of the phrase-table are reduced to less than 20MB using the new representation with no loss in translation quality and speed. Additionally, the new representation is not limited to a specific test set, which is important for online or real-time machine translation. One problem in speech translation is the matching of phrases in the input word graph and the phrase-table. We will describe a novel algorithm that effectively solves this combinatorial problem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality."
D07-1055,A Systematic Comparison of Training Criteria for Statistical Machine Translation,2007,21,30,1,1,30618,richard zens,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We address the problem of training the free parameters of a statistical machine translation system. We show significant improvements over a state-of-the-art minimum error rate training baseline on a large ChineseEnglish translation task. We present novel training criteria based on maximum likelihood estimation and expected loss computation. Additionally, we compare the maximum a-posteriori decision rule and the minimum Bayes risk decision rule. We show that, not only from a theoretical point of view but also in terms of translation quality, the minimum Bayes risk decision rule is preferable."
2007.iwslt-1.3,Improved chunk-level reordering for statistical machine translation,2007,20,30,2,0,7420,yuqi zhang,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"Inspired by previous chunk-level reordering approaches to statistical machine translation, this paper presents two methods to improve the reordering at the chunk level. By introducing a new lattice weighting factor and by reordering the training source data, an improvement is reported on TER and BLEU. Compared to the previous chunklevel reordering approach, the BLEU score improves 1.4{\%} absolutely. The translation results are reported on IWSLT Chinese-English task."
W06-3108,Discriminative Reordering Models for Statistical Machine Translation,2006,23,104,1,1,30618,richard zens,Proceedings on the Workshop on Statistical Machine Translation,0,"We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system."
W06-3110,N-Gram Posterior Probabilities for Statistical Machine Translation,2006,17,68,1,1,30618,richard zens,Proceedings on the Workshop on Statistical Machine Translation,0,"Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation. We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality. Additionally, we will introduce a sentence length model based on posterior probabilities.n n We will show significant improvements on the Chinese-English NIST task. The absolute improvements of the BLEU score is between 1.1% and 1.6%."
W06-3111,Partitioning Parallel Documents Using Binary Segmentation,2006,11,8,2,1,4017,jia xu,Proceedings on the Workshop on Statistical Machine Translation,0,"In statistical machine translation, large numbers of parallel sentences are required to train the model parameters. However, plenty of the bilingual language resources available on web are aligned only at the document level. To exploit this data, we have to extract the bilingual sentences from these documents.n n The common method is to break the documents into segments using predefined anchor words, then these segments are aligned. This approach is not error free, incorrect alignments may decrease the translation quality.n n We present an alternative approach to extract the parallel sentences by partitioning a bilingual document into two pairs. This process is performed recursively until all the sub-pairs are short enough.n n In experiments on the Chinese-English FBIS data, our method was capable of producing translation results comparable to those of a state-of-the-art sentence aligner. Using a combination of the two approaches leads to better translation performance."
P06-2061,Integration of Speech to Computer-Assisted Translation Using Finite-State Automata,2006,24,9,2,0,5805,shahram khadivi,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"State-of-the-art computer-assisted translation engines are based on a statistical prediction engine, which interactively provides completions to what a human translator types. The integration of human speech into a computer-assisted system is also a challenging area and is the aim of this paper. So far, only a few methods for integrating statistical machine translation (MT) models with automatic speech recognition (ASR) models have been studied. They were mainly based on N-best rescoring approach. N-best rescoring is not an appropriate search method for building a real-time prediction engine. In this paper, we study the incorporation of MT models and ASR models using finite-state automata. We also propose some transducers based on MT models for rescoring the ASR word graphs."
2006.iwslt-evaluation.8,The {JHU} workshop 2006 {IWSLT} system,2006,10,17,2,0,34780,wade shen,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the SMT we built during the 2006 JHU Summer Workshop for the IWSLT 2006 evaluation. Our effort focuses on two parts of the speech translation problem: 1) efficient decoding of word lattices and 2) novel applications of factored translation models to IWSLT-specific problems. In this paper, we present results from the open-track Chinese-to-English condition. Improvements of 5-10% relative BLEU are obtained over a high performing baseline. We introduce a new open-source decoder that implements the state-of-the-art in statistical machine translation."
2006.iwslt-evaluation.15,The {RWTH} statistical machine translation system for the {IWSLT} 2006 evaluation,2006,26,48,2,0,42680,arne mauser,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. The system was ranked first with respect to the BLEU measure in all language pairs it was used Using a two-pass aproach, we first generate the N best translation candidates. The second pass consists of rescoring and reranking these candidates. We will give a description of the search algorithm as well as of the models used in each pass. We will also describe our method for dealing with punctuation restoration, in order to overcome the difficulties of spoken language translation. This work also includes a brief description of the system combination done by the partners participating in the European TC-Star project."
2006.eamt-1.11,A Flexible Architecture for {CAT} Applications,2006,-1,-1,3,0.740741,34439,savsa hasan,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
W05-0831,Novel Reordering Approaches in Phrase-Based Statistical Machine Translation,2005,16,80,4,0,49189,stephan kanthak,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality."
W05-0834,Word Graphs for Statistical Machine Translation,2005,14,22,1,1,30618,richard zens,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"Word graphs have various applications in the field of machine translation. Therefore it is important for machine translation systems to produce compact word graphs of high quality. We will describe the generation of word graphs for state of the art phrase-based statistical machine translation. We will use these word graph to provide an analysis of the search process. We will evaluate the quality of the word graphs using the well-known graph word error rate. Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score.n n Experimental results are presented for two Chinese--English tasks: the small IWSLT task and the NIST large data track task. For both tasks, we achieve significant reductions of the graph error rate already with compact word graphs."
2005.mtsummit-papers.34,Statistical Machine Translation of {E}uropean Parliamentary Speeches,2005,-1,-1,4,0,5800,david vilar,Proceedings of Machine Translation Summit X: Papers,0,"In this paper we present the ongoing work at RWTH Aachen University for building a speech-to-speech translation system within the TC-Star project. The corpus we work on consists of parliamentary speeches held in the European Plenary Sessions. To our knowledge, this is the first project that focuses on speech-to-speech translation applied to a real-life task. We describe the statistical approach used in the development of our system and analyze its performance under different conditions: dealing with syntactically correct input, dealing with the exact transcription of speech and dealing with the (noisy) output of an automatic speech recognition system. Experimental results show that our system is able to perform adequately in each of these conditions."
2005.iwslt-1.18,Integrated {C}hinese Word Segmentation in Statistical Machine Translation,2005,12,36,3,1,4017,jia xu,Proceedings of the Second International Workshop on Spoken Language Translation,0,"A Chinese sentence is represented as a sequence of characters, and words are not separated from each other. In statistical machine translation, the conventional approach is to segment the Chinese character sequence into words during the pre-processing. The training and translation are performed afterwards. However, this method is not optimal for two reasons: 1. The segmentations may be erroneous. 2. For a given character sequence, the best segmentation depends on its context and translation. In order to minimize the translation errors, we take different segmentation alternatives instead of a single segmentation into account and integrate the segmentation process with the search for the best translation. The segmentation decision is only taken during the generation of the translation. With this method we are able to translate Chinese text at the character level. The experiments on the IWSLT 2005 task showed improvements in the translation performance using two translation systems: a phrase-based system and a finite state transducer based system. For the phrase-based system, the improvement of the BLEU score is 1.5% absolute."
2005.iwslt-1.20,The {RWTH} Phrase-based Statistical Machine Translation System,2005,24,46,1,1,30618,richard zens,Proceedings of the Second International Workshop on Spoken Language Translation,0,"We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation 2005. We use a two pass approach. In the first pass, we generate a list of the N best translation candidates. The second pass consists of rescoring and reranking this N -best list. We will give a description of the search algorithm as well as the models that are used in each pass. We participated in the supplied data tracks for manual transcriptions for the following translation directions: Arabic-English, Chinese-English, English-Chinese and Japanese-English. For Japanese-English, we also participated in the C-Star track. In addition, we performed translations of automatic speech recognition output for ChineseEnglish and Japanese-English. For both language pairs, we translated the single-best ASR hypotheses. Additionally, we translated Chinese ASR lattices."
2005.eamt-1.6,Comparison of generation strategies for interactive machine translation,2005,8,17,4,1,47115,oliver bender,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Fully automatic translations are far from being perfect. Non-grammatical sentences are often produced by automatic systems and there is even no guarantee that the meaning of the sentence is preserved. Nevertheless, automatic translation systems can be used to help human translators to produce high-quality translations. This is the goal of the TransType2 project, where an interactive translation tool is being developed that suggests, in real time, possible completions for the sentences that the human translator is typing. This leads to a modification of the generation strategy of the translation system, as now we are looking for the best translation of the given source sentence that is compatible with the prefix. In order to remain within the tight response time constraints of such a system, some simplifications have to be done. In this paper, we review possible generation strategies for an interactive statistical machine translation system and analyze what is the loss in performance when strict time constraints have to be met. Experiments are performed on the Spanish-English and German-English Xerox corpora, which consist of the translation of technical manuals, and the results show that the real time generation strategy causes only a small performance degradation."
2005.eamt-1.37,Sentence segmentation using {IBM} word alignment model 1,2005,12,30,2,1,4017,jia xu,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In statistical machine translation, word alignment models are trained on bilingual corpora. Long sentences pose severe problems: 1. the high computational requirements; 2. the poor quality of the resulting word alignment. We present a sentence-segmentation method that solves these problems by splitting long sentence pairs. Our approach uses the lexicon information to locate the optimal split point. This method is evaluated on two Chinese-English translation tasks in the news domain. We show that the segmentation of long sentences before training significantly improves the final translation quality of a state-of-the-art machine translation system. In one of the tasks, we achieve an improvement of the BLEU score of more than 20% relative."
W04-1118,Do We Need {C}hinese Word Segmentation for Statistical Machine Translation?,2004,10,43,2,1,4017,jia xu,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"In Chinese texts, words are not separated by white spaces. This is problematic for many natural language processing tasks. The standard approach is to segment the Chinese character sequence into words. Here, we investigate Chinese word segmentation for statistical machine translation. We pursue two goals: the first one is the maximization of the final translation quality; the second is the minimization of the manual effort for building a translation system. The commonly used method for getting the word boundaries is based on a word segmentation tool and a predefined monolingual dictionary. To avoid the dependence of the translation system on an external dictionary, we have developed a system that learns a domainspecific dictionary from the parallel training corpus. This method produces results that are comparable with the predefined dictionary. Further more, our translation system is able to work without word segmentation with only a minor loss in translation quality."
N04-1033,Improvements in Phrase-Based Statistical Machine Translation,2004,14,182,1,1,30618,richard zens,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system."
C04-1006,Improved Word Alignment Using a Symmetric Lexicon Model,2004,11,22,1,1,30618,richard zens,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing. We improve the well-known IBM alignment models, as well as the Hidden-Markov alignment model using a symmetric lexicon model. This symmetrization takes not only the standard translation direction from source to target into account, but also the inverse translation direction from target to source. We present a theoretically sound derivation of these techniques. In addition to the symmetrization, we introduce a smoothed lexicon model. The standard lexicon model is based on full-form words only. We propose a lexicon smoothing method that takes the word base forms explicitly into account. Therefore, it is especially useful for highly inflected languages such as German. We evaluate these methods on the German-English Verbmobil task and the French-English Canadian Hansards task. We show statistically significant improvements of the alignment quality compared to the best system reported so far. For the Canadian Hansards task, we achieve an improvement of more than 30% relative."
C04-1030,Reordering Constraints for Phrase-Based Statistical Machine Translation,2004,17,105,1,1,30618,richard zens,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We investigate different reordering constraints for phrase-based statistical machine translation, namely the IBM constraints and the ITG constraints. We present efficient dynamic programming algorithms for both constraints. We evaluate the constraints with respect to translation quality on two Japanese-English tasks. We show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search."
C04-1032,Symmetric Word Alignments for Statistical Machine Translation,2004,11,68,2,0.900901,5736,evgeny matusov,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we address the word alignment problem for statistical machine translation. We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships. We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models. Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair. Then, we use efficient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability). We evaluate the automatic alignments created in this way on the German--English Verbmobil task and the French--English Canadian Hansards task. We show statistically significant improvements of the alignment quality compared to the best results reported so far. On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%."
2004.iwslt-papers.7,Statistical machine translation of spontaneous speech with scarce resources,2004,15,7,3,0.900901,5736,evgeny matusov,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,This paper deals with the task of statistical machine translation of spontaneous speech using a limited amount of training data. We propose a method for selecting relevant additional training data from other sources that may come from other domains. We present two ways to solve the data sparseness problem by including morphological information into the EM training of word alignments. We show that the use of part-of-speech information for harmonizing word order between source and target sentences yields significant improvements in the BLEU score.
2004.iwslt-evaluation.13,Alignment templates: the {RWTH} {SMT} system,2004,18,33,2,1,47115,oliver bender,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe the RWTH statistical machine translation (SMT) system which is based on log-linear model combination. All knowledge sources are treated as feature functions which depend on the source language sentence, the target language sentence and possible hidden variables. The main feature of our approach are the alignment templates which take shallow phrase structures into account: a phrase level alignment between phrases and a word level alignment between single words within the phrases. Thereby, we directly consider word contexts and local reorderings. In order to incorporate additional models (the IBM-1 statistical lexicon model, a word deletion model, and higher order language models), we perform n-best list rescoring. Participating in the International Workshop on Spoken Language Translation (IWSLT 2004), we evaluate our system on the Basic Travel Expression Corpus (BTEC) Chinese-to-English and Japanese-to-English tasks."
P03-1019,A Comparative Study on Reordering Constraints in Statistical Machine Translation,2003,17,121,1,1,30618,richard zens,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schroder numbers.We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%."
E03-1032,Efficient Search for Interactive Statistical Machine Translation,2003,11,48,2,0,37712,franz och,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The goal of interactive machine translation is to improve the productivity of human translators. An interactive machine translation system operates as follows: the automatic system proposes a translation. Now, the human user has two options: to accept the suggestion or to correct it. During the post-editing process, the human user is assisted by the interactive system in the following way: the system suggests an extension of the current translation prefix. Then, the user either accepts this extension (completely or partially) or ignores it. The two most important factors of such an interactive system are the quality of the proposed extensions and the response time. Here, we will use a fully fledged translation system to ensure the quality of the proposed extensions. To achieve fast response times, we will use word hypotheses graphs as an efficient search space representation. We will show results of our approach on the Verbmobil task and on the Canadian Hansards task."
