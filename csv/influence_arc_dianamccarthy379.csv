2020.coling-main.423,N09-1003,0,0.0866242,"ings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datas"
2020.coling-main.423,2020.emnlp-main.618,0,0.013873,"ining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multiling"
2020.coling-main.423,D14-1034,1,0.80299,"er languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter SpA-Verb) comprises verb classes and unmatched coverage of nearly 30k verb similarity scores. In this work, we demonstrate that their large-scale dataset creation methodology based on spatial arrangement (SpAM) can be extended to o"
2020.coling-main.423,Q17-1010,0,0.0123746,"nge of direction). Whereas in Italian and English, verbs describing motion towards the speaker/listener form a distinct cluster. These preliminary analyses suggest that the collected semantic multi-arrangement data may support many other, fine-grained and in-depth lexical-typological analyses in future work, e.g., focusing on cross-lingual comparisons of the organisation of different semantic fields and examination of the most salient meaning dimensions underlying a given conceptual space. 4 Evaluation Evaluation is focused on two types of representation architectures: static word embeddings (Bojanowski et al., 2017) and more recently proposed large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabil"
2020.coling-main.423,N19-1423,0,0.404351,"he performance of large language-specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of lan"
2020.coling-main.423,C18-1323,0,0.017561,"or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently int"
2020.coling-main.423,D16-1235,1,0.79979,"Missing"
2020.coling-main.423,J15-4004,1,0.908022,"uced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets"
2020.coling-main.423,S13-2049,0,0.0308016,"Missing"
2020.coling-main.423,kipper-etal-2006-extending,1,0.609614,"n Word similarity has been widely used as a go-to intrinsic evaluation task, in which rankings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these d"
2020.coling-main.423,D19-1279,0,0.0158964,"erb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned represen"
2020.coling-main.423,J99-4009,0,0.0289894,"which optimises the evidence collected for the dissimilarity estimates (see Figure 1). The final representational dissimilarity matrix (RDM) estimate is produced by statistically combining the evidence from multiple subsequent 2D arrangements and contains a dissimilarity estimate for each pairing of words in the set (see Kriegeskorte and Mur (2012) for the details). The dissimilarities collected for each Phase 1 class are then normalised to ensure inter-class consistency in the final dataset. The main advantages of the spatial arrangement method lie in its intuitiveness, rooted in psychology (Lakoff and Johnson, 1999; Gärdenfors, 2004; Casasanto, 2008), and flexibility, due to the reliance on fluid item placements simultaneously expressing multi-way similarity judgments, rather than discrete numerical scores. By repeatedly considering subsets of items, the users reflect on relative differences in meaning between different configurations of words, which decreases bias from placement error, order of presentation and judgment context. The two-phase design offers a practical advantage for porting the method to other languages. The approach starts from a verb sample, rather than a set of word pairs, which allo"
2020.coling-main.423,K19-1004,1,0.892956,"Missing"
2020.coling-main.423,2020.lrec-1.705,1,0.859868,"Missing"
2020.coling-main.423,L18-1008,0,0.0142817,"large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabilities on diverse NLP tasks (Mikolov et al., 2018) and coverage of 157 languages. For multi-word expressions, we compute their representations by averaging the vectors of their constituent words. We contrast the performance of FT vectors with the omnipresent state-of-the-art BERT model (Devlin et al., 2019). We derive word-level BERT representations of words and multi-word expressions in two different ways: (a) in isolation and (b) in context. In method (a), we follow the steps of Liu et al. (2019) by (1) feeding each item to the pretrained model in isolation, (2) averaging 3 The easier, higher-IAA classes tend to include verbs whose meanings"
2020.coling-main.423,Q17-1022,1,0.893661,"Missing"
2020.coling-main.423,2020.acl-main.720,0,0.011818,"recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in"
2020.coling-main.423,P19-1493,0,0.0239074,"s in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of t"
2020.coling-main.423,2020.tacl-1.54,0,0.012083,"ular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in a typologically diverse selection of languages where no such datasets have hitherto been available. The motivation behind the specific focus on verbs is twofold: (i) the importance of accurate and nuanced representation of verb meaning in light of their pivotal role in sentence structure and the still subpar verbal reasoning ability of SOTA models (Rogers et al., 2020), and (ii) the scarcity of verb data in evaluation datasets currently available. To this end, we employ a recently proposed two-phase data collection method (Majewska et al., 2020) combining semantic clustering (Phase 1) and finer-grained spatial arrangements of words based on their similarity (Phase 2), and evaluate its cross-lingual applicability. Using cross-lingual mappings, This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 4810 Proceedings of the 28th International Conference on Computationa"
2020.coling-main.423,L18-1152,0,0.0326317,"(Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter"
2020.coling-main.423,C10-1119,1,0.808447,"ased for ZH, JA (BERT- BASE with and without whole word masking (+WWM)), PL, FI, and IT (BERT- BASE and BERT- BASE - XXL trained on a larger Italian corpus), available in the Transformers repository (Wolf et al., 2019).4 4.1 Semantic Verb Clustering First, we evaluate the models on semantic clustering, where the task is to group the starting verb sample (Table 1, N verbs) into clusters based on semantic similarity. For each vector collection, we apply the spectral clustering algorithm (Meila and Shi, 2001; Yu and Shi, 2003), shown to produce strong results in previous work on verb clustering (Sun et al., 2010; Scarton et al., 2014; Vuli´c et al., 2017), and evaluate the produced groupings against the Phase 1 classes in each language using standard clustering evaluation metrics, modified purity (M P UR) (i.e., mean precision of induced verb clusters) and weighted class accuracy (WACC), calculated as follows: P M P UR = C∈Clust,nprev(C) >1 ntest_verbs nprev(C) P (1) WACC = ndom(C) ntest_verbs C∈Gold (2) where (1) each cluster C from the set of all KClust automatically induced clusters Clust is associated with its prevalent Phase 1 class, and nprev(C) is the number of verbs in an induced cluster C ap"
2020.coling-main.423,D17-1270,1,0.90543,"Missing"
2020.coling-main.423,2020.cl-4.5,1,0.881421,"Missing"
2020.coling-main.423,D19-1575,0,0.0122398,"Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most lan"
2020.coling-main.423,D19-1077,0,0.0176768,"ng have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to addres"
2020.emnlp-main.333,D16-1250,0,0.0296999,"analysis (Sch¨onemann, 1966). W = arg min W |V | X kWti − si k2 s.t. WT W = I. (1) i=1 As described in Eq. 2, we then learn a linear mapping M to transform the source space towards the average of source and the rotated target space, by minimizing the squared Euclidean distance between each transformed source vector Msi and the mean vector µi (µi = (si + Wti )/2). M is the mapping we will use to transform the original contextualized space. Following Doval et al. (2018), M is found via a closed-form solution. M = arg min M |V | X kMsi − µi k2 (2) i For improved alignment quality, as advised by Artetxe et al. (2016), we normalize and meancenter4 the embeddings in S and T a priori. 4 Experiments Task Descriptions5 We evaluate on three Within Word tasks. Usage Similarity (Usim) (Erk et al., 2013) dataset measures graded similarity of the same word in pairs of different contexts on the scale from 1 to 5. Word in Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset challenges a system to predict a binary choice of whether a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or"
2020.emnlp-main.333,Q17-1010,0,0.0718176,"ngs, and offer insights on the reasons behind the improvement. Our method also has minimum computational complexity and requires no labeled data. 2 Background This section briefly introduces the contextualized/static models that we experimented in this study. For static models, we select three representative methods. SGNS (Mikolov et al., 2013), as the most successful variant of word2vec, trains a log linear model to predict context words given a target word with negative sampling in the objective. FastText improves over SGNS by training at the n-gram level and can generalize to unseen words (Bojanowski et al., 2017). In addition to these two prediction-based models, we also include 4066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4066–4075, c November 16–20, 2020. 2020 Association for Computational Linguistics one count-based model, GloVe (Pennington et al., 2014). GloVe is trained to encode semantic relations exhibited in the ratios of word-word occurrence probabilities into word vectors. As opposed to static embeddings, contextualized models provide dynamic lexical representations as hidden layers in deep neural networks typically pre-trained with langu"
2020.emnlp-main.333,N18-2031,0,0.0158169,"t changes (Shi et al., 2019). To summarize the analysis, our controlled experiments confirm our two hypotheses that the transformation brings two independent effects: improved overall inter-word semantic space and improved within-word contextualization. Our qualitative analysis shows that the improved within-word contextualization is likely to be the result of context variance reduction. 5 Related Work It has been shown that combining different static word representations (for example through averaging or concatenation) into a meta embedding can usually lead to better lexical representations (Coates and Bollegala, 2018; Yin and Sch¨utze, 2016). While these task-independent meta embedding techniques are mainly applied on static embeddings, research has started to explore leveraging ensemble contextualized models when performing fine-tuning on a specific task (Devlin et al., 2019; Xu et al., 2020). Our method, as a post-processing transformation over task-independent contextual representations, is inherently different from these meta embedding and ensemble approaches. Computationally, our method does not require maintaining multiple models at test time, and is therefore more efficient. Our method is also by f"
2020.emnlp-main.333,P19-1285,0,0.0406146,"Missing"
2020.emnlp-main.333,N19-1423,0,0.57062,"meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from either contextualized or static models, the static embeddings, which have lower computational requirements, provide the most gains. 1 Introduction Word representations are fundamental in Natural Language Processing (NLP) (Bengio et al., 2003). Recently, there has been a surge of contextualized models that achieve state-of-the-art in many NLP benchmark tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019). Even better performance has been reported from finetuning or training multiple contextualized models for a specific task such as question answering (Devlin et al., 2019; Xu et al., 2020). However, little has been explored on directly leveraging the many off-the-shelf pre-trained models to improve taskindependent representations for lexical semantics. Furthermore, classic static embeddings are often overlooked in this trend towards contextualized models. As opposed to contextualized embeddings that generate dynamic representations for words in context, s"
2020.emnlp-main.333,D18-1027,0,0.419572,"or the best performance, we use the Large Cased1 variant for each contextualized model. Since our study focuses on generic lexical representations and many of the lexical semantic tasks do not provide training data, we extract features2 from these contextualized models without fine-tuning the weights for a specific task. This feature-based approach is also more efficient compared with finetuning the increasingly larger models which can have hundreds of millions of parameters. 3 Method Our method3 is built from a recently proposed cross-lingual alignment technique called meeting in the middle (Doval et al., 2018). Their method relies on manual translations to learn a transformation over an orthogonal alignment for better cross-lingual static embeddings. We show that by a similar alignment + transformation technique, we can improve monolingual contextualized embeddings without resorting to any labeled data. The direct correspondence among contextualized and static embeddings for alignment is not straightforward, as contextualized models can compute infinite representations for infinite contexts. Inspired by previous study (Schuster et al., 2019) that found contextualized embeddings roughly form word cl"
2020.emnlp-main.333,J13-3003,1,0.857793,"average of source and the rotated target space, by minimizing the squared Euclidean distance between each transformed source vector Msi and the mean vector µi (µi = (si + Wti )/2). M is the mapping we will use to transform the original contextualized space. Following Doval et al. (2018), M is found via a closed-form solution. M = arg min M |V | X kMsi − µi k2 (2) i For improved alignment quality, as advised by Artetxe et al. (2016), we normalize and meancenter4 the embeddings in S and T a priori. 4 Experiments Task Descriptions5 We evaluate on three Within Word tasks. Usage Similarity (Usim) (Erk et al., 2013) dataset measures graded similarity of the same word in pairs of different contexts on the scale from 1 to 5. Word in Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset challenges a system to predict a binary choice of whether a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or better than the results from un-preprocessed representations. 5 Appendix C reports details for each task and experiment. 4067 meaning or not. We follow the advised training scheme in"
2020.emnlp-main.333,S19-1002,0,0.034744,"Missing"
2020.emnlp-main.333,J15-4004,1,0.786565,"r a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or better than the results from un-preprocessed representations. 5 Appendix C reports details for each task and experiment. 4067 meaning or not. We follow the advised training scheme in the original paper to learn a cosine similarity threshold on the representations. The recently proposed CoSimlex (Armendariz et al., 2019) task provides contexts for selected word pairs from the word similarity benchmark SimLex-999 (Hill et al., 2015) and measures the graded contextual effect. We use the English dataset from this task. Its first subtask, CoSimlex-I, evaluates the change in similarity between the same word pair under different contexts. As it requires a system to capture different contextual representations of the same word in order to correctly predict the change of similarity to the other word in the pair, CoSimlex-I indirectly measures within-word contextual effect and therefore provides our third Within Word task. The second CoSimLex subtask, CoSimlex-II, is an Inter Word task as it requires a system to predict the abso"
2020.emnlp-main.333,K19-1004,1,0.882179,"Missing"
2020.emnlp-main.333,2021.ccl-1.108,0,0.136444,"Missing"
2020.emnlp-main.333,S13-2035,0,0.0156121,"ic anchors with guidance from other contextualized/static embeddings. We show leveraging static embeddings, with no labeled data, consistently improves (across almost all configurations) on both Inter Word and surprisingly Within Word context-aware lexical semantic tasks. We also perform controlled analysis to highlight, in isolation, the improvement from the transformation on both contextualization and on an overall interword semantic space. In the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (Navigli and Vannella, 2013). 6 A simple meta embedding baseline that concatenates contextualized and static representations generally impairs the performance. (Appendix F) 4070 Acknowledgments We thank the anonymous reviewers for their helpful feedback on this work. We acknowledge Peterhouse College at University of Cambridge for funding Qianchu Liu’s PhD research. The work was also supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. We also appreciate many helpful discussions and feedback from our colleagues in the Language Technology Lab. Referenc"
2020.emnlp-main.333,D14-1113,0,0.0996041,"Missing"
2020.emnlp-main.333,D14-1162,0,0.0927143,"tative methods. SGNS (Mikolov et al., 2013), as the most successful variant of word2vec, trains a log linear model to predict context words given a target word with negative sampling in the objective. FastText improves over SGNS by training at the n-gram level and can generalize to unseen words (Bojanowski et al., 2017). In addition to these two prediction-based models, we also include 4066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4066–4075, c November 16–20, 2020. 2020 Association for Computational Linguistics one count-based model, GloVe (Pennington et al., 2014). GloVe is trained to encode semantic relations exhibited in the ratios of word-word occurrence probabilities into word vectors. As opposed to static embeddings, contextualized models provide dynamic lexical representations as hidden layers in deep neural networks typically pre-trained with language modeling objectives. In our study, we choose three state-of-the-art contextualized models. BERT (Devlin et al., 2019) trains bidirectional transformers (Vaswani et al., 2017) with masked language modeling and next sentence prediction objectives. Liu et al. (2019b)’s RoBERTa further improves upon BE"
2020.emnlp-main.333,N18-1202,0,0.0555882,"textual variations of meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from either contextualized or static models, the static embeddings, which have lower computational requirements, provide the most gains. 1 Introduction Word representations are fundamental in Natural Language Processing (NLP) (Bengio et al., 2003). Recently, there has been a surge of contextualized models that achieve state-of-the-art in many NLP benchmark tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019). Even better performance has been reported from finetuning or training multiple contextualized models for a specific task such as question answering (Devlin et al., 2019; Xu et al., 2020). However, little has been explored on directly leveraging the many off-the-shelf pre-trained models to improve taskindependent representations for lexical semantics. Furthermore, classic static embeddings are often overlooked in this trend towards contextualized models. As opposed to contextualized embeddings that generate dynamic representations fo"
2020.lrec-1.705,N09-1003,0,0.89788,"sed in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013;"
2020.lrec-1.705,N19-1050,0,0.0169763,"he dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al.,"
2020.lrec-1.705,W16-2519,0,0.0178536,"verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one ta"
2020.lrec-1.705,P98-1013,0,0.705002,"ding (Jackendoff, 1972; Levin, 1993; McRae et al., 1997; Altmann and Kamide, 1999; Resnik and Diab, 2000; Ferretti et al., 2001; Sauppe, 2016, inter alia). The demand for verb-specific resources to support NLP has been recognised in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased"
2020.lrec-1.705,D14-1034,1,0.83064,"g and spatial arrangements of words. Despite their wide usefulness, most available datasets used for intrinsic evaluation in distributional semantics are restricted in size and coverage, many do not distinguish similarity and relatedness, and only a few target verbs in particular. The prominent word pair datasets include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), comprising 353 noun pairs, and SimLex-999 (Hill et al., 2015), comprising 999 word pairs out of which 222 are verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relie"
2020.lrec-1.705,W11-2501,0,0.0337348,"e which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, generate similarity ratings for over 8 times as many verb pair"
2020.lrec-1.705,P14-1023,0,0.0147114,"e still limited, and few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-h"
2020.lrec-1.705,W16-2502,0,0.0255245,"i are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous"
2020.lrec-1.705,Q17-1010,0,0.0301069,"huler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et"
2020.lrec-1.705,P12-1015,0,0.111911,"as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 201"
2020.lrec-1.705,J06-1003,0,0.0402948,"inantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate cat"
2020.lrec-1.705,W16-2506,0,0.0148593,"ontext of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments,"
2020.lrec-1.705,D16-1235,1,0.902677,"Missing"
2020.lrec-1.705,W16-2507,0,0.0140033,"ample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments, SpAM addresses shortcoming"
2020.lrec-1.705,N16-2002,0,0.0123547,"relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, genera"
2020.lrec-1.705,J15-4004,1,0.936771,"publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 2014). We show how it c"
2020.lrec-1.705,S13-2049,0,0.448125,"om the final sample due to their very low frequency, resulting in a 825-verb sample. 5751 4.1. Figure 2: The rough clustering task layout (zoomed in). Verbs can be dragged onto the ‘new category’ circle to create a new grouping, onto ‘copy’ to create a duplicate label, or ‘Trash’ to dispose of the unwanted duplicate. Participants The rough clustering task was first tested by two native English speakers. They produced clusters with an encouraging degree of overlap. It was computed using the B-Cubed metric (Bagga and Baldwin, 1998) extended by Amig´o et al. (2009) to overlapping clusters and by Jurgens and Klapaftis (2013) to fuzzy clusters, as used in related work (Jurgens and Klapaftis, 2013; Majewska et al., 2018). B-Cubed, based on precision and recall, estimates the overlap between two clusterings X and Y at the item level. Let U represent the collection of items, Xi the set of clusters containing item i in clustering X, Yi the set of clusters containing i in clustering Y ; let Ji be the set of items in Xi but excluding i and Ki be the set of items in Yi but excluding i. B-Cubed precision P and recall R are defined as: P = 1 X 1 X min(|Xi ∩ Xj |, |Yi ∩ Yj |) |U |i∈U |Ji |j∈J |Xi ∩ Xj | i numerous as the bi"
2020.lrec-1.705,kipper-etal-2006-extending,1,0.837416,"Missing"
2020.lrec-1.705,N16-1095,0,0.0182253,"ts include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synony"
2020.lrec-1.705,P17-2074,0,0.121665,"rs, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on an"
2020.lrec-1.705,J99-4009,0,0.887765,"se combinations possible. However, in SpAM a subject arranges multiple stimuli simultaneously in a two-dimensional space (e.g. on a computer screen), expressing (dis)similarity through the relative positions of items within that space. The inter-stimulus Euclidean distances represent pairwise dissimilarities. This set-up ensures that all stimuli are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting"
2020.lrec-1.705,P14-2050,0,0.0227907,"few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhar"
2020.lrec-1.705,L18-1153,1,0.89982,"Missing"
2020.lrec-1.705,W16-2523,0,0.024829,"e in legibility. Furthermore, the two-phase set-up handles ambiguity by permitting copying verb labels to capture different senses in Phase 1. The rough clustering phase guarantees that each verb label is presented in the context of related verbs in the arena in Phase 2, a necessary prerequisite for meaningful similarity judgments in psychology (Turner et al., 1987).2 The actual sense is implied by the surrounding words: this helps avoid mismatches in similarity judgments between participants for ambiguous verbs. What is more, this avoids the common problem of ambiguous low similarity scores (Milajevs and Griffiths, 2016) that conflate similarity judgments on antonyms (vanish - appear) and completely unrelated notions (fry - appear), and focuses on judgments between comparable concepts. 3.3. Data To test the scaling-up potential of our approach and to enable direct comparisons with the standard pairwise similarity rating methods, we select the 827 verbs from SimVerb (Gerz et al., 2016) as our item sample.3 The sample presents a challenge due to its size (i.e., it is almost nine times as 2 Following Turner et al. (1987), ‘stimuli can only be compared in so far as they have already been categorised as identical,"
2020.lrec-1.705,Q17-1022,1,0.925243,"Missing"
2020.lrec-1.705,D14-1162,0,0.0815648,"expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as"
2020.lrec-1.705,D18-1169,0,0.013689,"arena (inhale - exhale and sink - swim). This tendency is also illustrated by the RDM in Figure 3: separate clusters are formed by verbs such as raise, rise, grow and diminish, decline, lower, and finish is kept separate from begin and start. Crucially, our spatial approach records simultaneous judgments on multiple related words, which helps improve judgment consistency (e.g., word pairs holding analogous relations have similar scores) and allows making subtle distinctions based on varying degrees of similarity by means of 9 The ρSV scores are promising compared to the ρ = 0.612 SimVerb IAA (Pilehvar et al., 2018), despite the fact that the easy cases of verb pairs involving very disparate verbs (in different classes) are not included in our results. 10 There are exceptions: positive (e.g. love) and negative (e.g. hate) emotion verbs form two different classes; there are also separate groupings with ‘construction’ and ‘destruction’ verbs. See Table 1. 5754 600 5000 500 4000 400 Frequency Frequency 6000 3000 300 2000 200 1000 100 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 SpA-Verb dissimilarity score 1 2 3 4 5 6 7 SimVerb similarity score 8 9 10 Figure 4: Score distributio"
2020.lrec-1.705,K15-1026,0,0.0190291,"er, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of"
2020.lrec-1.705,J06-3003,0,0.103961,".e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate category (Lupker, 1984; Resnik, 1995) (taxonomical similarity (Turney and Pantel, 2010)) or shared semantic features (Tversky, 1977; Frenck-Mestre and Bueno, 1999; Turney, 2006) has been proposed. Here, similarity is quantified in terms of degree of overlap in semantic properties, e.g., shared function or physical features, with synonyms occupying the top region of the similarity scale (e.g. fiddleviolin (Cruse, 1986)). In this work, we reserve the term (semantic) similarity for this latter definition of closeness of meaning, and distinguish it from the more general relatedness, which also includes association, as in previous work (Resnik, 1995; Resnik and Diab, 2000; Agirre et al., 2009; Hill et al., 2015; Gerz et al., 2016). We explore how this distinction is captu"
2020.lrec-1.705,D16-1157,0,0.0274248,"Missing"
2021.emnlp-main.571,D18-1025,0,0.0242468,"system performance. The SemEval7158 2021 shared task MCL-WiC does focus on crosslingual WiC, but covers only five high-resource languages from three language families (English, French, Chinese, Arabic, Russian). Both XL-WiC and MCL-WiC mainly focus on common words and do not include less frequent concepts (e.g., named entities). Further, their language coverage and data availability are heavily skewed towards Indo-European languages. There are several other ‘non-WiC’ datasets designed to evaluate cross-lingual context-aware lexical representations. Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018) challenges a model to predict graded similarity of crosslingual word pairs given sentential context, one in each language. In the Bilingual Token-level Sense Retrieval (BTSR) task (Liu et al., 2019), given a query word in a source language context, a system must retrieve a meaning-equivalent target language word within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as gene"
2021.emnlp-main.571,W16-2501,1,0.796206,"d within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as general intrinsic probes for pretrained models, but cannot support fine-tuning experiments and cannot fully expose the relevance of information available in pretrained models for downstream applications. This is problematic as intrinsic tasks in general do not necessarily correlate well with downstream performance (Chiu et al., 2016; Glavaš et al., 2019). AM2 I C O vs. Entity Linking. Our work is related to the entity linking (EL) task (Rao et al., 2013; Cornolti et al., 2013; Shen et al., 2014) similarly to how the original WiC (based on WordNet knowledge) is related to WSD. EL systems must map entities in context to a predefined knowledge base (KB). While WSD relies on the WordNet sense inventory, the EL task focuses on KBs such as Wikipedia and DBPedia. When each entity mention is mapped to a unique Wiki page, this procedure is termed wikification (Mihalcea and Csomai, 2007). The cross-lingual wikification task (Ji et"
2021.emnlp-main.571,2020.acl-main.747,0,0.527258,"e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference"
2021.emnlp-main.571,N19-1423,0,0.168088,"lable in different languages, i.e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 P"
2021.emnlp-main.571,P19-1070,1,0.872566,"Missing"
2021.emnlp-main.571,N18-2017,0,0.037706,"Missing"
2021.emnlp-main.571,N18-1170,0,0.0535351,"Missing"
2021.emnlp-main.571,D17-1215,0,0.0513424,"Missing"
2021.emnlp-main.571,K19-1004,1,0.870059,"Missing"
2021.emnlp-main.571,Q14-1019,0,0.0357826,"r on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologi"
2021.emnlp-main.571,S13-2040,0,0.0330702,"emonstrate the challenging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide wh"
2021.emnlp-main.571,D12-1128,0,0.0920929,"Missing"
2021.emnlp-main.571,P19-1459,0,0.0209984,"ty of AM2 I C O. For each dataset, 7153 we recruit two annotators who each validate a random sample of 100 examples, where 50 examples are shared between the two samples and are used to compute inter-rater agreement.3 2.2 2.3 Adversarial Examples Another requirement is assessing to which extent models can grasp the meaning of a target word based on the (complex) interaction with its context. However, recently it was shown that SotA pretrained LMs exploit superficial cues while solving language understanding tasks due to spurious correlations seeping into the datasets (Gururangan et al., 2018; Niven and Kao, 2019). This hinders generalizations beyond the particular datasets and makes the models brittle to minor changes in the 3 The annotators were recruited via two crowdsourcing platforms, Prolific and Proz, depending on target language coverage. The annotators were native speakers of the target language, fluent in English, and with an undergraduate degree. 4 E.g., ‘China’ can be linked to the page ‘Republic of China (1912–1949)’ and to the page ‘Empire of China (1915–1916)’. XL-WiC MCL-WiC AM2 I C O 7,466 7,466 4,130 4,130 17 14,510 1,676 7,255 1,201 22.7 3600 2000 2766 2072 26.13 13,074 8,570 9,868 8"
2021.emnlp-main.571,N19-1128,0,0.042815,"ent in other datasets, namely the Georgian alphabet and the Bengali script (a Northern Indian abugida), for a total of 8 distinct scripts. 3 Experimental Setup We now establish a series of baselines on AM2 I C O to measure the gap between current SotA models and human performance. XLM-R6 (Conneau et al., 2020), available in the HuggingFace repository (Wolf et al., 2020). Classification. Given two contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize th"
2021.emnlp-main.571,2020.emnlp-main.185,1,0.926148,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,J19-3005,1,0.863194,"Missing"
2021.emnlp-main.571,E17-1010,0,0.0897936,"e, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologically diverse set of 15 la"
2021.emnlp-main.571,2020.emnlp-main.584,0,0.272024,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,D19-1077,0,0.0167166,"6.3. However, this is still insufficient to equalize performances across the board, as the latter group of languages continues to lag behind: between DE and UR remains a gap of 7.2 points. We speculate that the reason behind this asymmetry is the fact that in addition to being resource-poor, UR, KK, BN, and KA are also typologically distant from languages where most of the examples are concentrated. Overall, these findings suggest that leveraging multiple sources is better than a single one by virtue of the transfer capabilities of massively multilingual encoders, as previously demonstrated (Wu and Dredze, 2019; Ponti et al., 2021). Few-shot Transfer. To study the differences between training on `s and `t with controlled train data size, we plot the model performance on two target languages (RU and JA) as a function of the amount of available examples across different transfer conditions in Figure 2. Comparing supervised learning (based on target language data) with zeroshot learning (based on DE data), it emerges how the former is always superior if the number of examples is the same. However, zero-shot learning may eventually surpass the peak performance of supervised learning by taking advantage"
2021.emnlp-main.571,2020.lrec-1.723,0,0.0355104,"ging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the sam"
2021.emnlp-main.571,P18-1072,1,0.894256,"Missing"
2021.emnlp-main.571,N16-1072,0,0.164299,"stantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the same meaning in both contexts, or not. However, the current WiC evaluation still allows"
2021.emnlp-main.571,D18-1270,0,0.0460866,"Missing"
2021.emnlp-main.571,2020.cl-4.5,1,0.894831,"Missing"
A00-2034,J87-3002,0,0.0276137,"of alternations. The fully automatic method outlined here is applied to the causative 256 RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have also been suggested for the recovery of predicate argument structure, necessary for SCF acquisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations. He used alternations to indicate where the argument head data from different slots can be combined since it occupies the same semantic relationship with the predicate. Different diathesis alternations give different emphasis and nuances of meaning to the same basic content. These subtle changes of meaning are important in natural language generation (Stede, 1998). Alternations provide a means of reducing r"
A00-2034,A97-1052,0,0.593804,"suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative 256 RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have also been suggested for the recovery of predicate argument structure, necessary for SCF acquisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations. He used alternations to indicate where the argument head data from different slots can be combined since it occupies the same semantic relationship with the predicate. Different diathesis alternations give different emphasis and nuances of meaning to the same basic content. These subtle changes of meaning are important in natural language generation (Stede, 1998). Alternations"
A00-2034,W96-0303,0,0.0167032,"remove the overlap between classes from the original scheme. Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet. What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource. Using corpora by-passes reliance on the availability and adequacy of MRDs. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999). Estimations of productivity have been suggested for controlling the application of alternations (Briscoe and Copestake, 1996). We propose a method to acquire knowledge of alternation participation directly from corpora, with frequency information available as a by-product. 3 e..t Method We use both syntactic and semantic information for identifying participants in RSAs. Firstly, syntactic processing is used to find candidates taking the alternating SeEs. Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb. We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic"
A00-2034,P98-1046,0,0.0492489,"ns would be a useful tool for extending the classification with new participants. Levin&apos;s taxonomy might also be used alongside observed behaviour, to predict unseen behaviour. Levin&apos;s classification has been extended by other NLP researchers (Doff and Jones, 1996; Dang et al., &lt;Root&gt; I bs,r i/ t,on ~ i uoao activ,t ....... l measure ] /&quot;( time ,, A ---0a- t . . . . . / time ~ ] construction war car ( relation ] migration meal T [time_period] fo .v,=t,4 week month afternoon speech yelling Figure 1: TCM drum time ceremonial migration for the object slot of the transitive frame of start. 1998). Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme. Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet. What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource. Using corpora by-passes reliance on the availability and adequacy of MRDs. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999). Estimations of productivity have been suggested for contro"
A00-2034,C96-1055,0,0.0205037,"alongside observed behaviour, to predict unseen behaviour. Levin&apos;s classification has been extended by other NLP researchers (Doff and Jones, 1996; Dang et al., &lt;Root&gt; I bs,r i/ t,on ~ i uoao activ,t ....... l measure ] /&quot;( time ,, A ---0a- t . . . . . / time ~ ] construction war car ( relation ] migration meal T [time_period] fo .v,=t,4 week month afternoon speech yelling Figure 1: TCM drum time ceremonial migration for the object slot of the transitive frame of start. 1998). Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme. Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet. What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource. Using corpora by-passes reliance on the availability and adequacy of MRDs. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999). Estimations of productivity have been suggested for controlling the application of alternations (Briscoe and Copestake, 1996). We propose a method to acquire knowledge of alternat"
A00-2034,W93-0307,0,0.049706,"Missing"
A00-2034,1997.iwpt-1.16,0,0.0145022,"ns (Briscoe and Copestake, 1996). We propose a method to acquire knowledge of alternation participation directly from corpora, with frequency information available as a by-product. 3 e..t Method We use both syntactic and semantic information for identifying participants in RSAs. Firstly, syntactic processing is used to find candidates taking the alternating SeEs. Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb. We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic processing. The corpus data is POS tagged and lemmatised before the LR parser is applied. Subcategorization patterns are extracted from the parses, these include both the syntactic categories and the argument heads of the constituents. These subcategorization patterns are then classified according to a set of 161 SeE classes. The SeE entries for each verb are then subjected to a statistical filter which removes SCFs that have occurred with 257 a frequency less than would be expected by chance. The resulting SCF lexicon lists each verb with the SCFs it takes. Each SCF entry inclu"
A00-2034,P99-1051,0,0.582826,"tion, the transitive form alternates with a prepositional phrase construction involving either at or on. An example of the conative alternation is given in (2). 1. The boy broke the window ~-* The window broke. 2. The boy pulled at the rope *-* The boy pulled the rope. We refer to alternations where a particular semantic role appears in different grammatical roles in alternate realisations as &quot;role switching alternations&quot; (RSAS). It is these alternations that our method applies to. Recently, there has been interest in corpus-based methods to identify alternations (McCarthy and Korhonen, 1998; Lapata, 1999), and associated verb classifications (Stevenson and Merlo, 1999). These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative 256 RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which determines whether"
A00-2034,P99-1004,0,0.0166669,"nion of the tWO T C M s which are not subsumed by another class in this union. Duplicates are removed. Probabilities are assigned to the classes of a base cut using the estimates on the original TCM. The probability estimate for a hypernym class is obtained by combining the probability estimates for all its hyponyms on the original cut. Figure 2 exemplifies this process for two TOMs (TCM1 and TCM2) in an imaginary hierarchy. The UBC is at the classes B, c and D. To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (aSD) proposed by Lee (1999). 1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence, p l ( x ) and p2(x) are the two probability distributions which are being compared. The ~ constant is a value between 0 and 1 We also e x p e r i m e n t e d with euclidian distance, the L1 n o r m , a n d cosine measures. T h e differences in p e r f o r m a n c e of these m e a s u r e s were not statistically significant. 258 1 which smooths p l ( x ) with p2(z) so that ~SD is always defined. We use the same value (0.99) for as Lee. If a is set to 1 then this measure is equivalent to the Kulbac"
A00-2034,P98-2247,1,0.84272,"iant. In the conative alternation, the transitive form alternates with a prepositional phrase construction involving either at or on. An example of the conative alternation is given in (2). 1. The boy broke the window ~-* The window broke. 2. The boy pulled at the rope *-* The boy pulled the rope. We refer to alternations where a particular semantic role appears in different grammatical roles in alternate realisations as &quot;role switching alternations&quot; (RSAS). It is these alternations that our method applies to. Recently, there has been interest in corpus-based methods to identify alternations (McCarthy and Korhonen, 1998; Lapata, 1999), and associated verb classifications (Stevenson and Merlo, 1999). These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative 256 RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which dete"
A00-2034,J98-3003,0,0.0572551,"(Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations. He used alternations to indicate where the argument head data from different slots can be combined since it occupies the same semantic relationship with the predicate. Different diathesis alternations give different emphasis and nuances of meaning to the same basic content. These subtle changes of meaning are important in natural language generation (Stede, 1998). Alternations provide a means of reducing redundancy in the lexicon since the alternating scFs need not be enumerated for each individual verb if a marker is used to specify which verbs the alternation applies to. Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related. Levin (1993) provides a classification of over 3000 verbs according to their participation in alternations involving NP and PP constituents. Levin&apos;s classification is not intended to be exhaustive. Automatic identification of alternation"
A00-2034,E99-1007,0,0.0284512,"ional phrase construction involving either at or on. An example of the conative alternation is given in (2). 1. The boy broke the window ~-* The window broke. 2. The boy pulled at the rope *-* The boy pulled the rope. We refer to alternations where a particular semantic role appears in different grammatical roles in alternate realisations as &quot;role switching alternations&quot; (RSAS). It is these alternations that our method applies to. Recently, there has been interest in corpus-based methods to identify alternations (McCarthy and Korhonen, 1998; Lapata, 1999), and associated verb classifications (Stevenson and Merlo, 1999). These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative 256 RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have suggested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have"
A00-2034,J93-1005,0,\N,Missing
A00-2034,C98-1046,0,\N,Missing
A00-2034,C98-2242,1,\N,Missing
apidianaki-etal-2014-semantic,J90-1003,0,\N,Missing
apidianaki-etal-2014-semantic,N12-1095,0,\N,Missing
apidianaki-etal-2014-semantic,D09-1056,0,\N,Missing
apidianaki-etal-2014-semantic,S07-1002,0,\N,Missing
apidianaki-etal-2014-semantic,J13-3008,0,\N,Missing
apidianaki-etal-2014-semantic,P01-1008,0,\N,Missing
apidianaki-etal-2014-semantic,D08-1021,0,\N,Missing
apidianaki-etal-2014-semantic,W06-1610,0,\N,Missing
apidianaki-etal-2014-semantic,S13-2049,0,\N,Missing
apidianaki-etal-2014-semantic,S07-1009,1,\N,Missing
apidianaki-etal-2014-semantic,N03-1003,0,\N,Missing
apidianaki-etal-2014-semantic,P05-1074,0,\N,Missing
apidianaki-etal-2014-semantic,N06-1003,0,\N,Missing
apidianaki-etal-2014-semantic,N10-1031,0,\N,Missing
apidianaki-etal-2014-semantic,2005.mtsummit-papers.11,0,\N,Missing
apidianaki-etal-2014-semantic,S10-1011,0,\N,Missing
apidianaki-etal-2014-semantic,2010.iwslt-papers.2,1,\N,Missing
apidianaki-etal-2014-semantic,W07-0716,0,\N,Missing
apidianaki-etal-2014-semantic,D07-1043,0,\N,Missing
C04-1146,W03-1812,0,0.0146853,"le 5: Correlation with compositionality for different similarity measures Compositionality of collocations In its most general sense, a collocation is a habitual or lexicalised word combination. However, some collocations such as strong tea are compositional, i.e., their meaning can be determined from their constituents, whereas others such as hot dog are not. Both types are important in language generation since a system must choose between alternatives but only non-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary. Baldwin et al. (2003) explore empirical models of compositionality for noun-noun compounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words. McCarthy et al. (2003) also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts. They extract co-occurrence data for 111 phras"
C04-1146,briscoe-carroll-2002-robust,0,0.0700259,"ur sets We have described a number of ways of calculating distributional similarity. We now consider whether there is substantial variation in a word’s distributionally nearest neighbours according to the chosen measure. We do this by calculating the overlap between neighbour sets for 2000 nouns generated using different measures from direct-object data extracted from the British National Corpus (BNC). 3.1 Experimental set-up The data from which sets of nearest neighbours are derived is direct-object data for 2000 nouns extracted from the BNC using a robust accurate statistical parser (RASP) (Briscoe and Carroll, 2002). For reasons of computational efficiency, we limit ourselves to 2000 nouns and directobject relation data. Given the goal of comparing neighbour sets generated by different measures, we would not expect these restrictions to affect our findings. The complete set of 2000 nouns (WScomp ) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency > 500), and WSlow contains the nouns ranked 3001-4000 (frequency ≈ 100). By excluding mid-frequency nouns, we obtain a clear separation between h"
C04-1146,J92-4003,0,0.0165264,"ept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations). 1 Introduction Over recent years, many Natural Language Processing (NLP) techniques have been developed that might benefit from knowledge of distributionally similar words, i.e., words that occur in similar contexts. For example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruse"
C04-1146,P99-1016,0,0.00756697,"ty measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures. This will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for a particular application. Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy. Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., “Xs and other Ys” is used as evidence that X is a hyponym of Y. Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results. The rest of this paper is organised as follows. In Section 2, we present ten distributional similarity measures that have been proposed for use in NLP. In Section 3, we analyse the variation in neighbour sets returned by these measures. In Section 4, we take one fundamental statistical property (word frequency) and"
C04-1146,W02-0908,0,0.13096,"ns of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be"
C04-1146,P99-1004,0,0.359404,"that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999). The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). Thus, applying a distributional similarity technique to a new application necessitates evaluating a large number of distributional simil"
C04-1146,P98-2127,0,0.979739,"combinations of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curra"
C04-1146,W03-1810,1,0.696641,"not. Both types are important in language generation since a system must choose between alternatives but only non-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary. Baldwin et al. (2003) explore empirical models of compositionality for noun-noun compounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words. McCarthy et al. (2003) also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts. They extract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents (e.g. rip) from the BNC using RASP and calculate the value of simlin between each phrasal verb and its simplex constituent. The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplex constituent. This ranking is correlated with human judgements of the comp"
C04-1146,W03-1011,1,0.907832,"eated semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999). The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). Thus, applying a distributional similarity technique to a new application necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm. We propose a shift in focus from attempting to discover the overall best distributional similarity measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures. This will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for a part"
C04-1146,C98-2122,0,\N,Missing
C04-1177,S01-1020,0,\N,Missing
C04-1177,W04-0837,1,\N,Missing
C04-1177,S01-1005,0,\N,Missing
C04-1177,S01-1027,0,\N,Missing
C04-1177,kilgarriff-rosenzweig-2000-english,0,\N,Missing
C04-1177,rose-etal-2002-reuters,0,\N,Missing
C04-1177,O97-1002,0,\N,Missing
C04-1177,briscoe-carroll-2002-robust,1,\N,Missing
C04-1177,P04-1036,1,\N,Missing
C04-1177,J04-1003,0,\N,Missing
C04-1177,P98-2127,0,\N,Missing
C04-1177,C98-2122,0,\N,Missing
C04-1177,magnini-cavaglia-2000-integrating,0,\N,Missing
C14-1154,E09-1013,0,0.0784519,"Missing"
C14-1154,P13-1141,0,0.0154691,"pproaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but identifies the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further extensions"
C14-1154,Y11-1028,1,0.838437,"about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches"
C14-1154,cook-stevenson-2010-automatically,1,0.937154,"d in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not"
C14-1154,W11-2508,0,0.203893,"senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic"
C14-1154,S13-2049,0,0.0594711,"Missing"
C14-1154,H05-1053,1,0.75209,"nexpensively constructed in the future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports 1632 Method NoveltyDiff NoveltyLLR NoveltyRatio RelevanceAuto RelevanceManual Rank SumDiff,auto Rank SumDiff,manual Upper-bound Baseline F-score BNC–ukWaC SiBol/Port 0.57 0.29 0.67 0.28 0.66 0.28 0.48 0.24 0.45 0.27 0.72 0.30 0.72 0.29 0.72 0.42 0.36 0.20 Table 2: Token-level F-score for the BNC–ukWaC and SiBol/Port datasets using variants of Novelty, Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown. and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed very high NoveltyRatio for many distractors (selected in a similar way to our other experiments). Unlike the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to cooccur with very different words in the corpora, and NoveltyRatio will consequently be high. To address vocabulary differences between corpora, in their experiments on identifying lexical semantic differences between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word to those with mod"
C14-1154,S13-2051,1,0.89377,"Missing"
C14-1154,S13-2039,1,0.871209,"Missing"
C14-1154,E12-1060,1,0.877532,"s for identifying new word-senses could benefit applied NLP by helping to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in addition to new words themselves; methods which identify new word-senses could therefore also help to keep dictionaries current. In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses. Specifically, we consider the identification of word-senses that are not attested in a reference corpus, taken to represent standard usage, but that are attested in a focus corpus of newer texts. Lau et al. (2012) introduced the task of novel sense identification. They presented a method for identifying novel word-senses — described here in Section 4 — and evaluated this method on a very small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al. (2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new wordsenses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
C14-1154,J07-4005,1,0.808136,"d senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but ide"
C14-1154,S13-2035,0,0.0486422,"Missing"
C14-1154,P11-2053,0,0.252002,"nge in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains."
C14-1154,W09-0214,0,0.490803,"arger than has been used in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergon"
C14-1154,W11-1102,0,0.0956413,"Missing"
C98-2242,A97-1052,0,\N,Missing
D07-1039,W99-0901,0,0.0786912,"references of verbs. We will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the e"
D07-1039,W03-1812,0,0.847965,"selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on"
D07-1039,W03-1809,0,0.49145,"use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-obj"
D07-1039,J98-2002,0,0.086539,"e of selectional preferences of verbs. We will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet a"
D07-1039,W04-3224,0,0.0402697,"Missing"
D07-1039,P98-2127,0,0.342873,"data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for finding the best classes for representation. This method has previously been tried for modelling compositionality of verb-particle constructions (Bannard, 2002). The other two methods (we refer to them as ‘typebased’) also calculate a probability distribution using argument head tokens but they select the classes over which the distribution is calculated using the number"
D07-1039,briscoe-carroll-2002-robust,0,0.0186821,"predicate in question, for example eat hat. In contrast to Bannard, our experiments are with verb-object combinations rather than verb particle constructions. We compare Li and Abe models with WordNet models which use the number of argument types to obtain the classes for representation of the selectional preferences. In addition to experiments with these WordNet models, we propose models using entries in distributional thesauruses for representing preferences. 3 Three Methods for Acquiring Selectional Preferences All models were acquired from verb-object data extracted using the RASP parser (Briscoe and Carroll, 2002) from the 90 million words of written English from the BNC (Leech, 1992). We extracted verb and common noun tuples where the noun is the argument head of the object relation. The parser was also used to extract the grammatical relation data used for acquisition of the thesaurus described below in section 3.3. 3.1 TCMs This approach is a reimplementation of Li and Abe (1998). Each selectional preference model (referred to as a tree cut model, or TCM) comprises a set of disjunctive noun classes selected from all the possibilities in the WordNet hyponym hierarchy 3 using MDL (Rissanen, 1978). The"
D07-1039,J90-1003,0,0.0331396,"nd Evert, 2001). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Although they are not specifically detecting compositionality, there is a strong correlation between syntactic rigidity and semantic idiosyncrasy. Venkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations. They explored seven features as measures of compositionality: 1. frequency 2. pointwise mutual information (Church and Hanks, 1990), 3. least mutual information difference with similar collocations, based on (Lin, 1999) and using Lin’s thesaurus (Lin, 1998) for obtaining the similar collocations. 4. The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold. 5. distributed frequency of an object, using the verb, which considers the similarity between the target verb and the verbs occurring with the target object above the specified threshold. 6. a latent semantic approach (LSA) based on (Sch¨utze, 1998; Baldwin et"
D07-1039,J02-2003,0,0.318475,"will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus o"
D07-1039,E06-1043,0,0.136054,"class words output from his parser and filtered these by the loglikelihood statistic. Lin proposed that if there is a phrase obtained by substitution of either the head or modifier in the phrase with a ‘nearest neighbour’ from the thesaurus then the mutual information of this and the original phrase must be significantly different for the original phrase to be considered noncompositional. He evaluated the output manually. As well as distributional similarity, researchers have used a variety of statistics as indicators of non-compositionality (Blaheta and Johnson, 2001; Krenn and Evert, 2001). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Although they are not specifically detecting compositionality, there is a strong correlation between syntactic rigidity and semantic idiosyncrasy. Venkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations. They explored seven features as measures of compositionality: 1. frequency 2. pointwise mutual information (Church and Hanks, 1990), 3. least mutual in"
D07-1039,C94-2119,0,0.0403517,"369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for finding the best classes for representation."
D07-1039,J03-3005,0,0.209835,"Missing"
D07-1039,P99-1041,0,0.868583,"ighly significant correlation between measures which use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investig"
D07-1039,W03-1810,1,0.953292,"s and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following p"
D07-1039,A00-2034,1,0.825144,"e for a noun may be indicative of peculiar semantics, this may not always be the case, for example chew the fat. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). We also believe it is worth targeting features to specific types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al., 2003) The selectional preference models we have proposed here might also be applied to other tasks. We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose. 6 Acknowledgements We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a specific combination with an object is noncompositional. We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task. Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesau"
D07-1039,P93-1024,0,0.381518,"Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the"
D07-1039,W01-0513,0,0.0977826,"Missing"
D07-1039,J98-1004,0,0.150997,"Missing"
D07-1039,W04-2605,0,0.0165565,"be indicative of peculiar semantics, this may not always be the case, for example chew the fat. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). We also believe it is worth targeting features to specific types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al., 2003) The selectional preference models we have proposed here might also be applied to other tasks. We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose. 6 Acknowledgements We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a specific combination with an object is noncompositional. We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task. Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesaurus. We find that the probab"
D07-1039,H05-1113,1,0.900041,"a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following previous research in this direction (McCarthy e"
D07-1039,W04-0401,0,\N,Missing
D07-1039,C98-2122,0,\N,Missing
D09-1046,D07-1007,0,0.0440408,"epts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between usages and because it is publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or"
D09-1046,P07-1005,0,0.0355219,"rned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between usages and because it is publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli,"
D09-1046,P09-1002,1,0.823755,"or thesaurus. However, clear cut sense boundaries are sometimes hard to define, and the meaning of words depends strongly on the context in which they are used (Cruse, 2000; Hanks, 2000). Some researchers in lexical semantics have suggested that word meanings lie on a continuum between i) clear cut cases of ambiguity and ii) vagueness where clear cut boundaries do not hold (Tuggy, 1993). Certainly, it seems that a more complex representation of word sense is needed with a softer, graded representation of meaning rather than a fixed listing of senses (Cruse, 2000). A recent annotation study ((Erk et al., 2009), hereafter GWS) marked a target word in context with graded ratings (on a scale of 1-5) on senses from WordNet (Fellbaum, 1998). Table 1 shows an example of a sentence with the target word in bold, and with the annotator judgments given 2 Related Work WSD has to date been a task where word senses are viewed as having clear cut boundaries. However, there are indications that word meanings do not behave in this way (Kilgarriff, 2006). Researchers in the field of WSD have acknowledged these problems but have used existing lexical resources in the hope that useful applications can be built with t"
D09-1046,N06-2015,0,0.0699944,"publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subt"
D09-1046,J04-1003,0,0.00597778,"2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object Answering task. However, it does not test its prediction against human annotator data. We concentrate on supervised models in this paper since they generally perform better than their unsupervised or knowledge-based counterparts (Navigli, 2009). We compare them against a baseline model which simply uses the training data to obtain a probability distribution over senses regardless of context, since marginal distributions are highly skewed making a prior distribution very informative (Chan and Ng, 2005; Lapata and Brew, 2004). Along with standard WSD models, we evaluate vector space models that use the training data to locate a word sense in semantic space. Word sense and vector space models have been related in two ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than c"
D09-1046,P93-1016,0,0.0234806,"or training. As it is to be expected that the vectors in this space will be very sparse, we also test a variant of the Prototype model with Sch¨utze-style secondorder vectors (Sch¨utze, 1998), called Prototype/2. Given a (first-order) feature vector, we compute a second-order vector as the centroid of vectors for all lemma features (omitting stopwords) in the first-order vector. For the feature vector in Table 3, ~ ~ this is the centroid of vectors sweet-sour, sauce, ~ ~ . . . , boil. We compute the vectors sweet-sour etc. as dependency vectors (Pad´o and Lapata, 2007) 4 over a Minipar parse (Lin, 1993) of the BNC. and recall, which can be seen as follows. Graded sense assignment is represented by assigning each sense a score between 0.0 and 1.0. The categorial case can be represented in the same way, the difference being that one single sense will receive a score of 1.0 while all other senses get a score of 0.0. With this representation for categorial sense assignment, consider a fixed token t of lemma `. P i∈S` min(assigned`,i,t , gold`,i,t ) will be 1 if the assigned sense is the gold sense, and 0 otherwise. 5 Models for Graded Word Sense Assignment In this section we discuss the computat"
D09-1046,S07-1009,1,0.861164,"As Spearman’s ρ compares the rankings of two sets of judgments, it abstracts from the absolute values of the judgments. It is useful to have a measure that abstracts from absolute values of judgments and magnitude of difference because the GWS dataset contains annotator judgments on a fixed scale, and it is quite possible that human judges will differ in how they use such a scale. Each judgment in the gold-standard can be represented as a 4-tuple hlemma, sense no, sentence no, gold judgmenti. For example, hadd.v, 1 2 The GWS data also contains data from the English Lexical Substitution Task (McCarthy and Navigli, 2007) but we do not use that portion of the data for these experiments. Mitchell and Lapata (2008) note that Spearman’s ρ tends to yield smaller coefficients than its parametric counterparts such as Pearson’s coefficient. 442 1, 1, 0.8i is the first sentence for target add.v, first WordNet sense, with a (normalized) judgment of 0.8. Likewise, each prediction by the model can be represented as a 4-tuple hlemma, sense no, sentence no, predicted judgmenti. We write G for the set of gold tuples, A for the set of assigned tuples, L for the set of lemmas, S` for the set of sense numbers that exist for le"
D09-1046,W03-1810,1,0.385229,"y be transformed to categorial judgments by making more coarse-grained senses. If human word sense judgments are best viewed as graded, it makes sense to explore models of word sense that can predict graded sense assignments. In this paper we look at the issue of graded applicability of word sense from the point of view of automatic graded word sense assignment, using the GWS graded word sense dataset. We make three primary contributions. Firstly, we propose evaluation metrics that can be used on graded word sense judgments. Some of these metrics, like Spearman’s ρ, have been used previously (McCarthy et al., 2003; Mitchell and Lapata, 2008), but we also introduce new metrics based on the traditional precision and recall. Secondly, we investigate how two classes of models perform on the task of graded word sense assignment: on the one hand classical WSD models, on the other hand prototype-based vector space models that can be viewed as simple one-class classifiers. We study supervised models, training on traditional WSD data and evaluating against a graded scale. Thirdly, the evaluation metrics we use also provides a novel analysis of annotator performance on the GWS dataset. Word sense disambiguation"
D09-1046,J98-1004,0,0.561455,"Missing"
D09-1046,H05-1051,0,0.0174344,"rmance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try to predict graded judgments of word sense applicability, using a recent dataset with graded annotation (Erk et al., 2009). Our hope is that models which can mimic graded human judgments on the same task should better reflect the underlying phenomena of word meaning compared to a system that focuses on ma"
D09-1046,W06-2503,1,0.779405,"arpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try t"
D09-1046,W04-0807,0,0.0214689,"As the GWS dataset is too small to accommodate both training and testing of a supervised model, we use all the data from GWS for testing our models, and train our models on traditional word sense annotation data. We use as training data all sentences from SemCor and the training portion of SE-3 that are not included in GWS. The quantity of training data available is shown in the last two columns of table 2. # training SemCor SE-3 171 238 14 195 386 236 106 73 125 11 111 160 46 207 88 53 1047 1173 Table 2: Lemmas used in this study with various sense-tagged datasets (e.g. (Miller et al., 1993; Mihalcea et al., 2004)) for comparison. 3 Data In this paper, we use a subset of the GWS dataset (Erk et al., 2009) where three annotators supplied ordinal judgments of the applicability of WordNet (v3.0) senses on a 5 point scale: 1 – completely different, 2 – mostly different, 3 – similar, 4 – very similar and 5 – identical. Table 1 shows a sample annotation. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al., 2004) (hereafter SE-3) and SemCor (Miller et al., 1993). 1 For 8 lemmas, 25 sentences were randomly sampled from SemCor"
D09-1046,H93-1061,0,0.387192,"mapped accordingly. As the GWS dataset is too small to accommodate both training and testing of a supervised model, we use all the data from GWS for testing our models, and train our models on traditional word sense annotation data. We use as training data all sentences from SemCor and the training portion of SE-3 that are not included in GWS. The quantity of training data available is shown in the last two columns of table 2. # training SemCor SE-3 171 238 14 195 386 236 106 73 125 11 111 160 46 207 88 53 1047 1173 Table 2: Lemmas used in this study with various sense-tagged datasets (e.g. (Miller et al., 1993; Mihalcea et al., 2004)) for comparison. 3 Data In this paper, we use a subset of the GWS dataset (Erk et al., 2009) where three annotators supplied ordinal judgments of the applicability of WordNet (v3.0) senses on a 5 point scale: 1 – completely different, 2 – mostly different, 3 – similar, 4 – very similar and 5 – identical. Table 1 shows a sample annotation. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al., 2004) (hereafter SE-3) and SemCor (Miller et al., 1993). 1 For 8 lemmas, 25 sentences were rand"
D09-1046,P08-1028,0,0.373728,"egorial judgments by making more coarse-grained senses. If human word sense judgments are best viewed as graded, it makes sense to explore models of word sense that can predict graded sense assignments. In this paper we look at the issue of graded applicability of word sense from the point of view of automatic graded word sense assignment, using the GWS graded word sense dataset. We make three primary contributions. Firstly, we propose evaluation metrics that can be used on graded word sense judgments. Some of these metrics, like Spearman’s ρ, have been used previously (McCarthy et al., 2003; Mitchell and Lapata, 2008), but we also introduce new metrics based on the traditional precision and recall. Secondly, we investigate how two classes of models perform on the task of graded word sense assignment: on the one hand classical WSD models, on the other hand prototype-based vector space models that can be viewed as simple one-class classifiers. We study supervised models, training on traditional WSD data and evaluating against a graded scale. Thirdly, the evaluation metrics we use also provides a novel analysis of annotator performance on the GWS dataset. Word sense disambiguation is typically phrased as the"
D09-1046,S07-1006,0,0.0658321,"Missing"
D09-1046,P06-1014,0,0.0164749,"., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try to predict graded judgments of"
D09-1046,J07-2002,0,0.0472049,"Missing"
D09-1046,P05-1016,0,0.0420628,"elated in two ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than clustering occurrences into word senses. The second way in which word sense and vector space models have been related is to assign disambiguated feature vectors to WordNet concepts (Pantel, 2005; Patwardhan and Pedersen, 2006). However those works do not use sense-tagged data and are not aimed at WSD, rather the applications are to insert new concepts into an ontology and to measure the relatedness of concepts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination o"
D09-1046,W06-2501,0,0.00374444,"ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than clustering occurrences into word senses. The second way in which word sense and vector space models have been related is to assign disambiguated feature vectors to WordNet concepts (Pantel, 2005; Patwardhan and Pedersen, 2006). However those works do not use sense-tagged data and are not aimed at WSD, rather the applications are to insert new concepts into an ontology and to measure the relatedness of concepts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between us"
E12-1060,S07-1002,0,0.718641,"assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple"
E12-1060,E09-1013,0,0.822927,"s a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighte"
E12-1060,cook-stevenson-2010-automatically,1,0.882901,"hese senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to"
E12-1060,de-marneffe-etal-2006-generating,0,0.00362124,"Missing"
E12-1060,H92-1045,0,0.355926,"ruction of semantic space models, e.g. for WSD. Based on these findings, we include dependency relations as additional features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1 Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2 We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T , the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P (t = z|d) then; (2) draw a word w from the chosen topic P (w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P (w|d) = T ∑ P (w|t = z)P (t = z|d). z=1 In stand"
E12-1060,W11-2508,0,0.0612863,"t 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new"
E12-1060,S10-1079,0,0.195775,"Missing"
E12-1060,S10-1011,0,0.457638,"rm of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple topics, with each documen"
E12-1060,D10-1012,0,0.0734893,"sk. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use t"
E12-1060,S07-1037,0,0.0562894,"Missing"
E12-1060,J07-2002,0,0.0570721,"Missing"
E12-1060,W09-0214,0,0.117489,"Missing"
E12-1060,J98-1004,0,0.680031,"sense detection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and"
E12-1060,W11-1102,0,0.612441,"Missing"
H05-1053,W04-0811,0,\N,Missing
H05-1053,W04-0807,0,\N,Missing
H05-1053,rose-etal-2002-reuters,0,\N,Missing
H05-1053,H93-1061,0,\N,Missing
H05-1053,O97-1002,0,\N,Missing
H05-1053,briscoe-carroll-2002-robust,1,\N,Missing
H05-1053,P04-1036,1,\N,Missing
H05-1053,H92-1045,0,\N,Missing
H05-1053,P98-2127,0,\N,Missing
H05-1053,C98-2122,0,\N,Missing
H05-1053,magnini-cavaglia-2000-integrating,0,\N,Missing
H05-1053,P99-1004,0,\N,Missing
I08-1073,D07-1007,0,0.019204,"ce and Technology University of Sussex 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan Falmer, East Sussex ryu-i@is.naist.jp BN1 9QH, UK {dianam,robk}@sussex.ac.uk Abstract many researches believe it will be important for applications which require, or would beneﬁt from, some degree of semantic interpretation. There has been considerable skepticism over whether WSD will actually improve performance of applications, but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval (Clough and Stevenson, 2004; Vossen et al., 2006) and machine translation (Carpuat and Wu, 2007; Chan et al., 2007) and we hope that other applications such as question-answering, text simpliﬁcation and summarisation might also beneﬁt as WSD methods improve. In recent years there have been various approaches aimed at automatic acquisition of predominant senses of words. This information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipﬁan distribution of word senses. Approaches which do not require manually sense-tagged data have been proposed for English exploiting lexical resources available, notably WordNet. In these approaches distributional"
I08-1073,P07-1007,0,0.0569397,"Missing"
I08-1073,P07-1005,0,0.0377788,"ersity of Sussex 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan Falmer, East Sussex ryu-i@is.naist.jp BN1 9QH, UK {dianam,robk}@sussex.ac.uk Abstract many researches believe it will be important for applications which require, or would beneﬁt from, some degree of semantic interpretation. There has been considerable skepticism over whether WSD will actually improve performance of applications, but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval (Clough and Stevenson, 2004; Vossen et al., 2006) and machine translation (Carpuat and Wu, 2007; Chan et al., 2007) and we hope that other applications such as question-answering, text simpliﬁcation and summarisation might also beneﬁt as WSD methods improve. In recent years there have been various approaches aimed at automatic acquisition of predominant senses of words. This information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipﬁan distribution of word senses. Approaches which do not require manually sense-tagged data have been proposed for English exploiting lexical resources available, notably WordNet. In these approaches distributional similarity is coupl"
I08-1073,W07-1428,0,0.0219632,"which are not always available. There are several issues for future directions of automatic detection of a ﬁrst sense heuristic. In this paper, we proposed an adaptation of the lesk measure of gloss-based similarity, by using the average similarity between nouns in the two glosses under comparison in a bag-of-words approach without recourse to other information. However, it would be worthwhile exploring other information in the glosses, such as words of other PoS and predicate argument relations. We also hope to investigate applying alignment techniques introduced for entailment recognition (Hickl and Bensley, 2007). Another important issue in WSD is to group ﬁnegrained word senses into clusters, making the task suitable for NLP applications (Ide and Wilks, 2006). We believe that our gloss-based similarity DSlesk might be very suitable for this task and we plan to investigate the possibility. There are other approaches we would like to explore in future. Mihalcea (2005) uses dictionary definitions alongside graphical algorithms for unsupervised WSD. Whilst the results are not directly comparable to ours because we have not included contextual evidence in our models, it would be worthwhile exploring if un"
I08-1073,P06-1013,0,0.0384264,"Missing"
I08-1073,O97-1002,0,0.819347,"g computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pedersen as Elesk. This paper investigates the potential of using the overlap of dictionary deﬁnitions with the McCarthy et al.’s method. We test the method for obtaining a ﬁrst sense heuristic using two publicly available datasets of sense-tagged data in Japanese, EDR (NICT, 2002) and the S ENSEVAL-2 Japanese dictionary task (Shirai, 2001). We contrast an implementation of lesk (Lesk, 1986) which uses only dictionary deﬁnitions with the Jiang-Conrath measure (jcn) (Jiang and Conrath, 1997) which uses man562 ually produced hyponym links and was used previously for this purpose on English datasets (McCarthy et al., 2004). The jcn measure is only applicable to the EDR dataset because the dictionary has hyponymy links which are not available in the S ENSEVAL-2 Japanese dictionary task. We also propose a new extension to lesk which does not require hand-crafted hyponym links but instead uses distributional similarity to increase the possibilities for overlap of the word deﬁnitions. We refer to this new measure as DSlesk. We compare this to the original lesk on both datasets and show"
I08-1073,H05-1053,1,0.893981,"we select one word sense at random for each word token and average the precision over 100 trials. For contrast with a supervised approach we show the performance if we use handlabelled training data for obtaining the predominant sense of the test words. This method usually outperforms an automatic approach, but crucially relies on there being hand-labelled data which is expensive to produce. The method cannot be applied where there is no hand-labelled training data, it will be unreliable for low frequency data and a general dataset may not be applicable when one moves to domain speciﬁc text (Koeling et al., 2005). Since we are not using context for disambiguation, but just a ﬁrst sense heuristic, we also give the upper-bound which is the ﬁrst sense heuristic calculated from the test data itself. 4.1 EDR We conduct empirical evaluation using 3,836 polysemous nouns in the sense-tagged corpus provided with EDR (183,502 instances) where the glosses are deﬁned in the EDR dictionary. We evaluated on this dataset using WSD precision and recall of this corpus using only our ﬁrst-sense heuristic (no context). The results are shown in Table 1. The WSD performance of all the automatic methods is much lower than"
I08-1073,W02-2016,0,0.0228183,"ties, jcn, lesk and DSlesk3 , for use in the method to 3 Elesk can be used when several semantic relations such as hypnoymy and meronomy are available. However, we cannot directly apply Elesk as it was used in (McCarthy et al., 2004) to 565 ﬁnd the most likely sense in the set of word senses deﬁned in each inventory following the approach of McCarthy et al. (2004). For the thesaurus construction we used <verb, case, noun&gt; triplets extracted from Japanese newspaper articles (9 years of the Mainichi Shinbun (1991-1999) and 10 years of the Nihon Keizai Shinbun (1991-2000)) and parsed by CaboCha (Kudo and Matsumoto, 2002). This resulted in 53 million triplet instances for acquiring the distributional thesaurus. We adopt the similarity score proposed by Lin (1998) as the distributional similarity score and use 50 nearest neighbours in line with McCarthy et al. For the random baseline we select one word sense at random for each word token and average the precision over 100 trials. For contrast with a supervised approach we show the performance if we use handlabelled training data for obtaining the predominant sense of the test words. This method usually outperforms an automatic approach, but crucially relies on"
I08-1073,P98-2127,0,0.902401,"en the candidate paraphrases. For each word in each of the two texts they obtain the maximum similarity between the word and any of the words from the putative paraphrase. The similarity scores for each word of both phrases contribute to an overall semantic similarity between 0 and 1 and a threshold of 0.5 is used to decide if the candidate phrases are paraphrases. In our work, we compare glosses of words senses (senses of the target word and senses of the nearest neighbour) rather than paraphrases. In this approach we extend the deﬁnition overlap by considering the distributional similarity (Lin, 1998) rather than identify of the words in the two deﬁnitions. In addition to McCarthy et al. (2004) there are other approaches to ﬁnding predominant senses. Chan and Ng (2005) use parallel data to provide estimates for sense frequency distributions to feed into a supervised WSD system. Mohammad and Hirst (2006) propose an approach to acquiring predominant senses from corpora which makes use of the category information in the Macquarie Thesaurus (Barnard, 1986). Lexical chains (Galley and McKeown, 2003) may also provide a useful ﬁrst sense heuristic (Brody et al., 2006) but are produced 563 using W"
I08-1073,P04-1036,1,0.934538,"incorporates information from the sense inventory. It is this semantic similarity measure which is the focus of our paper in the context of the method for acquiring predominant senses. Whilst the McCarthy et al.’s method works well for English, other inventories do not always have WordNet style resources to tie the nearest neighbours to the sense inventory. WordNet has many semantic relations as well as glosses associated with its synsets (near synonym sets). While traditional dictionaries do not organise senses into synsets, they do typically have sense deﬁnitions associated with the senses. McCarthy et al. (2004) suggest that dictionary deﬁnitions can be used with their method, however in the implementation of the measure based on dictionary deﬁnitions that they use, the dictionary deﬁnitions are extended to those of related words using the hierarchical structure of WordNet (Banerjee and Pedersen, 2002). This extension to the original method (Lesk, 1986) was proposed because there is not always sufﬁcient overlap of the individual words for which semantic similarity is being computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pe"
I08-1073,H05-1052,0,0.0519522,"Missing"
I08-1073,H93-1061,0,0.606995,"Missing"
I08-1073,E06-1016,0,0.205552,"Missing"
I08-1073,S07-1006,0,0.0881096,"Missing"
I08-1073,S01-1008,0,0.189448,"l method (Lesk, 1986) was proposed because there is not always sufﬁcient overlap of the individual words for which semantic similarity is being computed. In this paper we refer to the original method (Lesk, 1986) as lesk and the extended measure proposed by Banerjee and Pedersen as Elesk. This paper investigates the potential of using the overlap of dictionary deﬁnitions with the McCarthy et al.’s method. We test the method for obtaining a ﬁrst sense heuristic using two publicly available datasets of sense-tagged data in Japanese, EDR (NICT, 2002) and the S ENSEVAL-2 Japanese dictionary task (Shirai, 2001). We contrast an implementation of lesk (Lesk, 1986) which uses only dictionary deﬁnitions with the Jiang-Conrath measure (jcn) (Jiang and Conrath, 1997) which uses man562 ually produced hyponym links and was used previously for this purpose on English datasets (McCarthy et al., 2004). The jcn measure is only applicable to the EDR dataset because the dictionary has hyponymy links which are not available in the S ENSEVAL-2 Japanese dictionary task. We also propose a new extension to lesk which does not require hand-crafted hyponym links but instead uses distributional similarity to increase the"
I08-1073,W04-0811,0,0.069796,"Missing"
I08-1073,C98-2122,0,\N,Missing
I11-1024,W03-1812,0,0.900245,"their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 A"
I11-1024,D07-1039,1,0.914925,"Missing"
I11-1024,W03-1809,0,0.0884781,"(Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propose various constituent based models (section 4.3) which are intuitive and related to existing models of compositionality detection (section 4.1) and we evaluate these models in comparison to composition"
I11-1024,P08-1028,0,0.812426,"the second in determining compositionality. Results (both ρ and R2 ) clearly show that a relation exists between the constituent literality scores and the phrase compositionality. Existing compositionality approaches on noun-noun compounds such as (Baldwin et al., 2003; Korkontzelos and Manandhar, 2009) use the semantics of only one of the constituent words (generally the head word) Overall, this study suggests that it is possible to estimate the phrase level compositionality scores given the constituent word level literality scores. This motivates us to present constituent 214 vector models (Mitchell and Lapata, 2008; Widdows, 2008) which make use of the semantics of the constituents in a different manner. These models are described in section 4.4 and are evaluated in comparison with the constituent-based models. The vector space model used in all our experiments is described as follows. based models (section 4.3) for compositionality score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixe"
I11-1024,W11-1304,0,0.221301,"thy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propos"
I11-1024,W07-1106,0,0.0288709,"e lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disam"
I11-1024,W11-1306,0,0.0320891,"ty score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixedness and syntactic properties of the MWEs, and those which make use of the semantic similarities between the constituents and the MWE. Non compositional MWEs are known to have lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the vari"
I11-1024,W11-1310,1,0.854619,"Missing"
I11-1024,J09-1005,0,0.18527,"and reduce the impact of ambiguity. The second is that distributional models are greatly influenced by frequency and since we aim to work with distributional models for compositionality detection we base our findings on the most frequent sense of the compound noun. In this work we consider the compositionality of the noun-noun compound type without token based disambiguation which we leave for future work. atively scarce. In this paper, we only deal with compound nouns made up of two words separated by space. 2.1 Annotation setup In the literature (Nunberg et al., 1994; Baldwin et al., 2003; Fazly et al., 2009), compositionality is discussed in many terms including simple decomposable, semantically analyzable, idiosyncratically decomposable and non-decomposable. For practical NLP purposes, Bannard et al. (2003) adopt a straightforward definition of a compound being compositional if “the overall semantics of the multi-word expression (here compound) can be composed from the simplex semantics of its parts, as described (explicitly or implicitly) in a finite lexicon”. We adopt this definition and pose compositionality as a literality issue. A compound is compositional if its meaning can be understood f"
I11-1024,W01-0513,0,0.0462878,"Missing"
I11-1024,W11-0115,0,0.0607213,"2005; Biemann and Giesbrecht, 2011): higher correlation scores indicate better compositionality predictions. s3 = f (s1, s2) Composition function based models In these models (Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Giesbrecht, 2009) of compositionality detection, firstly a vector for the compound is composed from its constituents using a compositionality function ⊕. Then the similarity between the composed vector and true cooccurrence vector of the compound is measured to determine the compositionality: the higher the similarity, the higher the compositionality of the compound. Guevara (2011) observed that additive models performed well for building composition vectors of phrases from their parts whereas Mitchell and Lapata (2008) found in favor of multiplicative models. We experiment using both the compositionality functions simple addition5 and simple multiplication, which are the most widely used composition functions, known for their simplicity and good performance. Vector v1 ⊕ v2 for a compound w3 is composed from its constituent word vectors v1 and v2 using the vector addition av1 + bv2 and simple multiplication v1v2 where the ith element of v1 ⊕ v2 is defined as (av1 + bv2)"
I11-1024,D08-1027,0,0.0267317,"Missing"
I11-1024,W06-1203,0,0.263496,"york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of t"
I11-1024,E09-1086,0,0.00987205,"known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disambiguation) based on the lexical chain similarities of the constituents and the context of the MWE. Bannard et al. (2003) and McCarthy et al. (2003) study the compositionality in verb particles and they found that methods based on the similarity between simpl"
I11-1024,P09-2017,1,0.915764,"Missing"
I11-1024,H05-1113,0,0.257826,"Missing"
I11-1024,P99-1041,0,0.0571438,"s, additive models perform better than their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Ch"
I11-1024,W03-1810,1,0.568929,", UK Suresh Manandhar University of York, UK siva@cs.york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the"
I11-1079,J93-1003,0,0.133967,"ty. The selected clusters are then combined using a composition function, to produce a single vector representing the semantics of the target compound noun N . 3.1.1 Figure 2: Running example of WSI The aim of this stage is to capture words contextually related to tw. In the first step, the target word is removed from bc and part-of-speech tagging is applied to each context. Only nouns and verbs are kept and lemmatised. In the next step, the distribution of each word in the base corpus is compared to the distribution of the same noun in a reference corpus using the log-likelihood ratio (G2 ) (Dunning, 1993). Words that have a G2 below a pre-specified threshold (parameter p1 ) are removed from each context of the base corpus. The result of this stage is shown in the upper left part of Figure 2. Graph creation & clustering: Each context ci ∈ bc is represented as a vertex in a graph G. Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 2, where smcl (ci , cj ) is the collocational weight of contexts ci , cj and smwd (ci , cj ) is their bag-of-words weight. If the edge weight W (ci , cj ) is above a prespecified threshold (parameter p3 ), then an edge is"
I11-1079,D08-1094,0,0.15084,"Missing"
I11-1079,P10-2017,0,0.0445281,"Missing"
I11-1079,D10-1073,1,0.813692,"Missing"
I11-1079,N10-1010,1,0.912273,"y is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been pro"
I11-1079,P09-2017,1,0.880724,"Missing"
I11-1079,S07-1002,0,0.00838561,"the set of vertices which share a direct connection with vertex i. During the update step for a vertex i: each class Ck receives a score equal to the sum of the weights of edges (i, j), where j has been assigned class Ck . The maximum score determines the strongest class. In case of multiple strongest classes, one is chosen randomly. Classes are updated immediately, which means that a node can inherit classes from its LN that were introduced in the same iteration. Experimental setting The parameters of the WSI method were fine-tuned on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task, i.e. supervised (WSD) evaluation. We tried various parameter combinations shown in Table 1. Specifically, we selected the parameter combination p1 =15, p2 =10, p3 = 0.05 that maximized the performance in this evaluation. We use ukWaC (Ferraresi et al., 2008) corpus to retrieve all the instances of the target words. 3.2 Dynamic Prototype Based Sense Selection Kilgarriff (1997) argues that representing a word with a fixed set of senses is not a good way of modelling word senses. Instead word senses should be defined according to a given context."
I11-1079,P08-1028,0,0.849714,"ounds and evaluate on a compositionality-based similarity task. Our results show that: (1) selecting relevant senses of the constituent words leads to a better semantic composition of the compound, and (2) dynamic prototypes perform better than static prototypes. 1 animal house h 30 hunting h 90 buy 60 15 vector dimensions apartment price 90 55 12 20 rent 45 33 kill 10 i 90 i Figure 1: A hypothetical vector space model. Compositional Distributional Semantic methods formalise the meaning of a phrase by applying a vector composition function on the vectors associated with its constituent words (Mitchell and Lapata, 2008; Widdows, 2008). For example, the result of vector addition to compose the semantics of house hunting from the vectors house and hunting is the vector h120, 75, 102, 75, 78, 100i. As can be observed the resulting vector does not reflect the correct meaning of the compound house hunting due to the presence of irrelevant co-occurrences such as animal or kill. These cooccurrences are relevant to one sense of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with"
I11-1079,D10-1012,0,0.0071372,"e compound noun. The results are encouraging showing that polysemy is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of composit"
I11-1079,J07-2002,0,0.0285319,"of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with a single prototype (vector) by conflating all of its senses, the application of a composition function ⊕ is likely to include irrelevant co-occurrences in house ⊕ hunting. A potential solution to this problem would involve the following steps: Introduction Vector Space Models of lexical semantics have become a standard framework for representing a word’s meaning. Typically these methods (Sch¨utze, 1998; Pado and Lapata, 2007; Erk and Pad´o, 2008) utilize a bag-of-words model or 705 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 705–713, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP following Mitchell and Lapata (2008) are defined as follows: 1. build separate prototype vectors for each of the senses of house and hunting 2. select the relevant prototype vectors of house and hunting and then perform the semantic composition. In this paper we present two methods (section 3) for carrying out the above steps on noun-noun compounds. The first one (section 3.1) ap"
I11-1079,N10-1013,0,0.284791,"fers to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound"
I11-1079,P07-2011,0,0.0890041,"Missing"
I11-1079,J98-1004,0,0.706399,"Missing"
I11-1079,W11-1301,0,0.0217646,"xt section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound words from the semantics of the constituent words. Mitchell and Lapata (2008) discussed and evaluated various composition functions for phrases consisting of two words. Among these, the simple additive (ADD) and simple multiplicative (MULT) functions are easy to implement and competitive with respect to existing sophisticated methods (Widdows, 2008; Vecchi et al., 2011). Let us assume a target compound noun N that consists of two nouns n and n0 . Bold letters represent their corresponding distributional vectors obtained from corpora. ⊕(N) denotes the vector of N obtained by applying the composition function ⊕ on n and n0 . Real number vi denote the ith cooccurrence in v. The functions ADD and MULT 3 Sense Prototype Vectors for Semantic Composition In this section we describe two approaches for building sense specific prototype vectors of constituent words in a noun-noun compound. The first approach performs WSI to build static multi prototype vectors. The ot"
I11-1079,W06-3812,0,\N,Missing
I11-1079,W11-0115,0,\N,Missing
J03-4004,W99-0901,0,0.0636712,"ion to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantag"
J03-4004,W01-0703,0,0.0550584,"Missing"
J03-4004,J93-1002,1,0.184643,"Missing"
J03-4004,1995.iwpt-1.8,1,0.845886,"Missing"
J03-4004,E99-1042,1,0.728329,"Missing"
J03-4004,C00-1028,0,0.139046,"formance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at"
J03-4004,A94-1009,0,0.0172762,"Missing"
J03-4004,J98-2002,0,0.0845227,"Missing"
J03-4004,W97-0808,1,0.843014,"erage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized"
J03-4004,H93-1061,0,0.283296,"Missing"
J03-4004,P96-1006,0,0.013206,"Missing"
J03-4004,W02-0815,0,0.0206244,"yms, for example, substituting letter for missive. Our motivation for using WSD is to filter out inappropriate senses of a word token, so that the substituting synonym is appropriate given the context. For example, in the following sentence we would like to use strategy, rather than dodge, as a substitute for scheme: A recent government study singled out the scheme as an example to others. We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if subcategorization were acquired specific to sense, rather than verb form, then distinct senses of fire could have different subcategorization entries: fire(1) - sack: fire(2) - shoot: NP V NP NP V NP, NP V Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001). 3. Methodology We acquire selectional preferences from automatically preprocessed and parsed text during a training phase. The parser is applied to the test data as well in the runtime phase to identify grammatical relations among nouns, verbs,"
J03-4004,W97-0209,0,0.306333,". 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguation performance of automatically acquired selectional preferences in regard to nouns, verbs, and adjectives with respect to a standard test corpus and evaluat"
J03-4004,E95-1016,0,0.0149708,"racy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they c"
J03-4004,J01-3001,0,0.0222001,"ults to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguat"
J07-2005,W04-3204,0,0.0523912,"Missing"
J07-2005,W06-1669,0,0.0424829,"Missing"
J07-2005,J98-1004,0,0.239702,"Missing"
J07-4005,briscoe-carroll-2002-robust,1,0.276472,"Missing"
J07-4005,P89-1010,0,0.0609431,"e each word’s total feature frequency was at least 10. A thesaurus entry of size k for a target word w is then defined as the k most similar words to w. A large number of distributional similarity measures have been proposed in the literature (see Weeds 2003 for a review) and comparing them is outside the scope of this work. However, the study of Weeds and Weir (2005) provides interesting insights into what makes a “good” distributional similarity measure in the contexts of semantic similarity prediction and language modeling. In particular, weighting features by pointwise mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise mutual information (I(w, f )) between a word and a feature is calculated as I(w, f ) = log P( f |w) P( f ) (3) Intuitively, this means that the occurrence of a less-common feature is more important in describing a word than a more-common feature. For example, the verb eat is more selective and tells us more about the meaning of its arguments than the verb be. 13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor. We use sss for the semantic similarity between two WordNet senses, si and a sense of the neighbor"
J07-4005,W03-1022,0,0.00935984,"redominance with automatically induced inventories such as those produced by CBC. Evaluation of induced inventories should be done in the context of an application, because the senses will be keyed to the acquisition corpus and not to WordNet. Induction of senses allows coverage of senses appearing in the data that are not present in a predefined inventory. Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns. 9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the option to include all corpora (TREC-2002, TREC-9, and COSMOS). 562 McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses 4. Method In our method, the predominant sense for a target word is determined from a prevalence ranking of the possible senses for that word. The senses come from a predefined inventory (which might be a dictionary or WordNet-like resource). The ranking is derived using a distributional thesaurus automatically produ"
J07-4005,P05-1004,0,0.0109871,"duced inventories such as those produced by CBC. Evaluation of induced inventories should be done in the context of an application, because the senses will be keyed to the acquisition corpus and not to WordNet. Induction of senses allows coverage of senses appearing in the data that are not present in a predefined inventory. Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns. 9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the option to include all corpora (TREC-2002, TREC-9, and COSMOS). 562 McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses 4. Method In our method, the predominant sense for a target word is determined from a prevalence ranking of the possible senses for that word. The senses come from a predefined inventory (which might be a dictionary or WordNet-like resource). The ranking is derived using a distributional thesaurus automatically produced from a large c"
J07-4005,P00-1064,0,0.0293393,"Missing"
J07-4005,H92-1045,0,0.395155,"Missing"
J07-4005,P05-1050,0,0.030912,"Missing"
J07-4005,O97-1002,0,0.0133586,"measure in the package with no normalizing for gloss length, and the default set of relations: lesk(s1, s2) = |{W1 ∈ definition(s1)} |∩ |{W2 ∈ definition(s2)}| (5) where definitions(s) is the gloss definition of sense s concatenated with the gloss definitions of the senses related to s where the relationships are defined by the de566 McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses fault set of relations in the relations.dat file supplied with the WordNet Similarity package. W ∈ definition(s) is the set of words from the concatenated definitions. jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts. Each synset is incremented with the frequency counts (from the corpus) of all words belonging to that synset, directly or via the hyponymy relation. The frequency data is used to calculate the “information content” (IC; Resnik 1995) of a class as follows: IC(s) = −log(p(s)) (6) Jiang and Conrath specify a distance measure: Djcn (s1, s2) = IC(s1) + IC(s2) − 2 × IC(s3) (7) where the third class (s3) is the most informative, or most specific, superordinate synset of the two senses s1 and s2. This is co"
J07-4005,S07-1068,1,0.81821,"Missing"
J07-4005,H05-1053,1,0.810634,"Missing"
J07-4005,J04-1003,0,0.0393479,"elate to and weighting the contribution from these neighbors more. This may however give rise to further errors because of the noise introduced by focusing on individual neighbors. We will explore such directions in future work. 576 McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses In this experiment we did not assign any credit for near misses. In many cases of error the SemCor FS nonetheless received a high prevalence score. In the future we hope to use the score for probability estimation, and combine this with contextual information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng (2005). 6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic In the previous section we described an evaluation of the accuracy of automatically acquired predominant sense information. We carried out the evaluation with respect to SemCor in order to have as much test data as possible. To obtain reasonably reliable gold-standard first-sense data and first-sense heuristic upper bounds, we limited the evaluation to words occurring at least three times in SemCor. Clearly this scenario is unrealistic. For many words, and particularly for nouns, there is very little or n"
J07-4005,P97-1009,0,0.0567575,"is in the future for using our ranking score for estimating probability distributions of senses, because a sufficiently large value of k will be needed to include neighbors for rarer senses. 565 Computational Linguistics Volume 33, Number 4 We chose to use the distributional similarity score described by Lin (1998a) because it is an unparameterized measure which uses pointwise mutual information to weight features and which has been shown (Weeds 2003) to be highly competitive in making predictions of semantic similarity. This measure is based on Lin’s information-theoretic similarity theorem (Lin 1997): The similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are. In our application, if T(w) is the set of features f such that I(w, f ) is positive, then the similarity between two words, w and n, is    f ∈T(w)∩T(n) I(w, f ) + I(n, f )  dss(w, n) =  f ∈T(w) I(w, f ) + f ∈T(n) I(n, f ) (4) However, due to this choice of dss and the openness of the domain, we restrict ourselves to only considering words with a total feature frequency of at least 10. Weeds et"
J07-4005,P98-2127,0,0.763587,"tics Computational Linguistics Volume 33, Number 4 1. Introduction In word sense disambiguation, the “first sense” heuristic (choosing the first, or predominant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufficient to make a more informed choice. In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text (McCarthy et al. 2004a). The method uses distributionally similar words listed as “nearest neighbors” in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic s"
J07-4005,magnini-cavaglia-2000-integrating,0,0.0318076,"Missing"
J07-4005,S01-1027,0,0.0609412,"Missing"
J07-4005,W00-1326,0,0.0432788,"is based on lexicographer intuition, whereas in WordNet the senses are ordered according to their frequency in SemCor (Miller et al. 1993). There are two major problems with deriving a first sense heuristic from these types of resources. The first is that the predominant sense of a word varies according to the source of the document (McCarthy and Carroll 2003) and with the domain. For example, the first sense of star as derived from SemCor is celestial body, but if one were disambiguating popular news stories then celebrity would be more likely. Domain, topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al. 2002) and the sense-frequency distributions of words depend on all of these factors. Any dictionary will provide only a single sense ranking, whether this is derived from sensetagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpus data as in the Oxford Advanced Learner’s Dictionary (Hornby 1989). A fixed order of senses may not reflect the data that an NLP system is dealing with. The second problem with obtaining predominant sense information applies to the use of hand-tagged resources, such as SemCor. Such resources are relatively small due to"
J07-4005,W06-2503,1,0.837506,"Missing"
J07-4005,J03-4004,1,0.708726,"Missing"
J07-4005,P04-1036,1,0.070666,"November 2005; revised submission received: 12 July 2006; accepted for publication 16 February 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction In word sense disambiguation, the “first sense” heuristic (choosing the first, or predominant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufficient to make a more informed choice. In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text (McCarthy et al. 2004a). The method uses distributionally similar words listed as “nearest neighbors” in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea"
J07-4005,W04-0837,1,0.0784347,"November 2005; revised submission received: 12 July 2006; accepted for publication 16 February 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction In word sense disambiguation, the “first sense” heuristic (choosing the first, or predominant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufficient to make a more informed choice. In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text (McCarthy et al. 2004a). The method uses distributionally similar words listed as “nearest neighbors” in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea"
J07-4005,S07-1009,1,0.14586,"Missing"
J07-4005,H93-1061,0,0.94556,"acquiring predominant senses for words from raw text (McCarthy et al. 2004a). The method uses distributionally similar words listed as “nearest neighbors” in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic similarity which relies on the relationships between word senses, such as hyponyms (available in an inventory such as WordNet) or overlap in the definitions of word senses (available in most dictionaries), or both. In this article we provide a detailed discussion and quantitative analysis of the motivation behind the first sense heuristic, and a full description of our method. We extend previously reported w"
J07-4005,E06-1016,0,0.0343306,"Missing"
J07-4005,S07-1006,0,0.0392996,"Missing"
J07-4005,rose-etal-2002-reuters,0,0.0299999,"Missing"
J07-4005,W04-0811,0,0.114906,"bout 2,000 words, from the Brown corpus (Francis and Kuˇcera 1979) and the complete text of a 19th-century American novel, The Red Badge of Courage, which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and these have been linked to WordNet senses by human taggers using a software interface. The shortage of training data due to the high costs of tagging texts has motivated research into unsupervised methods for WSD. But in the English all-words tasks in Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make use of hand-tagged data (in some form or other) performed substantially worse than those that did. Table 1 summarizes the situation. It gives the precision and recall of the best1 two supervised (S) and unsupervised (U)2 systems for the English all words and English lexical sample for Senseval-23 and -3, along with the first sense baseline (FS) reported by the task organizers.4 This is a simple application of the “first sense” heuristic—that is, using the most common sense of a word for every instance of it in the test corpus, regardless of context. Although context"
J07-4005,J01-3001,0,0.00781404,"discuss directions for future work (Section 8). 2. Motivation The problem of disambiguating the meanings of words in text has received much attention recently, particularly since the inception of the Senseval evaluation exercises (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). One of the standard Senseval tasks (the “all words” task) is to tag each open class word with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum 1998). The most accurate word sense disambiguation (WSD) systems use supervised machine learning approaches (Stevenson and Wilks 2001), trained on text which has been sense tagged by hand. However, the performance of these systems is strongly 555 Computational Linguistics Volume 33, Number 4 dependent on the quantity of training data available (Yarowsky and Florian 2002), and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The largest all words sense tagged corpus is SemCor, which is 220,000 words taken from 103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuˇcera 1979) and the complete text of a 19th-century American novel, The Red Badge of Courage, which totals 45,"
J07-4005,J05-4002,1,0.911399,"eatures, f , each with an associated frequency, where each feature is a pair r, x consisting of a grammatical relation name and the other word in the relation. We computed distributional similarity scores for every pair of words of the same PoS where each word’s total feature frequency was at least 10. A thesaurus entry of size k for a target word w is then defined as the k most similar words to w. A large number of distributional similarity measures have been proposed in the literature (see Weeds 2003 for a review) and comparing them is outside the scope of this work. However, the study of Weeds and Weir (2005) provides interesting insights into what makes a “good” distributional similarity measure in the contexts of semantic similarity prediction and language modeling. In particular, weighting features by pointwise mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise mutual information (I(w, f )) between a word and a feature is calculated as I(w, f ) = log P( f |w) P( f ) (3) Intuitively, this means that the occurrence of a less-common feature is more important in describing a word than a more-common feature. For example, the verb eat is more selective and tells us mor"
J07-4005,J90-1003,0,\N,Missing
J07-4005,C98-2122,0,\N,Missing
J13-3003,D10-1115,0,0.0611901,"Missing"
J13-3003,P08-2063,0,0.182386,"Missing"
J13-3003,burchardt-etal-2006-salsa,1,0.867888,"Missing"
J13-3003,2007.tmi-papers.6,0,0.0326144,"Missing"
J13-3003,D07-1007,0,0.0507655,"rin.erk@mail.utexas.edu, nlgaylord@utexas.edu. ∗∗ Visiting Scholar, Department of Theoretical and Applied Linguistics, University of Cambridge, Sidgwick Avenue, Cambridge, CB3 9DA, UK. E-mail: diana@dianamccarthy.co.uk. Submission received: 3 November 2011; revised version received: 30 April 2012; accepted for publication: 25 June 2012. doi:10.1162/COLI a 000142 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 3 1. Introduction Word sense disambiguation (WSD) is a task that has attracted much work in computational linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview), including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, M`arquez, and Wicentowski 2007; Erk and Strapparava 2010), which were originally organized expressly as a forum for shared tasks in WSD. In WSD, polysemy is typically modeled through a dictionary, where the senses of a word are understood to be mutually disjoint. The meaning of an occurrence of a word is then characterized through the best-ﬁtting among its dictionary senses. The assumption of senses that are mutua"
J13-3003,D09-1003,0,0.0173312,"Missing"
J13-3003,D10-1113,0,0.101169,"Missing"
J13-3003,D09-1046,1,0.928864,"Missing"
J13-3003,P09-1002,1,0.734848,"Missing"
J13-3003,D08-1094,1,0.895479,"Mervis 1975; Hampton 2007). This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies? Recently, there have been several proposals for modeling word meaning in context that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word. The SemEval Lexical Substitution task (McCarthy and Navigli 2009) represents each occurrence through multiple weighted paraphrases. Other approaches represent meaning in context through a vector ¨ space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Furstenau, and Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again, this raises the question of annotation: Can human annotators give ﬁne-grained judgments about degrees of similarity between word occurrences, like these computational models predict? The question that we explore in this paper is: Can word meaning be described through annotation in the form of graded judgments? We want to know whether annotators can provide graded meaning annotation in a consistent fashion. Also, we want to know whether annotators will use"
J13-3003,P10-2017,1,0.917282,"Missing"
J13-3003,D11-1129,0,0.022525,"Missing"
J13-3003,N06-2015,0,0.0241474,"Missing"
J13-3003,W09-2413,0,0.0273464,"Missing"
J13-3003,W02-0816,1,0.776181,"Missing"
J13-3003,S07-1009,1,0.89977,"Missing"
J13-3003,W03-2408,0,0.0867752,"Missing"
J13-3003,H93-1061,0,0.77528,"Missing"
J13-3003,P08-1028,0,0.0717742,"on 2007). This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies? Recently, there have been several proposals for modeling word meaning in context that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word. The SemEval Lexical Substitution task (McCarthy and Navigli 2009) represents each occurrence through multiple weighted paraphrases. Other approaches represent meaning in context through a vector ¨ space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Furstenau, and Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again, this raises the question of annotation: Can human annotators give ﬁne-grained judgments about degrees of similarity between word occurrences, like these computational models predict? The question that we explore in this paper is: Can word meaning be described through annotation in the form of graded judgments? We want to know whether annotators can provide graded meaning annotation in a consistent fashion. Also, we want to know whether annotators will use the whole graded scale, or"
J13-3003,S07-1006,0,0.0333095,"Missing"
J13-3003,passonneau-etal-2010-word,0,0.0761967,"Missing"
J13-3003,S07-1016,0,0.0862518,"Missing"
J13-3003,J91-4003,0,0.514636,"Missing"
J13-3003,I11-1079,1,0.911679,"Missing"
J13-3003,N10-1013,0,0.059551,"Missing"
J13-3003,J98-1004,0,0.229525,"Missing"
J13-3003,W04-0811,0,0.0600943,"Missing"
J13-3003,H05-1051,0,0.071159,"Missing"
J13-3003,P10-1097,0,0.105721,"Missing"
J13-3003,D11-1094,0,0.0385112,"Missing"
J13-3003,W10-2807,0,0.0433242,"Missing"
J13-3003,P10-4014,0,0.016529,"Missing"
J13-3003,D08-1105,0,0.0469699,"Missing"
J13-3003,W04-0807,0,\N,Missing
J13-3003,W09-2412,1,\N,Missing
J13-3003,S10-1002,1,\N,Missing
J16-2003,apidianaki-2008-translation,1,0.788727,"different, but somewhat related. We want to know to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built f"
J16-2003,E09-1010,1,0.794351,"d for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will move to automatic"
J16-2003,apidianaki-etal-2014-semantic,1,0.867919,"Missing"
J16-2003,D09-1056,0,0.0820066,"Missing"
J16-2003,P05-1074,0,0.0289879,"ses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will"
J16-2003,N12-1095,0,0.0584039,"Missing"
J16-2003,D07-1007,0,0.030657,"Missing"
J16-2003,J13-3008,0,0.0609858,"Missing"
J16-2003,eom-etal-2012-using,0,0.0612982,"Missing"
J16-2003,P09-1002,1,0.923919,"Missing"
J16-2003,J13-3003,1,0.892716,"Missing"
J16-2003,N06-2015,0,0.0425995,"Missing"
J16-2003,W02-0808,0,0.182124,"Missing"
J16-2003,S13-2049,0,0.0416348,"Missing"
J16-2003,W09-2413,0,0.0378648,"Missing"
J16-2003,S10-1011,0,0.0219161,"pted for publication: 25 January 2016. doi:10.1162/COLI a 00247 © 2016 Association for Computational Linguistics Computational Linguistics Volume 42, Number 2 1. Introduction In computational linguistics, the field of word sense disambiguation (WSD)—where a computer selects the appropriate sense from an inventory for a word in a given context—has received considerable attention.1 Initially, most work focused on manually constructed inventories such as WordNet (Fellbaum 1998) but there has subsequently been a great deal of work on the related field of word sense induction (WSI) (Pedersen 2006; Manandhar et al. 2010; Jurgens and Klapaftis 2013) prior to disambiguation. This article concerns the phenomenon of word meaning and current practice in the fields of WSD and WSI . Computational approaches to determining word meaning in context have traditionally relied on a fixed sense inventory produced by humans or by a WSI system that groups token instances into hard clusters. Either sense inventory can then be applied to tag sentences on the premise that there will be one best-fitting sense for each token instance. However, word meanings do not always take the form of discrete senses but vary on a continuum b"
J16-2003,S07-1009,1,0.800354,"Missing"
J16-2003,S10-1002,1,0.886946,"Missing"
J16-2003,P08-1028,0,0.0875116,"Missing"
J16-2003,palmer-etal-2000-semantic,0,0.22024,"Missing"
J16-2003,passonneau-etal-2010-word,0,0.168918,"s (Tuggy 1993). For example, the noun crane is a clear-cut case of ambiguity between lifting device and bird, whereas the exact meaning of the noun thing can only be retrieved via the context of use rather than via a representation in the mental lexicon of speakers. Cases of polysemy such as the verb paint, which can mean painting a picture, decorating a room, or painting a mural on a house, lie somewhere between these two poles. Tuggy highlights the fact that boundaries between these different categories are blurred. Although specific context clearly plays a role (Copestake and Briscoe 1995; Passonneau et al. 2010) some lemmas are inherently much harder to partition than others (Kilgarriff 1998; Cruse 2000). There are recent attempts to address some of these issues by using alternative characterizations of word meaning that do not involve creating a partition of usages into senses (McCarthy and Navigli 2009; Erk, McCarthy, and Gaylord 2013), and by asking WSI systems to produce soft or graded clusterings (Jurgens and Klapaftis 2013) where tokens can belong to a mixture of the clusters. However, these approaches do not overtly consider the location of a lemma on the continuum, but doing so should help in"
J16-2003,W97-0213,0,0.220678,"ssible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefeve"
J16-2003,D07-1043,0,0.159065,"Missing"
J16-2003,J06-2001,0,0.0881139,"Missing"
J16-2003,D09-1067,0,0.028907,"Missing"
J16-2003,S07-1044,0,0.0306505,"now to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent w"
J16-2003,W09-2419,0,\N,Missing
J16-2003,passonneau-etal-2012-masc,0,\N,Missing
K19-1004,N19-1391,0,0.318028,"ly context-aware dataset for evaluating cross-lingual embeddings on the word level is Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018). It challenges a system to predict similarity scores between cross-lingual word pairs with sentential context provided in both languages. However, BCWS does not explicitly test for the retrieval of meaning-equivalent cross-lingual contextualized embeddings, which is explicitly tested in our test. Also, BCWS is only available for one language pair: English-Chinese. Another task used for evaluating contextualized embeddings is Sentence Retrieval (Aldarmaki and Diab, 2019): given a query source sentence, the task is to retrieve the corresponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an or"
K19-1004,D18-1027,0,0.0469943,"e also experimented with directly aligning embeddings obtained from the BERT multilingual model, which is a joint model trained with the same model parameters with shared subword vocabulary (Devlin et al., 2019). This means that identical words in two different languages will obtain the same embeddings. W = arg min kW S − T k2 s.t. W T W = I. (1) W The closed-form solution can be found by solving the orthogonal Procrustes problem (Sch¨onemann, 1966) as follows: T S T = U ΣV T ; W = U V T (2) We also optionally apply a post-processing Meeting-in-the-Middle (MIM) technique, recently proposed by Doval et al. (2018). It first calculates the average of each dictionary item representation in a pair after the orthogonal mapping: we denote the matrix U as the matrix where each column is such an average vector. Then, it finds a linear mapping M from both the source language (denoted as Ms ) and the target language (Mt ) after the previous step of orthogonal mapping to minimize the distance to U via a closed-form solution. Equation (3) formulates how to find Ms , and we do the same from target to source. Ms = arg min kMs W S − U k2 (3) Ms We apply the orthogonal mapping and MIM both on static embeddings (for b"
K19-1004,D16-1250,0,0.220628,"ponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an orthogonal mapping between pretrained embeddings (Xing et al., 2015; Artetxe et al., 2016). The orthogonality of the mapping is crucial as it preserves monolingual invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned w"
K19-1004,N13-1073,0,0.110811,"Missing"
K19-1004,Q17-1010,0,0.460088,"n a dictionary with item pairs from source and target languages (si , ti ), and matrices S and T that contain the vector representations corresponding to the item pairs in the columns, we follow the standard practice (Glavaˇs et al., 2019) to find an orthogonal alignment matrix W that minimizes the distance between the transformed matrix W S and T . For improved performance, following Artetxe et al. (2016), we normalize and mean center the embeddings in S and T . The mapping is as follows: Methods Monolingual Contextualized Embeddings Compared to static word embeddings (Mikolov et al., 2013b; Bojanowski et al., 2017), more recent contextualized embeddings provide dynamic representations for a word in context as hidden layers in a deep neural network. They are typically obtained by unsupervised pretraining based on language modeling objectives (Devlin et al., 2019; Yang et al., 2019). The underlying contextualized method in our study is the pretrained BERTbase cased model1 (Devlin et al., 2019). BERT is trained using a transformer architecture (Vaswani et al., 2017) with masked language modelling (MLM) and next sentence prediction (NSP) tasks. MLM predicts the vocabulary id of a randomly masked word in a s"
K19-1004,P19-1070,1,0.84668,"Missing"
K19-1004,P13-1133,0,0.0433986,"e pair are aligned. Therefore, the final contexts for the source and target word in the word pair are indeed non-parallel. The use of non-parallel contexts here is crucial because when we perform the token retrieval task, parallel contexts can be superficially retrieved by simply matching the contexts rather than repreCollecting Translation Pairs. We select a representative set of query words from WordNet (Miller, 1998) (one unique word per WordNet synset). For each source word, we retrieve its WordNet senses and the corresponding translations in the target language from Multilingual WordNet (Bond and Foster, 2013). As WordNet senses are too fine-grained, we collapse senses into clusters if they contain the same translation for the source word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word emb"
K19-1004,P12-1092,0,0.177851,"Missing"
K19-1004,S17-2002,0,0.031663,"uation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings is Bilingual Dictionary Induction (BDI) (Vuli´c and Moens, 2013; Mikolov et al., 2013a; Gouws et al., 2015): given a source query word, the task is to retrieve the translation word in the target language. The test words in BDI are out-of-context and polysemy cannot be addressed properly. The same issue is found in another relevant lexical task, Cross-lingual Semantic Similarity. (Camacho-Collados et al., 2017). (1) Cross-lingual Word Sense Disambigution: source query: the national [coach] of the Irish teams ... answer: allenatore (Italian); Fußbaltrainer; Nationaltrainer; Trainer (German); entrenador(Spanish) ... (2) Cross-lingual Lexical Substitution : source query: She looked as [severely] as she could muster at Draco. answer: rigurosamente, seriamente (3) BTSR: source query: The reflections included in this document are linked to discussions with many colleagues and friends, in the present [tense]. 34 3.2 answer: Scott Peterson meti´o la pata elfondo y us´o el [tiempo] pasado mientras afirmaba q"
K19-1004,W09-2413,0,0.043201,"trieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring"
K19-1004,K18-2005,0,0.0169691,"to serve as anchors for the alignment using static dictionaries, or we use parallel sentences as dictionary items to directly align contextualized word representations on the token level. We discuss this in what follows. 1 To produce the contextualized representation for a word in context, we average the 12 hidden layers of the word’s subword representations in BERT and then average the subword representations as input for the cross-lingual alignment. We leave other ways to extract the representations for future work. 2 We have also experimented with ELMo in lieu of BERT (Peters et al., 2018; Che et al., 2018). However, as we reach similar conclusions in terms of relative performance, while BERT-based cross-lingual embeddings outperform their ELMo-based counterparts in absolute terms, we do not report ELMo’s results for brevity. It should be noted that these pretrained models used different training data. 3.3 Alignment Levels We explore aligning contextualized models on two levels: type-level and token-level. Type-level word representation refers to static word representation that assigns one fixed embedding to a word. All the traditional word embedding models (e.g., skipgram, CBOW, fastText) provi"
K19-1004,D18-1025,0,0.354342,"R task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary. 1 We evaluate the methods on a variety of contextaware tasks. Besides two previously established evaluation tasks (1) Bilingual Contextual Word Similarity (Chi and Chen, 2018) and (2) Sentence Retrieval (Conneau et al., 2017), we introduce a new task: Bilingual Token-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have b"
K19-1004,N19-1386,0,0.0335393,"al invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextu"
K19-1004,N19-1423,0,0.296476,"eval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur mai"
K19-1004,N19-1392,0,0.0656437,"Missing"
K19-1004,D14-1113,0,0.123895,"Missing"
K19-1004,D19-1449,1,0.859792,"Missing"
K19-1004,N13-1011,1,0.878435,"Missing"
K19-1004,N18-1202,0,0.319663,"ken-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) alig"
K19-1004,N15-1104,0,0.0932394,"Missing"
K19-1004,P19-1493,0,0.0378202,"gual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur main findings are as follows. (1) Using the average of the contextualized word representations as"
K19-1004,N19-1162,0,0.117939,"ddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source lan"
K19-1004,W09-2412,1,0.746156,"se two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring full sense disambiguation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings"
K19-1004,tian-etal-2014-um,0,0.0293443,"urce word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word embeddings in a variant of our task which relies on parallel contexts. We set aside 1M parallel sentences from the UMCorpus (Tian et al., 2014) (EN–ZH) and the WMT13 news dataset (Bojar et al., 2013) (EN–ES) for extracting the sentence contexts. We end up with 14,604 distinct word pairs with contexts extracted for EN–ZH, and 9,623 pairs for EN–ES. target candidate. We experiment with 20k target candidates and 200k target candidates. 5 Experiments Training Setup. To test the effects of corpora size on the induction of the cross-lingual alignment, we vary the size of the parallel corpus from 100 up to 200k parallel sentences in the UMCorpus and the WMT13 corpus. Word alignment was produced by IBM Model 2 using Fastalign (Dyer et al., 2"
L18-1153,W15-0102,0,0.0258175,"ameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such"
L18-1153,P98-1013,0,0.0943859,"antic clustering, multilingual NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). Howev"
L18-1153,P13-1133,0,0.0212106,"re intentionally restricted, so as to avoid imposing any preconceived semantic categories or classification structure onto the annotators and elicit possibly spontaneous similarity judgments, such discrepancies in detecting ambiguity are inevitable. In order to have more control over which sense of a given verb is taken into consideration in the clustering task, word senses rather than word forms would have to be provided at the start of the task. Such a set-up would also allow comparison of the elicited classes with the existing multilingual sense inventories, like Open Multilingual WordNet (Bond and Foster, 2013) or BabelNet (Navigli and Ponzetto, 2012). Since the aim of the present study was to elicit judgments on basic word forms, without any guidance as to the different word senses available, such comparisons are beyond the scope of this study; however, in future work we intend to extend this analysis and compare our findings against the resources available. 5. Conclusion We have presented the first cross-lingual analysis and evaluation of semantic clustering of verbs by non-expert human annotators. The inter-annotator agreement scores reported for English, Polish, and Croatian are encouraging and"
L18-1153,P12-1090,0,0.0217686,"al verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to th"
L18-1153,D16-1235,1,0.9085,"Missing"
L18-1153,C94-1042,0,0.65147,"c lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and sy"
L18-1153,J15-4004,1,0.623982,"ints of view, for example, lend and borrow, which differ along only one dimension of meaning, that is, the direction of the action (the object of the verb either travels away from the participant (A lends something to B) or towards the participant (B borrows something from A)), and are essentially identical with regard to all other features, which makes them appear semantically close. 4.2. Semantic Similarity versus Relatedness The importance of distinguishing between the concepts of semantic similarity (e.g. cup and mug) and relatedness (e.g. coffee and cup) has been noted in the literature (Hill et al., 2015), and the analysis of our data provides more evidence illustrating the influence of loose association on how humans conceptualize similarity between words, and the difficulty of keeping similarity and relatedness apart. In all three languages we can observe instances of what can be described as a ‘storyline approach’ to judging semantic similarity and verb classification. This is particularly noticeable in Croatian classifications, where several classes formed by the annotators group verbs describing quite different actions, linked via loose thematic ties: (1) marry, conquer, approach, move, w"
L18-1153,S13-2049,0,0.415023,"lusters, and 1 We used the Fuzzy B-Cubed implementation of Jurgens and Klapaftis (2013) but did not associate the clusters with weights, and therefore the metric is equivalent to that of Amig´o et al. (2009). 953 Average B-Cubed English Polish Croatian All 0.262 0.338 0.172 0.205 1c1inst All-instances, One class 0.0 0.069 Table 3: The average B-Cubed F-score (i.e. harmonic mean of B-Cubed precision and recall) calculated for all possible pairings of annotators, for each language individually and across the three languages, and for two SemEval baselines: 1c1inst and All-instances, One class by Jurgens and Klapaftis (2013) to fuzzy clusters, used to evaluate the performance of Word Sense Induction systems in SemEval tasks (Jurgens and Klapaftis, 2013). The B-Cubed metrics (B-Cubed precision and recall) compare two clusterings (say, X and Y) at the item level: for an item i, precision measures how many items sharing a cluster with i in clustering X are placed in its cluster in clustering Y; whereas B-Cubed recall measures how many items sharing a cluster with i in Y are also placed in its cluster in X, with the final B-Cubed score equivalent to the harmonic mean of the two values. In our task, rather than compar"
L18-1153,P14-1097,0,0.0151587,"urrently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge"
L18-1153,korhonen-etal-2006-large,1,0.648455,"for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challeng"
L18-1153,J05-1004,0,0.143765,"NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resour"
L18-1153,S16-2012,0,0.0165565,"s NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge, is the first evaluation of this approach. D"
L18-1153,W11-2112,0,0.0252591,"ss architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the"
L18-1153,D12-1048,0,0.0248176,"tic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trai"
L18-1153,C10-1119,1,0.80849,"the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instruc"
L18-1153,D17-1270,1,0.883378,"Missing"
L18-1153,W11-0110,0,0.0318037,", prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate t"
mccarthy-2008-lexical,H05-1113,0,\N,Missing
mccarthy-2008-lexical,W03-1810,1,\N,Missing
mccarthy-2008-lexical,S07-1036,0,\N,Missing
mccarthy-2008-lexical,W03-1807,0,\N,Missing
mccarthy-2008-lexical,S07-1009,1,\N,Missing
mccarthy-2008-lexical,W03-1809,0,\N,Missing
mccarthy-2008-lexical,E06-1043,0,\N,Missing
N09-2059,W04-3204,0,0.323015,"Missing"
N09-2059,briscoe-carroll-2002-robust,1,0.778211,". (2004) provides a prevalence ranking score to produce a MFS heuristic. We make a slight modification to McCarthy et al.’s prevalence score and use it to estimate the probability distribution over the senses of a word. We use the same resources as McCarthy et al. (2004): a distributional similarity thesaurus and a WordNet semantic similarity measure. The thesaurus was produced using the metric described by Lin (1998) with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992) using the RASP parser (Briscoe and Carroll, 2002). The thesaurus consists of entries for each word (w) with the top 50 “nearest neighbours” to w, where the neighbours are words ranked by the distributional similarity that Proceedings of NAACL HLT 2009: Short Papers, pages 233–236, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics they share with w. The WordNet similarity score is obtained with the jcn measure (Jiang and Conrath, 1997) using the WordNet Similarity Package 0.05 (Patwardhan and Pedersen, 2003) and WordNet version 1.6. The jcn measure needs word frequency information, which we obtained from the BNC."
N09-2059,P00-1064,0,0.0877167,"Missing"
N09-2059,O97-1002,0,0.0370294,") with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992) using the RASP parser (Briscoe and Carroll, 2002). The thesaurus consists of entries for each word (w) with the top 50 “nearest neighbours” to w, where the neighbours are words ranked by the distributional similarity that Proceedings of NAACL HLT 2009: Short Papers, pages 233–236, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics they share with w. The WordNet similarity score is obtained with the jcn measure (Jiang and Conrath, 1997) using the WordNet Similarity Package 0.05 (Patwardhan and Pedersen, 2003) and WordNet version 1.6. The jcn measure needs word frequency information, which we obtained from the BNC. 2.1 Estimates of Predominance, Probability and Entropy Following McCarthy et al. (2004), we calculate prevalence of each sense of the word (w) using a weighted sum of the distributional similarity scores of the top 50 neighbours of w. The sense of w that has the highest value is the automatically detected MFS (predominant sense). The weights are determined by the WordNet similarity between the sense in question and"
N09-2059,N07-1044,0,0.277898,"ic estimation. As we can see, while precision is higher for lower polysemy, the automatic estimate of entropy can provide a greater increase in precision than polysemy, and frequency does not seem to be strongly correlated with precision. 3.2 precision S ENSEVAL-2 English All Words Dataset The SE 2- EAW task provides a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II (Palmer et al., 2001). Again, we examine whether precision of the MFS 235 4 Related Work There is promising related work on determining the predominant sense for a MFS heuristic (Lapata and Keller, 2007; Mohammad and Hirst, 2006) but our work is the first to use the ranking score to estimate entropy and apply it to determine the confidence in the MFS heuristic. It is likely that these methods would also have increased precision if the ranking scores were used to estimate entropy. We leave such investigations for further work. Chan and Ng (2005) estimate word sense distributions and demonstrate that sense distribution estimation improves a supervised WSD classifier. They use three sense distribution methods, including that of McCarthy et al. (2004). While the other two methods outperform the"
N09-2059,P98-2127,0,0.0508317,"1 It is also referred to as the first sense heuristic in the literature and in this paper. WSD 233 Method Given a listing of senses from an inventory, the method proposed by McCarthy et al. (2004) provides a prevalence ranking score to produce a MFS heuristic. We make a slight modification to McCarthy et al.’s prevalence score and use it to estimate the probability distribution over the senses of a word. We use the same resources as McCarthy et al. (2004): a distributional similarity thesaurus and a WordNet semantic similarity measure. The thesaurus was produced using the metric described by Lin (1998) with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992) using the RASP parser (Briscoe and Carroll, 2002). The thesaurus consists of entries for each word (w) with the top 50 “nearest neighbours” to w, where the neighbours are words ranked by the distributional similarity that Proceedings of NAACL HLT 2009: Short Papers, pages 233–236, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics they share with w. The WordNet similarity score is obtained with the jcn measure (J"
N09-2059,P04-1036,1,0.92448,"Missing"
N09-2059,E06-1016,0,0.229024,"see, while precision is higher for lower polysemy, the automatic estimate of entropy can provide a greater increase in precision than polysemy, and frequency does not seem to be strongly correlated with precision. 3.2 precision S ENSEVAL-2 English All Words Dataset The SE 2- EAW task provides a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II (Palmer et al., 2001). Again, we examine whether precision of the MFS 235 4 Related Work There is promising related work on determining the predominant sense for a MFS heuristic (Lapata and Keller, 2007; Mohammad and Hirst, 2006) but our work is the first to use the ranking score to estimate entropy and apply it to determine the confidence in the MFS heuristic. It is likely that these methods would also have increased precision if the ranking scores were used to estimate entropy. We leave such investigations for further work. Chan and Ng (2005) estimate word sense distributions and demonstrate that sense distribution estimation improves a supervised WSD classifier. They use three sense distribution methods, including that of McCarthy et al. (2004). While the other two methods outperform the McCarthy et al. method, 3 W"
N09-2059,S01-1005,0,0.039217,"ll due to frequency or polysemy. This is important, since both frequency and polysemy level (assuming a predefined sense inventory) could be obtained without the need for automatic estimation. As we can see, while precision is higher for lower polysemy, the automatic estimate of entropy can provide a greater increase in precision than polysemy, and frequency does not seem to be strongly correlated with precision. 3.2 precision S ENSEVAL-2 English All Words Dataset The SE 2- EAW task provides a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II (Palmer et al., 2001). Again, we examine whether precision of the MFS 235 4 Related Work There is promising related work on determining the predominant sense for a MFS heuristic (Lapata and Keller, 2007; Mohammad and Hirst, 2006) but our work is the first to use the ranking score to estimate entropy and apply it to determine the confidence in the MFS heuristic. It is likely that these methods would also have increased precision if the ranking scores were used to estimate entropy. We leave such investigations for further work. Chan and Ng (2005) estimate word sense distributions and demonstrate that sense distribut"
N09-2059,C98-2122,0,\N,Missing
P04-1036,S01-1020,0,\N,Missing
P04-1036,W04-0837,1,\N,Missing
P04-1036,S01-1005,0,\N,Missing
P04-1036,S01-1027,0,\N,Missing
P04-1036,rose-etal-2002-reuters,0,\N,Missing
P04-1036,W97-0808,1,\N,Missing
P04-1036,C04-1146,1,\N,Missing
P04-1036,W03-1022,0,\N,Missing
P04-1036,P00-1064,0,\N,Missing
P04-1036,H93-1061,0,\N,Missing
P04-1036,W02-0907,0,\N,Missing
P04-1036,O97-1002,0,\N,Missing
P04-1036,briscoe-carroll-2002-robust,1,\N,Missing
P04-1036,J04-1003,0,\N,Missing
P04-1036,W01-0715,0,\N,Missing
P04-1036,P98-2127,0,\N,Missing
P04-1036,C98-2122,0,\N,Missing
P04-1036,magnini-cavaglia-2000-integrating,0,\N,Missing
P09-1002,S07-1002,0,0.0639388,"dictionary senses or similarity between uses. In that case, it would be useful to have more extensive datasets with graded annotation, even though this annotation paradigm is more time consuming and thus more expensive than traditional word sense annotation. As a next step, we will automatically cluster the judgments we obtained in the WSsim and Usim experiments to further explore the degree to which the annotation gives rise to sense grouping. We will also use the ratings in both experiments to evaluate automatically induced models of word meaning. The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from OntoNotes. We hope that the Usim dataset will be particularly useful for evaluating methods which relate usages without necessarily producing hard clusters. Also, we will extend the current dataset using more annotators and exploring additional lexicon resources. Acknowledgments. We acknowledge support from the UK Royal Society for a Dorothy Hodkin Fellowship to the second author. We thank Sebastian Pado for many helpful discussions, and Andrew Young for help with the interface"
P09-1002,S07-1009,1,0.878001,"data taken from SemCor and SE-3 so that we can compare the annotations. Although we use WordNet for the annotation, our study is not a study of WordNet per se. We choose WordNet because it is sufficiently fine-grained to examine subtle differences in usage, and because traditionally annotated datasets exist to which we can compare our results. Predefined dictionaries and lexical resources are not the only possibilities for annotating lexical items with meaning. In cross-lingual settings, the actual translations of a word can be taken as the sense labels (Resnik and Yarowsky, 2000). Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. It uses paraphrases for words in context as a way of annotating meaning. The task was proposed following a background of discussions in the WSD community as to the adequacy of predefined word senses. The LEXSUB dataset comprises open class words (nouns, verbs, adjectives and adverbs) with token instances of each word appearing in the context of one sentence taken from the English Internet Corpus (Sharoff, 2006). The methodology can only work where there are paraphrases, so the"
P09-1002,W04-0807,0,0.0587557,"Missing"
P09-1002,D08-1094,1,0.107902,"the full spectrum of ratings, as shown in Figures 1 and 4. This may be because of a graded perception of the similarity of uses as well as senses, or because some uses and senses are very similar. Table 4 shows that for a large number of WSsim items, multiple senses that were not significantly positively correlated got high ratings. This seems to indicate that the ratings we obtained cannot simply be explained by more coarse-grained senses. It may hence be reasonable to pursue computational models of word meaning that are graded, maybe even models that do not rely on dictionary senses at all (Erk and Pado, 2008). Comparison to previous word sense annotation. Our graded WSsim annotations do correlate with traditional “best fitting sense” annotations from SemCor and SE-3; however, if annotators perceive similarity between uses and senses as graded, traditional word sense annotation runs the risk of introducing bias into the annotation. Comparison to lexical substitutions. There is a strong correlation between both Usim and WSsim and the overlap in paraphrases that annotators generated for LEXSUB. This is very encouraging, and especially interesting because LEXSUB annotators freely generated paraphrases"
P09-1002,S07-1006,0,0.142338,"Missing"
P09-1002,N06-2015,0,0.147668,"Missing"
P09-1002,H05-1051,0,0.0559669,"Missing"
P09-1002,J06-4002,0,\N,Missing
P13-2129,P03-1009,1,0.893953,"Missing"
P13-2129,W02-1016,0,0.490118,"Missing"
P13-2129,A00-2034,1,0.911844,"argely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson ("
P13-2129,P06-4020,0,0.011323,"(j)) sup{p(a|v)} = 1 XX Z= min(fv (m), fv (n)) m 1 n A relaxation is used in mathematical optimization for relaxing the strict requirement, by either substituting it with an easier requirement or dropping it completely. 737 Frame pair NP+PPon NP+PPwith NP+PPon PPwith NP+PPon PPon NP+PPwith PPwith NP+PPwith PPon PPwith PPon NP+PPon NP+PPon NP+PPwith NP+PPwith PPwith PPwith PPon PPon Possible DA Locative Causative(with) Causative(on) ? ? ? - Frequency 30 0 30 0 30 0 40 30 0 30 occurrence of a verb as a member of one of the 168 SCF s on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. Table 2: Example frame pair features for spray So we end up with a simple form: p(fv (i), fv (j)|v) ≈ Z −1 · min(fv (i), fv (j))"
P13-2129,J01-3003,0,0.300833,"chulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from"
P13-2129,J02-3001,0,0.0664675,"1993)’s seminal book provides a manual inventory both of DAs and verb classes where membership is determined according to participation in these alternations. For example, most of the C OOK verbs (e.g. bake, cook, fry . . . ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), 736 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Frame NP+PPon NP+PPwith PPwith PPon Example sentence Jessica sprayed paint on the wall Jessica sprayed the wall with paint *The wall sprayed with paint Jessica sprayed paint on the wall Freq 40 30 0 30 decomposed as: p(fv (i), fv (j)|v)0 , p(fv (i)|v) · p(fv (j)|v) (1) We assume that SCFs are dependent as they are generated by the underlying meaning components (Levin and Hovav, 2006). The frame dependenc"
P13-2129,D11-1025,1,0.920283,"d Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from childdirected speech parsed with a dependency parser to model"
P13-2129,J06-2001,0,0.371871,"ory University of Cambridge Cambridge, UK diana@dianamccarthy.co.uk alk23@cam.ac.uk Abstract predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson ("
P13-2129,D09-1067,1,0.928296,"(Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the ov"
P13-2129,D11-1095,1,0.848268,"y of Cambridge Cambridge, UK diana@dianamccarthy.co.uk alk23@cam.ac.uk Abstract predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (20"
P13-2129,W04-2605,0,0.563711,"tic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchica"
P13-2129,W09-0210,1,0.907059,"Missing"
P13-2129,C10-1113,1,\N,Missing
P13-2129,P98-2247,1,\N,Missing
P13-2129,C98-2242,1,\N,Missing
P13-2129,P99-1051,0,\N,Missing
P13-2129,P07-1115,1,\N,Missing
P14-1025,P13-1141,0,0.0173254,"ve of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W04-3204,0,0.195409,"Missing"
P14-1025,S07-1002,0,0.0143677,"nto a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in"
P14-1025,P06-1012,0,0.0155071,"Missing"
P14-1025,E09-1005,0,0.0121583,"he flexibility and robustness of our methodology. Future work could pursue a more sophisticated methodology, using non-linear combinations of sim(si , tj ) for computing the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. where f (tj ) is the frequency of topic tj in the corpus. The intuition behind novelty is that a target lemma with a novel sense should have a (somewhat-)frequent topic that has low association with any sense. That we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense. For each of our t"
P14-1025,cook-stevenson-2010-automatically,1,0.840918,"Missing"
P14-1025,I13-1041,1,0.812251,"one dataset), but HDP-WSI is better at inducing the overall sense distribution. It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained"
P14-1025,N06-1017,0,0.0586004,"A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability dist"
P14-1025,S07-1060,0,0.466214,"Missing"
P14-1025,D07-1109,0,0.017322,"tems, The University of Melbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable se"
P14-1025,E14-4042,1,0.869137,"Missing"
P14-1025,E09-1013,0,0.0180149,"let and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata a"
P14-1025,W11-2508,0,0.125836,"Missing"
P14-1025,D07-1108,0,0.0240323,"k@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition"
P14-1025,S13-2039,1,0.835591,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,I08-1073,1,0.825054,"d in Table 3. HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution. Testing for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FINANCE is significant (p &lt; 0.05) but the results for the other two datasets are not (p &gt; 0.1 in each case). 8 McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9 The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dict"
P14-1025,O97-1002,0,0.0490006,"n for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date ("
P14-1025,P10-1116,0,0.0142022,"lbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further"
P14-1025,N09-2059,1,0.960189,"senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguat"
P14-1025,P98-2127,0,0.0433121,"ing predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automa"
P14-1025,P12-3005,1,0.65009,"ary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Twitter, and the POS tags provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as “Other” in instances where the usage was not captured by any of the Macmillan senses. After quality control over the annotators/annotations (see 5.1 Learning Sense Distributions As in Section 4, we evaluate in terms of WSD accuracy (Table 4) and JS divergence over the gold-standard sen"
P14-1025,S13-2049,0,0.0136682,"the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignm"
P14-1025,S10-1011,0,0.00982955,"ds, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution"
P14-1025,C04-1177,1,0.882843,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,H05-1053,1,0.623299,"r of usages assigned to topic tj ), and T is the number of topics. The intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s). 4 WordNet annotations. The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. For the remainder of the paper, we denote their system as MKWC. To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of Koeling et al. (2005). For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the WordNet senses (Equation (1)), and rank the senses based on the prevalence scores (Equation (2)). In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) AccUB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation);7 and (2) ERR, the error rate reduction between the accuracy for a given system (Acc) and the upper bound (AccUB ), calculated a"
P14-1025,P04-1036,1,0.84242,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,J04-1003,0,0.0384531,"ta, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to"
P14-1025,J07-4005,1,0.874451,"e learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy e"
P14-1025,E12-1060,1,0.881316,"ity of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method f"
P14-1025,H93-1061,0,0.648686,"Missing"
P14-1025,S13-2051,1,0.840589,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,E06-1016,0,0.357917,"Missing"
P14-1025,S13-2035,0,0.0261491,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W11-1102,0,0.0307236,"Missing"
P14-1025,P10-4014,0,0.10309,"Missing"
P14-1025,S07-1006,0,0.0131677,"(Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl ov"
P14-1025,W04-2807,0,0.0704841,"move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of T"
P14-1025,N13-1079,0,0.0120313,"eyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distri"
P14-1025,W09-0214,0,0.0845474,"Missing"
P14-1025,S07-1053,0,\N,Missing
P14-1025,N06-2015,0,\N,Missing
P14-1025,C98-2122,0,\N,Missing
P16-1143,W04-3204,0,0.117847,"Missing"
P16-1143,N15-1132,0,0.200608,"the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of s"
P16-1143,P11-2087,0,0.0263279,"Missing"
P16-1143,S07-1060,0,0.165989,"nt sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on"
P16-1143,D07-1109,0,0.185536,"o the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been ma"
P16-1143,P06-1013,0,0.127078,"heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been manually tagged with W"
P16-1143,W10-0701,0,0.0326124,"our English Wikipedia corpus. The remaining lemmas were then split into 5 groups of approximately equal size based on S EM C OR frequency. The S EM C OR frequencies contained in each group are summarised in Table 1. From each of the S EM C OR frequency groups, we randomly sampled 10 lemmas, giving 50 lemmas in total. Then for each lemma, we randomly sampled 100 usages to be annotated from English Wikipedia. This was done in the same way as the sampling of lemma usages for L EX S EM TM (see Section 6). We obtained crowdsourced sense annotations for each lemma using Amazon Mechanical Turk (AMT: Callison-Burch and Dredze (2010)). The sentences for each lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions we"
P16-1143,P06-1012,0,0.19513,", 2014), semi-automatic dictionary construction (Cook et al., 2013), lexical simplification (Biran et al., 2011), and textual entailment (Shnarch et al., 2011). Automatically acquired sense distributions themselves are also used to improve unsupervised WSD, for example by providing a most frequent sense heuristic (McCarthy et al., 2004b; Jin et al., 2009) or by improving unsupervised usage sampling strategies (Agirre and Martinez, 2004). Furthermore, the improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency"
P16-1143,C14-1035,0,0.0254343,"others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 20"
P16-1143,D14-1110,0,0.186671,"e improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 wo"
P16-1143,C00-1027,0,0.202938,"Missing"
P16-1143,N13-1132,0,0.0174577,"ch lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions were created using MACE (Hovy et al., 2013), which is a generalpurpose tool for inferring item labels from multiannotator, multi-item tasks. It provides a Bayesian framework for modelling item annotations, modelling the individual biases of each annotator, and 17 We chose to restrict our scope in this evaluation to nouns because much of the prior work has also focussed on nouns, and these are the words we would expect others to care the most about disambiguating, since they are more often context bearing. Also, introducing other POS would require a greater quantity of expensive annotated data. 18 These were created manually, to be as c"
P16-1143,N09-2059,1,0.917589,"oyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan a"
P16-1143,H05-1053,1,0.832069,"ew/527 8 For simplicity we use HCA to refer to both the topic modelling algorithm implemented by Buntine and Mishra (2014) as well as the corresponding software suite, whereas elsewhere HCA often only refers to the software. 1516 HCA Topic Model Convergence 104 log2 (perplexity) Topic Model Training Time (s) Training Time for HDP-WSI vs. HCA-WSI 103 102 HDP-WSI HCA-WSI 0 20 40 Number of Lemma Usages (1000’s) 14 12 10 0 Figure 1: Comparison of the time taken to train the topic models of HDP-WSI and HCA-WSI for each lemma in the BNC dataset. For each method, one data point is plotted per lemma. Koeling et al. (2005),9 which was also used by Lau et al. (2014). This dataset consists of 40 English lemmas, and for each lemma it contains a set of usages of varying size from the BNC and a gold-standard sense distribution that was created by hand-annotating a subset of the usages with W ORD N ET 1.7 senses. Using this dataset, we can calculate the quality of a candidate sense distribution by calculating its Jensen Shannon divergence (JSD) with respect to the corresponding gold-standard distribution. JSD is a measure of dissimilarity between two probability distributions, so a lower JSD score means the distribut"
P16-1143,J04-1003,0,0.0262866,"iments is available via: https://github.com/awbennett/LexSemTm 2 Background and Related Work Given the difficulty and expense of obtaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar appro"
P16-1143,N07-1044,0,0.0165171,"lly similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense di"
P16-1143,P04-1036,1,0.835389,"posed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapt"
P16-1143,J07-4005,1,0.0500043,"btaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method"
P16-1143,H93-1061,0,0.858704,"Missing"
P16-1143,E06-1016,0,0.0192742,"roposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository co"
P16-1143,D14-1113,0,0.02152,"kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural n"
P16-1143,E12-1060,1,0.603835,"eed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan and Ng, 2005; Chan and Ng, 2006; Lau et al., 2014), while the others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Re"
P16-1143,S15-2058,0,0.0349866,"Missing"
P16-1143,P14-1025,1,0.218445,"f languages, which could be extremely computationally expensive. To make things worse, domain differences could require learning numerous distributions per word. Despite this, though, we would not want to make these techniques scalable at the expense of sense distribution quality. Therefore, we would like to understand the tradeoff between the accuracy and computation time of these techniques, and optimise this tradeoff. This could be particularly critical in applying them in an industrial setting. The current state-of-the-art technique for unsupervised sense distribution learning is HDP-WSI (Lau et al., 2014). In order to address the above concerns, we provide a series of investigations exploring how to best optimise HDP-WSI for largescale application. We then use our optimised technique to produce L EX S EM TM,1 a semantic and sense frequency dataset of unprecedented size, spanning the entire vocabulary of English. Finally, we use crowdsourced data to produce a new set of gold-standard sense distributions to accompany L EX S EM TM. We use these to investigate the quality of the sense frequency data in L EX S EM TM with respect to S EM C OR. 1 L EX S EM TM, as well as code for accessing L EX S EM"
P16-1143,P11-2098,0,0.0658532,"Missing"
P16-1143,N15-1074,0,0.0151679,"however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural networks (Larochelle and Murray, 2011; Cao et al., 2015), or incorporating distributional similarity of words (Xie et al., 2015). Of these approaches, we chose to experiment with that of Buntine and Mishra (2014) because a working implementation was readily available, it has previously shown very strong performance in terms of accuracy and speed, and it is similar to HDP and thus easy to incorporate into our work. 3 HDP-WSI Sense Learning HDP-WSI (Lau et al., 2014) is a state-of-theart unsupervised method for learning sense distributions, given a sense repository with per-sense glosses. It takes as input a collection of example usages of the target lemma2 and the glosses 2 Except where stated otherwise, a lemma usage i"
P98-2247,A97-1052,0,0.376514,"in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.I. 1 Introduction Diathesis alternations are regular variations in the syntactic expressions of verbal arguments, for example The boy broke the window ~-. The window broke. Levin&apos;s (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation information for classifying verbs. Some studies have recently recognised the potential for using diathesis alternations within automatic lexical acquisition (Ribas, 1995; Korhonen, 1997; Briscoe and Carroll, 1997). This paper shows how corpus data can be used to automatically detect which verbs undergo these alternations. Automatic acquisition avoids the costly overheads of a manual approach and allows for the fact that predicate behaviour varies between sublanguages, domains and across time. Subcategorization frames (SCFs) are acquired for each verb and 1This work was partially funded by CEC LE1 project &quot;SPARKLE&quot;. We also acknowledge support from UK EPSRC project &quot;PSET: Practical Simplification of English Text&quot;. 1493 a hand-crafted classification of diathesis alternations filters potential candidates"
S01-1029,W96-0209,1,0.870646,"Missing"
S01-1029,A94-1009,0,0.0297692,"essor and parser are used for identifying predicates and argument heads for application of the acquired selectional preferences for disambiguation. Anaphora resolution is used at run-time to make links between antecedents of nouns, where the antecedents or the predicates may be in subject or object relationships. Preprocessor and Parser The preprocessor consists of three modules applied in sequence: a tokeniser, a part-of-speech (PoS) tagger, and a lemmatiser. The tokeniser comprises a small set of manually-developed finite-state rules for identifying word and sentence boundaries. The tagger (Elworthy, 1994) uses a bigram HMM augmented with a statistical unknown word guesser. Vhen applied to the training data for selectional preference acquisition it produces the single highest-ranked tag for each word; at run-time it returns multiple tags whose associated forward-backward probabilities are incorporated into parse probabilities. The lemmatiser ( Minnen et al., 2001) reduces inflected verbs and nouns to their base forms. The parser uses a &apos;shallow&apos; unification-based grammar of English PoS tags, performs .disambiguation using a context-sensitive probabilistic model (Carroll and Briscoe, 1996), and"
S01-1029,C96-1021,0,0.0341865,"Missing"
S01-1029,J98-2002,0,0.0668346,"Missing"
S01-1029,W97-0209,0,0.0431338,"ses when they are not. However, this means we loose out in cases where preferences do not provide the necessary information and other complementary information would help. Another disadvantage of using selectional preferences alone for disambiguation is that the preferences only apply to the grammatical slots for which they have been acquired. In addition, selectional preferences only help disambiguation for slots where there is a strong enough tie between predicate and argument. In this work, we use subject and object relationships, since these appear to work better than other relationships (Resnik, 1997; McCarthy, 2001), and we use argument heads, rather than the entire argument phrase. Our basic system is restricted to using only selectional information, and no other source of disambiguating information. However, we ex119 that is returned from the disambiguation phase. For selectional preference acquisition we applied the analysis system to the 90 million words of the written portion of the British National Corpus (BNC); both in the acquisition phase and at run-time we extracted from the analyser output only subject-verb and verb-direct object dependencies2. Thus we did not use the SENSEVAL"
S07-1009,P06-1057,0,0.102043,"Missing"
S07-1009,P99-1004,0,0.00750796,"target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC frequency data. 4. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of all synsets of the target, ranked with the BNC frequency data. We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin (Lin, 1998) and αSD (Lee, 1999) 4 . We took the word with the largest similarity (or smallest distance for αSD and l1) for best and the top 10 for oot. For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word. 4 Systems 9 teams registered and 8 participated, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, U"
S07-1009,W02-0816,1,0.782675,"st researchers believe that it will ultimately prove useful for applications which need some degree of semantic interpretation, the jury is still out on this point. One problem is that WSD systems have been tested on fine-grained inventories, rendering the task harder than it need be for many applications (Ide and Wilks, 2006). Another significant problem is that there is no clear choice of inventory for any given task (other than the use of a parallel corpus for a specific language pair for a machine translation application). The lexical substitution task follows on from some previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by researchers on a task which has potential for NLP applications. Finding alternative words that can occur in given contexts would potentially be useThe task involves a lexical sample of nouns, verbs, adjectives and adverbs. Both annotators and systems select one or more substitutes for the target word in the context of a sentence. The data was selected from the English Internet Corpus of English produced by Sharoff (2006) from the Internet (http://corpus.leeds.ac.uk/internet.html). This is a balanced corpus similar in flavour to the BNC, thou"
S07-1009,H93-1061,0,0.832213,"multiword response from a majority of at least 2 annotators. Let mwi ∈ M W be the multiword identified by majority vote for item i. Let M W sys be the subset of T for which there is a multiword response from the system and mwsysi be a multiword specified by the system for item i. detection P = P if mwi exists at i |M W sys| mwsysi ∈M W sys 1 (9) detection R = P mwsysi ∈M W 1 if mwi exists at i |M W | (10) identif ication P = P if mwsysi = mwi |M W sys| mwsysi ∈M W sys 1 (11) identif ication R = P 3.1 mwsysi ∈M W 1 if mwsysi = mwi |M W | (12) Baselines We produced baselines using WordNet 2.1 (Miller et al., 1993a) and a number of distributional similarity measures. For the WordNet best baseline we found the best ranked synonym using the criteria 1 to 4 below in order. For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1. Synonyms from the first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC freque"
S07-1068,H05-1053,1,0.801689,"asible to produce large manually sense-annotated corpora for every domain of interest. Since the method described in section 2 works with raw text, we can specialize our sense rankings for a particular topic domain, simply by feeding a domain specific corpus to the algorithm. Previous experiments have shown that unsupervised estimation of the predominant sense of certain words using corpora whose domain has been determined by hand outperforms estimates based on domainindependent text for a subset of words and even outperforms the estimates based on counting occurrences in an annotated corpus (Koeling et al., 2005). A later experiment (using S ENSEVAL2 and 3 data) showed that using domain specific predominant senses can slightly improve the results for some domains (Koeling et al., 2007). However, a firm idea of when domain specilisation should be considered could not (yet) be given. 4.1 Creating the Domain Corpora In order to estimate topic domain specific sense rankings, we need to specify what we consider ’domains’ and we need to collect corpora of texts for 315 these domains. We decided to use text classification for determining the topic domain and adopted the domain hierarchy as defined for the to"
S07-1068,briscoe-carroll-2002-robust,0,0.0557572,"’story’. We are using all the data. The five documents were fed to the classifier. The results are given in table 1. Unfortunately, only one document (d004) was considered to be a clear-cut example of a particular domain by the classifier (i.e. a high score is given to the first class and a much lower score to the following classes). 4.2 Domain rankings We created domain corpora by feeding the GigaWord documents to the classifier and adding each document to the domain corpus corresponding to the classifier’s first choice. The five corpora we needed for these documents were parsed using RASP (Briscoe and Carroll, 2002) and the resulting grammatical relations were used to create a distributional similarity thesaurus, which in turn was used for computing the predominant senses (see section 2). The only pre-processing we performed was stripping the XML codes from the documents. No other filtering was undertaken. This resulted in five sets of sense inventories with domain-dependent sense rankings. Each of them has a slightly different set of words. The words they have in common do have the same senses, but not necessarily the same estimated most frequently used sense. 5 Results from Semeval Coarse Disambiguatio"
S07-1068,O97-1002,0,0.00822915,"ibutional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable results for (McCarthy et al., 2004) and it is efficient at run time given precompilation of frequency information. The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier relations. 314 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314–317, c Prague, June 2007. 2007 Association for Computational Linguistics 3 Coarse Sense Inventory Adaptation We c"
S07-1068,P98-2127,0,0.0174099,"domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system. 2 Predominant Sense Acquisition We use the method described in (McCarthy et al., 2004) for finding predominant senses from raw text. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like (McCarthy et al., 2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable"
S07-1068,P04-1036,1,0.842291,"ortunate errors. (Magnini et al., 2002) have shown that information about the domain of a document is very useful for WSD. This is because many concepts are specific to particular domains, and for many words their most likely meaning in context is strongly correlated to the domain of the document they appear in. Thus, since word sense distributions are skewed and depend on the domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system. 2 Predominant Sense Acquisition We use the method described in (McCarthy et al., 2004) for finding predominant senses from raw text. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like (McCarthy et al., 2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity sc"
S07-1068,C98-2122,0,\N,Missing
S10-1002,D07-1007,0,0.0762143,"16 July 2010. 2010 Association for Computational Linguistics 4 The Cross-Lingual Lexical Substitution Task conducted experiments using words in context, rather than a predefined sense-inventory however in these experiments the annotators were asked for a single preferred translation. In our case, we allowed annotators to supply as many translations as they felt were equally valid. This allows us to examine more subtle relationships between usages and to allow partial credit to systems that get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word. The Cross-Lingual Lexical Substitution task follows LEXSUB except that the annotations are translations rather than paraphrases. Given a target word in context, the task is to provide several correct translations for that word in a given language. We used English as the source language and Spanish as the target language. We provided both development and test sets, but no training data. As for LEXSUB, any systems requiring training data had to obtain it from other sources. We included nouns, verbs"
S10-1002,S01-1009,0,0.110328,"Missing"
S10-1002,S10-1003,0,0.148904,"D tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (Sinha et al., 2009). In our task, we collected a dataset which allows instances of the same word to have some translations in common, while not necessitating a clustering of translations from a specific resource into senses (in comparison to Lefever and Hoste (2010)). 1 Resnik and Yarowsky (2000) also In this paper we describe the SemEval2010 Cross-Lingual Lexical Substitution task, where given an English target word in context, participating systems had to find an alternative substitute word or phrase in Spanish. The task is based on the English Lexical Substitution task run at SemEval-2007. In this paper we provide background and motivation for the task, we describe the data annotation process and the scoring system, and present the results of the participating systems. 1 Introduction In the Cross-Lingual Lexical Substitution task, annotators and syste"
S10-1002,S07-1009,1,0.504538,"nt and test sets, but no training data. As for LEXSUB, any systems requiring training data had to obtain it from other sources. We included nouns, verbs, adjectives and adverbs in both development and test data. We used the same set of 30 development words as in LEXSUB , and a subset of 100 words from the LEX SUB test set, selected so that they exhibit a wide variety of substitutes. For each word, the same example sentences were used as in LEXSUB. 3 Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval2007 (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning, such as those proposed by Sch¨utze (1998) and Pantel and Lin (2002). 4.1 Annotation We used four annotators for the task, all native Spanish speakers from Mexico, with a high level of proficiency in English. As in LEXSUB, the annotators were allowed to use any resources they wanted to, and were required to provide as many substitutes as they could think of. The i"
S10-1002,S07-1010,0,0.0605658,"lation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, in our task we provided actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (Sinha et al., 2009). In our task, we collected a dataset which allows instances of the same word to have some translations in common, while not necessitating a clustering of translations from a specific resource into senses (in comparison to Lefever and Hoste (2010)). 1 Resnik and Yarowsky (2000) also In this paper we describe the SemEval2010 Cross-Lingual Lexica"
S10-1002,J98-1004,0,0.385291,"Missing"
S10-1002,W02-0816,1,\N,Missing
S10-1002,W09-2413,0,\N,Missing
S10-1002,H05-1051,0,\N,Missing
S10-1002,W09-2412,1,\N,Missing
S10-1087,H93-1061,0,0.352349,"echniques using the background documents released in the task were used to assign ranking scores to the words and their senses. The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses. In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5). 1 Introduction The senses in WordNet are ordered according to their frequency in a manually tagged corpus, SemCor (Miller et al., 1993). Senses that do not occur in SemCor are ordered arbitrarily after those senses of the word that have occurred. It is known from the results of SENSEVAL2 (Cotton et al., 2001) and SENSEVAL3 (Mihalcea and Edmonds, 2004) that first sense heuristic outperforms many WSD systems (see McCarthy et al. (2007)). The first sense baseline’s strong performance is due to the skewed frequency distribution of word senses. WordNet sense distributions based on SemCor are clearly useful, however in a given domain these distributions may not hold true. For example, the first sense for “bank” in WordNet refers to"
S10-1087,E09-1005,0,0.435374,", our approach aims to use these sense distributions collected from domain specific corpora as a knowledge source and combine this with information from the context. Our approach focuses on the strong influence of domain for WSD (Buitelaar et al., 2006) and the benefits of focusing on words salient to the domain (Koeling et al., 2005). Words are assigned a ranking score based on its keyness (salience) in the given domain. We use these word scores as another knowledge source. Graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007). These approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular lexical knowledge base (LKB), such as WordNet. These graphbased algorithms are appealing because they take into account information drawn from the entire graph as well as from the given context, making them superior to other approaches that rely only on local information individually derived for each word. Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph We describe two systems that part"
S10-1087,N04-3012,0,0.0579884,"e of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based on the method described by Lin (1998). This provides the top k nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour. The senses of a word w are each assigned a score by summing over the distributional similarity scores of its neighbours. These are weighted by a semantic similarity score (using WordNet Similarity score (Pedersen et al., 2004) between the sense of w and the sense of the neighbour that maximizes the semantic similarity score. More formally, let Nw = {n1 , n2 , . . . nk } be the ordered set of the top k scoring neighbours of w from the thesaurus with associated distributional similarity scores {dss(w, n1 ), dss(w, n2 ), . . . dss(w, nk )}. Let senses(w) be the set of senses of w. For each sense of w (wsi ∈ senses(w)) a ranking score is obtained by summing over the dss(w, nj ) of each neighbour (nj ∈ Nw ) multiplied by a weight. This weight is the WordNet similarity score (wnss) between the target sense (wsi ) and the"
S10-1087,W00-0901,0,0.0576778,"the domain specific corpus. In the next section we describe the way in which we compute krs(nj ). WordNet::Similarity::lesk (Pedersen et al., 2004) was used to compute word similarity wnss. IIITH1 and IIITH2 systems differ in the way senses are ranked. IIITH1 uses srs(wsj ) whereas IIITH2 system uses msrs(wsj ) for computing sense ranking scores in the given domain. 3 Domain Keyword Ranking We extracted keywords in the domain by comparing the frequency lists of domain corpora (background documents) and a very large general corpus, ukWaC (Ferraresi et al., 2008), using the method described by Rayson and Garside (2000). For each word in the frequency list of the domain corpora, words(domain), we calculated the loglikelihood (LL) statistic as described in Rayson and Garside (2000). We then normalized LL to compute keyword ranking score krs(w) of word w words(domain) using wnss(wsi , nj ) X wnss(wsi , nj ) X wsi senses(w) srs(wsi ) = X dss(w, nj )× wnss(wsi , nj ) wsi senses(w) 388 krs(w) = XLL(w) Keyword Ranking scores with PPR (KRS + PPR): This is same as PPR except that context words are initialized with krs. Sense Ranking scores with PPR (SRS + PPR): Edges connecting words and their synsets are assigned"
S10-1087,kilgarriff-etal-2010-corpus,1,0.850105,"Missing"
S10-1087,P03-1054,0,0.00449573,"ver the SemEval data are provided in Section 5. 2 where wnss(wsi , nj ) = maxnsx ∈senses(nj ) (wnss(wsi , nsx )) Since this approach requires only raw text, sense rankings for a particular domain can be generated by simply training the algorithm using a corpus representing that domain. We used the background documents provided to the participants in this task as a domain specific corpus. In general, a domain specific corpus can be obtained using domain-specific keywords (Kilgarriff et al., 2010). A thesaurus is acquired from automatically parsed background documents using the Stanford Parser (Klein and Manning, 2003). We used k = 5 to built the thesaurus. As we increased k we found the number of non-domain specific words occurring in the thesaurus increased and negatively affected the sense distributions. To counter this, one of our systems IIITH2 used a slightly modified ranking score by multiplying the effect of each neighbour with its domain keyword ranking score. The modified sense ranking msrs(wsj ) score of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based o"
S10-1087,H05-1053,1,\N,Missing
S10-1087,J07-4005,1,\N,Missing
S10-1087,P04-1036,1,\N,Missing
S10-1087,P98-2127,0,\N,Missing
S10-1087,C98-2122,0,\N,Missing
S12-1031,P08-1037,1,0.945589,"l candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data, and indeed, there has been some prior work investigating this possibility. Agirre et al. (2008) applied two state-of-the-art treebank parsers to the sensetagged subset of the Brown corpus version of the Penn Treebank (Marcus et al., 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the ori"
S12-1031,P11-2123,0,0.472393,"Missing"
S12-1031,A00-1031,0,0.0177658,"open class token is labelled with multiple synsets, starting with the assigned leaf synset and travelling as high as possible up the hierarchy, with no distinction made between the different levels in the hierarchy. Configurations using this are designated HP, for ‘hypernym path’. 3.4 Word Sense Annotations We return now to the question of determination of the synset for a given token. One frequentlyused and robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense"
S12-1031,briscoe-carroll-2002-robust,0,0.0322519,"n, instead leaving it the MaxEnt parse ranker to pick those labels from the hierarchy which are most useful. Each 1 This could be useful for verbs since senses interact strongly subcategorisation frames, but that is not our focus here. 231 3.4.2 Disambiguating senses 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. root_frag Adding Word Sense to Parse Selection Models np_frg_c We noted above that parse selection using the methodology established by Velldal (2007) uses hdn_bnp_c human-annotated incorrect and correct derivation aj-hdn_norm_c trees to train a maximum entropy parse selection model. More specifically, the model is trained using legal_a1 n_pl_olr features extracted from the candidate HPSG deriva&quot;legal&quot; issue_n1 tion trees, using the labels of each node (which are the rule names from the grammar) and those of a &quot;issues&quot; lim"
S12-1031,W96-0209,0,0.0869887,"g. In all configurations, there were instances of F-score decreases, sometimes substantial. It is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above. One possible explanation is that the model training parameters were suboptimal for this data set since the characteristics of the data are somewhat different than without sense annotations. The failure to improve some235 what mirrors the results of Clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of Carroll and Briscoe (1996). Clark (2001) used dependencies to rank parses, and WordNet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser. Additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by Fujita et al. (2007), although we outlined reasons why we wished to avoid this approach. This work is preliminary and there is room for more exploration in this space. There is sco"
S12-1031,A00-2018,0,0.0387452,"performance. 1 (1) I saw a tree with my telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the"
S12-1031,W06-1670,0,0.155466,"d robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense labels, it can only provide SS annotations, as we do not get the leaf synsets needed for HP. The input we feed in is POStagged with TnT as above, for comparability with the WNF method, and to ensure that it is compatible with the configuration in which the corpora were parsed – specifically, the unknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSe"
S12-1031,P04-1014,0,0.0471892,"1) I saw a tree with my telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An interm"
S12-1031,P97-1003,0,0.0706991,"telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An intermediate level of"
S12-1031,W11-2927,1,0.833287,"ra are summarised in Table 1. With these corpora, we are able to investigate indomain and cross-domain effects, by testing on a 230 different corpus to the training corpus, so we can examine whether sense-tagging alleviates the crossdomain performance penalty noted in MacKinlay et al. (2011). We can also use a subset of each training corpus to simulate the common situation of sparse training data, so we can investigate whether sensetagging enables the learner to make better use of a limited quantity of training data. 3.2 Evaluation Our primary evaluation metric is Elementary Dependency Match (Dridan and Oepen, 2011). This converts the semantic output of the ERG into a set of dependency-like triples, and scores these triples using precision, recall and F-score as is conventional for other dependency evaluation. Following MacKinlay et al. (2011), we use the EDMNA mode of evaluation, which provides a good level of comparability while still reflecting most the semantically salient information from the grammar. Other work on the ERG and related grammars has tended to focus on exact tree match, but the granular EDM metric is a better fit for our needs here – among other reasons, it is more sensitive in terms o"
S12-1031,W07-1204,0,0.430351,"impact on parse selection of semantic annotations such as coarse sense labels or synonyms from a distributional theTotal Sentences Parseable Sentences Validated Sentences Train/Test Sentences Tokens/sentence Training Tokens W E S CIENCE 9632 9249 7631 6149/1482 15.0 92.5k LOGON 9410 8799 8550 6823/1727 13.6 92.8k Table 1: Corpora used in our experiments, with total sentences, how many of those can be parsed, how many of the parseable sentences have a single gold parse (and are used in these experiments), and average sentence length saurus. Our work here differs from the aforementioned work of Fujita et al. (2007) in a number of ways. Firstly, we use purely syntactic parse selection features based on the derivation tree of the sentence (see Section 3.4.3), rather than ranking using dependency triples, meaning that our method is in principle able to be integrated into a parser more easily, where the final set of dependencies would not be known in advance. Secondly, we do not use humancreated sense annotations, instead relying on heuristics or trained sense-taggers, which is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpo"
S12-1031,P98-2127,0,0.131146,"only to PP attachment, but may help in a range of other syntactic phenomena, such as distinguishing between complements and modifiers of verbs. 228 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228–236, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics The synonyms or hypernyms could take the form of any grouping which relates word forms with semantic or syntactic commonality – such as a label from the WordNet (Miller, 1995) hierarchy, a subcategorisation frame (for verbs) or closely related terms from a distributional thesaurus (Lin, 1998). We present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the English Resource Grammar (ERG: Flickinger (2000)), a precision HPSG-based grammar of English. 2 Related Work 2.1 Parse Selection for Precision Grammars The focus of this work is on parsing using handcrafted precision HPSG-based grammars, and in particular the ERG. While these grammars are carefully crafted to avoid overgeneration, the ambiguity of natural languages means that there will unavoidably be multiple candidate parses licensed by the grammar for any non-"
S12-1031,W02-2018,0,0.0515672,"the ERG, the number of parses postulated for a given sentence can be anywhere from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a"
S12-1031,J93-2004,0,0.0394835,"Missing"
S12-1031,P04-1036,1,0.851121,"making assumptions about which level of the hierarchy will be most useful for parse disambiguation, instead leaving it the MaxEnt parse ranker to pick those labels from the hierarchy which are most useful. Each 1 This could be useful for verbs since senses interact strongly subcategorisation frames, but that is not our focus here. 231 3.4.2 Disambiguating senses 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. root_frag Adding Word Sense to Parse Selection Models np_frg_c We noted above that parse selection using the methodology established by Velldal (2007) uses hdn_bnp_c human-annotated incorrect and correct derivation aj-hdn_norm_c trees to train a maximum entropy parse selection model. More specifically, the model is trained using legal_a1 n_pl_olr features extracted from the candidate HPSG deriva&quot;legal&quot; issue_n1 tion trees,"
S12-1031,W02-1210,0,0.0694168,"Missing"
S12-1031,W07-2207,0,0.0129779,"here from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers"
S12-1031,C98-2122,0,\N,Missing
S12-1081,S12-1051,0,0.0425111,"systems. For this reason we only provide a brief description of that. The results are promising, with Pearson’s coefficients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets. We provide some analysis of the results and also provide results for our data using Spearman’s, which as a nonparametric measure which we argue is better able to reflect the merits of the different systems (average is ranked between the others). 1 Introduction Our motivation for the systems entered in the STS task (Agirre et al., 2012) was to model the contribution of each linguistic component of a sentence to the similarity of a candidate match and vice versa. Siva Reddy Lexical Computing Ltd, UK siva@sivareddy.in Ultimately such a system could be exploited for ranking candidate paraphrases of a chunk of text of any length. We envisage a system as outlined in the future work section. The systems reported are simple baselines to such a system. We have two main systems (alignheuristic and wordsim) and also a system which simply uses the average score for each item from the two main systems (average). In our systems we: • onl"
S12-1081,W03-1004,0,0.0313257,"e similarity, where similarity is cast as meaning equivalence. 1 Textual entailment the relation under question is the more specific relation of entailment, where the meaning of one sentence is entailed by another and a system needs to determine the direction of the entailment. Lexical substitution relates to semantic textual similarity though the task involves a lemma in the context of a sentence, candidate substitutes are not provided, and the relation at question in the task is one of substitutability. 2 Paraphrase recognition is a highly related task, for example using comparable corpora (Barzilay and Elhadad, 2003) and it is likely that semantic textual similarity measures might be useful for ranking candidates in paraphrase acquisition. In addition to various works related to textual entailment, lexical substitution and paraphrasing, there has been some prior work explicitly on semantic text similarity. Semantic textual similarity has been explored in various works. Mihalcea et al. (2006) extend earlier work on word similarity using various WordNet similarity measures (Patwardhan et al., 2003) and a couple of corpus-based distributional measure PMI-IR (Turney, 2002) and LSA (Berry, 1992) using a measur"
S12-1081,de-marneffe-etal-2006-generating,0,0.0195891,"Missing"
S12-1081,P08-1028,0,0.0411168,"ystems for each item, is ranked between the other two systems. This gives a similar ranking of our three systems as the mean score. We also show average ρ. This is a macro average of the Spearman’s value for the 5 datasets without weighting by the number of sentence pairs. 10 6 Conclusions The systems were developed in less than a week including the time with the test data. There are many trivial fixes that may improve the basic algorithm, such as decapitalising proper nouns. There are many things we would like to try, such as val9 Note that Spearman’s ρ is often a little lower than Pearson’s Mitchell and Lapata (2008) 10 We do recognise the difficulty in determining metrics on a new pilot study. The task organisers are making every effort to make it clear that this enterprise is a pilot, not a competition and that they welcome feedback. idating the dependency matching process with the thesaurus matching. We would like to match larger units rather than tokens, with preferences towards the longer matching blocks. In parallel to the development of alignheuristic, we developed a system which measures the similarity between a node in the dependency tree of s1 and a node in the dependency tree of s2 as the sum o"
S12-1081,P11-1076,0,0.0197745,"distributional) weighted by the inverse document frequency of that word. The distributional similarity measures perform at a similar level to the knowledge-based 1 See the guidelines given to the annotators at http://www.cs.columbia.edu/ weiwei/workshop/instructions.pdf 2 This is more or less semantic equivalence since the annotators were instructed to focus on meaning http://www.dianamccarthy.co.uk/files/instructions.pdf. measures that use WordNet. Mohler and Mihalcea (2009) adapt this work for automatic short answer grading, that is matching a candidate answer to one supplied by the tutor. Mohler et al. (2011) take this application forward, combining lexical semantic similarity measures with a graph-alignment which considers dependency graphs using the Stanford dependency parser (de Marneffe et al., 2006) in terms of lexical, semantic and syntactic features. A score is then provided for each node in the graph. The features are combined using machine learning. The systems we propose likewise use lexical similarity and dependency relations, but in a simple heuristic formulation without a man-made thesaurus such as WordNet and without machine learning. 3 Systems We lemmatize and part-of-speech tag the"
S12-1081,E09-1065,0,0.0331374,"d LSA (Berry, 1992) using a measure which takes a summation over all tokens in both sentences for each finding the maximum similarity (WordNet or distributional) weighted by the inverse document frequency of that word. The distributional similarity measures perform at a similar level to the knowledge-based 1 See the guidelines given to the annotators at http://www.cs.columbia.edu/ weiwei/workshop/instructions.pdf 2 This is more or less semantic equivalence since the annotators were instructed to focus on meaning http://www.dianamccarthy.co.uk/files/instructions.pdf. measures that use WordNet. Mohler and Mihalcea (2009) adapt this work for automatic short answer grading, that is matching a candidate answer to one supplied by the tutor. Mohler et al. (2011) take this application forward, combining lexical semantic similarity measures with a graph-alignment which considers dependency graphs using the Stanford dependency parser (de Marneffe et al., 2006) in terms of lexical, semantic and syntactic features. A score is then provided for each node in the graph. The features are combined using machine learning. The systems we propose likewise use lexical similarity and dependency relations, but in a simple heurist"
S12-1081,W07-1401,0,\N,Missing
S19-1007,S13-2050,0,0.0121286,"a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling task. A related application of second-order substitutes is word sense induction. Baskaya et al. (2013) represent contexts as second-order substitutes and apply co-occurrence modelling on top of the instance id - substitute pairs. Alagi´c et al. (2018) propose a similar method to our paper and showed that second-order lexical substitutes and first-order contexts complement each other in word sense induction. Our paper provides alternative evidence for the use of lexical substitutes in the setting of rare word modelling with analysis on the effect from different contexts. . 3 cosine(ContextVec, S0i ) f (S0i ) = P20 (1) 0 j=1 cosine(ContextVec, Sj ) SC = 20 X f (S0i ) ∗ Si (2) i=1 To directly com"
S19-1007,Q17-1010,0,0.070915,"ategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–67 c Minneapolis, June 6–7,"
S19-1007,E17-2062,0,0.0170927,"As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–67 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics and their fitness scores are generated from context2vec. Compared with the context2vec repre"
S19-1007,P18-1002,0,0.397604,"nd-order contexts from lexical substitutes for few-shot learning of word representations Qianchu Liu, Diana McCarthy, Anna Korhonen Language Technology Lab, University of Cambridge English Faculty Building, 9 West Road, Cambridge CB3 9DA, United Kingdom ql261@cam.ac.uk, diana@dianamccarthy.co.uk, alk23@cam.ac.uk Abstract to the traditional bag of word context representations. In line with previous research in this area, we evaluate our methodology on three tasks that measure the quality of the induced unseen word representation from contexts (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018). Our results reveal that the three tasks involve different types of contexts which put different emphasis on first or second order contexts. Our second-order substitute-based method achieves the best performance for modelling rare words in natural contexts from corpora. In the tasks in which both first order and second order contexts are important, the ensemble of these two types of contexts yields superior performance. 1 There is a growing awareness of the need to handle rare and unseen words in word representation modelling. In this paper, we focus on few-shot learning of emerging concepts"
S19-1007,D17-1010,0,0.112609,"te and processing strategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–6"
S19-1007,S18-2004,0,0.0385523,"achieved superior performance on naturallyoccurring contexts from corpora. 1 2 2.1 Related work First-order context The most naive way of inducing new word representation from contexts is to simply take the average of context word embeddings that co-occur with the target word in a sentence. With stop words removed, this simple method has proven to be a strong baseline as shown in Lazaridou et al. (2017) and Herbelot and Baroni (2017). A potential improvement from the simple additive baseline model is that we weigh words with ISF (inverse sentence frequency). We follow the definition of ISF in Samardzhiev et al. (2018) and implement it as a baseline model in our study. More recently, Khodak et al. (2018) learn a transformation matrix to reconstruct pre-trained word embeddings, which essentially learns to highlight informative dimensions. Along a different line, Herbelot and Baroni (2017) take a high-risk learning rate and processing strategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how"
S19-1007,I17-1048,0,0.0251347,"ng fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling task. A related application of second-order substitutes is word sense induction. Baskaya et al. (2013) represent contexts as second-order substitutes and apply co-occurrence modelling on top of the instance id - substitute pairs. Alagi´c et al. (2018) propose a similar method to our paper and showed that second-order lexical substitutes and first-order contexts complement each other in word sense induction. Our paper provides alternative evidence for the use of lexical substitu"
S19-1007,D18-1173,0,0.0780631,"ontextVec 3 be the context representation produced by context2vec, S 0 be the set of the top 20 substitute target word vectors produced by context2vec, S be the same 20 substitutes that we look up in our base word embedding space, and f (S0i ) be the normalized fitness score of S0i as defined in equation 1. The substitute-based context (SC), and thus the unseen word representation for this context, is defined in equation 2. If the unseen word occurs multiple times, we average the unseen word representations across the multiple contexts. work implements a memory-augmented word embedding model (Sun et al., 2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamu"
S19-1007,W13-3512,0,0.0858804,"um information about the target word. We show such an example with the nearest neighbours of the representations induced by our substitutes model and additive ISF in Table 2. We can see that while the additive ISF representation is easily affected by unrelated words in the sentence, the substitutes approach clearly has at least identified that the target word is likely to be a kind of animal. The Contextual Rare Words dataset (CRW) The Contextual Rare Words dataset (CRW) was introduced by Khodak et al. (2018). It consists of a subset of 562 word pairs from the original Rare Word (RW) Dataset (Luong et al., 2013). For each pair, the second word is the rare word and is accompanied by 255 contexts. We follow the experiment setup in Khodak et al. (2018) and use their pre-trained vectors on the subcorpus that does not contain any of the rare words from the dataset. This subcorpus is also used to train the context2vec model that generates substitutes. As in Khodak et al. (2018), we randomly choose 2, 4, 6..128 number of contexts as separate conditions for 100 trials, and use these contexts to predict the rare word representations. Cosine similarity is computed between the rare word representation from the"
S19-1007,D12-1086,0,0.067739,"Missing"
S19-1007,N15-1050,0,0.0155688,"rk implements a memory-augmented word embedding model (Sun et al., 2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to comput"
S19-1007,K16-1006,0,0.0541498,"2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling"
S19-1007,N18-1202,0,0.176787,"Missing"
U11-1011,W10-2923,1,0.729062,"Missing"
U11-1011,P93-1041,0,0.194791,"tional model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th Edition (1977). Because of the lack of a machine-readable copy of the thesaurus at the time, the lexical chains were built by hand. Research in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), text summarisation (Barzilay and Elhadad, 1997), topic detection and tracking (Stokes and Carthy, 2001), information retrieval (Stairmand, 1997), malapropism detection (Hirst and St-Onge, 1998), and question answering (Moldovan and Novischi, 2002). Many types of lexical chaining algorithms rely on examining lexicographical relationships (i.e. semantic measures) between words using domainindependent thesauri such as the Longmans Dictionary of Contemporay English (Kozima, 1993), Roget’s Thesaurus (Jarmasz and Szpakowic"
U11-1011,P98-2127,0,0.0550504,"., 2011b; Wang et al., 2011a; Aumayr et al., 2011), with CRF models frequently being reported to deliver superior performance. While there is research that attempts to conduct cross-forum classification (Wang et al., 2011a) — where classifiers are trained over linking labels from one forum and tested over threads from other forums — the results have not been promising. This research explores unsupervised methods for thread 77 cept or category) inventory from the Macquarie Thesaurus (Bernard, 1986) to build a word-category cooccurrence matrix (WCCM), based on the British National Corpus (BNC). Lin (1998a)’s measure of distributional similarity based on point-wise mutual information (PMI) is then used to measure the association between words. This research will explore two thesaurus-based lexical chaining algorithms, as well as a novel lexical chaining approach which relies solely on statistical word associations. linking structure recovery, by exploiting lexical cohesion between posts via lexical chaining. The first computational model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th E"
U11-1011,W97-0703,0,0.69674,"erent solution again to the original question (link = 4). Lexical chaining is a technique for identifying lists of related words (lexical chains) within a given discourse. The extracted lexical chains represent the discourse’s lexical cohesion, or “cohesion indicated by relations between words in the two units, such as use of an identical word, a synonym, or a hypernym” (Jurafsky and Martin, 2008, pp. 685). Lexical chaining has been investigated in many research tasks such as text segmentation (Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), and text summarisation (Barzilay and Elhadad, 1997). The lexical chaining algorithms used usually rely on domain-independent thesauri such as Roget’s Thesaurus, the Macquarie Thesaurus (Bernard, 1986) and WordNet (Fellbaum, 1998), with some algorithms also utilising statistical associations between words (Stokes et al., 2004; Marathe and Hirst, 2010). 2 Related Work The linking structure of web user forum threads can be used in tasks such as IR (Xi et al., 2004; Seo et al., 2009; Elsas and Carbonell, 2009) and threading visualisation. However, many user forums don’t support the user input of linking information. Automatically recovering the li"
U11-1011,W06-1605,0,0.0603238,"pon, and often only apply to nouns. Some lexical chaining algorithms also make use of statistical associations (i.e. distributional measures) between words which can be automatically generated from domain-specific corpora. For example, Stokes et al. (2004)’s lexical chainer extracts significant noun bigrams based on the G2 statistic (Pedersen, 1996), and uses these statistical word associations to find related words in the preceding context, building on the work of Hirst and StOnge (1998). Marathe and Hirst (2010) use distributional measures of conceptual distance, based on the methodology of Mohammad and Hirst (2006) to compute the relation between two words. This framework uses a very coarse-grained sense (con3 Lexical Chaining Algorithms Three lexical chaining algorithms are experimented with in this research, as detailed in the following sections. 3.1 ChainerRoget ChainerRoget is a Roget’s Thesaurus based lexical chaining algorithm (Jarmasz and Szpakowicz, 2003) based on an off-the-shelf package, namely the Electronic Lexical Knowledge Base (ELKB) (Jarmasz and Szpakowicz, 2001). The underlying methodology of ChainerRoget is shown in Algorithm 1. Methods used to calculate the chain strength/weight are p"
U11-1011,C02-1167,0,0.0291959,"h in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), text summarisation (Barzilay and Elhadad, 1997), topic detection and tracking (Stokes and Carthy, 2001), information retrieval (Stairmand, 1997), malapropism detection (Hirst and St-Onge, 1998), and question answering (Moldovan and Novischi, 2002). Many types of lexical chaining algorithms rely on examining lexicographical relationships (i.e. semantic measures) between words using domainindependent thesauri such as the Longmans Dictionary of Contemporay English (Kozima, 1993), Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003), Macquarie Thesaurus (Marathe and Hirst, 2010) or WordNet (Barzilay and Elhadad, 1997; Hirst and StOnge, 1998; Moldovan and Novischi, 2002; Galley and McKeown, 2003). These lexical chaining algorithms are limited by the linguistic resources they depend upon, and often only apply to nouns. Some lexical chaining algo"
U11-1011,J91-1002,0,0.785889,"1986) to build a word-category cooccurrence matrix (WCCM), based on the British National Corpus (BNC). Lin (1998a)’s measure of distributional similarity based on point-wise mutual information (PMI) is then used to measure the association between words. This research will explore two thesaurus-based lexical chaining algorithms, as well as a novel lexical chaining approach which relies solely on statistical word associations. linking structure recovery, by exploiting lexical cohesion between posts via lexical chaining. The first computational model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th Edition (1977). Because of the lack of a machine-readable copy of the thesaurus at the time, the lexical chains were built by hand. Research in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown"
U11-1011,J98-1004,0,0.109223,"Missing"
U11-1011,D11-1002,1,0.817214,"heer scale and diversity of the data. Previous research shows that the thread linking structure can be used to improve information retrieval (IR) in forums, at both the post level (Xi et al., 2004; Seo et al., 2009) and thread level (Seo et al., 2009; Elsas and Carbonell, 2009). These interpost links also have the potential to enhance threading visualisation, thereby improving information access over complex threads. Unfortunately, linking information is not supported in many forums. While researchers have started to investigate the task of thread linking structure recovery (Kim et al., 2010; Wang et al., 2011b), most research efforts focus on supervised methods. To illustrate the task of thread linking recovery, we use an example thread, made up of 5 posts from 4 distinct participants, from the CNET forum dataset of Kim et al. (2010), as shown in Figure 1. The linking structure of the thread is modelled as a rooted directed acyclic graph (DAG). In this example, UserA initiates the thread with a question in the first post, by asking how to create an interactive input box on a webpage. This post is linked to a virtual root with link label 0. In response, UserB and UserC proLi Wang, Diana Mccarthy an"
U11-1011,widdows-ferraro-2008-semantic,0,0.0125278,"ple.virginia.edu/˜ma5ke/ classes/files/cs65lexicalChain.pdf 78 Algorithm 1 ChainerRoget select a set of candidate nouns for each candidate noun do build all the possible chains, where each pair of nouns in each chain are either the same word or included in the same Head of Roget’s Thesaurus, and select the strongest chain for each candidate noun. end for merge two chains if they contain at least one noun in common derived from words that co-occur with w. A dimensionality reduction technique is often used to reduce the dimension of the vector. We build the WORDSPACE model with SemanticVectors (Widdows and Ferraro, 2008), which is based on Random Projection dimensionality reduction (Bingham and Mannila, 2001). The underlying methodology of ChainerSV is shown in Algorithm 2. This algorithm requires a method to calculate the similarity between two tokens (i.e. words): simtt (x, y), which is done by computing the cosine similarity of the two tokens’ semantic vectors. The similarity between a token ti and a lexical chain cj is then calculated by: the discourse one by one. Each node in the graph represents a noun instance with all its senses, and each weighted edge represents the semantic relation between two sens"
U11-1011,C00-2137,0,0.0531232,"Missing"
U11-1011,J00-4006,0,\N,Missing
U11-1011,C98-2122,0,\N,Missing
U12-1006,E09-1005,0,0.029899,"a multinomial distribution over words, and LDA places a Dirichlet prior on word distributions in topics. Although exact inference of LDA parameters is intractable, the model has gained prominence due to the availability of computationally efﬁcient approximations, the most popular being based on Gibbs sampling (Grifﬁths and Steyvers, 2004). For brevity, we do not give a detailed description of the LDA model. 2.3 Related Work Stevenson (2011) experimented with the use of LDA topic modelling in word sense disambiguation, where he used topic models to provide context for a graph-based WSD system (Agirre and Soroa, 2009), replacing a local context derived from adjacent words. This approach is of limited relevance to our work, as the graph-based approach considered state-of-the-art in unsupervised WSD (De Cao et al., 2010) maps senses to individual nodes in a graph. This presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme ("
U12-1006,E09-1013,0,0.0261533,"Related Work Stevenson (2011) experimented with the use of LDA topic modelling in word sense disambiguation, where he used topic models to provide context for a graph-based WSD system (Agirre and Soroa, 2009), replacing a local context derived from adjacent words. This approach is of limited relevance to our work, as the graph-based approach considered state-of-the-art in unsupervised WSD (De Cao et al., 2010) maps senses to individual nodes in a graph. This presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributio"
U12-1006,W10-2304,0,0.047835,"Missing"
U12-1006,D10-1113,0,0.0466316,"mbines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributional semantics of words in context (Erk and Pad´o, 2008). Dinu and Lapata (2010) propose a probabilistic framework for representing word meaning and measuring similarity of words in context. One of the parametrizations of their framework uses LDA to automatically induce latent senses, which is conceptually very similar to our approach. One key difference is that Dinu and Lapata focus on inferring the similarity in use of different words given their context, whereas in this work we focus on estimating the similarity of use of a single word in a number of different contexts. 3 Methodology Our basic framework is to produce a vector representation for each item in a L EX S UB"
U12-1006,D08-1094,0,0.170801,"Missing"
U12-1006,P09-1002,1,0.850568,"ord sense induction (WSI), but differs in that Usim does not pre-suppose a predeﬁned sense inventory. It also captures the fact that word senses may not always be distinct, and that the applicability of word senses is not necessarily mutually exclusive. In Usim, we consider pairs of sentences at a time, and quantify the similarity of the sense of the target word being used in each sentence. An example of a sentence pair (SPAIR) using similar but not identical senses of the word dry is given in Figure 1. Usim is a relatively new NLP task, partly due to the lack of resources for its evaluation. Erk et al. (2009) recently produced a corpus of sentence Figure 1: Example of an SPAIR judged by annotators to use similar but not identical senses of the word dry. pairs annotated for usage similarity judgments, allowing Usim to be formulated as a distinct task from the related tasks of word sense disambiguation and word sense induction. In this work, we propose a method to estimate word usage similarity in an entirely unsupervised fashion through the use of a topic model. We make use of the well-known Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) to model the distribution of topics in a sentenc"
U12-1006,J06-4002,0,0.0318208,"ixture is too dry, add some water ; if it is too soft, add some ﬂour. ���� ���� ρ Figure 5: Sentences for dry(a) with a strong component of Topic 0 given the 8-topic model illustrated in ﬁgure 3 ���� ���� tor averages and the automated word usage similarity computation was a statistically signiﬁcant 0.202. A detailed breakdown of the best overall result is given in Table 1. Alongside this breakdown, we also provide: (1) the average inter-annotator agreement (IAA); and (2) the Spearman’s ρ for the optimal number of topics for the given lemma. The IAA is computed using leave-one-out resampling (Lapata, 2006), and is a detailed breakdown of the result reported by Erk et al. (2009). In brief, the IAA reported is the mean Spearman’s ρ between the ratings given by each annotator and the average rating given by all other annotators. We also present the Spearman’s ρ for the best number of topics in order to illustrate the impact of the number of topics parameter for the model of the background collection. We ﬁnd that for some lemmas, a lower topic count is optimal, whereas for other lemmas, a higher topic count is preferred. In aggregate terms, we found that verbs, adverbs and nouns performed better wi"
U12-1006,E12-1060,1,0.841355,"presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributional semantics of words in context (Erk and Pad´o, 2008). Dinu and Lapata (2010) propose a probabilistic framework for representing word meaning and measuring similarity of words in context. One of the parametrizations of their framework uses LDA to automatically induce latent senses, which is conceptually very similar to our approach. One key difference is that Dinu and Lapata focus on inferring the similarity in use of different words given their conte"
U12-1006,S07-1009,1,0.888227,"Missing"
U12-1006,W11-1102,0,0.0498483,"Missing"
W00-1325,P87-1027,0,0.307011,"scribed in section 2.2, along with the details of the threshold applied to the relative frequencies o u t p u t from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 F r a m e w o r k for S C F Acquisition Briscoe and Carroll's (1997) verbal acquisition system distinguishes 163 SCFs and returns relative frequencies for each SCF found for a given predicate. The SCFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997"
W00-1325,P91-1027,0,0.108727,"has been done to tional lexicons which include valuable freremove the noise that arises when dealing with quency information. However, the accuracy naturally occurring data, and from mistakes of the resulting lexicons shows room for immade by the SCF acquisition system, for exprovement. One significant source of error ample, parser errors. lies in the statistical filtering used by some reFiltering is usually done with a hypothesearchers to remove noise from automatically sis test, and frequently with a variation of acquired subcategorization frames. In this pathe binomial filter introduced by Brent (1991, per, we compare three different approaches to 1993). Hypothesis testing is performed by forfiltering out spurious hypotheses. Two hymulating a null hypothesis, (H0), which is aspothesis tests perform poorly, compared to sumed true unless there is evidence to the confiltering frames on the basis of relative fretrary. If there is evidence to the contrary, quency. We discuss reasons for this and conH0 is rejected and the alternative hypothesider directions for future research. sis (H1) is accepted. In SCF acquisition, H0 is 1 Introduction that there is no association between a particular verb ("
W00-1325,J93-2002,0,0.866559,"Missing"
W00-1325,A97-1052,0,0.938899,"y, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acquired SCFs. They applied BHT as follows. The system recorded the total number of sets of SCF cues (n) found for a given predicate, and the number of these sets for a given SCF (ra). The system estimated the error probability (pe) that a cue for a SCF (scfi) occurred with a verb which did not take scfi. pe was estimated in two stages, as shown in equation 1. Firstly, the number of verbs which are members of the target SCF in the ANLT dictionary were extracted. This number was converted to a probability of class membership by dividing by th"
W00-1325,W98-1505,0,0.139722,"Missing"
W00-1325,J93-1003,0,0.0405344,"ve frequencies produced slightly betrecall 1 ter results than those achieved with a BrentzWhere token recall is the percentage .of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. 199 style binomial filter when establishing SCFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when filtering SCFs automatically acquired for Czech. This test has been recommended for use in NLP since it does not assume a normal distribution, which invalidates many other parametric tests for use with natural language phenomena. LLR can be used in a form (-2logA) which is X2 distributed. Moreover, this asymptote is appropriate at quite low frequencies, which makes the hypothesis test particularly useful when dealing with natural language phenomena, where low frequency events are commonplace. A problem with using hypothesis testing for fi"
W00-1325,P98-1071,0,0.264875,"Missing"
W00-1325,C94-1042,0,0.0922367,"eshold applied to the relative frequencies o u t p u t from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 F r a m e w o r k for S C F Acquisition Briscoe and Carroll's (1997) verbal acquisition system distinguishes 163 SCFs and returns relative frequencies for each SCF found for a given predicate. The SCFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acq"
W00-1325,P99-1051,0,0.121006,"p u t e r Laboratory, University of Cambridge Pembroke Street, Cambridge CB2 3QG, U K alk23@cl, cam. ac. uk, genevieve, gorrell@netdecisions, co. uk Diana McCarthy School of Cognitive and Computing Sciences University of Sussex, Brighton, BN1 9QH, UK dianam@cogs, s u s x . a c . uk Abstract The approaches to extracting SCF information from corpora have frequently employed Research ""into the automatic acquisition of statistical methods for filtering (e.g. Brent, subcategorization frames (SCFS) from corpora 1993; Manning 1993; Briscoe and Carroll, is starting to produce large-scale computa1997; Lapata, 1999). This has been done to tional lexicons which include valuable freremove the noise that arises when dealing with quency information. However, the accuracy naturally occurring data, and from mistakes of the resulting lexicons shows room for immade by the SCF acquisition system, for exprovement. One significant source of error ample, parser errors. lies in the statistical filtering used by some reFiltering is usually done with a hypothesearchers to remove noise from automatically sis test, and frequently with a variation of acquired subcategorization frames. In this pathe binomial filter introdu"
W00-1325,C00-2100,0,0.837786,"Missing"
W00-1325,W93-0109,0,0.905976,"Missing"
W00-1325,H91-1067,0,\N,Missing
W00-1325,E95-1016,0,\N,Missing
W00-1325,P93-1032,0,\N,Missing
W00-1325,C98-1068,0,\N,Missing
W02-0816,P00-1041,0,\N,Missing
W02-0816,P91-1034,0,\N,Missing
W02-0816,J98-1004,0,\N,Missing
W03-1810,pearce-2002-comparative,0,\N,Missing
W03-1810,W01-0513,0,\N,Missing
W03-1810,J90-1003,0,\N,Missing
W03-1810,H94-1020,0,\N,Missing
W03-1810,briscoe-carroll-2002-robust,1,\N,Missing
W03-1810,W03-1812,0,\N,Missing
W03-1810,W03-1809,0,\N,Missing
W03-1810,P98-2127,0,\N,Missing
W03-1810,C98-2122,0,\N,Missing
W03-1810,W02-2001,0,\N,Missing
W03-1810,P99-1041,0,\N,Missing
W03-1810,S01-1029,1,\N,Missing
W04-0837,S01-1020,0,\N,Missing
W04-0837,O97-1002,0,\N,Missing
W04-0837,briscoe-carroll-2002-robust,1,\N,Missing
W04-0837,J04-1003,0,\N,Missing
W04-0837,P98-2127,0,\N,Missing
W04-0837,C98-2122,0,\N,Missing
W04-0837,W04-0861,1,\N,Missing
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W06-2503,briscoe-carroll-2002-robust,0,0.0162184,"th specify a distance measure: Djcn (s1, s2) = IC(s1)+IC(s2)−2×IC(s3) where the third class (s3) is the most informative, or most specific, superordinate synset of the two senses s1 and s2. This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal: jcn(s1, s2) = 1/Djcn (s1, s2) We use raw BNC data for calculating IC values. JCN DIST We use a distributional similarity measure (Lin, 1998) to obtain a fixed number (50) of the top ranked nearest neighbours for the target nouns. For input we used grammatical relation data extracted using an automatic parser (Briscoe and Carroll, 2002). We used the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992). For each noun we collect co-occurrence triples featuring the noun in a grammatical relationship with another word. The words and relationships considered are cooccurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjectivenoun relations. Using this data, we compute the distributional similarity proposed by Lin between each pair of nouns, where the nouns have at least 10 triples. Each noun (w) is then listed with"
W06-2503,J98-1003,0,0.0317561,"Related Work There is a significant amount of previous work on grouping WordNet word senses using a number of different information sources, such as predicate argument structure (Palmer et al., forthcoming), information from WordNet (Mihalcea and Moldovan, 2001; Tomuro, 2001) 2 and other lexical resources (Peters and Peters, 1998) translations, system confusability, topic signature and contextual evidence (Agirre and Lopez de Lacalle, 2003). There is also work on grouping senses of other inventories using information in the inventory (Dolan, 1994) along with information retrieval techniques (Chen and Chang, 1998). One method presented here (referred to as DIST and described in section 3) relates most to that of Agirre and Lopez de Lacalle (2003). They use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of “monosemous relatives” which are monosemous words related to one of the target word senses in WordNet. We use contexts of occurrence indirectly. We obtain “nearest neighbours” which occur in similar contexts to the target word. A vector is created for each word sense with a WordNet similarity score between the sense and each nearest neighbour of"
W06-2503,C94-2113,0,0.569576,"ics. We follow this with a discussion and conclusion. 18 2 Related Work There is a significant amount of previous work on grouping WordNet word senses using a number of different information sources, such as predicate argument structure (Palmer et al., forthcoming), information from WordNet (Mihalcea and Moldovan, 2001; Tomuro, 2001) 2 and other lexical resources (Peters and Peters, 1998) translations, system confusability, topic signature and contextual evidence (Agirre and Lopez de Lacalle, 2003). There is also work on grouping senses of other inventories using information in the inventory (Dolan, 1994) along with information retrieval techniques (Chen and Chang, 1998). One method presented here (referred to as DIST and described in section 3) relates most to that of Agirre and Lopez de Lacalle (2003). They use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of “monosemous relatives” which are monosemous words related to one of the target word senses in WordNet. We use contexts of occurrence indirectly. We obtain “nearest neighbours” which occur in similar contexts to the target word. A vector is created for each word sense with a WordNe"
W06-2503,O97-1002,0,0.120294,"s of different words, but leave that for future research. data in SemCor) to determine the fine-grained output. This shows that the structure of WordNet is indeed helpful when selecting coarse senses for WSD . We use the JCN measure to contrast with our DIST measure which uses a combination of distributional neighbours and JCN. We have experimented only with nouns to date, although in principle our method can be extended for other POS. 3 Methods for producing RLISTs This is a measure from the WordNet similarity package (Patwardhan and Pedersen, 2003) originally proposed as a distance measure (Jiang and Conrath, 1997). JCN uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts. Each synset is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation. The frequency data is used to calculate the “information content” (IC) of a class (IC(s) = −log(p(s))) and with this, Jiang and Conrath specify a distance measure: Djcn (s1, s2) = IC(s1)+IC(s2)−2×IC(s3) where the third class (s3) is the most informative, or most specific, superordinate synset of the two senses s1 and s2. This is transformed from a di"
W06-2503,S01-1004,0,0.0235039,"ed such as the number of clusters required for each word. This has been done with the numbers determined by the gold-standard for the purposes of evaluation (Agirre and Lopez de Lacalle, 2003) but ultimately the right number of classes for each word cannot usually be predetermined even if one knows the application, unless only a sample of words are being handled. In cases where a goldstandard is provided by humans it is clear that further relationships could be drawn. For example, in the groups (hereafter referred to as SEGR ) made publicly available for the SENSEVAL-2 English lexical sample (Kilgarriff, 2001) (hereafter referred to as S EVAL -2 ENG LEX) child is grouped as shown in table 1. Whilst it is perfectly reasonable the grouping decision was determined by the ‘youth’ vs ‘descendant’ distinction, the relationships between non-grouped senses, notably sense numbers 1 and 2 are apparent. It is quite possible that these senses will share contextual cues useful for WSD and distinction between the two might not be relevant in a given application, for example because they are translated in the same way (ni˜no/a in Spanish can mean both young boy/girl and son/daughter) or have common substitutions"
W06-2503,H05-1053,1,0.808913,"Missing"
W06-2503,P05-1005,0,0.0140173,"similar contexts to the target word. A vector is created for each word sense with a WordNet similarity score between the sense and each nearest neighbour of the target word. 3 While related senses may not have a lot of shared contexts directly, because of sparse data, they may have semantic associations with the same subset of words that share similar distributional contexts with the target word. This method avoids reliance on sense-tagged data or monosemous relatives because the distributional neighbours can be obtained automatically from raw text. Our other method relates to the findings of Kohomban and Lee (2005). We use the JiangConrath score (JCN) in the WordNet Similarity Package. This is a distance measure between WordNet senses given corpus frequency counts and the structure of the WordNet hierarchy. It is described in more detail below. Kohomban and Lee (2005) get good results on disambiguation of the SENSEVAL all-words tasks using the 25 unique beginners from the WordNet hierarchy for training a coarse-grained WSD system and then using a first sense heuristic (provided using the frequency 2 Mihalcea and Moldovan group WordNet synonym sets (synsets) rather than word senses. 3 We have not tried u"
W06-2503,P04-1036,1,0.915343,"icient training data for supervised WSD systems. One response to this is WNs# 1 gloss your basis for belief or disbelief; knowledge on which to base belief; ‘the evidence that smoking 2 causes lung cancer is very compelling’ an indication that makes something evident; 3 (law) all the means by which any alleged ‘his trembling was evidence of his fear’ matter of fact whose truth is investigated at judicial trial is established or disproved Figure 1: The senses of evidence in WordNet to exploit the natural skew of the data and focus on finding the first (predominant) sense from a sample of text (McCarthy et al., 2004). Further contextual WSD may be required, but the technique provides a useful unsupervised back-off method. The other major problem for WSD is the granularity of the sense inventory since a pre-existing lexical resource is often too fine-grained, with narrow sense distinctions that are irrelevant for the intended application. For example, WordNet (Fellbaum, 1998) which is widely used and publicly available, has a great many subtle distinctions that may in the end not be required. For example, in figure 1 we show the three senses (WNs#) for evidence from WordNet version 1.7. 1 These are all cle"
W06-2503,H93-1061,0,0.129723,"lso look at the effect of the RLISTs and the goldstandards themselves on WSD . Since the focus of this paper is not the WSD system, but the sense inventory, we use a simple WSD heuristic which uses the first sense of a word in all contexts, where the first sense of every word is specified by a resource. While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an allwords task (Snyder and Palmer, 2004). We contrast the performance of first sense heuristics i) from SemCor (Miller et al., 1993) and ii) derived automatically from the BNC following (McCarthy et al., 2004) and also iii) an upper-bound first sense heuristic extracted from the test data. The paper is organised as follows. In the next section we describe some related work. In section 3 we describe the two methods we will use to relate senses. Our experiments are described in section 4. In 4.1 we describe the construction of a new gold-standard produced using the same sense inventory used for SEGR , and give inter-annotator agreement figures for the task. In section 4.2 we compare our methods to the new gold-standard and i"
W06-2503,J98-1004,0,0.362446,"Missing"
W06-2503,W04-0811,0,0.0582718,"ion with figures for inter-tagger agreement. As well as evaluating against a gold-standard, we also look at the effect of the RLISTs and the goldstandards themselves on WSD . Since the focus of this paper is not the WSD system, but the sense inventory, we use a simple WSD heuristic which uses the first sense of a word in all contexts, where the first sense of every word is specified by a resource. While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an allwords task (Snyder and Palmer, 2004). We contrast the performance of first sense heuristics i) from SemCor (Miller et al., 1993) and ii) derived automatically from the BNC following (McCarthy et al., 2004) and also iii) an upper-bound first sense heuristic extracted from the test data. The paper is organised as follows. In the next section we describe some related work. In section 3 we describe the two methods we will use to relate senses. Our experiments are described in section 4. In 4.1 we describe the construction of a new gold-standard produced using the same sense inventory used for SEGR , and give inter-annotator agreemen"
W06-2503,N01-1010,0,0.548982,"nse inventory used for SEGR , and give inter-annotator agreement figures for the task. In section 4.2 we compare our methods to the new gold-standard and in section 4.3 we investigate how much effect coarser grained sense distinctions have on WSD using naive first sense heuristics. We follow this with a discussion and conclusion. 18 2 Related Work There is a significant amount of previous work on grouping WordNet word senses using a number of different information sources, such as predicate argument structure (Palmer et al., forthcoming), information from WordNet (Mihalcea and Moldovan, 2001; Tomuro, 2001) 2 and other lexical resources (Peters and Peters, 1998) translations, system confusability, topic signature and contextual evidence (Agirre and Lopez de Lacalle, 2003). There is also work on grouping senses of other inventories using information in the inventory (Dolan, 1994) along with information retrieval techniques (Chen and Chang, 1998). One method presented here (referred to as DIST and described in section 3) relates most to that of Agirre and Lopez de Lacalle (2003). They use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of “mon"
W06-2503,P98-2127,0,\N,Missing
W06-2503,C98-2122,0,\N,Missing
W08-2211,briscoe-carroll-2002-robust,0,0.0298302,"d from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier relations. Thus we rank each sense wsi ∈ W Sw using Prevalence Score wsi = (11) ∑ n j ∈Nw dssn j × wnss(wsi , n j ) ∑wsi′ ∈W Sw wnss(wsi′ , n j ) where the WordNet similarity score (wnss) is defined as: wnss(wsi , n j ) = max (wnss(wsi , nsx )) nsx ∈NSn j 2.2 Building the Thesaurus The thesaurus was acquired using the method described by Lin (1998). For input we used grammatical relation data extracted using an automatic parser (Briscoe and Carroll, 2002). For the experiments in this paper we used the 90 million words of written English from the BNC. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. This limited set of grammatical relations was chosen since accuracy of the parser is particularly high for these 4 relations. We could easily extend the set of relations to more in the future. A noun, w, is thus described by a set of co-occurrence triples &lt; w, r, x > and associated frequencies, where r is"
W08-2211,I08-1073,1,0.755593,"sense distributions are expensive to create and therefore hard to find. The one resource that is used most widely, SemCor (Miller et al., 1993), is only available for English and only representative for ’general’ (non domain specific) text. McCarthy et al’s method was successfully applied to a corpus of modern English text (the BNC (Leech, 1992)) and the predicted predominant senses compared well with the gold standard given by SemCor. Other experiments showed that the method can successfully be adapted to domain specific text (Koeling et al., 2005) and other languages (for example, Japanese (Iida et al., 2008)). Even though the first sense heuristic is powerful, it would be preferable to only use it for WSD, when either the sense distribution is so skewed that the most commonly used sense is by far the most dominant, or as a back-off when few other clues are available to decide otherwise. The use of local context is ultimately necessary to find evidence for the intended sense of an ambiguous word. In this paper we investigate how we can exploit results from intermediate steps taken when calculating the predominant senses to this end. The work on automatically finding predominant senses1 was partly"
W08-2211,O97-1002,0,0.0260307,"score which sums over the distributional similarity scores of 2 We use the same corpus used for generating the thesaurus as for the reference corpus (in all our experiments). Koeling and McCarthy 132 the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score on nouns (Jiang and Conrath, 1997) since this gave reasonable results for McCarthy et al. and it is efficient at run time given precompilation of frequency information. The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier relations. Thus we rank each sense wsi ∈ W Sw using Prevalence Score wsi = (11) ∑ n j ∈Nw dssn j × wnss(wsi , n j ) ∑wsi′ ∈W Sw wnss(wsi′ , n j ) where the WordNet similarity score (wnss) is defined as: wnss(wsi , n j ) = max (wnss("
W08-2211,H05-1053,1,0.749432,"Missing"
W08-2211,P98-2127,0,0.444309,"his section we explain how we exploit local context for SD. 2.1 Finding Predominant Senses We use the method described in McCarthy et al. (2004) for finding predominant senses from raw text. It can be applied to all parts of speech, but the experiments in this paper all focus on nouns only. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like McCarthy et al. (2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). We use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of 2 We use the same corpus used for generating the thesaurus as for the reference corpus (in all our experiments). Koeling and McCarthy 132 the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and and the"
W08-2211,P04-1036,1,0.808619,"x (UK) email: robk@sussex.ac.uk Abstract Recent work on automatically predicting the predominant sense of a word has proven to be promising (McCarthy et al., 2004). It can be applied (as a first sense heuristic) to Word Sense Disambiguation (WSD) tasks, without needing expensive hand-annotated data sets. Due to the big skew in the sense distribution of many words (Yarowsky and Florian, 2002), the First Sense heuristic for WSD is often hard to beat. However, the local context of an ambiguous word can give important clues to which of its senses was intended. The sense ranking method proposed by McCarthy et al. (2004) uses a distributional similarity thesaurus. The k nearest neighbours in the thesaurus are used to establish the predominant sense of a word. In this paper we report on a first investigation on how to use the grammatical relations the target word is involved with, in order to select a subset of the neighbours from the automatically created thesaurus, to take the local context into account. This unsupervised method is quantitatively evaluated on SemCor. We found a slight improvement in precision over using the predicted first sense. Finally, we discuss strengths and weaknesses of the method and"
W08-2211,J07-4005,1,0.907182,"sense ranking algorithm introduced in McCarthy et al. (2004). Then we explain how we can use the database of grammatical relations that we used for creating the thesaurus, for selecting a subset of neighbours in the thesaurus. The following section describes an evaluation performed on the SemCor data. In the last two sections we discuss the results and especially why both recall and precision are lower than we had hoped and what can be done to improve the results. 2 Predominant Senses and Local Context For a full review of McCarthy et al’s ranking method, we refer to McCarthy et al. (2004) or McCarthy et al. (2007). Here we give a short description of the method. Since we need the grammatical relations used for building the thesaurus, for selecting a subset of the neighbours, we explain the procedure for building the thesaurus in 2.2. In the last part of this section we explain how we exploit local context for SD. 2.1 Finding Predominant Senses We use the method described in McCarthy et al. (2004) for finding predominant senses from raw text. It can be applied to all parts of speech, but the experiments in this paper all focus on nouns only. The method uses a thesaurus obtained from the text by parsing,"
W08-2211,H93-1061,0,0.188914,"ishing the predominant sense of ambiguous words automatically using untagged texts (McCarthy et al., 2004, 2007). The motivation for that work is twofold: on the one hand it builds on the strength of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of choosing the most commonly used sense of a word, irrespective of the context in which the word occurs) and on the other hand it recognizes that manually created resources for establishing word sense distributions are expensive to create and therefore hard to find. The one resource that is used most widely, SemCor (Miller et al., 1993), is only available for English and only representative for ’general’ (non domain specific) text. McCarthy et al’s method was successfully applied to a corpus of modern English text (the BNC (Leech, 1992)) and the predicted predominant senses compared well with the gold standard given by SemCor. Other experiments showed that the method can successfully be adapted to domain specific text (Koeling et al., 2005) and other languages (for example, Japanese (Iida et al., 2008)). Even though the first sense heuristic is powerful, it would be preferable to only use it for WSD, when either the sense di"
W08-2211,C98-2122,0,\N,Missing
W09-2401,P09-1002,1,0.86456,"Missing"
W09-2401,S07-1009,1,0.890402,"Missing"
W09-2401,S07-1006,0,0.0387272,"Missing"
W09-2401,W09-2412,1,0.868421,"Missing"
W09-2401,N06-2015,0,\N,Missing
W09-2401,S10-1002,1,\N,Missing
W09-2412,D07-1007,0,0.11801,"efined sense inventory. Resnik and Yarowsky (2000) also conducted their experiments using words in context, rather than a predefined sense-inventory as in (Ng and Chan, 2007; Chan and Ng, 2005), however in these experiments the annotators were asked for a single preferred translation. We intend to allow annotators to supply as many translations as they feel are equally valid. This will allow us to examine more subtle relationships between usages and to allow partial credit to systems which get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. 4 The Cross-Lingual Lexical Substitution Task Here we discuss our proposal for a Cross-Lingual Lexical Substitution task. The task will follow LEX SUB except that the annotations will be translations rather than paraphrases. Given a target word in context, the task is to provide several correct translations for that word in a given language. We will use English as the source language and Spanish as the target language. Multiwords are ‘part and parcel’ of natural language. For this reason, rat"
W09-2412,S01-1009,0,0.0946596,"llowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1 The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 77 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the S ENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of a test word, also referred to as a translation memory. Systems could either select the most appropriate translation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit e"
W09-2412,S07-1009,1,0.708115,"ta. Any system that relied on training data, such as sense annotated corpora, had to use resources available from other sources. The task had eight participating teams. Teams were allowed to submit up to two systems and there were a total of ten different systems. The scoring was conducted using recall and precision measures using: • the frequency distribution of responses from the annotators and • the mode of the annotators (the most frequent response). The systems were scored using their best guess as well as an out-of-ten score which allowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1 The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 77 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the S ENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of"
W09-2412,W02-0816,1,0.911884,"be in English, but the substitutes will be in Spanish. An automatic system for cross-lingual lexical substitution would be useful for a number of applications. For instance, such a system could be used to assist human translators in their work, by providing a number of correct translations that the human translator can choose from. Similarly, the system Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1"
W09-2412,S07-1010,0,0.0538714,"nslation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not. An example from Resnik and Yarowsky (2000) (table 4 in that paper) is the first two senses from WordNet for the noun interest: WordNet sense monetary e.g. on loan stake/share Spanish Translation inter´es, r´edito inter´es,participaci´on For WSD tasks, a decision can be made to lump senses with such overlap, or split them using the distinctive translation and then"
W09-2412,J98-1004,0,0.130057,"Missing"
W09-2412,H05-1051,0,0.0572677,"Similarly, the system Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing on one aspect of the problem and therefore not requiring a complete system that might mask system capab"
W09-2412,S10-1002,1,\N,Missing
W11-1310,W03-1812,0,0.465554,"im(Vw1 ⊕w2 , Vw1 w2 ) &gt; γ, the compound is classified as compositional, where γ is a threshold for deciding compositionality. Global values of a and b were chosen by optimizing the performance on the development set. It was found that no single threshold value γ held for all compounds. Changing the threshold alters performance arbitrarily. This might be due to the polysemous nature of the constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCar"
W11-1310,W03-1809,0,0.532911,"Missing"
W11-1310,W11-1304,0,0.520526,"s.york.ac.uk diana@dianamccarthy.co.uk Suresh Manandhar University of York, UK Spandana Gella University of York, UK suresh@cs.york.ac.uk spandana@cs.york.ac.uk Abstract In this paper, we highlight the problems of polysemy in word space models of compositionality detection. Most models represent each word as a single prototype-based vector without addressing polysemy. We propose an exemplar-based model which is designed to handle polysemy. This model is tested for compositionality detection and it is found to outperform existing prototype-based models. We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations. 1 Introduction In the field of computational semantics, to represent the meaning of a compound word, two mechanisms are commonly used. One is based on the distributional hypothesis (Harris, 1954) and the other is on the principle of semantic compositionality (Partee, 1995, p. 313). The distributional hypothesis (DH) states that words that occur in similar contexts tend to have similar meanings. Using this hypothesis, distributional models like the Word-space model (WSM, Sahl"
W11-1310,P10-2017,0,0.0416216,"Missing"
W11-1310,W06-1203,0,0.555699,"s as a PSC-based vector. So a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for"
W11-1310,W03-1810,1,0.953329,"Missing"
W11-1310,P08-1028,0,0.553899,"attern using corpus query language. Let w1 w2 be a compound word with constituent words w1 and w2 . Ew denotes the set of exemplars of w. Vw is the prototype vector of the word w, which is built by merging all the exemplars in Ew 1 Sketch Engine http://www.sketchengine.co.uk 55 For the purposes of producing a PSC-based vector for a compound, a vector of a constituent word is built using only the exemplars which do not contain the compound. Note that the vectors are sensitive to a compound’s word-order since the exemplars of w1 w2 are not the same as w2 w1 . We use other WSM settings following Mitchell and Lapata (2008). The dimensions of the WSM are the top 2000 content words in the given corpus (along with their coarse-grained part-of-speech information). Cosine similarity (sim) is used to measure the similarity between two vectors. Values at the specific positions in the vector representing context words are set to the ratio of the probability of the context word given the target word to the overall probability of the context word. The context window of a target word’s exemplar is the whole sentence of the target word excluding the target word. Our language of interest is English. We use the ukWaC corpus"
W11-1310,N10-1013,0,0.0369443,"ionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1 w2 , we build its DH-based prototype vector Vw1 w2 from all its exemplars Ew1 w2 . Secondly, we remove irrelevant exemplars in Ew1 and Ew2 of constituent words and build the refined prototype vectors Vw1r and Vw2r of the constituent words w1 and w2 respectively. These refined vectors are used to compose the PSC-based vectors 2 of the compound. Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. This work does not relate to compositionality but to measuring semantic similarity of single words. As such, their clusters are not influenced by other words whereas in our approach for detecting compositionality, the other constituent word plays a major role. We use the compositionality functions, simple addition and simple multiplication to build Vw1r +w2r and Vw1r ×w2r respectively. Based on the similarities sim(Vw1 w2 , Vw1r ), sim(Vw1 w2 , Vw2r ), sim(Vw1 w2 , Vw1r +w2r ) and sim(Vw1 w2 , Vw1r ×w2r ), we"
W11-1310,W11-0115,0,0.0621038,"he constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCarthy et al. (2003) observed that methods based on distributional similarities between a phrase and its constituent words help when determining the compositionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1"
W11-1310,P07-2011,0,0.060728,"Missing"
W11-1310,W01-0513,0,0.791299,"a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics di"
W14-3004,W13-5503,1,0.873988,"ic role labeling performance (Bauer & Rambow, 2011; Dipanjan, et al., 2010; Giuglea & Moschitti, 2006; Merlo & der Plas, 2009; Yi, et al., 2007). That is one of the primary goals of SemLink. The first release of SemLink (1.1) contained mappings between these three lexical resources as well as a set of PropBank instances from the Wall Street Journal data with mappings to VerbNet classes and thematic roles (Palmer, 2009). Our most recent release, SemLink 1.2,3 now includes mappings to FrameNet frames and Frame Elements wherever they are available (FN version 1.5), as well as ON sense groupings (Bonial, et al., 2013). The mapping files between PropBank and VerbNet (version 3.2), and FrameNet have also been checked for consistency and updated to more accurately reflect the current relations between these resources. This annotated corpus can now be used to train and evaluate VerbNet Class and FrameNet Frame classifiers, to explore clusters of Frame Elements that map to the same VerbNet and PropBank semantic roles, and to evaluate approaches to semantic role labeling that use the type-to-type mappings to bootstrap VerbNet and FrameNet role labels from automatic PropBank semantic role labels. 4 the kitchen ev"
W14-3004,N10-1138,0,0.0836314,"Missing"
W14-3004,P09-1033,0,0.0602151,"Missing"
W14-3004,P06-1117,0,0.0798849,"Missing"
W14-3004,J05-1004,1,0.286109,"l give us myriads of alternatives for “coining a phrase.” This causes immense difficulty for NLP systems. No one has made greater contributions to advancing the state of the art of lexical semantics, and its applications to NLP, than Chuck Fillmore. In this paper we focus on the central role that FrameNet has played in our development of SemLink+ and in our current explorations into event ontologies that can play a practical role in accurate automatic event extraction. 2 3 SemLink+ and Semantic Roles SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2004), and the recently added OntoNotes (ON) sense groupings (Weischedel, et al., 2011). They all associate semantic information with the propositions in a sentence. Each was created independently with somewhat differing goals, and they vary in the level and nature of semantic detail represented. FrameNet is the Detecting events An elusive goal of current NLP systems is the accurate detection of events – recognizing the meaningful relations among the topics, people, 13 Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuc"
W14-3004,lopez-de-lacalle-etal-2014-predicate,0,\N,Missing
W14-3004,bonial-etal-2014-propbank,1,\N,Missing
W14-3004,W14-2903,1,\N,Missing
W14-3004,fillmore-etal-2004-framenet,0,\N,Missing
W97-0808,H93-1061,0,0.175376,"Missing"
W97-0808,P93-1024,0,0.068055,"t and the MDL principle is detailed in the papers (Li & Abe, 1995; Abe & Li, 1996). The WordNet hypernym noun hierarchy is used here as it is available and ready made. Using a resource produced by humans has its drawbacks, particularly that the classification is not tailored to the task and data at hand and is prone to the inconsistencies and errors that beset any man-made lexical resource. Still the alternative of using an automatically clustered hierarchy has other disadvantages, a particular problem being that techniques so far developed often give rise to semantically incongruous classes (Pereira, Tishby, & Lee, 1993). Calculation of the class frequencies is key to the process of acquisition of selectional preferences. Li and Abe estimate class frequencies by dividing the frequencies of nouns occurring in the set of synonyms of a class between all the classes in which they appear. Class frequencies are then inherited up the hierarchy. In order to keep to their definition of a ""tree cut"" all nouns in the hierarchy need to be positioned at leaves. WordNet does not adhere to this stipulation and so they prune the hierarchy at classes where a noun featured in the set of synonyms has occurred in the data. This"
W97-0808,H93-1054,0,0.0847244,"anaphora as well as being important for identifying the underlying semantic roles in argument slots. The work reported here concentrates on acquisition for verbal predicates since verbs are of such obvious importance for the lexicon. However it could also be applied to any other type of predication. The main contribution of this work is that it uses shallow parses produced by a fully automatic parser and that some word sense disambiguation (WSD) is performed on the heads collected from these parses. Most current research on selectional preference acquisition has used the Penn Treebank parses (Resnik, 1993a, 1993b; Ribas, 1995; Li & Abe, 1995; Abe & Li, 1996) These are obtained semi- automatically with a deterministic parser and manual correction. Additionally the other approaches do not perform any WSD on the input data and most report a major source of error arising from the contribution of erroneous senses sometimes giving incorrect preferences and at other times a noticeable effect of over-generalisation (Ribas, 1995; Resnik, 1993a). The relationship between selectional preference acquisition and WSD is a circular one. One potential use of selectional preferences is WSD yet their acquisitio"
W97-0808,E95-1016,0,0.160525,"ing important for identifying the underlying semantic roles in argument slots. The work reported here concentrates on acquisition for verbal predicates since verbs are of such obvious importance for the lexicon. However it could also be applied to any other type of predication. The main contribution of this work is that it uses shallow parses produced by a fully automatic parser and that some word sense disambiguation (WSD) is performed on the heads collected from these parses. Most current research on selectional preference acquisition has used the Penn Treebank parses (Resnik, 1993a, 1993b; Ribas, 1995; Li & Abe, 1995; Abe & Li, 1996) These are obtained semi- automatically with a deterministic parser and manual correction. Additionally the other approaches do not perform any WSD on the input data and most report a major source of error arising from the contribution of erroneous senses sometimes giving incorrect preferences and at other times a noticeable effect of over-generalisation (Ribas, 1995; Resnik, 1993a). The relationship between selectional preference acquisition and WSD is a circular one. One potential use of selectional preferences is WSD yet their acquisition appears to require"
W97-0808,P95-1026,0,0.164698,"Missing"
W97-0808,J98-2002,0,\N,Missing
W97-0808,A97-1052,0,\N,Missing
Y10-1086,C00-2157,0,0.30041,"s these processes are fast and available for many languages, and have been applied to most of our corpora) but not parsed. 2 Related Work There are numerous other corpus query tools available. Here we give brief references to several that are either widely used on large corpora or which concentrate on syntactic search. • the IMS Stuttgart CorpusWorkBench (Christ and Schulze, 1994), limited to 2 billion-words corpora, • a system presented in (Davies, 2009) which uses standard relational database technology, not closely related to syntactic search, • tools being part of the Tigersearch project (König and Lezius, 2000), currently unmaintained, • the Gsearch corpus query system (Steffan et al., 2001), 741 742 Workshop on Advanced Corpus Solutions 3 Core CQL A CQL query is a pattern which may match a token or series of tokens in the corpus. Each token is assigned a set of attributes (word form, lemma, part-of-speeech tag etc.) and each corpus might be assigned a set of structures. Structures may identify any sequence of tokens and are typically used to mark up documents, paragraphs, sentences, utterances, syntactic phrases of various kinds and named entities. Zero-length structures can also be used. Structure"
Y10-1086,kilgarriff-etal-2010-corpus,1,0.793715,"t for much linguistic research to find datasets relating to hunches and hypotheses. Linguists and system developers would like to be able to find large numbers of examples quickly and easily. Our tool computes the Corpus Query Language (CQL) (Christ and Schulze, 1994) queries rapidly on large corpora using the Manatee corpus manager system as its backend which is based on stream processing techniques and has been described in (Rychlý, 2000). We have large (more than 100 million-word) corpora loaded into the tool and accessible over the web for 25 languages and the number rises month by month (Reddy et al., 2010). For English, Italian and German we have very large (more than 1.5 billion-word) corpora, and for English, also a corpus of 5.5 billion words (Pomikálek et al., 2009). We have recently augmented the CQL formalism with extensions to the ‘within’ operator and a new ‘containing’ operator, to better support queries relating to phrase structure. In this paper we first summarise standard CQL, then describe the extensions to the ‘within’ operator, then introduce the ‘containing’ operator, and finally indicate computation times for sample queries. For the same purpose, the ‘meet’ and ‘union’ operator"
