2021.repl4nlp-1.1,Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting,2021,-1,-1,5,1,2443,irene li,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters. We evaluate this framework over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4{\%} in single-source transfer and 8{\%} in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer."
2021.naacl-main.37,{DART}: Open-Domain Structured Data Record to Text Generation,2021,-1,-1,2,0,3324,linyong nan,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart."
2021.naacl-main.57,Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation,2021,-1,-1,7,1,3404,alexander fabbri,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation."
2021.naacl-main.472,{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization,2021,-1,-1,11,0,4653,ming zhong,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \url{https://github.com/Yale-LILY/QMSum}."
2021.findings-emnlp.377,An Exploratory Study on Long Dialogue Summarization: What Works and What{'}s Next,2021,-1,-1,9,0,7346,yusen zhang,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent transformer-based pre-trained models, and the interactive nature of dialogues makes relevant information more context-dependent and sparsely distributed than news articles. In this work, we perform a comprehensive study on long dialogue summarization by investigating three strategies to deal with the lengthy input problem and locate relevant information: (1) extended transformer models such as Longformer, (2) retrieve-then-summarize pipeline models with several dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding models such as HMNet. Our experimental results on three long dialogue datasets (QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline models yield the best performance. We also demonstrate that the summary quality can be further improved with a stronger retrieval model and pretraining on proper external summarization datasets."
2021.findings-acl.435,{D}oc{NLI}: A Large-scale Dataset for Document-level Natural Language Inference,2021,-1,-1,2,0.156578,3569,wenpeng yin,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eval4nlp-1.8,Testing Cross-Database Semantic Parsers With Canonical Utterances,2021,-1,-1,6,0,142,heather lent,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2021.emnlp-demo.37,{S}ummer{T}ime: Text Summarization Toolkit for Non-experts,2021,-1,-1,8,0,7347,ansong ni,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Recent advances in summarization provide models that can generate summaries of higher quality. Such models now exist for a number of summarization tasks, including query-based summarization, dialogue summarization, and multi-document summarization. While such models and tasks are rapidly growing in the research field, it has also become challenging for non-experts to keep track of them. To make summarization methods more accessible to a wider audience, we develop SummerTime by rethinking the summarization task from the perspective of an NLP non-expert. SummerTime is a complete toolkit for text summarization, including various models, datasets, and evaluation metrics, for a full spectrum of summarization-related tasks. SummerTime integrates with libraries designed for NLP researchers, and enables users with easy-to-use APIs. With SummerTime, users can locate pipeline solutions and search for the best model with their own data, and visualize the differences, all with a few lines of code. We also provide explanations for models and evaluation metrics to help users understand the model behaviors and select models that best suit their needs. Our library, along with a notebook demo, is available at https://github.com/Yale-LILY/SummerTime."
2021.acl-short.127,Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders,2021,-1,-1,5,1,2443,irene li,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. Our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. In addition, we expand an existing dataset by introducing two new domains{---}-CV and Bioinformatics (BIO). The annotated data and resources as well as the code will be made publicly available."
2021.acl-long.535,{C}onvo{S}umm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining,2021,-1,-1,7,1,3404,alexander fabbri,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues{--}viewpoints{--}assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations."
2020.emnlp-main.660,Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,2020,-1,-1,3,0.156578,3569,wenpeng yin,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited."
2020.coling-main.99,{R}-{VGAE}: Relational-variational Graph Autoencoder for Unsupervised Prerequisite Chain Learning,2020,31,0,4,1,2443,irene li,Proceedings of the 28th International Conference on Computational Linguistics,0,"The task of concept prerequisite chain learning is to automatically determine the existence of prerequisite relationships among concept pairs. In this paper, we frame learning prerequisite relationships among concepts as an unsupervised task with no access to labeled concept pairs during training. We propose a model called the Relational-Variational Graph AutoEncoder (R-VGAE) to predict concept relations within a graph consisting of concept and resource nodes. Results show that our unsupervised approach outperforms graph-based semi-supervised methods and other baseline methods by up to 9.77{\%} and 10.47{\%} in terms of prerequisite relation prediction accuracy and F1 score. Our method is notably the first graph-based model that attempts to make use of deep learning representations for the task of unsupervised prerequisite learning. We also expand an existing corpus which totals 1,717 English Natural Language Processing (NLP)-related lecture slide files and manual concept pair annotations over 322 topics."
2020.acl-main.706,{ESPRIT}: Explaining Solutions to Physical Reasoning Tasks,2020,64,0,10,0,3344,nazneen rajani,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events. We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach. Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions. Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations. Dataset, code and documentation are available at \url{https://github.com/salesforce/esprit}."
P19-1102,Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model,2019,36,2,5,1,3404,alexander fabbri,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."
P19-1306,Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations,2019,38,0,8,1,3325,rui zhang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks."
P19-1443,{SP}ar{C}: Cross-Domain Semantic Parsing in Context,2019,36,1,19,1,3340,tao yu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2{\%} over all questions and less than10{\%} over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc."
N19-1075,Syntax-aware Neural Semantic Role Labeling with Supertags,2019,26,1,4,0.416667,3374,jungo kasai,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We introduce a new syntax-aware model for dependency-based semantic role labeling that outperforms syntax-agnostic models for English and Spanish. We use a BiLSTM to tag the text with supertags extracted from dependency parses, and we feed these supertags, along with words and parts of speech, into a deep highway BiLSTM for semantic role labeling. Our model combines the strengths of earlier models that performed SRL on the basis of a full dependency parse with more recent models that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system."
D19-1204,{C}o{SQL}: A Conversational Text-to-{SQL} Challenge Towards Cross-Domain Natural Language Interfaces to Databases,2019,0,8,24,1,3340,tao yu,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql."
D19-1537,Editing-Based {SQL} Query Generation for Cross-Domain Context-Dependent Questions,2019,0,8,10,1,3325,rui zhang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to error propagation. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at \url{https://github.com/ryanzhumich/sparc_atis_pytorch}."
P18-2017,Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering,2018,26,2,5,1,3325,rui zhang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Coreference resolution aims to identify in a text all mentions that refer to the same real world entity. The state-of-the-art end-to-end neural coreference model considers all text spans in a document as potential mentions and learns to link an antecedent for each possible mention. In this paper, we propose to improve the end-to-end coreference resolution system by (1) using a biaffine attention model to get antecedent scores for each possible mention, and (2) jointly optimizing the mention detection accuracy and mention clustering accuracy given the mention cluster labels. Our model achieves the state-of-the-art performance on the CoNLL-2012 shared task English test set."
P18-1033,Improving Text-to-{SQL} Evaluation Methodology,2018,0,29,7,1,18974,catherine finegandollak,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development."
P18-1057,"{T}utorial{B}ank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation",2018,18,4,8,1,3404,alexander fabbri,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research."
N18-2093,{T}ype{SQL}: Knowledge-Based Type-Aware Neural Text-to-{SQL} Generation,2018,31,9,5,1,3340,tao yu,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Interacting with relational databases through natural language helps users with any background easily query and analyze a vast amount of data. This requires a system that understands users{'} questions and converts them to SQL queries automatically. In this paper, we present a novel approach TypeSQL which formats the problem as a slot filling task in a more reasonable way. In addition, TypeSQL utilizes type information to better understand rare entities and numbers in the questions. We experiment this idea on the WikiSQL dataset and outperform the prior art by 6{\%} in much shorter time. We also show that accessing the content of databases can significantly improve the performance when users{'} queries are not well-formed. TypeSQL can reach 82.6{\%} accuracy, a 17.5{\%} absolute improvement compared to the previous content-sensitive model."
N18-1089,Robust Multilingual Part-of-Speech Tagging via Adversarial Training,2018,0,15,3,1,3368,michihiro yasunaga,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks."
D18-1193,{S}yntax{SQLN}et: Syntax Tree Networks for Complex and Cross-Domain Text-to-{SQL} Task,2018,0,20,7,1,3340,tao yu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5{\%} in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at \url{https://yale-lily.github.io/seq2sql/spider}."
D18-1425,{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task,2018,0,12,12,1,3340,tao yu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}."
K17-1045,Graph-based Neural Multi-Document Summarization,2017,34,18,6,1,3368,michihiro yasunaga,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems."
P16-1062,Effects of Creativity and Cluster Tightness on Short Text Clustering Performance,2016,44,5,5,1,18974,catherine finegandollak,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms. We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset, but do not help with less creative texts. Yet the choice of similarity metric interacts with the choice of clustering method. We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data. Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset. Thus, the best performing clustering systems could not use semantic metrics."
N16-1008,A Low-Rank Approximation Approach to Learning Joint Embeddings of News Stories and Images for Timeline Summarization,2016,33,13,3,0,3991,william wang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
N16-1177,Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents,2016,34,53,3,1,3325,rui zhang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long ShortTerm Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification."
L16-1076,Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest,2016,-1,-1,1,1,2447,dragomir radev,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community."
L16-1452,Sentence Similarity based on Dependency Tree Kernels for Multi-document Summarization,2016,26,5,3,0,12023,csaziye ozatecs,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce an approach based on using the dependency grammar representations of sentences to compute sentence similarity for extractive multi-document summarization. We adapt and investigate the effects of two untyped dependency tree kernels, which have originally been proposed for relation extraction, to the multi-document summarization problem. In addition, we propose a series of novel dependency grammar based kernels to better represent the syntactic and semantic similarities among the sentences. The proposed methods incorporate the type information of the dependency relations for sentence similarity calculation. To our knowledge, this is the first study that investigates using dependency tree based sentence similarity for multi-document summarization."
L16-1493,Extractive Summarization under Strict Length Constraints,2016,21,2,4,0,3127,yashar mehdad,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we report a comparison of various techniques for single-document extractive summarization under strict length budgets, which is a common commercial use case (e.g. summarization of news articles by news aggregators). We show that, evaluated using ROUGE, numerous algorithms from the literature fail to beat a simple lead-based baseline for this task. However, a supervised approach with lightweight and efficient features improves over the lead-based baseline. Additional human evaluation demonstrates that the supervised approach also performs competitively with a commercial system that uses more sophisticated features."
D16-1006,Nested Propositions in Open Information Extraction,2016,23,13,3,0,16414,nikita bhutani,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
P15-1043,Content Models for Survey Generation: A Factoid-Based Evaluation,2015,26,5,5,1,4654,rahul jha,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present a new factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics in natural language processing. We also introduce a novel HITS-based content model for automated survey article generation called HITSUM that exploits the lexical network structure between sentences from citing and cited papers. Using the factoid-annotated data, we conduct a pyramid evaluation and compare HITSUM with two previous state-of-the-art content models: C-Lexrank, a network based content model, and TOPICSUM, a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outperforms C-Lexrank by 4% and TOPICSUM by 7% in pyramid evaluation."
W14-5317,Experiments in Sentence Language Identification with Groups of Similar Languages,2014,14,9,2,1,37492,ben king,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"Language identification is a simple problem that becomes much more difficult when its usual assumptions are broken. In this paper we consider the task of classifying short segments of text in closely-related languages for the Discriminating Similar Languages shared task, which is broken into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish, and (F) American and British English. We consider a number of different methods to boost classification performance, such as feature selection and data filtering, but we ultimately find that"
Q14-1001,"Heterogeneous Networks and Their Applications: Scientometrics, Name Disambiguation, and Topic Modeling",2014,61,3,3,1,37492,ben king,Transactions of the Association for Computational Linguistics,0,"We present heterogeneous networks as a way to unify lexical networks with relational data. We build a unified ACL Anthology network, tying together the citation, author collaboration, and term-cooccurence networks with affiliation and venue relations. This representation proves to be convenient and allows problems such as name disambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms."
J14-3003,A Random Walk{--}Based Model for Identifying Semantic Orientation,2014,44,3,4,1,576,ahmed hassan,Computational Linguistics,0,"Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification, text filtering, product review analysis, survey response analysis, and on-line discussion mining. We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph, and produces a polarity estimate for any given word. The model can accurately and quickly assign a polarity sign and magnitude to any word. It can be used both in a semi-supervised setting where a training set of labeled words is used, and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes. The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon. We also show how our method can be used for three-way classification which identifies neutral words in addition to positive and negative words. Our experiments show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting and is comparable to the best reported values in the weakly supervised setting. In addition, the proposed method is faster and does not need a large corpus. We also present extensions of our methods for identifying the polarity of foreign words and out-of-vocabulary words."
W13-3403,Introducing Computational Concepts in a Linguistics Olympiad,2013,5,1,4,0,12351,patrick littell,Proceedings of the Fourth Workshop on Teaching {NLP} and {CL},0,"Linguistics olympiads, now offered in more than 20 countries, provide secondary-school students a compelling introduction to an unfamiliar field. The North American Computational Linguistics Olympiad (NACLO) includes computational puzzles in addition to purely linguistic ones. This paper explores the computational subject matter we seek to convey via NACLO, as well as some of the challenges that arise when adapting problems in computational linguistics to an audience that may have no background in computer science, linguistics, or advanced mathematics. We present a small library of reusable design patterns that have proven useful when composing puzzles appropriate for secondary-school students. 1 What is a Linguistics Olympiad? A linguistics olympiad (LO) (Payne and Derzhanski, 2010) is a puzzle contest for secondary-school students in which contestants compete to solve self-contained linguistics problem sets. LOs have their origin in the Moscow Traditional Olympiad in Linguistics, established in 1965, and have since spread around the world; an international contest (http://www.ioling.org) has been held yearly since 2003. In an LO, every problem set is self-contained, so no prior experience in linguistics is necessary to compete. In fact, LO contests are fun and rewarding for exactly this reason: by the end of the contest, contestants are managing to read hieroglyphics, conjugate verbs in Swahili, and perform other amazing feats. Furthermore, they have accomplished this solely through their own analytical abilities and linguistic intuition. Based on our experience going into high schools and presenting our material, this xe2x80x9clinguisticxe2x80x9d way of thinking about languages almost always comes as a novel surprise to students. They largely think about languages as collections of known facts that you learn in classes and from books, not something that you can dive into and figure out for yourself. This is a hands-on antidote to the common public misconception that linguists are fundamentally polyglots, rather than language scientists, and students come out of the experience having realized that linguistics is a very different field (and hopefully a more compelling one) than they had assumed it to be. 2 Computational Linguistics at the LO Our goal, since starting the North American Computational Linguistics Olympiad (NACLO) in 2007 (Radev et al., 2008), has been to explore how this LO experience can be used to introduce students to computational linguistics. Topics in computational linguistics have been featured before in LOs, occasionally in the Moscow LO and with some regularity in the Bulgarian LO. Our deliberations began with some troubling statistics regarding enrollments in computer science programs (Zweben, 2013). Between 2003 and 2007 enrollments in computer science dropped dramatically. This was attributed in part to the dip in the IT sector, but it also stemmed in"
W13-1710,Experimental Results on the Native Language Identification Shared Task,2013,16,5,4,1,39972,amjad abujbara,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present a system for automatically identifying the native language of a writer. We experiment with a large set of features and train them on a corpus of 9,900 essays written in English by speakers of 11 different languages. our system achieved an accuracy of 43% on the test data, improved to 63% with improved feature normalization. In this paper, we present the features used in our system, describe our experiments and provide an analysis of our results."
P13-2045,Random Walk Factoid Annotation for Collective Discourse,2013,9,3,3,1,37492,ben king,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set."
P13-2102,A System for Summarizing Scientific Topics Starting from Keywords,2013,11,11,3,1,4654,rahul jha,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user. We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents. We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing. We have manually annotated 2,625 sentences with these factoids (around 375 sentences per topic) to build an evaluation corpus for this task. We present evaluation results for the performance of our system using this annotated data."
P13-2144,Identifying Opinion Subgroups in {A}rabic Online Discussions,2013,37,22,4,1,39972,amjad abujbara,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results."
N13-1067,Purpose and Polarity of Citation: Towards {NLP}-based Bibliometrics,2013,40,50,3,1,39972,amjad abujbara,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Bibliometric measures are commonly used to estimate the popularity and the impact of published research. Existing bibliometric measures provide xe2x80x9cquantitativexe2x80x9d indicators of how good a published paper is. This does not necessarily reflect the xe2x80x9cqualityxe2x80x9d of the work presented in the paper. For example, when hindex is computed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. In this paper, we propose using NLP to add a xe2x80x9cqualitativexe2x80x9d aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation."
W12-4102,Extracting Signed Social Networks from Text,2012,-1,-1,3,1,576,ahmed hassan,Workshop Proceedings of {T}ext{G}raphs-7: Graph-based Methods for Natural Language Processing,0,None
W12-3201,Rediscovering {ACL} Discoveries Through the Lens of {ACL} {A}nthology Network Citing Sentences,2012,61,8,1,1,2447,dragomir radev,Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries,0,"The ACL Anthology Network (AAN) is a comprehensive manually curated networked database of citations and collaborations in the field of Computational Linguistics. Each citation edge in AAN is associated with one or more citing sentences. A citing sentence is one that appears in a scientific article and contains an explicit reference to another article. In this paper, we shed the light on the usefulness of AAN citing sentences for understanding research trends and summarizing previous discoveries and contributions. We also propose and motivate several different uses and applications of citing sentences."
S12-1043,{UM}ichigan: A Conditional Random Field Model for Resolving the Scope of Negation,2012,18,6,2,1,39972,amjad abujbara,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"In this paper, we present a system for detecting negation in English text. We address three tasks: negation cue detection, negation scope resolution and negated event identification. We pose these tasks as sequence labeling problems. For each task, we train a Conditional Random Field (CRF) model on lexical, structural, and syntactic features extracted from labeled data. The models are trained and tested using the dataset distributed with the *sem Shared Task 2012 on resolving the scope and focus of negation. The system detects negation cues with 90.98% F1 measure (94.3% and 87.88% recall). It identifies negation scope with 82.70% F1 on token-by-token level and 64.78% F1 on full scope level. Negated events are detected with 51.10% F1 measure."
P12-3023,Subgroup Detector: A System for Detecting Subgroups in Online Discussions,2012,13,10,2,1,39972,amjad abujbara,Proceedings of the {ACL} 2012 System Demonstrations,0,"We present Subgroup Detector, a system for analyzing threaded discussions and identifying the attitude of discussants towards one another and towards the discussion topic. The system uses attitude predictions to detect the split of discussants into subgroups of opposing views. The system uses an unsupervised approach based on rule-based opinion target detecting and unsupervised clustering techniques. The system is open source and is freely available for download. An online demo of the system is available at: http://clair.eecs.umich.edu/SubgroupDetector/"
P12-1042,Subgroup Detection in Ideological Discussions,2012,43,44,4,1,39972,amjad abujbara,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results."
N12-3009,{A}ttitude{M}iner: Mining Attitude from Online Discussions,2012,14,2,3,1,39972,amjad abujbara,Proceedings of the Demonstration Session at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This demonstration presents AttitudeMiner, a system for mining attitude from online discussions. AttitudeMiner uses linguistic techniques to analyze the text exchanged between participants of online discussion threads at different levels of granularity: the word level, the sentence level, the post level, and the thread level. The goal of this analysis is to identify the polarity of the attitude the discussants carry towards one another. Attitude predictions are used to construct a signed network representation of the discussion thread. In this network, each discussant is represented by a node. An edge connects two discussants if they exchanged posts. The sign (positive or negative) of the edge is set based on the polarity of the attitude identified in the text associated with the edge. The system can be used in different applications such as: word polarity identification, identifying attitudinal sentences and their signs, signed social network extraction from text, subgroup detect in discussion. The system is publicly available for download and has an online demonstration at http://clair.eecs.umich.edu/AttitudeMiner/."
N12-1009,Reference Scope Identification in Citing Sentences,2012,31,32,2,1,39972,amjad abujbara,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"A citing sentence is one that appears in a scientific article and cites previous work. Citing sentences have been studied and used in many applications. For example, they have been used in scientific paper summarization, automatic survey generation, paraphrase identification, and citation function classification. Citing sentences that cite multiple papers are common in scientific writing. This observation should be taken into consideration when using citing sentences in applications. For instance, when a citing sentence is used in a summary of a scientific paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary. In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results."
D12-1006,Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants,2012,39,53,3,1,576,ahmed hassan,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them."
W11-1107,Simultaneous Similarity Learning and Feature-Weight Learning for Document Clustering,2011,-1,-1,2,1,44364,pradeep muthukrishnan,Proceedings of {T}ext{G}raphs-6: Graph-based Methods for Natural Language Processing,0,None
P11-4021,"{C}lairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis",2011,12,2,2,1,39972,amjad abujbara,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"In this paper we present Clairlib, an open-source toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos."
P11-2104,Identifying the Semantic Orientation of Foreign Words,2011,22,30,4,1,576,ahmed hassan,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results."
P11-1051,Coherent Citation-Based Summarization of Scientific Papers,2011,24,76,2,1,39972,amjad abujbara,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency."
P11-1110,Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice,2011,40,14,2,1,42828,vahed qazvinian,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity."
D11-1147,Rumor has it: Identifying Misinformation in Microblogs,2011,26,395,3,1,42828,vahed qazvinian,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"A rumor is commonly defined as a statement whose true value is unverifiable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-specific memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Finally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations."
P10-1041,Identifying Text Polarity Using Random Walks,2010,24,114,2,1,576,ahmed hassan,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word related-ness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus."
P10-1057,Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.,2010,25,70,2,1,42828,vahed qazvinian,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Identifying background (context) information in scientific articles can help scholars understand major contributions in their research area more easily. In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientific papers. We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create, and employ a Belief Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone."
D10-1121,What{'}s with the Attitude? Identifying Sentences with Attitude in Online Discussions,2010,63,67,3,1,576,ahmed hassan,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Mining sentiment from user generated content is a very important task in Natural Language Processing. An example of such content is threaded discussions which act as a very important tool for communication and collaboration in the Web. Threaded discussions include e-mails, e-mail lists, bulletin boards, newsgroups, and Internet forums. Most of the work on sentiment analysis has been centered around finding the sentiment toward products or topics. In this work, we present a method to identify the attitude of participants in an online discussion toward one another. This would enable us to build a signed network representation of participant interaction where every edge has a sign that indicates whether the interaction is positive or negative. This is different from most of the research on social networks that has focused almost exclusively on positive links. The method is experimentally tested using a manually labeled set of discussion posts. The results show that the proposed method is capable of identifying attitudinal sentences, and their signs, with high accuracy and that it outperforms several other baselines."
C10-1101,Citation Summarization Through Keyphrase Extraction,2010,25,62,2,1,42828,vahed qazvinian,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper presents an approach to summarize single scientific papers, by extracting its contributions from the set of citation sentences written in other papers. Our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary. Comparisons show how this methodology excels at the task of single paper summarization, and how it out-performs other multi-document summarization methods."
W09-3607,The {ACL} {A}nthology Network,2009,2,6,1,1,2447,dragomir radev,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL}),0,"We introduce the ACL Anthology Network (AAN), a manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks."
W09-1416,Supervised Classification for Extracting Biomedical Events,2009,5,12,2,1,1778,arzucan ozgur,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"We introduce a supervised approach for extracting bio-molecular events by using linguistic features that represent the contexts of the candidate event triggers and participants. We use Support Vector Machines as our learning algorithm and train separate models for event types that are described with a single theme participant, multiple theme participants, or a theme and a cause participant. We perform experiments with linear kernel and edit-distance based kernel and report our results on the BioNLP'09 Shared Task test data set."
N09-1066,Using Citations to Generate surveys of Scientific Paradigms,2009,27,81,7,0,13005,saif mohammad,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role."
D09-1145,Detecting Speculations and their Scopes in Scientific Text,2009,15,49,2,1,1778,arzucan ozgur,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Distinguishing speculative statements from factual ones is important for most biomedical text mining applications. We introduce an approach which is based on solving two sub-problems to identify speculative sentence fragments. The first sub-problem is identifying the speculation keywords in the sentences and the second one is resolving their linguistic scopes. We formulate the first sub-problem as a supervised classification task, where we classify the potential keywords as real speculation keywords or not by using a diverse set of linguistic features that represent the contexts of the keywords. After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes."
W08-0211,The {N}orth {A}merican Computational Linguistics Olympiad ({NACLO}),2008,2,5,1,1,2447,dragomir radev,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"NACLO (North American Computational Linguistics Olympiad) is an annual Olympiad-style contest for high school students, focusing on linguistics, computational linguistics, and language technologies."
bird-etal-2008-acl,The {ACL} {A}nthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics,2008,15,134,9,0,8953,steven bird,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research."
otterbacher-radev-2008-modeling,Modeling Document Dynamics: an Evolutionary Approach,2008,10,1,2,1,33596,jahna otterbacher,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"News articles about the same event published over time have properties that challenge NLP and IR applications. A cluster of such texts typically exhibits instances of paraphrase and contradiction, as sources update the facts surrounding the story, often due to an ongoing investigation. The current hypothesis is that the stories evolve over time, beginning with the first text published on a given topic. This is tested using a phylogenetic approach as well as one based on language modeling. The fit of the evolutionary models is evaluated with respect to how well they facilitate the recovery of chronological relationships between the documents. Over all data clusters, the language modeling approach consistently outperforms the phylogenetics model. However, on manually collected clusters in which the documents are published within short time spans of one another, both have a similar performance, and produce statistically significant results on the document chronology recovery evaluation."
C08-1040,Tracking the Dynamic Evolution of Participants Salience in a Discussion,2008,19,7,7,1,576,ahmed hassan,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,We introduce a technique for analyzing the temporal evolution of the salience of participants in a discussion. Our method can dynamically track how the relative importance of speakers evolve over time using graph based techniques. Speaker salience is computed based on the eigenvector centrality in a graph representation of participants in a discussion. Two participants in a discussion are linked with an edge if they use similar rhetoric. The method is dynamic in the sense that the graph evolves over time to capture the evolution inherent to the participants salience. We used our method to track the salience of members of the US Senate using data from the US Congressional Record. Our analysis investigated how the salience of speakers changes over time. Our results show that the scores can capture speaker centrality in topics as well as events that result in change of salience or influence among different participants.
C08-1077,Detecting Multiple Facets of an Event using Graph-Based Unsupervised Methods,2008,18,3,3,1,44364,pradeep muthukrishnan,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,We propose a new unsupervised method for topic detection that automatically identifies the different facets of an event. We use pointwise Kullback-Leibler divergence along with the Jaccard coefficient to build a topic graph which represents the community structure of the different facets. The problem is formulated as a weighted set cover problem with dynamically varying weights. The algorithm is domain-independent and generates a representative set of informative and discriminative phrases that cover the entire event. We evaluate this algorithm on a large collection of blog postings about different news events and report promising results.
C08-1087,Scientific Paper Summarization Using Citation Summary Networks,2008,27,148,2,1,42828,vahed qazvinian,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others' viewpoint of the target article's contributions and the study of its citation summary network using a clustering approach."
D07-1024,Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing,2007,28,108,3,1,49348,gunecs erkan,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We introduce a relation extraction method to identify the sentences in biomedical text that indicate an interaction among the protein names mentioned. Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences. Given two dependency trees, we define two separate similarity functions (kernels) based on cosine similarity and edit distance among the paths between the protein names. Using these similarity functions, we investigate the performances of two classes of learning algorithms, Support Vector Machines and k-nearest-neighbor, and the semisupervised counterparts of these algorithms, transductive SVMs and harmonic functions, respectively. Significant improvement over the previous results in the literature is reported as well as a new benchmark dataset is introduced. Semi-supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited."
D07-1069,{M}aven{R}ank: Identifying Influential Members of the {US} Senate Using Lexical Centrality,2007,22,17,2,1,34024,anthony fader,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We introduce a technique for identifying the most salient participants in a discussion. Our method, MavenRank is based on lexical centrality: a random walk is performed on a graph in which each node is a participant in the discussion and an edge links two participants who use similar rhetoric. As a test, we used MavenRank to identify the most influential members of the US Senate using data from the US Congressional Record and used committee ranking to evaluate the output. Our results show that MavenRank scores are largely driven by committee status in most topics, but can capture speaker centrality in topics where speeches are used to indicate ideological position instead of influence legislation."
P06-4012,{L}ex{N}et: A Graphical Environment for Graph-Based {NLP},2006,9,2,1,1,2447,dragomir radev,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification). All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment."
P06-2096,Adding Syntax to Dynamic Programming for Aligning Comparable Texts for the Generation of Paraphrases,2006,12,15,2,0,49896,siwei shen,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Multiple sequence alignment techniques have recently gained popularity in the Natural Language community, especially for tasks such as machine translation, text generation, and paraphrase identification. Prior work falls into two categories, depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text) or (b) comparable texts (non-parallel but on the same topic). So far, only techniques based on parallel texts have successfully used syntactic information to guide alignments. In this paper, we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts. Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences. Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammaticality and fidelity when computed over the novel sentences generated by alignment-induced finite state automata."
N06-5003,Graph-based Algorithms for Natural Language Processing and Information Retrieval,2006,0,3,2,0,1124,rada mihalcea,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Tutorial Abstracts",0,"Graph theory is a well studied discipline, and so are the fields of natural language processing and information retrieval. However, most of the times, they are perceived as different disciplines, with different algorithms, different applications, and different potential end-users."
patel-radev-2006-lexical,Lexical similarity can distinguish between automatic and manual translations,2006,6,2,2,0,49948,agam patel,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We consider the problem of identifying automatic translations from manual translations of the same sentence. Using two different similarity metrics (BLEU and Levenshtein edit distance), we found out that automatic translations are closer to each other than they are to manual translations. We also use phylogenetic trees to provide a visual representation of the distances between pairs of individual sentences in a set of translations. The differences in lexical distance are statistically significant, both for Chinese to English and for Arabic to English translations."
H05-1115,Using Random Walks for Question-focused Sentence Retrieval,2005,17,144,3,1,33596,jahna otterbacher,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. How many victims have been found?) Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."
W04-3247,{L}ex{P}age{R}ank: Prestige in Multi-Document Text Summarization,2004,6,238,2,1,49348,gunecs erkan,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
N04-4031,Computational Linkuistics: Word Triggers across Hyperlinks,2004,11,0,1,1,2447,dragomir radev,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"It is known that context words tend to be self-triggers, that is, the probability of a content word to appear more than once in a document, given that it already appears once, is significantly higher than the probability of the first occurrence. We look at self-triggerability across hyperlinks on the Web. We show that the probability of a word wj to appear in a Web document di depends on the presence of wj in documents pointing to di. In Document Modeling, we will propose the use of a correction factor, R, which indicates how much more likely a word is to appear in a document given that another document containing the same word is linked to it."
N04-3007,A Scaleable Multi-document Centroid-based Summarizer,2004,4,1,1,1,2447,dragomir radev,Demonstration Papers at {HLT}-{NAACL} 2004,0,"We are presenting the most recent version of MEAD (v. 3.08), a large-scale public-domain summarizer that has been used in a number of applications, including the 2001 JHU summer workshop and the NewsInEssence project (www.newsinessence.com). A version of MEAD finished in first place on task 4 at DUC 2003 and finished in the top three on two other tasks."
N04-1021,A Smorgasbord of Features for Statistical Machine Translation,2004,12,254,13,0,37712,franz och,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation.
otterbacher-radev-2004-revisionbank,{R}evision{B}ank: A Resource for Revision-based Multi-document Summarization and Evaluation,2004,9,2,2,1,33596,jahna otterbacher,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Multi-document summaries produced via sentence extraction often suffer from a number of cohesion problems, including dangling anaphora, sudden shifts in topic and incorrect or awkward chronological ordering. Therefore, the development of an automated revision process to correct such problems is a research area of current interest. We present the RevisionBank, a corpus of 240 extractive, multi-document summaries that have been manually revised to promote cohesion. The summaries were revised by six linguistic students using a constrained set of revision operations that we previously developed. In the current paper, we describe the process of developing a taxonomy of cohesion problems and corrective revision operators that address such problems, as well as an annotation schema for our corpus. Finally, we discuss how our taxonomy and corpus can be used for the study of revision-based multi-document summarization as well as for summary evaluation."
radev-etal-2004-cst,{CST} Bank: A Corpus for the Study of Cross-document Structural Relationships,2004,9,21,1,1,2447,dragomir radev,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Clusters of multiple news stories related to the same topic exhibit a number of interesting properties. For example, when documents have been published at various points in time or by different authors or news agencies, one finds many instances of paraphrasing, information overlap and even contradiction. The current paper presents the Cross-document Structure Theory (CST) Bank, a collection of multi-document clusters in which pairs of sentences from different documents have been annotated for cross-document structure theory relationships. We will describe how we built the corpus, including our method for reducing the number of sentence pairs to be annotated by our hired judges, using lexical similarity measures. Finally, we will describe how CST and the CST Bank can be applied to different research areas such as multi-document summarization."
radev-etal-2004-mead,{MEAD} - A Platform for Multidocument Multilingual Text Summarization,2004,11,273,1,1,2447,dragomir radev,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Abstract This paper describes the functionality of MEAD, a comprehensive, public domain, open source, multidocument multilingual summarization environment that has been thus far downloaded by more than 500 organizations. MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection."
C04-1184,Comparing Semantically Related Sentences: The Case of Paraphrase Versus Subsumption,2004,8,2,2,1,33596,jahna otterbacher,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Paraphrases and other semantically related sentences present a challenge to NLP and IR applications such as multi-document summarization and question answering systems. While it is generally agreed that paraphrases contain approximately equivalent ideas, they often differ from one another in subtle, yet non-trivial, ways. In this paper, we examine semantic differences in cases of paraphrase and subsumption, in an effort to understand what makes one sentence significantly more informative than another. Using manually annotated data from the news domain, we concentrate on developing a framework for analyzing and comparing pairs of related sentences."
W03-0502,Sub-event based multi-document summarization,2003,13,54,2,0,52758,naomi daniel,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,"The production of accurate and complete multiple-document summaries is challenged by the complexity of judging the usefulness of information to the user. Our aim is to determine whether identifying sub-events in a news topic could help us capture essential information to produce better summaries. In our first experiment, we asked human judges to determine the relative utility of sentences as they related to the sub-events of a larger topic. We used this data to create summaries by three different methods, and we then compared these summaries with three automatically created summaries. In our second experiment, we show how the results of our first experiment can be applied to a cluster-based automatic summarization system. Through both experiments, we examine the use of inter-judge agreement and a relative utility metric that accounts for the complexity of determining sentence quality in relation to a topic."
W03-0503,Multi-document summarization using off the shelf compression software,2003,14,4,4,0,52759,amardeep grewal,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,"This study examines the usefulness of common off the shelf compression software such as gzip in enhancing already existing summaries and producing summaries from scratch. Since the gzip algorithm works by removing repetitive data from a file in order to compress it, we should be able to determine which sentences in a summary contain the least repetitive data by judging the gzipped size of the summary with the sentence compared to the gzipped size of the summary without the sentence. By picking the sentence that increased the size of the summary the most, we hypothesized that the summary will gain the sentence with the most new information. This hypothesis was found to be true in many cases and to varying degrees in this study."
P03-1048,Evaluation Challenges in Large-Scale Document Summarization,2003,18,92,1,1,2447,dragomir radev,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We present a large-scale meta evaluation of eight evaluation measures for both single-document and multi-document summarizers. To this end we built a corpus consisting of (a) 100 Million automatic summaries using six summarizers and baselines at ten summary lengths in both English and Chinese, (b) more than 10,000 manual abstracts and extracts, and (c) 200 Million automatic document and summary retrievals using 20 queries. We present both qualitative and quantitative results showing the strengths and draw-backs of all evaluation methods and how they rank the different summarizers."
W02-0404,Revisions that improve cohesion in multi-document summaries: a preliminary study,2002,13,45,2,1,33596,jahna otterbacher,Proceedings of the {ACL}-02 Workshop on Automatic Summarization,0,"Extractive summaries produced from multiple source documents suffer from an array of problems with respect to text cohesion. In this preliminary study, we seek to understand what problems occur in such summaries and how often. We present an analysis of a small corpus of manually revised summaries and discuss the feasibility of making such repairs automatically. Additionally, we present a taxonomy of the problems that occur in the corpus, as well as the operators which, when applied to the summaries, can address these concerns. This study represents a first step toward identifying and automating revision operators that could work with current summarization systems in order to repair cohesion problems in multi-document summaries."
saggion-etal-2002-developing,Developing Infrastructure for the Evaluation of Single and Multi-document Summarization Systems in a Cross-lingual Environment,2002,29,29,2,0,5986,horacio saggion,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe our work on the development of Language and Evaluation Resources for the evaluation of summaries in English and Chinese. The language resources include a parallel corpus of English and Chinese texts which are translations of each other, a set of queries in both languages, clusters of documents relevants to each query, sentence relevance measures for each sentence in the document clusters, and manual multi-document summaries at different compression rates. The evaluation resources consist of metrics for measuring the content of automatic summaries against reference summaries. The framework can be used in the evaluation of extractive, non-extractive, single and multi-document summarization. We focus on the resources developed that are made available for the research community."
radev-etal-2002-evaluating,Evaluating Web-based Question Answering Systems,2002,5,75,1,1,2447,dragomir radev,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The official evaluation of TREC-style Q&A systems is done manually, which is quite expensive and not scalable to web-based Q&A systems. An automatic evaluation technique is needed for dynamic Q&A systems. This paper presents a set of metrics that have been implemented in our web-based Q&A system, namely NSIR. It also shows the correlations between the different metrics."
J02-4001,Introduction to the Special Issue on Summarization,2002,54,319,1,1,2447,dragomir radev,Computational Linguistics,0,"generation based on rhetorical structure extraction. In Proceedings of the International Conference on Computational Linguistics, Kyoto, Japan, pages 344xe2x80x93348. Otterbacher, Jahna, Dragomir R. Radev, and Airong Luo. 2002. Revisions that improve cohesion in multi-document summaries: A preliminary study. In ACL Workshop on Text Summarization, Philadelphia. Papineni, K., S. Roukos, T. Ward, and W-J. Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Research Report RC22176, IBM. Radev, Dragomir, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Arda Celebi, Hong Qi, Elliott Drabek, and Danyu Liu. 2002. Evaluation of text summarization in a cross-lingual information retrieval framework. Technical Report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, June. Radev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies. In ANLP/NAACL Workshop on Summarization, Seattle, April. Radev, Dragomir R. and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469xe2x80x93500. Rau, Lisa and Paul Jacobs. 1991. Creating segmented databases from free text for text retrieval. In Proceedings of the 14th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, New York, pages 337xe2x80x93346. Saggion, Horacio and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4), 497xe2x80x93526. Salton, G., A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing & Management, 33(2):193xe2x80x93207. Silber, H. Gregory and Kathleen McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4), 487xe2x80x93496. Sparck Jones, Karen. 1999. Automatic summarizing: Factors and directions. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 1xe2x80x9313. Strzalkowski, Tomek, Gees Stein, J. Wang, and Bowden Wise. 1999. A robust practical text summarizer. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 137xe2x80x93154. Teufel, Simone and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4), 409xe2x80x93445. White, Michael and Claire Cardie. 2002. Selecting sentences for multidocument summaries using randomized local search. In Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July. Association for Computational Linguistics, New Brunswick, NJ, pages 9xe2x80x9318. Witbrock, Michael and Vibhu Mittal. 1999. Ultra-summarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, pages 315xe2x80x93316. Zechner, Klaus. 2002. Automatic summarization of open-domain multiparty dialogues in diverse genres. Computational Linguistics, 28(4), 447xe2x80x93485."
C02-1073,Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics,2002,17,41,2,0,5986,horacio saggion,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We describe a framework for the evaluation of summaries in English and Chinese using similarity measures. The framework can be used to evaluate extractive, non-extractive, single and multi-document summarization. We focus on the resources developed that are made available for the research community."
H01-1006,Answering What-Is Questions by Virtual Annotation,2001,8,34,2,0,43764,john prager,Proceedings of the First International Conference on Human Language Technology Research,0,"We present the technique of Virtual Annotation as a specialization of Predictive Annotation for answering definitional What is questions. These questions generally have the property that the type of the answer is not given away by the question, which poses problems for a system which has to select answer strings from suggested passages. Virtual Annotation uses a combination of knowledge-based techniques using an ontology, and statistical techniques using a large corpus to achieve high precision."
H01-1056,"{N}ews{I}n{E}ssence: A System For Domain-Independent, Real-Time News Clustering and Multi-Document Summarization",2001,1,69,1,1,2447,dragomir radev,Proceedings of the First International Conference on Human Language Technology Research,0,"NEWSINESSENCE is a system for finding, visualizing and summarizing a topic-based cluster of news stories. In the generic scenario for NEWSINESSENCE, a user selects a single news story from a news Web site. Our system then searches other live sources of news for other stories related to the same event and produces summaries of a subset of the stories that it finds, according to parameters specified by the user."
W00-1110,Automatic summarization of search engine hit lists,2000,12,47,1,1,2447,dragomir radev,{ACL}-2000 Workshop on Recent Advances in Natural Language Processing and Information Retrieval,0,"We present our work on open-domain multi-document summarization in the framework of Web search. Our system, SNS (pronounced essence), retrieves documents related to an unrestricted user query and summarizes a subset of them as selected by the user. We present a task-based extrinsic evaluation of the quality of the produced multi-document summaries. The evaluation results show that summarization quality is relatively high and does help improve the reading speed and judge the relevance of the retrieved URLs."
W00-1009,A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure,2000,21,173,1,1,2447,dragomir radev,1st {SIG}dial Workshop on Discourse and Dialogue,0,"We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."
W00-0403,"Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",2000,10,427,1,1,2447,dragomir radev,NAACL-ANLP 2000 Workshop: Automatic Summarization,0,"We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization."
A00-1021,Ranking suspected answers to natural language questions using predictive annotation,2000,8,53,1,1,2447,dragomir radev,Sixth Applied Natural Language Processing Conference,0,"In this paper, we describe a system to rank suspected answers to natural language questions. We process both corpus and query using a new technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions. Given a natural language question, our IR system returns a set of matching passages, which we then rank using a linear function of seven predictor variables. We provide an evaluation of the techniques based on results from the TREC Q&A evaluation in which our system participated."
P98-2176,Learning Correlations between Linguistic Indicators and Semantic Constraints: Reuse of Context-Dependent Descriptions of Entities,1998,13,8,1,1,2447,dragomir radev,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method."
J98-3005,Generating Natural Language Summaries from Multiple On-Line Sources,1998,57,362,1,1,2447,dragomir radev,Computational Linguistics,0,"We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing."
C98-2171,Learning Correlations between Linguistic Indicators and Semantic Constraints: Reuse of Context-Dependent Descriptions of Entities,1998,13,8,1,1,2447,dragomir radev,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method."
W97-0903,Software Re-Use and Evolution in Text Generation Applications,1997,11,1,4,0,54349,karen kukich,From Research to Commercial Applications: Making {NLP} Work in Practice,0,None
A97-1033,Building a Generation Knowledge Source using {I}nternet-Accessible Newswire,1997,16,3,1,1,2447,dragomir radev,Fifth Conference on Applied Natural Language Processing,0,"In this paper, we describe a method for automatic creation of a knowledge source for text generation using information extraction over the Internet. We present a prototype system called PROFILE which uses a client-server architecture to extract noun-phrase descriptions of entities such as people, places, and organizations. The system serves two purposes: as an information extraction tool, it allows users to search for textual descriptions of entities; as a utility to generate functional descriptions (FD), it is used in a functional-unification based generation system. We present an evaluation of the approach and its applications to natural language generation and summarization."
W96-0512,An Architecture For Distributed Natural Language Summarization,1996,4,4,1,1,2447,dragomir radev,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,None
W96-0101,Using Word Class for Part-of-speech Disambiguation,1996,16,16,2,0,4939,evelyne tzoukermann,Fourth Workshop on Very Large Corpora,0,None
