2008.eamt-1.6,A94-1016,0,0.0304311,"ng such combinations of rule-based and statistical knowledge sources, one of the approaches being an integration of existing rule-based MT systems into a multi-engine architecture. This paper describes several incarnations of such multi-engine architectures within the project. A careful analysis of the results will guide us in the choice of further steps towards the construction of hybrid MT systems for practical applications. 2 2.1 Merging multiple MT results via a SMT decoder Architecture Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with [4]. Multi-engine systems can be roughly divided into simple architectures that try to select the best output from a number of systems but leave the individual hypotheses as is on the one hand [5–10], and more sophisticated setups on the other hand that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in [11–16]. Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not"
2008.eamt-1.6,C00-2122,0,0.0602105,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.3,0,0.0464118,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.12,0,0.0319946,"Missing"
2008.eamt-1.6,C02-1076,0,0.0555903,"Missing"
2008.eamt-1.6,P04-1063,0,0.0385466,"Missing"
2008.eamt-1.6,W05-0828,1,0.796901,"Missing"
2008.eamt-1.6,hogan-frederking-1998-evaluation,0,0.0443059,"Missing"
2008.eamt-1.6,P05-3026,0,0.029568,"Missing"
2008.eamt-1.6,E06-1005,0,0.0371106,"Missing"
2008.eamt-1.6,N07-1029,0,0.0640876,"Missing"
2008.eamt-1.6,W08-0328,1,0.816746,"combinations of highly probable partial translations. Instead of implementing a special-purpose search procedure from scratch, we transform the information contained in the MT output into a form that is suitable as input for an existing SMT decoder. This has the additional advantage that it is simple to combine resources used in standard phrase-based SMT with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. This architecture is described in more detail in [17], where also examples of results are given. It should be noted that this is certainly not the only way to combine systems. In particular, as this proposed 28 12th EAMT conference, 22-23 September 2008, Hamburg, Germany setup gives the last word to the SMT decoder, there is the risk that linguistically well-formed constructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a"
2008.eamt-1.6,P07-2045,0,0.00439401,"tructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a set of six rule-based MT engines that are partly available via web interfaces and partly installed locally. In addition to these engines, we generated phrase tables from the training data following the baseline methodology given in the description of the shared task and using the scripts included in the Moses toolkit [18]. In order to improve alignment quality, the source text and the output text of the MT systems were aligned with the help of a modified version of GIZA++ that it is able to load given models and which is embedded into a client-server setup, as described in [19]. The original Moses phrase table and separate phrase tables for each of the RBMT systems were then combined into a unified phrase table. By combining domain-specific lexical knowledge learned from the training data with more general knowledge contained in the linguistic rules, the hybrid system can both handle a wider range of syntactic"
2008.eamt-1.6,W08-0309,0,0.0966573,"sess about the particular vocabulary of the source text. 2.2 Results We submitted the results of the hybrid system as well as the results from each of the rule-based systems (suitably anonymized) to the shared task of the WMT 2008 workshop. This gives us the opportunity to compare the results with many other systems under fair conditions, both using automatic evaluation metric and comparisons involving human inspection. Detailed results of this evaluation are Fig. 1. Relative performance of system types for in-domain (EuroParl, upper row) and out-of-domain (News, lower row) data documented in [20]. By condensing several of the tables into a joint plot, it 29 12th EAMT conference, 22-23 September 2008, Hamburg, Germany becomes easier to see some of the salient patterns contained in these datasets. Fig. 1 project the results of two different types of human evaluation into twodimensional plots, and it is interesting to study the different behavior of the systems that depend strongly on whether the tests are done on data from the same or from a different domain as the training data. The plot displays the relative performance of the systems for the directions German ↔ English according to s"
2008.eamt-1.6,2001.mtsummit-papers.39,0,0.0119217,"lemma information. The two representations are then combined and filters based on PoS sequences on both sides are used to obtain a set of candidates for the lexicon. A list of acceptable pairs of PoS sequences is generated by inspecting several hundred of the most frequently occurring PoS sequences and excluding those that either do not form a pair of linguistic phrases or where the interpretation on both sides is incompatible. Morphological classification is applied to these lexical entries to augment them with inflection classes, following the open lexicon interchange format (OLIF) standard [21]. Even if statistical alignment and linguistic preprocessing can lead us a long way towards the automatic creation of lexical entries, it is crucial to manually inspect and correct the results because rule-based MT systems have no other mechanism for preventing errors from incorrect lexical entries. In cases of technical terminology, the validation of the terminology requires both linguistic and technical competence and it may be necessary to distribute some steps over different groups of people. In order to facilitate this process, we have built up a web-based front end for lexical database m"
2008.eamt-1.6,W07-0732,0,0.0135969,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,W07-0728,0,0.0198984,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,vilar-etal-2006-error,0,0.026094,"SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if one or both of these approaches can be made to deliver significant improvements under fairly general conditions, the improvements will essentially only alleviate the problem of lexical coverage but will not touch some other well"
2013.mtsummit-wptp.2,2003.mtsummit-systems.1,0,0.0711019,"1 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), se"
2013.mtsummit-wptp.2,federmann-2010-appraise,0,0.0122073,"e sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), select the translation output which is easiest to post-edit and perform the editing. Post-edit all: for each source sentence in the selected subset (4070 sentences in total), postedit all four produced translation outputs. For both post-editing tasks, the translators"
2013.mtsummit-wptp.2,P10-1064,0,0.032193,"Missing"
2013.mtsummit-wptp.2,W12-3123,0,0.0120168,"proved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professi"
2013.mtsummit-wptp.2,2011.eamt-1.12,0,0.0122675,", five types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, si"
2013.mtsummit-wptp.2,2012.amta-wptp.9,0,0.0175217,"dding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by trans"
2013.mtsummit-wptp.2,W10-1738,1,0.708534,"n-de es-de fr-de Total News 1788 514 912 1744 101 1852 6911 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and p"
2014.eamt-1.38,W13-2201,0,0.100476,"Missing"
2014.eamt-1.38,W11-2107,0,0.017106,"n examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result of these limitations, there has been a"
2014.eamt-1.38,2010.eamt-1.12,0,0.209351,"Missing"
2014.eamt-1.38,1994.amta-1.9,0,0.935888,"lity Metric” MQM designed by the QTLaunchPad project (http://www.qt21.eu/launchpad). The metric was designed to facilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model,"
2014.eamt-1.38,P02-1040,0,0.0903186,"ticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result o"
2014.eamt-1.38,stymne-ahrenberg-2012-practice,0,0.494563,"cilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model, SAE J2450, TAUS DQF, ATA assessment, and various tool-specific metrics) that served as the basis for a descriptive framework f"
2014.eamt-1.38,vilar-etal-2006-error,0,0.85614,"Missing"
2014.eamt-1.41,2011.mtsummit-papers.17,0,0.162598,"– Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas removing additions ha"
2014.eamt-1.41,W12-3102,0,0.0667545,"Missing"
2014.eamt-1.41,2012.eamt-1.35,0,0.0190046,"(Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2012 ok 323 31 200 quality level edit+ edit edit1559 0 544 399 0 550 548 856 576 make the translation acceptable. Post-editing time is measured on the sentence level in a controlled way in order to isolate factors such as pauses between sentences. The technical effort is represented by following five types of edit operations: bad 99 20 74 Table 1: Corpus statistics: number of sentences assigned to each"
2014.eamt-1.41,W12-3123,0,0.0661415,"under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 191 More details about the technical effort can be obtained by analysing particular edit operations. (Blain et al., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al"
2014.eamt-1.41,J11-4002,1,0.862953,"Missing"
2014.eamt-1.41,specia-etal-2010-dataset,0,0.058975,"Missing"
2014.eamt-1.41,2011.eamt-1.12,0,0.111983,"szkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas"
2014.eamt-1.41,2009.mtsummit-posters.20,0,0.295234,"e Lommel, Aljoscha Burchardt, Eleftherios Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most c"
2014.eamt-1.41,2010.jec-1.6,0,0.0874217,"Missing"
2014.eamt-1.41,temnikova-2010-cognitive,0,0.116741,"Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most"
2014.eamt-1.41,2013.mtsummit-papers.15,0,0.0298537,"., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2"
A97-1014,C96-1020,0,0.0273106,"Missing"
A97-1014,A92-1018,0,0.0608614,"as the option of altering the assigned tags. 3) Additionally, the program performs simple bracketing, i.e., finds &apos;kernel&apos; phrases. 4) Tile tagger suggests partial or cornplete parses. So far, about 1100 sentences of our corpus have been annotated. This amount of data suffices as training material to reliably assign the g r a m m a t i c a l functions if the user determines the elements of a phrase and its type (step 1 of the list above). 5.4 Assigning GramInatical Function Labels G r a m m a t i c a l functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). For a phrase Q with children of type T . . . . . . T~: and g r a m m a t i c a l fimctions G , , . . . , (7~:, we use the lexical probabilities PO(GiITi) and the contextual (trigram) probabilities PQ(T; [Ti-,, Ti-~ ) 92 i=1 To keep the h u m a n annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label Gi. If its probability is close to the winner (closeness is defined by a threshold on the quotient), the assignment is regarded as unreliable, and the annotator is asked to confirm the assignment. For evaluation, th"
A97-1014,P95-1024,0,0.0151924,"ally, the structural handling of free word order means stating well-formedness constraints on structures involving m a n y trace-filler dependencies, which ha:s proved tedious. Since most methods of handling discontinuous constituents make the fornaalism more powerfifl, the efficiency of processing deteriorates, too. An Mternative solution is to make argurnent structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent ba.ckbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic ibr phrase-structure g r a m m a r s (clause union, extraposition, diverse second-position phenomena). 2.4 Annotating Adv~ V NP#2 NP I I V / e#e e#.~erkennen, Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees s"
A97-1014,C96-2120,0,0.00975821,"Missing"
A97-1014,H94-1020,0,0.445291,"Missing"
A97-2016,C96-1020,0,0.0235451,"Missing"
A97-2016,H94-1020,0,0.0905533,"Missing"
A97-2016,A97-1014,1,0.804177,"Missing"
adolphs-etal-2010-question,W08-1301,0,\N,Missing
ai-etal-2014-sprinter,copestake-flickinger-2000-open,0,\N,Missing
ai-etal-2014-sprinter,bunt-2006-dimensions,0,\N,Missing
ai-etal-2014-sprinter,C12-2127,0,\N,Missing
ai-etal-2014-sprinter,ai-charfuelan-2014-mat,1,\N,Missing
avramidis-etal-2014-taraxu,federmann-2010-appraise,0,\N,Missing
avramidis-etal-2014-taraxu,avramidis-etal-2012-involving,1,\N,Missing
avramidis-etal-2014-taraxu,W10-1738,1,\N,Missing
avramidis-etal-2014-taraxu,P07-2045,0,\N,Missing
avramidis-etal-2014-taraxu,2013.mtsummit-posters.5,1,\N,Missing
baumann-etal-2004-muli,J98-2001,0,\N,Missing
baumann-etal-2004-muli,W01-1612,0,\N,Missing
baumann-etal-2004-muli,H94-1020,0,\N,Missing
buitelaar-etal-2004-evaluation,W03-1302,1,\N,Missing
buitelaar-etal-2004-evaluation,E03-2012,1,\N,Missing
buitelaar-etal-2004-evaluation,A00-1031,0,\N,Missing
buitelaar-etal-2004-evaluation,vintar-etal-2002-efficient,1,\N,Missing
C00-1005,C88-1010,0,0.0708934,"the specification of a shared grammar can furthermore be exploited for simplifying the transfer process. Without much ado, computational linguists engaged in multilingual grammar development have always tried to reduce their labour by importing existing grammar components in a simple ""copy-pastemodify"" fashion. But there were also a number of systematic attempts to create and describe shared grammars that are convincingly documented in publications. [Kam88] demonstrates the concept for a relatively restricted domain, the grammatical description of simple nominal expressions in five languages. [BOP88] were able to exploit the grammatical overlap of two Slavic languages, for the design of a lean transfer process in Russian to Czech machine translation. In multilingual application development within Microsoft research, grammar sharing has extensively been exploited ± [Pin96], [GLPR97]. However, all these approaches are rather opportunistic in the sense that existing grammatical descriptions based on existing grammar models were explored. We went a step further and started grammar design with a notion of a shared grammar for a family of related languages. Pursuing the goal of designing lingui"
C00-1005,W97-0908,0,0.0278334,"copy-pastemodify"" fashion. But there were also a number of systematic attempts to create and describe shared grammars that are convincingly documented in publications. [Kam88] demonstrates the concept for a relatively restricted domain, the grammatical description of simple nominal expressions in five languages. [BOP88] were able to exploit the grammatical overlap of two Slavic languages, for the design of a lean transfer process in Russian to Czech machine translation. In multilingual application development within Microsoft research, grammar sharing has extensively been exploited ± [Pin96], [GLPR97]. However, all these approaches are rather opportunistic in the sense that existing grammatical descriptions based on existing grammar models were explored. We went a step further and started grammar design with a notion of a shared grammar for a family of related languages. Pursuing the goal of designing linguistically motivated grammatical Hans USZKOREIT Language Technology Lab, DFKI Computational Linguistics, Saarland University Saarbrücken, Germany, D-66041 uszkoreit@dfki.de resources for Slavic languages to be used in computational linguistics, one is inevitably confronted with primary pr"
C00-1005,P88-1024,0,0.0688361,"rammar also facilitates the difficult task of maintaining consistency within and across the individual parallel grammars. In machine translation, the specification of a shared grammar can furthermore be exploited for simplifying the transfer process. Without much ado, computational linguists engaged in multilingual grammar development have always tried to reduce their labour by importing existing grammar components in a simple ""copy-pastemodify"" fashion. But there were also a number of systematic attempts to create and describe shared grammars that are convincingly documented in publications. [Kam88] demonstrates the concept for a relatively restricted domain, the grammatical description of simple nominal expressions in five languages. [BOP88] were able to exploit the grammatical overlap of two Slavic languages, for the design of a lean transfer process in Russian to Czech machine translation. In multilingual application development within Microsoft research, grammar sharing has extensively been exploited ± [Pin96], [GLPR97]. However, all these approaches are rather opportunistic in the sense that existing grammatical descriptions based on existing grammar models were explored. We went a"
C10-2065,P96-1009,0,0.0218864,"Missing"
C10-2065,W07-1207,0,0.0198766,"nline product of the Berlin startup company Metaversum1 . The KomParse NPCs provide various services through con1 http://www.metaversum.com/ In contrast to existing systems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the machine-learning based recognition. The classifier is trained on a corpus originating from a Wizard-of-Oz experiment which was semiautomatically annotated. It contains automatically annotated syntactic relations namely, predicate argument structures, which were checked and corrected manually af"
C10-2065,J05-1004,0,0.033644,"rogative. • The topic of the utterance. The topic value is coreferent with the currently discussed object. Topic can consist of an object class (e.g. sofa) or an special object instance (sofa 1836). The topic of the directly preceding utterance was chosen as a feature too. 4.2 Annotation with Predicate Argument Structure The second annotation step, applied to the utterance level of the input, automatically enriches the annotation with predicate argument structures. Each utterance is parsed with a predicate argument parser and annotated with syntactic relations organized according to PropBank (Palmer et al., 2005) containing the following features: Predicate, Subject, Objects, Negation, Modifiers, Copula Complements. A single relation mainly consists of a predicate and the belonging arguments. Verb modifiers like attached PPs are classified as “argM” together with negation (“argM neg”) and modal verbs (“argM modal”). Arguments are labeled with numbers according to the found information for the actual structure. PropBank is organized in two layers, the first one being an underspecified representation of a sentence with numbered arguments, the second one containing fine-grained information about the sema"
C10-2065,P98-1013,0,0.0288322,", Objects, Negation, Modifiers, Copula Complements. A single relation mainly consists of a predicate and the belonging arguments. Verb modifiers like attached PPs are classified as “argM” together with negation (“argM neg”) and modal verbs (“argM modal”). Arguments are labeled with numbers according to the found information for the actual structure. PropBank is organized in two layers, the first one being an underspecified representation of a sentence with numbered arguments, the second one containing fine-grained information about the semantic frames for the predicate comparable to FrameNet (Baker et al., 1998). While the information in the second layer is stable for each verb, the values of the numbered arguments can change from verb to verb. While for one verb the “arg0” may refer to the subject of the verb, another verb may encapsulate a direct object behind the same notation “arg0”. This is very complicated to handle in a computational setup, which needs continuous labeling for the successive components. Therefore the arguments were in general named as in PropBank but consistently numbered by syntactic structure. This means for example that the subject is always labeled as “arg1”. Consider the e"
C10-2065,W08-1301,0,0.0219048,"Missing"
C10-2065,W98-0319,0,0.194776,"Missing"
C10-2065,W02-0213,0,0.0246234,"Missing"
C10-2065,P10-4007,1,0.807992,"Missing"
C10-2065,N04-1020,0,0.0168352,"Missing"
C10-2065,J00-3003,0,0.221944,"ng from the user input. The work presented in this paper is part of a dialogue system called KomParse (Kl¨uwer et al., 2010), which is an application of a NL dialogue system combined with various question answering technologies in a three-dimensional virtual world named Twinity, a web-based online product of the Berlin startup company Metaversum1 . The KomParse NPCs provide various services through con1 http://www.metaversum.com/ In contrast to existing systems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the mach"
C10-2065,N09-1064,0,0.0626592,"Missing"
C10-2065,C08-1123,0,0.0478388,"Missing"
C10-2065,P07-1074,1,0.293455,"ystems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the machine-learning based recognition. The classifier is trained on a corpus originating from a Wizard-of-Oz experiment which was semiautomatically annotated. It contains automatically annotated syntactic relations namely, predicate argument structures, which were checked and corrected manually afterwards. Furthermore these relations are enriched by manual annotation with semantic frame information from VerbNet to gain an additional level of semantic richne"
C10-2065,C98-1013,0,\N,Missing
C10-2155,P07-1073,0,0.0346878,"her relations. In (Xu et al., 2007) we make use of domain relevance values of terms occurring in rules. This method is not applicable to general relations. Parallel to confidence estimation strategies, the learning of negative rules is useful for identifying wrong rules straightforwardly. Yangarber (2003) and Etzioni et al. (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. Examples of one certain domain or class are regarded as negative examples for the other ones. Bunescu and Mooney (2007) follow a classification-based approach to RE. They use positive and negative sentences of a target relation for a SVM classifier. Uszkoreit et al. (2009) exploit negative examples as seeds for learning further negative instances and negative rules. The disadvantage of the above four approaches is that the selected negative domains or classes or negative instances cover only a subset of the negative domains/classes/relations of the target domain/class/relation. 3 DARE Baseline System Our baseline system DARE is a minimally supervised learning system for relation extraction, initialized by so-c"
C10-2155,W06-0204,0,0.755395,"e selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 20"
C10-2155,P03-1029,0,0.622006,"egree depending on the domain and the selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and G"
C10-2155,P07-1074,1,0.955045,"d by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006),"
C10-2155,P03-1044,0,0.167727,"venson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006), (Etzioni et al., 2005), (Xu et al., 2007) and (Uszkoreit et al., 2009)). In this paper, we present a new approach to estimating or ranking the confidence value of learned rules by utilizing limited closed-world knowledge. As many predecessors, our ranking method is built on the “Duality Principle” (e. g., (Brin, 1998), (Yangarber, 2001) and (Agichtein, 2006)). We extend the validation method by an evaluation of extracted instances against some limited closed-world knowledge, while also allowing cases in which knowledge for informed decisions is not available"
C10-2155,P06-1015,0,0.447985,"chtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006), (Etzioni et al., 2005), (Xu et al., 2007) and (Uszkoreit et al., 2009)). In this paper, we present a new approach to estimating or ranking the confidence value of learned rules by utilizing limited closed-world knowledge. As many predecessors, our ranking method is built on the “Duality Principle” (e. g., (Brin, 1998), (Yangarber, 2001) and (Agichtein, 2006)). We extend the validation method by an evaluation of extracted instances against some limited closed-world knowledge, while also allowing cases in which knowledge for informed decisions is not available. In comparison to previous approa"
C86-1045,C86-1016,0,0.0498798,"t also with its categorial subgraphs. The @-notation permits this use of templates preceding member of the same valency listfi In the proposed model, the dependency between the extraposed phrase and its antecendent is neither established by functional application/composition nor by feature passing. It is assumed that there is a different matching process that combines the noncontiguous phrases. A process of this kind is independently needed for the matching of adjuncts with thematic roles that are embedded in the meaning of the functor: (26a) Tellme about French history. (26b) Start in 1700. (Karttunen, 1986)3 3. A CUG G r a m m a r Model t h a t A e c o m o d a t e s Word Order Variation Worder order variation has always been one of the hardest problems for categorial grammars. Functional composition together with type-raising can be used to obtain all permutations of the sentences that are generated by a traditional categorial grammar. Totally free word order does therefore not pose an unsurmountable problem to the categorial approach. As with other types of grammar formalisms, it is semi-free word order that is difficult to accommedate. The year 1700 is obviously not the start time for the tell"
C86-1045,P85-1018,0,\N,Missing
C86-1045,P84-1075,0,\N,Missing
capstick-etal-2002-collate,W01-1506,0,\N,Missing
capstick-etal-2002-collate,W01-1502,1,\N,Missing
capstick-etal-2002-collate,declerck-etal-2000-new,1,\N,Missing
cotik-etal-2017-annotation,W16-2921,1,\N,Missing
cotik-etal-2017-annotation,W17-1807,0,\N,Missing
cotik-etal-2017-annotation,W16-4210,1,\N,Missing
D16-1065,D15-1198,0,0.636214,"tem P R F1 JAMR(fixed) System 1 System 2 JAMR(fixed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing an"
D16-1065,W13-2322,0,0.620283,"w between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources. 1 Introduction Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node"
D16-1065,P13-2131,0,0.178848,"te inference, it is very natural to employ violation-fixing perceptron here for AMR parsing training. Specifically, we use an improved update method “max-violation” which updates at the worst mistake, 686 4.1 Experiments Dataset and Evaluation Metric Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation. Finally, we also show our parsers performance on the full LDC2014T12 dataset. We evaluate the performance of our parser using Smatch v2.0 (Cai and Knight, 2013), which counts the precision, recall and F1 of the concepts and relations together. 4.2 Development Results Generally, larger beam size will increase the computational cost while smaller beam size may reduce the performance. As a tradeoff, we set the beam size as 4 throughout our experiments. Figure 3 shows the training curves of the averaged violation-fixing perceptron with respect to the performance on the both development sets. As we can see the curves converge very quickly, at around iteration 3. 4.4 0.72 F-measure 0.71 0.7 0.69 0.68 LDC2014T112 LDC2103E117 0.67 0.66 0 1 2 3 4 5 6 7 8 9 10"
D16-1065,P04-1015,0,0.0592557,"cantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments de"
D16-1065,W02-1001,0,0.38374,"1. Name Description 1 Fragment given words 2 3 Span length NER 4 Bias Relative frequency estimates of the probability of a concept fragment given the span of words. The length of the span. 1 if the span corresponds to a named entity, 0 otherwise. 1 for any concept fragment from the alignment table, 0 otherwise. 5 6 7 8 9 10 11 12 13 14 15 16 c c+w c + lem c + pos c + w−1 c + w+1 c + pos−1 c + pos+1 c + w−2 c + w+2 c + pos−2 c + pos+2 To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron (Collins, 2002). Additionally, in our training algorithms, the implementation of the oracle function is rela-tively straightforward. Specifically, when the i-th span is processed in the incremental parsing process, the partial gold-standard AMR graph up to the i-th span consists of the edges and nodes that appear before the end position of the i-th span, over which the gold-standard feature vectors are calculated. 4 c represents the current concept label, w represents the current words, lem represents the current lemmas, pos represents the current POS tags. w−1 denotes the first word to the left of current w"
D16-1065,P14-1134,0,0.701726,"only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawback of the staged architecture, errors in upstream component are often compounded and"
D16-1065,P16-1001,0,0.650129,"nal semantic resources. Dataset System P R F1 LDC2014T12 JAMR(fixed) CAMR* CAMR SMBT-based Our approach .64 .68 .70 .70 .53 .60 .62 .62 .58 .64 .66 .67 .66 Table 5: Final results on the full LDC2014T12 dataset. 5 Related Work Our work is motivated by JAMR (Flanigan et al., 2014), which is based on a pipelined model, resulting in a large drop in overall performance when moving from gold concepts to system concepts. Wang et al. (2015a) uses a two-stage approach; dependency parses are modified by executing a sequence of actions to resolve dis-crepancies between dependency tree and AMR structure. Goodman et al. (2016) improves the transition-based parser with the imitation learning algorithms, achieving almost the same performance as that of Wang et al. 5 The code is available at https://github.com/ Juicechuan/AMRParsing 688 (2015b), which exploits the extended features from additional trained analysers, including co-reference and semantic role labelers. Artzi et al. (2015) introduces a new CCG grammar induction algorithm for AMR parsing, combined with a factor graph to model non-compositional phenomena. Pust et al. (2015) adapts the SBMT parsing framework to AMR parsing by designing an AMR transformation,"
D16-1065,P12-1110,0,0.0117722,"pelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole"
D16-1065,N12-1015,0,0.0303754,"2016. 2016 Association for Computational Linguistics cent”, it should evoke the concept “affect-02”. Obviously, the correct concept choice for the verb “affect” should exploit a larger context, and even the whole semantic structure of the sentence, which is more probable to be unfolded at the downstream relation identification stage. This example indicates that it is necessary to allow for the interaction of information between the two stages. try-01 :ARG1 :ARG0 :ARG1 :mod :name He accent 2 Background 2.1 country affect-02 :ARG0 designed specifically for inexact search in structured learning (Huang et al., 2012). Experimental results show that the proposed joint framework significantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as"
D16-1065,D14-1036,0,0.0398291,"a factor graph to model non-compositional phenomena. Pust et al. (2015) adapts the SBMT parsing framework to AMR parsing by designing an AMR transformation, and adding external semantic resources. More recently, Damonte et al. (2016) also presents an incremental AMR parser based on a simple transition system for dependency parsing. However, compared to our parser, their parser cannot parse non-projective graphs, resulting in a limited coverage. Our work is also inspired by a new computational task of incremental semantic role labeling, in which semantic roles are assigned to incomplete input (Konstas et al., 2014). 6 Conclusions and Future Work In this paper, we present a new approach to AMR parsing by using an incremental model for performing the concept identification and relation identification jointly, which alleviates the error propagation in the pipelined model. In future work, we plan to improve the parsing performance by exploring more features from the coreference resolution, word sense disambiguation system and other external semantic resources. In addition, we are interested in further incorporating the incremental semantic role labeling into our incremental framework to allow bi-directional"
D16-1065,P14-1038,0,0.0994402,"and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are requi"
D16-1065,N15-1114,0,0.122135,"Missing"
D16-1065,J08-4003,0,0.0403371,"GHT-ARCS (lines 26-27): Add current concept and highest-scoring left arc and right arc to the partial graph. The first three actions are similar in form to those in the Arc-Standard algorithm for transition-based 2 3 The constant B denotes the beam size. The right-to-left order reflects the principle of local priority. 683 Figure 2: An illustrative diagram for CWBS algorithm. Each dotted box corresponds to a connected component in the partial graph, each of which consists one or multiple concept fragments. The rightmost subgraph corresponds to the current concept fragment. dependency parsing (Nivre, 2008; Zhang and Clark, 2008a). The last one is defined to cope with the cases where there may be multiple parents for some nodes in an AMR graph. Note that the “SHIFT” action does not add any edges. This operation is particularly necessary because the partial graphs are not always connected during the search process. In our experiments, we also found that the number of connected components during search process is relatively small, which is generally less than 6. It is important to note that, in order to guarantee the output graph connected, when the last concept fragment is encountered, the “SHIF"
D16-1065,N15-1119,0,0.0396957,"es. 1 Introduction Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawbac"
D16-1065,D15-1136,0,0.517497,"ures generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluate our parser on the full LDC2014T12 dataset. We use the training/development/test split recommended in the release: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences for testing. For comparison, we include the results of JAMR, CAMR*, CAMR and SMBT-based parser (Pust et al., 2015), which are also trained on the same dataset. The results in Table 5 show that our approach outperforms CAMR*, and obtains comparable performance with CAMR. However, our approach achieves slightly lower performance, compared to the SMBT-based parser, which adds data and features drawn from various external semantic resources. Dataset System P R F1 LDC2014T12 JAMR(fixed) CAMR* CAMR SMBT-based Our approach .64 .68 .70 .70 .53 .60 .62 .62 .58 .64 .66 .67 .66 Table 5: Final results on the full LDC2014T12 dataset. 5 Related Work Our work is motivated by JAMR (Flanigan et al., 2014), which is based"
D16-1065,N15-1040,0,0.667706,"xed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluat"
D16-1065,P15-2141,0,0.302746,"xed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluat"
D16-1065,P15-1095,0,0.212769,"h their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream prediction. The downstream components, however, cannot impact earlier decision. For example, for the verb “affect” in the example shown in Figure 1, there exist two possible concepts: “affect-01” and “affect-02”. Comparatively, the first concept has mo"
D16-1065,D08-1059,0,0.189991,"on, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR. Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm (Zhang and Clark, 2008b) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model. For parameter estimation, “violation-fixing” perceptron is adopted since it is 681 AMR Parsing Task Nodes of an AMR graph are labeled with concepts, and edges are labeled with relations. Concepts can be English words (“He”), PropBank event predicates (“try-01”, “affect-02”), or special keywords (“British”). For example, “affect-02” represents a PropBank roleset that corresponds to the first sense of “affect”. According"
D16-1065,P08-1101,0,0.126709,"on, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR. Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm (Zhang and Clark, 2008b) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model. For parameter estimation, “violation-fixing” perceptron is adopted since it is 681 AMR Parsing Task Nodes of an AMR graph are labeled with concepts, and edges are labeled with relations. Concepts can be English words (“He”), PropBank event predicates (“try-01”, “affect-02”), or special keywords (“British”). For example, “affect-02” represents a PropBank roleset that corresponds to the first sense of “affect”. According"
E09-2004,P07-1074,1,0.799928,"develop and implement cognitively enhanced artificial agents, using technologies in natural language processing, question answering, web-based information extraction, semantic web and interaction driven profiling with cognitive modelling (Krenn, 2008). This paper describes a conversational agent “Gossip Galore”, an active self-learning system that can learn, update and interpret information from the web, and can make conversations with users and provide answers to their questions in the domain of celebrity gossip. In more detail, by applying a minimally supervised relation extraction system (Xu et al., 2007; Xu et al., 2008), the agent automatically collects the knowledge from relevant websites, and also communicates with the users using a question-answering engine via a 3D graphic interface. This paper is organized as follows. Section 2 gives an overview of the system architecture and presents the design and functionalities of the components. Section 3 explains the system setup and discusses implementation details, and finally Section 4 draws conclusions. 2 System Overview Figure 1 shows a use case of the system. Given a query “Tell me something about Carla Bruni”, the application would trigger"
E17-3002,W15-0514,0,0.030732,"r, they do not offer effective functionalities for 1) easy access to the argumentative structure of debate content, and 2) quick overviews of the various semantic facets, the polarity and the relevance of the arguments. Some platforms1 allow users to label posts as pro or con arguments, to cite external sources, to assess debate content or to create structured debates across the web, but do not offer any deeper automatic language technologybased analyses. Argumentation mining research, which could help in automatically structuring debates, has only recently been applied to web debate corpora (Boltuzic and Snajder, 2015; Petasis and Karkaletsis, 2016; Egan et al., 2016). Our goal is to address these issues by developing a debate platform that: • Supports debate participants in making substantial and clear contributions • Facilitates an overview of debate contents • Associates relevant information available on the web with debate topics • Connects regional discussions to global deliberations • Supports advanced participation in deliberations, without sacrificing transparency and usability. In this paper, we present the Common Round platform, which implements various functions towards these goals, with followi"
E17-3002,W16-2816,0,0.0376372,"Missing"
E17-3002,P09-1113,0,0.0119288,"odels to recognize standard entity types, such as persons, organizations, locations and date/time expressions. For non-standard concept types, we use SProUT (Drozdzynski et al., 2004), which implements a regular expression-like rule formalism and gazetteers for detecting domain-specific concepts in text. Relation extraction is performed by matching dependency parse trees of sentences to a set of automatically learned dependency patterns (Krause et al., 2012). Relation patterns are learned from corpora manually annotated with event type, argument types, and roles, or using distant supervision (Mintz et al., 2009). For sentiment analysis, we apply a lexicon-based approach that additionally makes use of syntactic information in order to handle negation. Text Analytics and Association with External Material The Common Round platform enriches the contents of debates and posts by extracting information about topics, sentiment, entities and relations. Topic detection helps to find semantically related debates. Sentiment analysis allows determining the emotion level in discussions. By identifying, for example, instances of the relation MayTreatDisorder in a discussion about the legalization of cannabis, we c"
E17-3002,W16-2811,0,\N,Missing
fu-etal-2010-determining,J96-1002,0,\N,Missing
fu-etal-2010-determining,J91-3001,0,\N,Missing
jorg-etal-2010-lt,councill-etal-2008-parscit,0,\N,Missing
jorg-etal-2010-lt,bird-etal-2008-acl,0,\N,Missing
jorg-etal-2010-lt,capstick-etal-2002-collate,1,\N,Missing
K16-1024,P98-1013,0,0.603699,"Missing"
K16-1024,P10-1143,0,0.227749,"Missing"
K16-1024,W09-3208,0,0.378324,"as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective. 6 Related work We briefly point out other relevant approaches and efforts from the vast amount of literature. Event coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sac"
K16-1024,W09-4303,0,0.401158,"Missing"
K16-1024,P15-1017,0,0.402323,"angeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisio"
K16-1024,cybulska-vossen-2014-using,0,0.0549016,"ctured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency."
K16-1024,W15-0801,0,0.262634,"can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexical-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean"
K16-1024,P15-1061,0,0.0841438,"2; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event men"
K16-1024,N15-1133,0,0.0151487,"output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a"
K16-1024,D13-1137,0,0.015828,"an (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task"
K16-1024,W13-1203,0,0.0233806,"n by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocumen"
K16-1024,liu-etal-2014-supervised,0,0.55287,"quence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexical-semantic resour"
K16-1024,P09-1113,0,0.0614883,"nd Mitamura, 2015). to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, presen"
K16-1024,P08-1030,0,0.0213297,"ent aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performan"
K16-1024,P02-1014,0,0.145167,"η 10−5 dw 300 dp 8 β1 0.2 dc 256 β2 0.999 de 50  10−2 dsim 2 batch size n 3 epochs ≤ 2000 Dropout no `2 reg. no 512 Table 2: Hyperparameter settings. strategy performed worse than the less elaborate algorithm in Figure 2. The pairwise coreference decisions of our model induce a clustering of a document’s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)’s “BestLink” strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion. 4 ACE++ Table 1: Dataset properties. Figure 2: Generation of examples Pd for a document d with a sequence of event mentions Md . 1: 2: 3: 4: 5: 6: 7: ACE Experimental setting, model training We implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et a"
K16-1024,P11-1115,0,0.0957174,"Missing"
K16-1024,P15-2060,0,0.128067,"ejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make"
K16-1024,W12-4501,0,0.0283451,"n judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level informatio"
K16-1024,N15-1120,1,0.821269,"E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference cand"
K16-1024,D12-1045,0,0.185055,"al, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexic"
K16-1024,D12-1110,0,0.00798676,"mple, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model o"
K16-1024,D15-1278,0,0.0151945,"be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint"
K16-1024,J01-4004,0,0.355159,"ters θ = {Ww , Wp , {wc }, {bc }, We , be , Wsim , bsim , Wout , bout } (8) by minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014). 3.3 Example generation and clustering We investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the “anaphors”) with all preceding ones (the “antecedent” candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training creates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this 242 1: 2: 3: 4: 5: 6: 7: procedure G ENERATE E XAMPLES(Md ): Md = (m1 , . . . , m|Md |) Pd ← ∅ for i = 2, . . . , |Md |do for j = 1, . . . , i − 1 do Pd ← Pd ∪ {(mi , mj )} return Pd # documents # event instances # event mentions procedure G ENERATE C LUSTERS(Pd , score): Pd = {(mi , mj )}i,j score : Pd 7→ [0, 1] Cd ← {(mi , mj ) ∈ Pd : score(mi"
K16-1024,P10-1081,0,0.031717,"ce. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto"
K16-1024,P11-1053,0,0.0267238,"to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event ty"
K16-1024,swampillai-stevenson-2010-inter,0,0.0721242,"Missing"
K16-1024,M95-1005,0,0.935591,"Missing"
K16-1024,P15-1137,0,0.0418208,"from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. On the other hand, separating anaphoricity detection from antecedent scoring, as is often done for the task of entity coreference resolution (e.g., by Wiseman et al. (2015)), might result in performance gains; also the generation of sentential features from recurrent neural networks seems promising. Regarding our mediumterm research agenda, we would like to investigate if the model can benefit from more fine-grained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis. Acknowledgments This research was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (contract 01IW14002) and BBDC"
K16-1024,D15-1206,0,0.0142118,"d classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument set"
K16-1024,D10-1099,0,0.0773609,"combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the ta"
K16-1024,C14-1220,0,0.441852,"rence resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step"
K16-1024,D15-1203,0,0.155321,"make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the p"
K16-1024,P05-1053,0,0.0500769,"example in (Araki and Mitamura, 2015). to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (en"
K16-1024,C98-1013,0,\N,Missing
K16-1024,D15-1247,0,\N,Missing
K16-1024,D14-1159,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
kluewer-etal-2012-evaluation,W10-4353,0,\N,Missing
kluewer-etal-2012-evaluation,adolphs-etal-2010-question,1,\N,Missing
kluewer-etal-2012-evaluation,W04-2304,0,\N,Missing
kluewer-etal-2012-evaluation,W09-3951,0,\N,Missing
kluewer-etal-2012-evaluation,C10-2065,1,\N,Missing
kluewer-etal-2012-evaluation,J00-3003,0,\N,Missing
kluewer-etal-2012-evaluation,P10-4007,1,\N,Missing
kluewer-etal-2012-evaluation,P97-1035,0,\N,Missing
kluewer-etal-2012-evaluation,J05-1004,0,\N,Missing
krause-etal-2014-language,P07-1074,1,\N,Missing
krause-etal-2014-language,li-etal-2014-annotating,1,\N,Missing
krause-etal-2014-language,li-etal-2012-annotating,1,\N,Missing
krause-etal-2014-language,doddington-etal-2004-automatic,0,\N,Missing
krause-etal-2014-language,hasler-etal-2006-nps,0,\N,Missing
krause-etal-2014-language,P05-1045,0,\N,Missing
krieger-etal-2014-information,P03-1054,0,\N,Missing
krieger-etal-2014-information,P07-1074,1,\N,Missing
krieger-etal-2014-information,P99-1052,0,\N,Missing
L16-1383,W14-2907,0,0.0279161,"Missing"
L16-1383,P98-1013,0,0.0704039,"red via a pattern discovery method based on distant supervision (Mintz et al., 2009; Krause et al., 2012). Thus sar-graphs can be directly applied to free texts for relation extraction. Early work on lexical-semantics resources has focused on gathering information about individual words and their different meanings in varying contexts, the famous example being WordNet (Fellbaum, 1998). Linguistic knowledge resources that go beyond the level of lexical items are scarce and of limited coverage due to significant investment of human effort and expertise required for their construction. FrameNet (Baker et al., 1998) is such a resource and provides fine-grained semantic relations of predicates and their arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczy"
L16-1383,W13-5503,0,0.0237209,"rce and provides fine-grained semantic relations of predicates and their arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Ofte"
L16-1383,W15-1601,0,0.0135592,"ia the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach higher coverage in extraction experiments. In"
L16-1383,J14-1002,0,0.0215939,"i, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this"
L16-1383,D15-1112,0,0.034165,"Missing"
L16-1383,J02-3001,0,0.0693277,"ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work an"
L16-1383,P06-1117,0,0.0245124,"on for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach hi"
L16-1383,E12-1059,0,0.0233527,"arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas"
L16-1383,W15-4204,1,0.904146,"ch of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach higher coverage in extraction experiments. In the following, we discuss two ways of linking sar-graphs with FrameNet, which are in spirit of the large-scale efforts mentioned above. We believe that both resources an"
L16-1383,P09-1113,0,0.0504113,"Missing"
L16-1383,P15-2067,0,0.044101,"Missing"
L16-1537,P12-2011,0,0.141426,"of inference types underlying the entailment decisions. Keywords: Entailment Graphs, Relation Extraction, Textual Entailment 1. Introduction The task of relation extraction (RE) is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data, resulting in large numbe"
L16-1537,P10-1124,0,0.304804,"e propose a new approach to structuring extraction patterns by utilizing entailment graphs. The textual entailment paradigm captures the semantic relationship holding between two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et a"
L16-1537,P11-1062,0,0.0285902,"ween two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system"
L16-1537,P12-1013,0,0.203706,"essions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system (Agichtein, 2006), Web"
L16-1537,H05-1091,0,0.0646459,"ties or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data, resulting in large numbers of unique candidate patterns acquired from suspected mentions of relation instances. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a r"
L16-1537,W14-1610,0,0.113191,"ls H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system (Agichtein, 2006), WebDARE rules assign the semantic role labels"
L16-1537,P09-1113,0,0.109532,"Missing"
L16-1537,N13-1008,0,0.217571,"Missing"
L16-1537,E06-1052,0,0.504574,"wledge for partially labeling large volumes of data, resulting in large numbers of unique candidate patterns acquired from suspected mentions of relation instances. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a relation that entails the target relation or that is entailed by the target relation. Others are semantically unrelated to the target relation. The basic assumption made in this work is that patterns are truly reliable if they express a relation that semantically entails the target relation (Romano et al., 2006). This includes all patterns that express the target relation explicitly or a semantically equivalent relation. As an example, the pattern “org|B UYER bought org|ACQUIRED” can be considered to be semantically equivalent to the pattern “org|B UYER purchased org|ACQUIRED”, whereas “per|SPOUSE divorced per|SPOUSE” is not semantically equivalent to “per|SPOUSE married per|SPOUSE”, but entails a marriage relation. We propose a new approach to structuring extraction patterns by utilizing entailment graphs, hierarchical structures representing entailment relations, and present a novel resource of ent"
L16-1537,W11-0201,0,0.223261,"evaluating automatically generated entailment graphs and systems for recognizing textual entailment. 2. Related Work While relation extraction would clearly benefit from considering semantic relationships between patterns, there has been only a limited amount of prior work in structuring patterns. Matrix factorization approaches cluster semantically similar patterns based on argument co-occurrence information (e.g., Riedel et al. (2013)). Other approaches focus on the tree structure of the patterns, and compute similarity metrics based on graph matching techniques or tree edit distance (e.g., Thomas et al. (2011), Liu et al. (2013)). We propose a new approach to structuring extraction patterns by utilizing entailment graphs. The textual entailment paradigm captures the semantic relationship holding between two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to"
L16-1537,C00-2136,0,0.154294,"ce as well as an analysis of inference types underlying the entailment decisions. Keywords: Entailment Graphs, Relation Extraction, Textual Entailment 1. Introduction The task of relation extraction (RE) is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data,"
L18-1142,2003.mtsummit-systems.1,0,0.331074,"Missing"
L18-1142,D16-1025,0,0.0177636,"pes of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that was involved in the evaluation procedure. In this paper we will present the n"
L18-1142,L16-1100,0,0.139109,"vi´c and Ney, 2011) have not yet become standard. For the detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that"
L18-1142,D17-1263,0,0.0946452,"e detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that was involved in the evaluation procedure. In this"
L18-1142,1995.mtsummit-1.35,0,0.455422,"tasks where test suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as dif"
L18-1142,C90-2037,0,0.870085,"be extended to other NLP tasks where test suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can b"
L18-1142,2001.mtsummit-papers.35,0,0.814551,"st suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as difficult as the tran"
L18-1142,C96-2120,0,0.256926,"the manual evaluation procedure we have applied in the past. Section 3. describes the new TQ-AutoTest framework that supports the evaluation procedure. A use case of the TQ-AutoTest will be shown in Section 4.. Finally, in Section 5. we will conclude and give an outlook on future work. 2. Test Suites for German – English We have built a test suite for a fine-grained evaluation of MT quality for the language pair German – English. In brief, it contains segments selected from various parallel corpora and drawn from other sources such as grammatical resources, e.g., the TSNLP Grammar Test Suite (Lehmann et al., 1996) and online lists of typical translation errors. Each test sentence is annotated with a phenomenon category and the phenomenon it represents. An example showing these fields can be seen in Table 1 with the first column containing the source segment and the second and third column containing the phenomenon category and the phenomenon, respectively. The fourth column shows an example machine translation1 and the last column contains a 1 As example we have used the “old” Google Translate system that was used before Google changed to a neural system in September 2016, cf. https://research.googlebl"
L18-1142,J11-4002,0,0.0624529,"Missing"
L18-1142,W14-0821,0,0.0650591,"Missing"
L18-1142,W16-2323,0,0.0425685,"Missing"
L18-1142,W16-3409,0,0.0131884,"fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as difficult as the translation itself. Today, MT quality is still usually assessed by shallow automatic comparisons of MT outputs with reference corpora resulting in a number. Early attempts to automatically classify errors based on post-edits or reference translations like (Popovi´c and Ney, 2011) have not yet become standard. For the detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT"
li-etal-2014-annotating,li-etal-2012-annotating,1,\N,Missing
P02-1056,P01-1019,0,0.0162156,"Missing"
P02-1056,A97-1035,0,0.0548928,"SG feature structures) is available in XML format with hyperlinks to full feature structure representations externally stored in corresponding data files. Fig. 1 gives an overview of the architecture of the WHITEBOARD Annotation Machine (WHAM). Applications feed the WHAM with input texts and a specification describing the components and configuration options requested. The core WHAM engine has an XML markup storage (external “offline” representation), and an internal “online” multi-level annotation chart (index-sequential access). Following the trichotomy of NLP data representation models in (Cunningham et al., 1997), the XML markup contains additive information, while the multi-level chart contains positional and abstraction-based information, e.g., feature structures representing NLP entities in a uniform, linguistically motivated form. Applications and the integrated components access the WHAM results through an object-oriented programming (OOP) interface which is designed as general as possible in order to abstract from component-specific details (but preserving shallow and deep paradigms). The interfaces of the actually integrated components form subclasses of the generic interface. New components ca"
P02-1056,P01-1034,0,0.116035,"Missing"
P02-1056,W97-0802,0,0.0694652,"Missing"
P02-1056,C02-1093,1,\N,Missing
P03-2019,C94-2144,1,0.720999,"ral edge automatically causes the failure of several, more specialized edges, without applying the unifiability test. Such information can in fact be precompiled. This and other optimization techniques are described in (Krieger and Piskorski, 2003). When compared to symbol-based finite state approaches, our method leads to smaller grammars and automata, which usually better approximate a given language. 3 XTDL – The Formalism in SProUT XTDL combines two well-known frameworks, viz., typed feature structures and regular expressions. XTDL is defined on top of TDL, a definition language for TFSs (Krieger and Schäfer, 1994) that is used as a descriptive device in several grammar systems (LKB, PAGE, PET). Apart from the integration into the rule definitions, we also employ TDL in SProUT for the establishment of a type hierarchy of linguistic entities. In the example definition below, the morph type inherits from sign and introduces three more morphologically motivated attributes with the corresponding typed values: morph := sign & [ POS atom, STEM atom, INFL infl ]. A rule in XTDL is straightforwardly defined as a recognition pattern on the left-hand side, written as a regular expression, and an output descriptio"
P03-2019,W03-2415,0,0.0311393,"roUT offers three online components: a tokenizer, a gazetteer, and a morphological analyzer. The tokenizer maps character sequences to tokens and performs fine-grained token classification. The gazetteer recognizes named entities based on static named entity lexica. The morphology unit provides lexical resources for English, German (equipped with online shallow compound recognition), French, Italian, and Spanish, which were compiled from the full form lexica of MMorph (Petitpierre and Russell, 1995). Considering Slavic languages, a component for Czech presented in (Hajiþ, 2001), and Morfeusz (Przepiórkowski and Wolinski, 2003) for Polish. For Asian languages, we integrated Chasen (Asahara and Matsumoto, 2000) for Japanese and Shanxi (Liu, 2000) for Chinese. The XTDL-based grammar engineering platform has been used to define grammars for English, German, French, Spanish, Chinese and Japanese allowing for named entity recognition and extraction. To guarantee a comparable coverage, and to ease evaluation, an extension of the MUC-7 standard for entities has been adopted. ne-person := enamex & [ TITLE list-of-strings, GIVEN_NAME list-of-strings, SURNAME list-of-strings, P-POSITION list-of-strings, NAME-SUFFIX string, DE"
P03-2019,piskorski-etal-2002-flexible,1,\N,Missing
P06-4010,J95-4004,0,0.0143991,"xts from Internet or Disk Word Seg. and POS Tag. Resources Word Seg. and POS Tag. Error Repair Resources Texts with Word Seg. and POS Tags NE Recognition Lexical Ontology NE Recognition Resources NE-Recognized Texts NER Identification NER Identification Resources NER-Identified Texts Figure 1. A three-stage Chinese IE computational model. In general, the accuracy of the first stage has considerable influence on the performance of the consequent two stages. It has been demonstrated by our experiments (Yao et al., 2002). In order to reduce unfavorable influence, we utilize a trainable approach (Brill, 1995) to automatically generate effective rules, by which the first component can repair different errors caused by word segmentation and POS tagging. At the second stage, there are two kinds of NE constructions to be processed (Yao et al., 2003). One is the NEs which involve trigger words; the other those without trigger words. For the former NEs, a shallow parsing mechanism, i.e., finitestate cascades (FSC) (Abney, 1996) which are automatically constructed by sets of NE recognition rules, is adopted for reliably identifying different categories of NEs. For the latter NEs, however, some special st"
P06-4010,W03-1708,1,0.82992,"ER Identification Resources NER-Identified Texts Figure 1. A three-stage Chinese IE computational model. In general, the accuracy of the first stage has considerable influence on the performance of the consequent two stages. It has been demonstrated by our experiments (Yao et al., 2002). In order to reduce unfavorable influence, we utilize a trainable approach (Brill, 1995) to automatically generate effective rules, by which the first component can repair different errors caused by word segmentation and POS tagging. At the second stage, there are two kinds of NE constructions to be processed (Yao et al., 2003). One is the NEs which involve trigger words; the other those without trigger words. For the former NEs, a shallow parsing mechanism, i.e., finitestate cascades (FSC) (Abney, 1996) which are automatically constructed by sets of NE recognition rules, is adopted for reliably identifying different categories of NEs. For the latter NEs, however, some special strategies, such as the valence constraints of domain verbs, the constituent analysis of NE candidates, the global context clues and the analysis for preposition objects etc., are designed for identifying them. 3 System Implementation During t"
P06-4010,W05-0401,1,0.839473,"curately and quickly as possible; • Evaluating the performance of this sys tem in a specific domain. 37 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 37–40, c Sydney, July 2006. 2006 Association for Computational Linguistics - 2 After the recognition for NEs, NER identification is performed in the last stage. Because of the diversity and complexity of NERs, at the same time, considering portability requirement in the identification, we suggest a novel supervised machine learning approach called positive and negative case-based learning (PNCBL) used in this stage (Yao and Uszkoreit, 2005). The learning in this approach is a variant of memory-based learning (Daelemans et al., 2000). The goal of that is to capture valuable information from NER and non-NER patterns, which is implicated in different features. Because not all features we predefine are necessary for each NER or non-NER, we should select them by a reasonable measure mode. According to the selection criterion we propose - self-similarity, which is a quantitative measure for the concentrative degree of the same kind of NERs or nonNERs in the corresponding pattern library, the effective feature sets - General-Character"
P07-1074,W06-0204,0,0.879054,"and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verbobject, chain m"
P07-1074,W06-0202,0,0.108538,"Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verbobject, chain model, linked chain model and sub"
P07-1074,P03-1029,0,0.672803,"Xu, Hans Uszkoreit and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation mode"
P07-1074,C00-2136,0,0.0322115,"the seed-driven bottom-up pattern acquisition is presented. Section 5 describes our experiments with pattern ranking, filtering and rule induction. Section 6 presents the experiments and evaluations for the two application domains. Section 7 provides a conclusion and an outline of future work. 2 System Architecture Given the framework, our system architecture can be depicted as follows: Figure 1. Architecture This architecture has been inspired by several existing seed-oriented minimally supervised machine learning systems, in particular by Snowball (Agichtein and Gravano, 2000) and ExDisco (Yangarber et al., 2000). We call our system DARE, standing for “Domain Adaptive Relation Extraction based on Seeds”. DARE contains four major components: linguistic annotation, classifier, rule learning and relation extraction. The first component only applies once, while the last three components are integrated in a bootstrapping loop. At each iteration, rules will be learned based on the seed and then new relation instances will be extracted by applying the learned rules. The new relation instances are then used as seeds for the next iteration of the learning cycle. The cycle terminates when no new relations can b"
P07-1074,P03-1044,0,0.584108,"Complexity Feiyu Xu, Hans Uszkoreit and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered"
P07-1074,xu-etal-2002-domain,1,0.798353,"erms “prize” and “win” respectively. Given n different domains, the domain relevance score (DR) of a term t in a domain di is: DR(t, di)= 0, if df(t, di) =0; df(t,di ) df(t,di ) × LOG(n × n ) , otherwise N×D ∑df(t,d j ) j=1 where • df(t, di): is the document frequency of a term t in the domain di • D: the number of the documents in di • N: the total number of the terms in di Here the domain relevance of a term is dependent both on its document frequency and its document frequency distribution in other domains. Terms mentioned by more documents within the domain than outside are more relevant (Xu et al., 2002). In the case of n=3 such different domains might be, e.g., management succession, book review or biomedical texts. Every domain corpus should ideally have the same number of documents and similar average document size. In the calculation of the trustworthiness of the origin, we follow Agichtein and Gravano (2000) and Yangarber (2003). Thus, the relevance of a pattern is dependent on the relevance of its terms and the score value of the most trustworthy seed from which it origins. Finally, the score of a pattern p is calculated as follows: T score(p)= ∑ DR(t i ) × max{score( s ) :s ∈ Seeds} i"
P10-4007,adolphs-etal-2010-question,1,0.873423,"Missing"
P10-4007,W08-1301,0,0.035559,"Missing"
P10-4007,P07-1074,1,0.665395,".C.E. chatbot that uses its own understanding and generation components (Wallace and Bush, 2001). &lt;PREDICATE, ARG1, ARG2, [message-type]&gt; The following examples show the structure used for different input: • ”Who is the boyfriend of Madonna?” &lt;hasBoyfriend, Madonna, ?, [wh]&gt; • ”I want to buy a sofa.” &lt;buy, I, &quot;a sofa&quot;, [declarative]&gt; 5.2 Information Extraction Both scenarios make use of state-of-the-art information extraction approaches to extract the important pieces from the user input. While the bartender depends on relation extraction to detect the fact or relation questioned by the user (Xu et al., 2007), the sales agent uses information extraction methods to recognize user wishes and demands. As a result, the questioned fact or the demanded object feature equals the ontology structure containing the knowledge needed to handle the user input. The input “Do you have any red couches?” for example needs to get processed by the system in such a way that the information regarding the sofa with red color is extracted. This is done by the system in a data-driven way. The input analysis first tries to find a demanded object in the input via asking the ontology: Every object which can be discussed in"
P12-1026,J92-4003,0,0.053679,"h other clustering algorithms. 3.1.2 Data Chinese Gigaword is a comprehensive archive of newswire text data that has been acquired over several years by the Linguistic Data Consortium (LDC). The large-scale unlabeled data we use in our experiments comes from the Chinese Gigaword (LDC2005T14). We choose the Mandarin news text, i.e. Xinhua newswire. This data covers all news published by Xinhua News Agency (the largest news agency in China) from 1991 to 2004, which contains over 473 million characters. Brown Clustering Our first choice is the bottomup agglomerative word clustering algorithm of (Brown et al., 1992) which derives a hierarchical clustering of words from unlabeled data. This algorithm generates a hard clustering – each word belongs to exactly one cluster. The input to the algorithm is sequences of words w1 , ..., wn . Initially, the algorithm starts with each word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languag"
P12-1026,A00-2018,0,0.748666,"Missing"
P12-1026,W02-1001,0,0.220434,"Missing"
P12-1026,J03-4003,0,0.494678,"Missing"
P12-1026,gimenez-marquez-2004-svmtool,0,0.060584,"Missing"
P12-1026,P10-1110,0,0.109628,"es includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the prin"
P12-1026,N09-2054,0,0.505726,"011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. punctuations. From this table, we can see that words with low frequency, especially the out-of-vocabulary (OOV) words, are hard to label. However, when a word is very frequently used, its behavior is very complicated and therefore hard to predict. A typical example of such words is"
P12-1026,D07-1117,0,0.056847,"rds. To conveniently illustrate, we denote a word in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed ana"
P12-1026,P08-1068,0,0.128971,"Missing"
P12-1026,P10-1052,0,0.0840783,"Missing"
P12-1026,D11-1109,0,0.173648,"Missing"
P12-1026,P05-1010,0,0.184469,"Missing"
P12-1026,N04-1043,0,0.145546,"Missing"
P12-1026,E99-1010,0,0.0303481,"matic or substitutional similarity among words. 3.1.1 Clustering Algorithms Various clustering techniques have been proposed, some of which, for example, perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms. In this paper, we focus on distributional word clustering that is based on the assumption that words that appear in similar contexts (especially surrounding words) tend to have similar meanings. They have been successfully applied to many NLP problems, such as language modeling. We use the publicly available implementation MKCLS3 (Och, 1999) to train this model. We choose to work with these two algorithms considering their prior success in other NLP applications. However, we expect that our approach can function with other clustering algorithms. 3.1.2 Data Chinese Gigaword is a comprehensive archive of newswire text data that has been acquired over several years by the Linguistic Data Consortium (LDC). The large-scale unlabeled data we use in our experiments comes from the Chinese Gigaword (LDC2005T14). We choose the Mandarin news text, i.e. Xinhua newswire. This data covers all news published by Xinhua News Agency (the largest n"
P12-1026,P06-1055,0,0.305586,"Missing"
P12-1026,N07-1051,0,0.0474247,"Missing"
P12-1026,P07-1096,0,0.0619281,"Missing"
P12-1026,C10-2139,1,0.854465,"t least two clusters left, the algorithm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units, i.e. words, segmentation is a necessary pre-processing step for word clustering. Previous research shows that character-based segmentation models trained on labeled data are reasonably accurate (Sun, 2010). Furthermore, as shown in (Sun and Xu, 2011), appropriate string knowledge acquired from large-scale unlabeled data can significantly enhance a supervised model, especially for the prediction of out-of-vocabulary (OOV) words. P (wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) In this paper, we employ such supervised and semisupervised segmenters4 to process raw texts. where the function C maps a word w to its class 3.2 Improving Tagging with Cluster Features C(w). We use a publicly available package2 (Liang Our discriminative sequential tagger is easy to be exet al., 2005) to train this"
P12-1026,D11-1090,1,0.847189,"hm merges the two clusters that maximizes the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units, i.e. words, segmentation is a necessary pre-processing step for word clustering. Previous research shows that character-based segmentation models trained on labeled data are reasonably accurate (Sun, 2010). Furthermore, as shown in (Sun and Xu, 2011), appropriate string knowledge acquired from large-scale unlabeled data can significantly enhance a supervised model, especially for the prediction of out-of-vocabulary (OOV) words. P (wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) In this paper, we employ such supervised and semisupervised segmenters4 to process raw texts. where the function C maps a word w to its class 3.2 Improving Tagging with Cluster Features C(w). We use a publicly available package2 (Liang Our discriminative sequential tagger is easy to be exet al., 2005) to train this model. tended with arbitrary features and the"
P12-1026,N03-1033,0,0.0975865,"Missing"
P12-1026,I05-3005,0,0.155187,"Missing"
P12-1026,P06-1054,0,0.104993,"window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setti"
P12-1026,D08-1059,0,0.0687714,"rrent token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments following the setting of the CoNLL 2009 shared task. The setting"
P12-1026,W09-3825,0,0.244173,"in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is the current token. Our features includes: Word unigrams: w−2 , w−1 , w, w+1 , w+2 ; Word bigrams: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; In order to better handle unknown words, we extract morphological features: character n-gram prefixes and suffixes for n up to 3. 2.3 Evaluation 2.3.1 Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and 1 http://wapiti.limsi.fr/ Xu, 2011), POS tagging (Huang et al., 2007, 2009), constituency parsing (Zhang and Clark, 2009; Wang et al., 2006) and dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010; Li et al., 2011). In this paper, we use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we define the training, development and test sets following two settings. To compare our tagger with the state-of-the-art, we conduct an experiment using the data setting of (Huang et al., 2009). For detailed analysis and evaluation, we conduct further experiments"
P12-1026,I05-3027,0,\N,Missing
P15-1058,D07-1074,0,0.138324,"Missing"
P15-1058,C10-1032,0,0.0663769,"Missing"
P15-1058,D11-1072,0,0.0796333,"Missing"
P15-1058,H93-1061,0,0.552476,"Missing"
P15-1058,Q14-1019,0,0.402417,"Missing"
P15-1058,S13-2040,0,0.208108,"Missing"
P15-1058,C10-1145,0,0.0251144,"Missing"
P15-1058,P10-4014,0,0.721567,"Missing"
P15-1058,J14-4005,0,0.133748,"Missing"
P15-1058,P10-1154,0,0.0170223,"Missing"
P15-1058,S07-1016,0,0.2072,"Missing"
P15-1058,S13-1003,0,0.256707,"Missing"
P15-1058,W04-0811,0,0.178741,"Missing"
P15-1058,E06-1002,0,\N,Missing
P15-1058,J14-1003,0,\N,Missing
P15-4008,doddington-etal-2004-automatic,0,0.0800995,"tically correct if there are no parsing or other preprocessing errors, and it is semantically correct if its source sentences express the target relation. Correspondingly, we label a dependency pattern as “INCORRECT” if it is grammatically incorrect, or if its sentences do not express the target relation. Typically, the annotators aim to identify one or more of the error classes discussed in Section 5 to decide whether a pattern is incorrect. For deciding whether a sentence expresses a given relation, we use the ACE annotation guidelines’ conceptual definition of relations and their mentions (Doddington et al., 2004), and define the semantics of relations based on Freebase descriptions. In contrast to the ACE tasks, we also consider n-ary relations in addition to binary relations. Sentences must express the target relation explicitly, e.g., “Obama was awarded the Nobel Peace Prize.” explicitly expresses the relation award honor. We treat implicit mentions as semantically incorrect, e.g., the previous example only implies an award nomination. A third feedback category, “CORRECT, BUT TOO SPECIFIC ”, was added based on our initial analysis of the dataset, and applies to dependency patterns mostly found in th"
P15-4008,P09-1113,0,0.170511,"Missing"
P15-4008,Q13-1030,0,0.0289449,"Missing"
P15-4008,P05-1047,0,0.0242473,"d allows searching for specific patterns or sentences. The center part visualizes the currently selected dependency pattern in AVM notation. In this notation, the IN PUT element contains the dependency pattern, and the OUTPUT element lists the relation arguments extracted by this pattern. In the example pattern shown in Figure 2, these correspond to the spouses and the wedding date. Thus, the patterns also contain the semantic role label information of the target relation for the corresponding linguistic arguments, which is not included in most traditional pattern extraction approaches (e.g., Stevenson and Greenwood (2005)). Pattern Extraction In this section, we briefly describe our approach for extracting relation-specific dependency patterns in a distantly supervised setting, called WebDARE (Krause et al., 2012). In contrast to most other approaches, we consider not only binary, but arbitrary n-ary relations, with n &gt;= 2. For example, we can define a 4-ary marriage relation with the spouses as essential (required) arguments, and optional arguments such as the wedding date and location. Given a knowledge base (KB) containing such relations and their arguments, we select a set of seed relation instances from t"
P15-4008,D12-1042,0,0.0708671,"Missing"
P15-4008,P07-1074,1,0.780952,"which matches this sentence, the pattern extraction algorithm first identifies the argument mentions of the seed relation instance occurring in the sentence, and then determines and composes the set of shortest paths connecting the arguments in the dependency parse in a bottom-up manner. Figure 1 visualizes the pattern extraction process for an example sentence expressing the marriage relation. The extracted pattern is shown in attribute-value-matrix (AVM) notation in Figure 1c. For more details on the algorithm we refer the interested reader to the DARE pattern extraction method described in Xu et al. (2007). 3 The area below the representation of the pattern lists the source sentences that it was observed in, as well as some statistics about the frequency of the pattern. Sentences are formatted to highlight the important elements of the pattern. Relation arguments are marked in red, content words occurring in the pattern are marked in blue. Listing the source sentences is important because it enables the human expert to verify both the extracted dependency pattern (e.g., to detect a parse error), and the semantic correctness of the pattern, i.e., whether the sentences express the target relation"
P16-4007,P14-5007,0,0.190071,"Missing"
P16-4007,C10-1011,0,\N,Missing
P16-4007,P07-1074,1,\N,Missing
P83-1004,P82-1014,0,\N,Missing
P91-1031,J90-1004,0,0.0607033,"Missing"
P91-1031,C86-1045,1,0.836965,"nation with a number of processing strategies and algorithms. The modularity has a number of advantages: Introduction Feature term formalisms (FTF) have proven extremely useful for the declarative representation of linguistic knowledge. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al. 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al. 1987]. • freedom for experimentation with different processing schemes, • compatibility of the grammar with improved system versions, • use of the same grammar for analysis and generation, • reusability of a grammar in different systems. Unification grammars have been used by theoretical linguists for describing linguistic competence. There exist no processing models for unification grammars yet that incorporate at least a few of the most widely accepted observations about human linguistic Research for this paper was carried out in parts at DFKI in the project DIsco which is fu"
P91-1031,C90-3002,0,0.0625948,"Missing"
P91-1031,C90-2025,0,0.0376306,"Missing"
P91-1031,P86-1038,0,0.0523643,"Missing"
P91-1031,P84-1018,0,0.0221123,"cs is their purely declarative nature. Since these grammars are not committed to any particular processing model, they can be used in combination with a number of processing strategies and algorithms. The modularity has a number of advantages: Introduction Feature term formalisms (FTF) have proven extremely useful for the declarative representation of linguistic knowledge. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al. 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al. 1987]. • freedom for experimentation with different processing schemes, • compatibility of the grammar with improved system versions, • use of the same grammar for analysis and generation, • reusability of a grammar in different systems. Unification grammars have been used by theoretical linguists for describing linguistic competence. There exist no processing models for unification grammars yet that incorporate at least a few of the m"
P91-1031,C90-2039,0,0.0450106,"no result can be derived using the grammar without relaxation, the relaxation level is increased and backtracking to the continuation points is activated. The Finally there might good reasons to process some conjuncts before others simply because processing them will bring in additional constraints that can reduce the 2Implicitely the ordered pair <0, c> is part of the control information for every subterm. Therefore it can be omitted. Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]). 239 subterm that is marked for relaxation is replaced by the relaxed equivalent. Unification continues. Whenever a (sub)term c from the grammar is encountered for which re(i) is defined, the relaxed constraint is used. This method also allows processing with an initial relaxation level greater than 0 in applications or discourse situations with a high probability of ungrammatical inpuL For a grammar G let Gi be the grammar G except that every constraint is replaced by rc(i). Let L i stand for the language generated or recognized by a grammar G i. If constraints are always properly relaxed,"
P91-1031,P84-1075,0,0.0824732,"Missing"
P92-1026,E89-1014,0,0.0945707,"andle LP constraints in head domains by building up a list of constituents over which the LP constraints are enforced, but also requires an addition to the parsing algorithm for checking LP constraints during as well as after processing. Our encoding of LP constraints does not require any particular format of the grammar, such as left- or right-branching structures. Therefore it can be incorporated into a variety of linguistic analyses. There is no need to work out the formal semantics of LP constraints because feature unification formalisms already have a well-defined formal semantics. Reape (1989) proposes a different strategy for treating partially free word order. His approach also permits the application of LP constraints across local trees. This is achieved by separating word order variation from the problem of building a semantically motivated phrase structure. Permutation across constituents can be described by merging the fringes (terminal yields) of the constituents using the operation of sequence union. All orderings imposed on the two merged fringes by LP constraints are preserved in the merged fringe. Reape treats clause union and scrambling as permutation that does not affe"
P92-1026,P91-1031,1,0.886116,"Missing"
R11-1003,P05-1061,0,0.0279728,"adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE applies acquired rules"
R11-1003,P03-1029,0,0.0794554,"nce in another domain. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which"
R11-1003,H05-1091,0,0.0432423,"in. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE"
R11-1003,P00-1002,0,0.0464215,"different effects of negative examples depending on the domain data properties and (iv) the potential of reusing rules from one domain for improving the relation extraction performance in another domain. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small s"
R11-1003,de-marneffe-etal-2006-generating,0,0.0127206,"Missing"
R11-1003,P07-1074,1,0.951825,"2010a; Kozareva and Hovy, 2010b)). The advantage of the minimally supervised approaches for IE rule learning is that only initial seed knowledge is needed. Therefore the adaptation might be limited to substituting the seed examples. However, different domains/corpora exhibit rather different properties of their learning/extraction data with respect to the learning algorithm. Depending on the domain, the need for improving precision by utilizing negative examples may differ. An important research goal is the exploitation of more benign domains for improving extraction in less suitable domains. Xu et al. (2007) and Xu (2007) present a minimally supervised learning system for relation extraction, initialized by a so-called semantic seed, i.e., examples of the target relations. We dub our system DARE for Domain Adaptive Relation Extraction. The system supports the domain adaptation with a compositional rule representation and a bottom-up rule discovery strategy. In this way, DARE can handle target relations of various complexities and arities. Relying on a few examples of a target relation as semantic seed dispenses with the costly acquisition of domain knowledge through experts or specialized resourc"
R11-1003,W06-0204,0,0.729222,"rtant for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE applies acquired rules to texts in order to discover m"
R11-1003,C96-1079,0,0.0407676,"ARE can handle target relations of various complexities and arities. Relying on a few examples of a target relation as semantic seed dispenses with the costly acquisition of domain knowledge through experts or specialized resources. In practice, this does not work equally well for any given domain. Xu (2007) and Uszkoreit et al. (2009) concede that DARE’s performance strongly depends on the specific type of relation and domain. In our experiments, we apply DARE to the extraction of two different 4-ary relations from different domains (Nobel Prize awards and MUC-6 management succession events (Grishman and Sundheim, 1996)). In the data set of the first domain, the connectivity between relation instances and linguistic patterns (rules) approximates the small world property (Amaral et al., 2005). In MUC-6 data on the other hand, the redundancy of both mentions of instances and patterns as well as their connectivity are very low. This paper investigates the application of an existing seed-based minimally supervised learning algorithm to different social domains exhibiting different properties of the available data. A systematic analysis studies the respective data properties of the three domains including the dis"
R11-1003,P03-1044,0,0.0769525,"Missing"
R11-1003,P10-1150,0,0.0496944,"Missing"
R11-1003,N10-1087,0,0.0473886,"Missing"
R11-1003,W06-0200,0,\N,Missing
R11-1052,W06-0204,0,0.0333742,"f the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations are stored in XML format. formance. As a web service, it offers a very user-friendly visualization of the"
R11-1052,C92-2082,0,0.0557225,"les and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations a"
R11-1052,R11-1003,1,0.886343,"hich learns relation extraction rules for dealing with relations of various complexity by utilizing some relation examples as semantic seed in the initialization and has achieved very promising results for the extraction of complex relations. In the recent years, more and more researchers are interested in understanding the underlying process behind this approach and attempt to identify relevant learning parameters to improve the system performance. Xu (2007) investigates the role of the seed selection in connection with the data properties in a careful way with our DARE system. Xu (2007) and Li et al. (2011) describe the applications of DARE system in different domains for different relation extraction tyes, for example, the NobelPrize-Winning event, management succession relations defined in MUC-6, marriage relationship, etc. Uszkoreit et al. (2009) describe a further empirical analysis of the seed construction and its influence on the learning performance and show that size, arity and distinctiveness of the seed examples play various important roles for the learning performance. Thus, the system demonstrated here, called META-DARE, serves as a monitoring and analysis system for conducting vario"
R11-1052,P03-1029,0,0.0125486,"gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations are stored in XML format. formance. As a web service, it offers a very user"
R11-1052,P07-1074,1,0.912011,"pecificity to start experiments on the example domains. Moreover, it provides a detailed survey of all learning iterations including the learned rules and extracted instances and their respective properties. Finally, it delivers a qualitative analysis of the learning perThis paper demonstrates a web-based online system, called META-DARE1 . META-DARE is built to assist researchers to obtain insights into seed-based minimally supervised machine learning for relation extraction. META-DARE allows researchers and students to conduct experiments with an existing machine learning system called DARE (Xu et al., 2007). Users can run their own learning experiments by constructing initial seed examples and can monitor the learning process in a very detailed way, namely, via interacting with each node in the learning graph and viewing its content. Furthermore, users can study the learned relation extraction rules and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a"
R11-1052,C10-2155,1,0.746712,". The following example shows two sentences from which relation 3 is extracted. (4) Visualization of Evaluation Results 1. Canadian economist Robert Mundell won the Nobel in economics for introducing foreign trade, capital movements, and currency swings into 3 http://dare.dfki.de/graph.jsp?f_id= example 382 id 1 2 3 4 instance number 1 1 2 3 2 12 2 1 3 1 prize area chemistry chemistry peace medicine chemistry peace literature physics economics year 1999 1998 1998 As illustrated in Figure 7 and 8, META-DARE also highlights the dangerous or bad rules and wrong relation instance. As described in Xu et al. (2010), the acquired rules are divided into four groups according to the extraction results: 1998 • useless, if the rule does not extract any instances. • good, if the rule extracts only correct instances. • dangerous, if the rule extract both correct and wrong instances. Table 2: Different seed constructions id 1 2 3 4 bootstrapping steps 7 10 6 5 extracted instances sum 4-arity 372 156 374 156 373 159 374 163 learned rules 1151 1146 1147 1117 • bad, if the rule extract only bad instances. In the learning graph, the rules from different group are colored in the following way: • useless rules: not f"
R11-1095,R11-1003,1,0.827965,"eyer” are the authors of publications about the same technology, they might be identified as name variants of the same person by our method. 3.2 DARE7 (Domain Adaptive Relation Extraction) is a minimally supervised machine learning framework for extracting relations of various complexity. It consists two major parts: 1) rule learning, 2) relation extraction. Rule learning and relation extraction feed each other in a bootstrapping framework. The bootstrapping starts from socalled “semantic seed” as a search query, which is a small set of instances of the target relation. (Uszkoreit, 2011) and (Li et al., 2011) describe the application and evaluation of DARE on different corpora for different relation extraction tasks. Currently DARE provides linguistic components which process English and German free texts. In T ECH WATCH T OOL, DARE is used to learn linguistic patterns to recognize sentences that potentially contain the trend information and also relations between persons and organizations. To learn patterns from trend sentences, we used the corpus offered by the project partner ThyssenKrupp AG, which is annotated with trend sentences and terms by the experts. From the annotation, we acquire examp"
R11-1095,P07-1074,1,0.774237,"we used the corpus offered by the project partner ThyssenKrupp AG, which is annotated with trend sentences and terms by the experts. From the annotation, we acquire examples as seed for DARE to learn patterns, e.g., NLP Tools In T ECH WATCH T OOL, named entity (NE) recognition and information extraction (IE) tools are applied to extract named entities (persons, organizations, etc.) and to detect relations or mentions of trends. Two tools are integrated in our system: 1. SProUT as named entity (Drozdzynski et al., 2004) and recognizer 2. DARE as relation extractor and trend sentence detector (Xu et al., 2007; Xu, 2007). 3.1 SProUT • (“lithium-ion battery”, “car”, “future”) SProUT6 (Shallow Processing with Unification and Typed Feature Structures) is a platform for development of multilingual shallow text processing and information extraction systems. It is a generic rule-based recognizer to extract named entities or concept terms. Users can write corresponding recognition patterns and specify linguistic resources, such as lexicons, gazetteers and tokenizers. The platform provides linguistic processing resources for several languages including English, German, etc. SProUT uses typed feature struct"
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S15-2056,E06-1002,0,0.0362967,"Missing"
S15-2056,D11-1072,0,0.101095,"Missing"
S15-2056,H93-1061,0,0.0294046,"Missing"
S15-2056,Q14-1019,0,0.0474194,"Missing"
S15-2056,P10-1040,0,0.0324483,"Missing"
S15-2056,P10-4014,0,0.0880334,"Missing"
S15-2056,S15-2049,0,\N,Missing
S17-1026,W04-3205,0,0.10063,"cktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it produces a binary decision (entailment, non-entailment) along with a computed confidence score. As the RTE engine was originally designed for sentences, rather than patterns, we converted each pattern into its textual representation. The variables expressing type and semantic role of the entities linked by the pattern were excluded in this representation, as the resulting variable alignments would skew the RTE engine’s entailment decision. For our experiments, we used the original MultiAlign implementation as well as an adapted version, in which we made some changes to the Word"
S17-1026,P15-1034,0,0.0497792,"semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities l"
S17-1026,W14-3348,0,0.0515783,"makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it produces a binary decision (entailment, non-entailment) along with a computed confidence score. As the RTE engine was originally designed for sentences, rather than patterns, we converted each pattern into its textual representation. The variables expressing type and semantic role of the entities linked by the pattern were excluded in this representation, as the resulting variable alignments would skew the RTE engine’s entailment decision. For our experiments, we used the ori"
S17-1026,L16-1537,1,0.852149,"xperiments, we split the evaluation set into a development set for optimizing the graph building parameters and a test set for the final evaluation. In our experiments, we tested several strategies for selecting patterns and measured performance over the annotated relation mentions in the evaluation dataset. For evaluating the graphbased methods, we selected all patterns entailing the base patterns [PERSON1 &lt;marry&gt; PERSON2 ] (for marriage) and [ORGANIZATION1 &lt;acquire&gt; For evaluating our method on the relation extraction task, we conducted experiments on two freely available datasets: TEG-REP (Eichler et al., 2016) and FB15k-237 (Toutanova et al., 2015). On the TEG-REG dataset, we carried out a detailed evaluation of several pattern filtering strategies with respect to two semantic relations. On the FB15k-237 corpus, we evaluate the scalability of our method to other semantic relations. 5.1 TEG-REP The TEG-REP corpus contains automatically derived relation extraction patterns as well as goldstandard entailment graphs created from these patterns for three relations typically considered in RE tasks: marriage, acquisition, and award honor. The patterns underlying this corpus are a subset of the patterns us"
S17-1026,P12-1013,0,0.116832,"Sample set of RTE decisions (YES: entailment, NO: no entailment) with associated confidence Figure 3: Sample outputs using greedy (left) and global (right) graph optimizer to a set of decisions that is invalid given the transitivity of the entailment relation. For deriving a consistent graph, we applied two different strategies: First, a simple greedy strategy that assumes each computed positive entailment relation with a confidence exceeding a pre-defined threshold to be valid, and adds missing entailment edges to ensure transitive closure. Second, the global graph optimization algorithm by Berant et al. (2012), which searches for the best graph under a global transitivity constraint, approaching the optimization problem by Integer Linear Programming. The selection of the optimization strategy is crucial, as illustrated in Figure 3, which shows two sample outputs from each of the two strategies for the decisions in Figure 2. 4 2. Generate an entailment graph EG expressing entailment relations among the patterns in P . 3. Choose a base pattern5 , expressing the target relation explicitly and select all patterns entailing the base pattern according to EG. 4. Apply the selected patterns to extract rela"
S17-1026,P10-1124,0,0.194142,"proach taken in the RTE challenges (Dagan et al., 2005). 220 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 220–229, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work for individual T/H pairs. We propose to exploit entailment relationships holding among RE patterns by structuring the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast"
S17-1026,D15-1075,0,0.0473261,"t is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it p"
S17-1026,N06-1039,0,0.0590712,"This is motivated by the fact that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it fa"
S17-1026,P14-5008,1,0.818781,", 2010). 3.2 RTE Engine For recognizing entailment relations between individual T/H pairs of patterns, we make use 3 For reasons of simplicity, the figure shows the text representation of the patterns, which are in fact represented as dependency structures. Since entailment is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning"
S17-1026,W11-0201,0,0.0273779,"ct that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressi"
S17-1026,N13-1018,0,0.0152046,"llenges (Dagan et al., 2005). 220 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 220–229, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work for individual T/H pairs. We propose to exploit entailment relationships holding among RE patterns by structuring the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast to their work, we construct"
S17-1026,W15-4007,0,0.0134313,"ate if the models trained on T/H pairs for one relation are general enough to be used for computing entailment relations among pattern candidates for other semantic relations. To this end, we used the FB15k237 corpus (Toutanova et al., 2015), which contains knowledge-base relation triples and textual mentions of Freebase entity pairs. For our experiments on this corpus, we generated candidate patterns by extracting the first 1000 tuples matching a particular relation from the pattern files in the corpus, and then extracting all patterns linking any of the tuples in the textual triples used by Toutanova and Chen (2015). This way, our candidate pattern set contains both patterns expressing the target relation as well as patterns expressing other relations. For creating the entailment graph, we converted all patterns into a textual representation, removed patterns with no lexical item, and, from the remaining patterns, built an entailment graph applying the RTE engine described in 3.2 with the model trained on the marriage relation and the best parameter setting derived based on the TEG-REP corpus. For evaluating the result, we selected 10 relations, defined a base pattern for each of them, and checked, for e"
S17-1026,D15-1174,0,0.133243,"et into a development set for optimizing the graph building parameters and a test set for the final evaluation. In our experiments, we tested several strategies for selecting patterns and measured performance over the annotated relation mentions in the evaluation dataset. For evaluating the graphbased methods, we selected all patterns entailing the base patterns [PERSON1 &lt;marry&gt; PERSON2 ] (for marriage) and [ORGANIZATION1 &lt;acquire&gt; For evaluating our method on the relation extraction task, we conducted experiments on two freely available datasets: TEG-REP (Eichler et al., 2016) and FB15k-237 (Toutanova et al., 2015). On the TEG-REG dataset, we carried out a detailed evaluation of several pattern filtering strategies with respect to two semantic relations. On the FB15k-237 corpus, we evaluate the scalability of our method to other semantic relations. 5.1 TEG-REP The TEG-REP corpus contains automatically derived relation extraction patterns as well as goldstandard entailment graphs created from these patterns for three relations typically considered in RE tasks: marriage, acquisition, and award honor. The patterns underlying this corpus are a subset of the patterns used by Moro et al. (2013) and were acqui"
S17-1026,D12-1104,0,0.0324044,"the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast to their work, we construct pattern-based entailment graphs using RTE technology. This is motivated by the fact that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in vario"
S17-1026,S15-1022,1,0.856577,"s of patterns, we make use 3 For reasons of simplicity, the figure shows the text representation of the patterns, which are in fact represented as dependency structures. Since entailment is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provi"
S17-1026,D11-1135,0,0.0349594,"in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities linked by patterns P1 to P3 are mentions of the marriage relation, whereas this is not necessarily true of entities linked"
S17-1026,N13-1008,0,0.0620861,"tering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities linked by patterns P1 to P3 are mentions of the marriage relation, whereas this is not necessarily true of entities linked by pattern P4. Similarly, clustering can identify patterns P1 and P3 as semantically related. However, it cannot express that the relation expressed by pattern P3 entails the relation expressed by pattern P1, but not vice versa. These asymmetric relationships have been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above,"
S17-1026,E06-1052,0,0.0373823,"ce versa. These asymmetric relationships have been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above, RE can clearly benefit from considering semantic relationships holding among extraction patterns. However, previous work in RE has either focussed on grouping related patterns without considering non-symmetric relations, or, on computing entailment decisions 3 3.1 Entailment Graph Generation Pattern-Based Entailment Graphs A pattern-based entailment graph refers to a directed graph, in which each node represents a unique RE pattern, and each edge (→) denotes an entailment relationship. Bidirectional edges (↔) denote that the pa"
S17-1026,P09-2015,0,0.0353095,"e been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above, RE can clearly benefit from considering semantic relationships holding among extraction patterns. However, previous work in RE has either focussed on grouping related patterns without considering non-symmetric relations, or, on computing entailment decisions 3 3.1 Entailment Graph Generation Pattern-Based Entailment Graphs A pattern-based entailment graph refers to a directed graph, in which each node represents a unique RE pattern, and each edge (→) denotes an entailment relationship. Bidirectional edges (↔) denote that the patterns represented by the two nodes are con"
S17-1026,W14-1610,0,\N,Missing
schafer-etal-2008-extracting,copestake-flickinger-2000-open,0,\N,Missing
schafer-etal-2008-extracting,schafer-2006-ontonerdie,1,\N,Missing
thomas-etal-2017-streaming,C10-1032,0,\N,Missing
thomas-etal-2017-streaming,C10-1011,0,\N,Missing
thomas-etal-2017-streaming,P09-1113,0,\N,Missing
thomas-etal-2017-streaming,P07-1074,1,\N,Missing
thomas-etal-2017-streaming,P14-5010,0,\N,Missing
thomas-etal-2017-streaming,doddington-etal-2004-automatic,0,\N,Missing
thomas-etal-2017-streaming,P14-5007,0,\N,Missing
thomas-etal-2017-streaming,D13-1100,0,\N,Missing
W05-0401,W96-0211,0,0.0315055,"weights and identification thresholds for different NERs or non-NERs. Thus, the learning results provide an identification references for the forthcoming NER identification. 3.1 Relation Features Relation features, by which we can effectively identify different NERs, are defined for capturing critical information of the Chinese language. According to the features, we can define NER / nonNER patterns. The following essential factors motivate our definition for relation features: • The relation features should be selected from multiple linguistic levels, i.e., morphology, grammar and semantics (Cardie, 1996); • They can help us to identify NERs using positive and negative case-based machine learning as their information do not only deal with NERs but also with non-NERs; and • They should embody the crucial information of Chinese language processing (Dang et al., 2002), such as word order, the context of words, and particles etc. There are a total of 13 relation features shown in Table 2, which are empirically defined according to the above motivations. It should be explained that in order to distinguish feature names from element names of the NER / non-NER patterns, we add a capital letter “F” in"
W05-0401,C02-1143,0,0.0176865,"t NERs, are defined for capturing critical information of the Chinese language. According to the features, we can define NER / nonNER patterns. The following essential factors motivate our definition for relation features: • The relation features should be selected from multiple linguistic levels, i.e., morphology, grammar and semantics (Cardie, 1996); • They can help us to identify NERs using positive and negative case-based machine learning as their information do not only deal with NERs but also with non-NERs; and • They should embody the crucial information of Chinese language processing (Dang et al., 2002), such as word order, the context of words, and particles etc. There are a total of 13 relation features shown in Table 2, which are empirically defined according to the above motivations. It should be explained that in order to distinguish feature names from element names of the NER / non-NER patterns, we add a capital letter “F” in the ending of feature names. In addition, a sentence group in the following definitions can contain one or multiple sentences. In other words, a sentence group must end with a stop, semicolon, colon, exclamation mark, or question mark. Feature Category SGTF NESPF"
W05-0401,W03-1708,1,0.262975,"Missing"
W05-0401,W00-1210,0,0.0277709,"Average Precision 91.67 87.50 75 68.75 68.19 81.67 66.67 65 50 66.67 75 37.50 30 31.25 63.92 Average F-measure 95.65 93.33 85.71 81.48 77.93 76.65 76.19 71.72 66.67 66.67 66.67 46.15 35.30 34.09 70.46 Table 4: Identification Performance for 14 NERs by PNCBL Finally, we have to acknowledge that it is difficult to compare the performance of our method to others because the experimental conditions and corpus domains of other NER identification efforts are quite different from ours. Nevertheless, we would like to use the performance of Chinese NER identification using memory-based learning (MBL) (Zhang and Zhou, 2000) for a comparison with our approach in Table 5. In the table, we select similar NERs in our domain to correspond to the three types of the relations (employee-of, product-of, and location-of). From the table we can deduce that the identification performance of relations for PNCBL is roughly comparable to that of the MBL. Method MBL&I PNCBL&I Relation Type Recall Precision F-measure employee-of 75.60 92.30 83.12 product-of 56.20 87.10 68.32 location-of 67.20 75.60 71.15 PS_TM PS_CP PS_ID 80 60 72.22 65 75 81.67 71.72 66.67 76.65 ID_TM TM_CP 90.91 100 68.19 87.50 77.93 93.33 CP_LOC PS_CPC TM_CPC"
W06-3001,W04-2509,0,0.215646,"Missing"
W06-3001,E89-1039,0,\N,Missing
W06-3001,P83-1025,0,\N,Missing
W06-3001,J86-3001,0,\N,Missing
W06-3001,A00-1041,0,\N,Missing
W08-2126,C04-1186,0,0.0550371,"ypes (POSes marked as predicates for at least 50 times in the training set). This helps to significantly improve the system efficiency in both training and prediction time without sacrificing prediction accuracy. It should be noted that the prediction of nominal predicates are generally much more difficult (based on CoNLL 2008 shared task annotation). The PI model achieved 96.32 F-score on WSJ with verbal predicates, but only 84.74 on nominal ones. Argument Identification After PI, the arguments to the predicted predicates are identified with the AI component. Similar to the approach taken in Hacioglu (2004), we use a statistical classifier to select from a set of candidate nodes in a dependency tree. However, instead of selecting from a set of neighboring nodes from the predicate word 2 , we define the concept of argument path as a chain of dependency relations from the predicate to the argument in the dependency tree. For instance, an argument path [ |] indicates that if the predicate is syntactically depending as  on a node which has a  child, then the  node 2 Hacioglu (2004) defines a tree-structured family of a predicate as a measure of locality. It is a set of dependency rela"
W08-2126,H05-1066,0,0.0287923,"predictions. In particular, the second part can be further divided into four stages: predicate identification (PI), argument identification (AI), argument classification (AC), and predicate classification (PC). Maximum entropy-based machine learning techniques are used in both components which we will see in detail in the following sections. 3 • Root attachments: the number of tokens attached to the ROOT node by the parser in one sentence Syntactic Dependency Parsing For obtaining syntactic dependencies, we have combined the results of two state-of-the-art dependency parsers: the MST parser (McDonald et al., 2005) and the MaltParser (Nivre et al., 2007). The MST parser formalizes dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. A major advantage of their framework is the ability to naturally and efficiently model both projective and non-projective parses. To learn these structures they used online largemargin learning that empirically provides state-ofthe-art performance. The MaltParser is a transition-based incremental dependency parser, which is language-independent and data-driven. It contains a deterministic algorithm, which can be viewed as a variant of the bas"
W08-2126,W08-2121,0,0.168141,"Missing"
W08-2126,U05-1006,1,0.88562,"Missing"
W08-2126,W07-1217,1,0.85909,"Missing"
W09-0405,W07-0726,1,0.843237,"ata is included in this year’s With the wealth of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton"
W09-0405,W08-0328,1,0.915116,"ction 4 its application in a number of experiments. Finally, Section 5 concludes this paper with a summary and some thoughts on future work. We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU. 1 2 Introduction Integrating Multiple Systems of Unknown Type and Quality When comparing (Eisele et al., 2008) to the present work, our proposal is more general in a way that the requirement for knowledge about the systems is minimum. The types and the identities of the participated systems are assumed unknown. Accordingly, we are not able to restrict ourselves to a certain class of systems as (Eisele et al., 2008) did. We rely on a standard phrase-based SMT framework to extract the valuable pieces from the system outputs. These extracted segments are also used to improve an existing SMT system that we have access to. While (Eisele et al., 2008) included translations from all of a fixed number of RBMT"
W09-0405,P07-1040,0,0.046861,"ion systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translation"
W09-0405,W08-0332,0,0.037797,"h directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez, 2008). In our experiments, we selected a subset of 5 systems for the combination, in most cases, based on BLEU. On the other hand, some systems may be designed in a way that they deliver interesting unique translation segments. Therefore, we also measure the similarity among system outputs as shown in Table 2 in a given collection by calculating average similarity scores across every pair of outputs. Num. Median Range Top 5 Median Range de-en 20 19.87 16.37 de-en 22.26 4.31 fr-en 23 26.55 17.06 fr-en 27.93 4.76 es-en 28 22.50 9.74 es-en 26.43 5.71 en-de 15 13.78 4.75 en-de 15.21 1.71 en-fr 16 24.76"
W09-0405,D08-1011,0,0.0384327,"e nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translations should be produc"
W09-0405,W07-0733,0,0.0224942,"ur hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. The method requires tuning on at least six more features, which expands the search space for the translation task unnecessarily. We instead integrate the translation models from multiple sources by extending the phrase table. In contrast to the prior approach presented in (Chen et al., 2007) and (Eisele et al., 2008) which concatenates the phrase tables and adds new features as system markers, our extension method avoids duplicate entries in the final combined table. Given a set of hypothesis translation models"
W09-0405,2005.mtsummit-papers.11,0,0.132271,"ing sentence set. task. In this paper, we use the Moses decoder to construct translations from the given system outputs. We mainly propose two slightly different ways: One is to construct translation models solely from the given translations and the other is to extend an existing translation model with these additional translations. 3 3.2 Sometimes, the goal of system combination is not only to produce a translation but also to improve one of the systems. In this paper, we aim at incorporating the additional system outputs to improve an out-of-domain SMT system trained on the Europarl corpus (Koehn, 2005). Our hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at onc"
W09-0405,E06-1005,0,0.0367832,"th of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the ad"
W09-0405,J03-1002,0,0.0039123,"parseness problem). However, in the system combination task, this is no longer an issue as the system only needs to translate sentences within the data set. When more translation engines are available, the size of this set becomes larger. Hence, we collect translations from all available systems and pair them with the corresponding input text, thus forming a medium-sized “hypothesis” corpus. Our system starts processing this corpus with a standard phrase-based SMT setup, using the Moses toolkit (Koehn et al., 2007). The hypothesis corpus is first tokenized and lowercased. Then, we run GIZA++ (Och and Ney, 2003) on the corpus to obtain word alignments in both directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combin"
W09-0405,P03-1021,0,0.0175625,"15.21 1.71 en-fr 16 24.76 11.05 en-fr 26.62 0.68 input texts. Moreover, we also generate “hypothesis” LMs solely based on the given system outputs, that is, LMs that model how the candidate systems convey information in the target language. These LMs do not require any additional training data. Therefore, we do not require any training data other than the given system outputs by using the “hypothesis” language model and the “hypothesis” translation model. 3.5 After building the models, it is essential to tune the SMT system to optimize the feature weights. We use Minimal Error Rate Training (Och, 2003) to maximize BLEU on the complete development data. Unlike the standard tuning procedure, we do not tune the final system directly. Instead, we obtain the weights using models built from the tuning portion of the system outputs. For each combination variant, we first train models on the provided outputs corresponding to the tuning set. This system, called the tuning system, is also tuned on the tuning set. The initial weights of any additional features not included in the standard setting are set to 0. We then adapt the weights to the system built with translations corresponding to the test se"
W09-0405,P02-1040,0,\N,Missing
W09-0405,W05-0909,0,\N,Missing
W09-0405,P07-2045,0,\N,Missing
W09-0411,W07-0732,0,0.028283,"d MT technology as summarized by (Lopez, 2008), including the best statistical, rulebased and example-based systems, produces output rife with errors. Those systems may employ different algorithms or vary in the linguistic resources they use which in turn leads to different characteristic errors. Besides continued research on improving MT techniques, one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages (Macherey and Och, 2007; Rosti et al., 2007a). Current approaches for system combination involve post-editing methods (Dugast et al., 2007; Theison, 2007), re-ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is needed to identify translation options within the different systems’ translations. Word alignment is computed using"
W09-0411,gimenez-amigo-2006-iqmt,0,0.0426571,"Missing"
W09-0411,D07-1105,0,0.0237618,"tion (MT) in the last decade, automatic translation is still far away from satisfactory quality. Even the most advanced MT technology as summarized by (Lopez, 2008), including the best statistical, rulebased and example-based systems, produces output rife with errors. Those systems may employ different algorithms or vary in the linguistic resources they use which in turn leads to different characteristic errors. Besides continued research on improving MT techniques, one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages (Macherey and Och, 2007; Rosti et al., 2007a). Current approaches for system combination involve post-editing methods (Dugast et al., 2007; Theison, 2007), re-ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is"
W09-0411,J03-1002,0,0.00263951,"ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is needed to identify translation options within the different systems’ translations. Word alignment is computed using the GIZA++ toolkit (Och and Ney, 2003), only one-to-one word alignments are employed. Select substitution candidates. For the shared task, we decide to substitute nouns, verbs and adjectives based on the available POS tags. Initially, any such source word is considered as a possible substitution candidate. As we do not want to require substitution canProceedings of the Fourth Workshop on Statistical Machine Translation , pages 70–74, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 70 • University of Karlsruhe (uka) didates to have exactly the same POS tag as the source, we use groups of “"
W09-0411,N07-1029,0,0.212533,"2 , Andreas Eisele1,2 , Hans Uszkoreit1,2 , Yu Chen2 , Michael Jellinghaus2 , Sabine Hunsicker2 1: Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz GmbH, Saarbr¨ucken, Germany 2: Universit¨at des Saarlandes, Saarbr¨ucken, Germany {cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de Abstract frame selected due to the high quality of its syntactic structure. The underlying idea of the approach is the improvement of a given (original) translation through the exploitation of additional translations of the same text. This can be seen as a simplified version of (Rosti et al., 2007b). Considering our submission from the shared translation task as the ”trusted” frame, we add translations from four additional MT systems that have been chosen based on their performance in terms of automatic evaluation metrics. In total, the combination system performs 1,691 substitutions, i.e., an average of 0.67 substitutions per sentence. We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precompute"
W09-0411,P07-1040,0,0.0901855,"2 , Andreas Eisele1,2 , Hans Uszkoreit1,2 , Yu Chen2 , Michael Jellinghaus2 , Sabine Hunsicker2 1: Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz GmbH, Saarbr¨ucken, Germany 2: Universit¨at des Saarlandes, Saarbr¨ucken, Germany {cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de Abstract frame selected due to the high quality of its syntactic structure. The underlying idea of the approach is the improvement of a given (original) translation through the exploitation of additional translations of the same text. This can be seen as a simplified version of (Rosti et al., 2007b). Considering our submission from the shared translation task as the ”trusted” frame, we add translations from four additional MT systems that have been chosen based on their performance in terms of automatic evaluation metrics. In total, the combination system performs 1,691 substitutions, i.e., an average of 0.67 substitutions per sentence. We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precompute"
W10-1708,2001.mtsummit-papers.68,0,0.0402506,"n-matching insertions. For this, we have (manually) derived a set of factors that are checked for each of the phrase pairs that are processed. The factors are described briefly below: prepositions finally, we give prepositions a special treatment. Experience from last year’s shared task had shown that things like double prepositions contributed to a large extent to the amount of avoidable errors. We tried to circumvent this class of error by identifying the correct prepositions; erroneous prepositions are removed. 3 Hybrid Translation Analysis We evaluated the intermediate outputs using BLEU (Papineni et al., 2001) against human references as in table 3. The BLEU score is calculated in lower case after the text tokenization. The translation systems compared are Moses, Lucy, Google and our hybrid system with different configurations: identical? simply checks whether two candidate phrases are identical. too complex? a Lucy phrase is “too complex” to substitute if it contains more than 2 embedded noun phrases. many-to-one? this factor checks if a Lucy phrase containing more than one word is mapped to a Moses phrase with only one token. contains pronoun? checks if the Lucy phrase contains a pronoun. contain"
W10-1708,W09-0411,1,0.72292,"shows that the substitution method works for different language configurations. Introduction In recent years the quality of machine translation (MT) output has improved greatly, although each paradigm suffers from its own particular kind of errors: statistical machine translation (SMT) often shows poor syntax, while rule-based engines (RBMT) experience a lack in vocabulary. Hybrid systems try to avoid these typical errors by combining techniques from both paradigms in a most useful manner. In this paper we present the improved version of the hybrid system we developed last year’s shared task (Federmann et al., 2009). We take the output from an RBMT engine as basis for our hybrid translations and substitute noun phrases by translations from an SMT engine. Even though a general increase in quality could be observed, our system introduced errors of its own during the substi2 Architecture Our hybrid translation system takes translation output from a) the Lucy RBMT system (Alonso and Thurmair, 2003) and b) a Moses-based SMT system (Koehn et al., 2007). We then identify noun phrases inside the rule-based translation and compute the most likely correspondences in the statistical translation output. For these, w"
W10-1708,P07-2045,0,0.00999536,"niques from both paradigms in a most useful manner. In this paper we present the improved version of the hybrid system we developed last year’s shared task (Federmann et al., 2009). We take the output from an RBMT engine as basis for our hybrid translations and substitute noun phrases by translations from an SMT engine. Even though a general increase in quality could be observed, our system introduced errors of its own during the substi2 Architecture Our hybrid translation system takes translation output from a) the Lucy RBMT system (Alonso and Thurmair, 2003) and b) a Moses-based SMT system (Koehn et al., 2007). We then identify noun phrases inside the rule-based translation and compute the most likely correspondences in the statistical translation output. For these, we apply a factored substitution method that decides whether the original RBMT phrase should be kept or rather be replaced by the Moses phrase. As this shallow substitution process may introduce problems at 77 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77–81, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Moses SMT System We used a state-of-the-art"
W10-1708,P02-1040,0,\N,Missing
W11-2161,J93-2003,0,0.0156366,"ule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smooth"
W11-2161,J07-2003,0,0.052603,"dings of the 6th Workshop on Statistical Machine Translation, pages 485–489, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics improves the translation performance more than the tuning on test2008 in a small-scale experiment for the tree-based system. 2.3 Hierarchical phrase-based system For the hierarchical system, we used the open source hierarchical phrased-based system Jane, developed at RWTH and free for non-commercial use (Vilar et al., 2010). This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps (Chiang, 2007). In this way long-range dependencies and reorderings can be modeled in a consistent statistical framework. The system uses a fairly standard setup, trained using the bilingual data provided by the organizers, word aligned using the mgiza. Two 5-gram language models were used during decoding: one trained on the monolingual part of the bilingual training data, and a larger one trained on the additional news data. Decoding was carried out using the cube pruning algorithm. The tuning is performed on test2008 without further experiments. 2.4 Rule-based systems We applied two rule-based translation"
W11-2161,2008.tc-1.2,0,0.191203,"tems are combined at the level of the final translation output. The translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations. 1 Individual translation systems 2.1 Introduction Machine translation (MT), in particular the statistical approach to it, has undergone incremental improvements in recent years. While rule-based machine translation (RBMT) maintains competitiveness in human evaluations. Combining the advantages of both approaches have been investigated by many researchers such as (Eisele et al., 2008). Nonetheless, significant improvements over statistical approaches still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on th"
W11-2161,W10-1708,1,0.849156,"translation systems 2.1 Introduction Machine translation (MT), in particular the statistical approach to it, has undergone incremental improvements in recent years. While rule-based machine translation (RBMT) maintains competitiveness in human evaluations. Combining the advantages of both approaches have been investigated by many researchers such as (Eisele et al., 2008). Nonetheless, significant improvements over statistical approaches still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual"
W11-2161,D08-1011,0,0.0236919,"stem is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with ma"
W11-2161,2010.amta-papers.34,0,0.0250441,"es still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalt"
W11-2161,P07-2045,0,0.00338684,"hrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smoothing using SRILM toolkit (Stolcke, 2002). We did not perform any tuning, because it hurts the evaluation performance in our experiments. 2.2 Syntax-based sy"
W11-2161,2005.mtsummit-papers.11,0,0.0347851,"Missing"
W11-2161,W11-2100,0,0.0883295,"uage models for tuning and all target side of parallel data to train language models for decoding. The beam size is set to 80, and 300 nbest is considered. 4 4.1 Translation experiments MT Setup The parallel training corpus consists of 1.8 million German-English parallel sentences from Europarl-v6 (Koehn, MT Summit 2005) and newscommentary with 48 million tokenized German words and 54 million tokenized English words respectively. The monolingual training corpus contains the target side of the parallel training corpus and the additional monolingual language model training data downloaded from (SMT, 2011). We did not apply the large-scale Gigaword corpus, because it does not significantly reduce the perplexity of our language model but raises the computational requirement heavily. 4.2 Single systems For each individual translation system, different configurations are experimented to achieve a higher translation quality. We take phrase- and syntaxbased translation system as examples. Table 1 presents official submission result on DE-EN by PBT+Syntax PBT+Syntax+HPBT PBT+HPBT+Linguatec+Lucy PBT+Syntax+HPBT+Linguatec+Lucy 20.37 20.78 20.27 20.81 Hybrid-2010 PBT Syntax HPBT Linguatec Lucy Hybrid-20"
W11-2161,C08-1115,0,0.0196628,"ms We applied two rule-based translation systems, the Lucy system (Lucy, 2011) and the Linguatec system (Aleksi´c and Thurmair, 2011). The Lucy system is a recent offspring of METAL. The Linguatec system is a modular system consisting of grammar, lexicon and morphological analyzers based on logic programming using slot grammar. 3 PBT-2010 Max80words Max100words +Compound +Newparallel Hybrid translation A hybrid approach combining rule-based and statistical machine translation is usually investigated with an in-box integration, such as multi-way translation (Eisele et al., 2008), post-editing (Ueffing et al., 2008) or noun phrase substitution (Federmann et al., 2010). However, significant improvements over state-of-the-art statistical machine translation are still expected. In the meanwhile system combination methods for instance described in (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008) are mostly evaluated to combine statistical translation systems, rule-based systems are not considered. In this work, we integrate the rule-based and statistical machine translation system on the level of the final 486 PBT 18.32 20.65 20.78 21.52 21.77 Syntax 21.10 22.13 Table 1: Translation performance BLEU"
W11-2161,W10-1738,1,0.847962,"eature weights using mert, because the tuning on test2007 1 http://geek.kyloo.net/software/doku.php/mgiza:overview 485 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485–489, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics improves the translation performance more than the tuning on test2008 in a small-scale experiment for the tree-based system. 2.3 Hierarchical phrase-based system For the hierarchical system, we used the open source hierarchical phrased-based system Jane, developed at RWTH and free for non-commercial use (Vilar et al., 2010). This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps (Chiang, 2007). In this way long-range dependencies and reorderings can be modeled in a consistent statistical framework. The system uses a fairly standard setup, trained using the bilingual data provided by the organizers, word aligned using the mgiza. Two 5-gram language models were used during decoding: one trained on the monolingual part of the bilingual training data, and a larger one trained on the additional news data. Decoding was carried out using the cube pruning algorithm. The tu"
W11-2161,C96-2141,0,0.206957,"ion, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smoothing using SRILM toolkit (Stolcke, 2002). We did not"
W11-2915,P05-1022,0,0.149875,"Missing"
W11-2915,W08-1301,0,0.0169911,"do et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The out"
W11-2915,D10-1068,0,0.0167683,"rd making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hybrid approaches. However, before targeting these medium-term goals we plan to investigate whether our approach can also be applied to other parsers wit"
W11-2915,P10-1074,0,0.0388319,"Missing"
W11-2915,P06-2034,0,0.0499667,"Missing"
W11-2915,W06-0204,0,0.0712755,"Missing"
W11-2915,C02-2025,0,0.0824744,"uistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of R"
W11-2915,C96-1079,0,0.059077,"erformance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambig"
W11-2915,W10-2105,0,0.0515468,"Missing"
W11-2915,I05-1018,0,0.0300548,"election of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by"
W11-2915,W09-2205,0,0.0249408,"e relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between th"
W11-2915,W07-2202,0,0.0200102,"ct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward"
W11-2915,D08-1050,0,0.0271645,"of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfo"
W11-2915,P06-1043,0,0.025829,"not necessarily correspond to a better parse ranking for other purposes or for generic parsing. This should not be surprising since relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treeba"
W11-2915,P03-1029,0,0.0867986,"Missing"
W11-2915,N10-1004,0,0.026464,"that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutano"
W11-2915,P05-1073,0,0.167111,"e gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less"
W11-2915,W10-1905,0,0.143777,"r the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutanova et al., 2005b), P"
W11-2915,P07-1074,1,0.945583,"it, Sebastian Krause DFKI, LT-Lab, Germany {feiyu,lihong,Yi.Zhang,uszkoreit,sebastian.krause}@dfki.de Abstract exploit domain knowledge with minimal human effort. Many IE systems benefit from combining generic NLP components with task-specific extraction methods. Various machine learning approaches have been employed for adapting the IE methods to new domains and extraction tasks (e.g., (Yangarber, 2001; Sudo et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handc"
W11-2915,C10-2155,1,0.922211,"Therefore, the confidence values can be utilized as feedback to the parser to help it to rerank its readings. Figure 1: DARE core architecture Relying entirely on semantic seeds as domain knowledge, DARE can accommodate new relation types and domains with minimal effort. Since we had already reported on experiments applying the framework to different relation types and corpora including MUC-6 data in the cited papers, including comparisons with other ML approaches to RE (Xu, 2007; Uszkoreit et al., 2009), we omit a comparative discussion here. For confidence estimation, the method proposed by Xu et al. (2010) is adopted.1 Actually, in (2) we propose an extended version of the rule scoring, since the rule scoring in (Xu et al., 2010) did not consider the case when a learned rule does not extract any new instances. Thus, given the scoring of instances, the confidence value of a rule is the average score of all instances (Iextracted ) extracted by this rule or the average score of seed instances (Irule ) from which they are learned. Through the factor δ we reduce the score of rules that have not proven yet their potential for extracting instances. 4.2 Reranking Architecture and Method Figure 2 depict"
W11-2915,P09-1043,1,0.850491,"and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue is recently acknowledged in most parsing systems and known as the domain adaptation task. 3 DARE and Confidence Estimation DARE (Xu et al., 2007; Xu, 2007) is a min"
W11-2915,W07-2207,1,0.836424,"m. Section 6 discusses related work. Finally, Section 7 summarizes the results and suggests directions for further research. 2 where w is the given input sentence and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue"
W11-2915,N10-1002,1,0.846736,"similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward toward making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hy"
W11-2915,P02-1062,0,\N,Missing
W15-4204,P09-1113,0,0.153013,"Missing"
W15-4204,P98-1013,0,0.707625,"hese relations in natural language text. Lexicalsemantic resources focus on linkage at the level of individual lexical items. For example, BabelNet integrates entity information from Wikipedia with word senses from WordNet, UWN is a multilingual WordNet built from various resources, and UBY integrates several linguistic resources by linking them at the word-sense level. Linguistic knowledge resources that go beyond the level of lexical items are scarce and of limited coverage due to significant investment of human effort and expertise required for their construction. Among these are FrameNet (Baker et al., 1998), which provides fine-grained semantic relations of predicates and their arguments, and VerbNet (Schuler, 2005), which models verb-class specific syntactic and semantic preferences. What is missing, therefore, is a large-scale, preferably automatically constructed linguistic resource that links language expressions at the phrase or sentence level to the semantic relations of knowledge bases, as well as to existing terminological resources. Such a repository would be very useful for many information extraction tasks, e.g., for relation extraction and knowledge base population. We aim to fill th"
W15-4204,Q14-1019,0,0.0348974,"ing sar-graphs to the linguistic LOD cloud, this mapping allows us to augment the lexico-syntactic and semantic information specified in sar-graphs with lexical semantic knowledge from the linked resources. In particular, we introduce new vertices for synonyms, and add new edges based on the lexical semantic relations specified in BabelNet. In Figure 1, these additional graph elements are represented as dashed vertices and edges. To link sar-graph vertices to Babelnet, we disambiguate content words in our pattern extraction pipeline (see Section 5), using the graph-based approach described by Moro et al. (2014). The disambiguation is performed on a per-sentence basis, considering all content words in the sentence as potentially ambiguous mentions if they correspond to at least one candidate meaning in BabelNet. This includes multi-token sequences containing at least one noun. The candidate senses (synset identifiers) of all mentions in a sentence are linked to each other via their BabelNet relations to create a graph. The approach then iteratively prunes low-probability candidate senses from the graph to select the synset assignment that maximizes the semantic agreement within a given sentence. Once"
W15-4204,E12-1059,0,0.106744,"Missing"
W15-4204,C98-1013,0,\N,Missing
W15-4405,ai-etal-2014-sprinter,1,0.880256,"Missing"
W15-4405,D11-1142,0,0.0572628,"Missing"
W15-4405,D12-1104,0,0.0139533,"Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 26–33, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2.2 Supervision by teacher Named-entity recognition Identified entity mentions Relation extraction Texts Relation instances Answers generation & multiple choices compilation A key part of our approach is the application of a pattern-based relation extraction (RE) system. Such systems, e.g., NELL (Carlson et al., 2010; Mitchell et al., 2015), PATTY (Nakashole et al., 2012), DARE (Xu et al., 2007), rely on lexicosyntactic patterns that pose restrictions on the surface level or grammatical level of sentences. Their underlying assumption is that whenever a given sentence matches a given pattern (i.e., a sentence template), the sentence expresses the pattern’s corresponding semantic relation. This assumption does not always hold, hence the system output usually contains a certain amount of noise, which makes a human-in-the-loop necessary for high-precision applications. Typically, RE systems associate patterns with a confidence score of some kind, allowing downstre"
W15-4405,P05-1045,0,0.00821473,"a teacher to compile actual reading-comprehension exercises suitable for presentation to learners, given only our approach’s resulting candidates. 2.1 Relation Extraction Reading-comprehension exercises The reading-comprehension exercises generated by our approach ask for relational facts mentioned in a text. In order to automatically identify these, we apply a series of processing steps, as depicted in Figure 1. In the first step, the input texts, e.g., news articles, are processed by a standard component for named-entity recognition (e.g., the well-known Stanford Named Entity Recognizer by Finkel et al. (2005)), in order to identify persons, organization, locations, etc. mentioned in the text, followed by application of a relation extraction system for the identification of facts. The information about mentioned facts and entities is passed on to a further processing step in which a mentioned instance of a semantic relation is transformed into a natural-language statement, paraphrasing the original occurrence of the fact. Furthermore, the information about named-entity occurrences is used to create false statements about relations between the entities. For each fact identified in a text, four choic"
W15-4405,P14-1084,0,0.0295347,"Missing"
W15-4405,W12-2039,0,0.0280881,"ticated reading-comprehension capabilities of language learners. An area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, com"
W15-4405,W00-0603,0,0.0168134,"pable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit that once created such tests require relatively few work on the teacher’s side in order to assess a learner’s skill level. At the core of our approach is the application of existing informationextraction approaches, mainly from the sub-area of relation extraction, for the identif"
W15-4405,S13-1041,0,0.0129197,"language learners. An area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit"
W15-4405,W14-3505,0,0.0165894,"area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit that once created suc"
W15-4405,krause-etal-2014-language,1,0.834374,"same meaning. For example, a sentence template “wife had a kid from husband” is formed from one of the patterns used in the marriage relation. The paraphrasing engine takes this template as input and provides templates with the same words in natural language, e.g., “from husband wife had a kid”. Both templates are treated as valid and randomly chosen to create answers. 2.4 Human supervision As already noted earlier, employing automatic information-extraction methods has the disadvantage of inevitable noise in the system output. with sans-serif font represent entity placeholders. 28 articles (Krause et al., 2014), and measured the productivity of our approach for automatic question and answer generation. For these first experiments, we used the available gold-standard entity annotation. For the relation-extraction part, we applied the RE patterns of Moro et al. (2013) to automatically extract the relations between the annotated entities in the text. These patterns are based on the dependency-grammar analysis of sentences and were extracted from a large web corpus, hence they should provide enough variation for both the detection of relation mentions in texts as well as the generation of statements abo"
W15-4405,C12-2127,0,0.0624695,"ally generated by filling arguments into sentence templates. These templates are created based on patterns that were used for relation extraction in the previous step, i.e., RE patterns are utilized for two purposes in our approach. Depending on the specific kind of RE pattern, this step involves a few straight-forward processing steps, e.g., for the case of surface-level RE patterns it involves restoring correct inflections of poten27 tially lemmatized lexical pattern elements; for the case of depenency-grammar based patterns it additionally includes a step of tree linearization, see, e.g., (Wang and Zhang, 2012). In the following, we present some example sentence templates, used for the generation of multiplechoice tests1 : • marriage relation: another in the source text. The best case would be that they appear in the same sentence from which a relationship is extracted. Consider the following example sentence from which the instance marriage(Madonna, Ritchie) is extracted: Example 2: If Penn was Madonna’s temperamental match and boyfriend Carlos Leon, father of Lourdes, her physical ideal, Ritchie --- who reportedly calls his new wife ‘Madge’ in private --- is a man who holds his own against his hig"
W15-4405,W13-1722,0,0.0425492,"Missing"
W15-4405,P07-1074,1,0.707314,"on Natural Language Processing Techniques for Educational Applications, pages 26–33, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2.2 Supervision by teacher Named-entity recognition Identified entity mentions Relation extraction Texts Relation instances Answers generation & multiple choices compilation A key part of our approach is the application of a pattern-based relation extraction (RE) system. Such systems, e.g., NELL (Carlson et al., 2010; Mitchell et al., 2015), PATTY (Nakashole et al., 2012), DARE (Xu et al., 2007), rely on lexicosyntactic patterns that pose restrictions on the surface level or grammatical level of sentences. Their underlying assumption is that whenever a given sentence matches a given pattern (i.e., a sentence template), the sentence expresses the pattern’s corresponding semantic relation. This assumption does not always hold, hence the system output usually contains a certain amount of noise, which makes a human-in-the-loop necessary for high-precision applications. Typically, RE systems associate patterns with a confidence score of some kind, allowing downstream components to trade p"
W15-5702,2003.mtsummit-systems.1,0,0.0185567,"(6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation occurs in three phases, namely analysis, transfer, and generation. All three phases consist of hand-coded linguistic rules which have shown to perform well for capturing the structural and semantic differences between German and other language"
W15-5702,W14-3302,0,0.0706052,"Missing"
W15-5702,eisele-chen-2010-multiun,0,0.0162457,"gure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we als"
W15-5702,W11-2141,0,0.122729,"dent target language structures. A RestAPI allows the different processing steps and/or intermediate results to be influenced. Deep features for empirical enhancement Although deep techniques indicate good coverage of a number of linguistic phenomena, each of the three phases may frequently encounter serious robustness issues and/or the inability to fully process a given sentence. Erroneous analysis from early phases may aggregate along the pipeline and cause further sub-optimal choices in later phases, thus severely deteriorating the quality of the produced translation. Preliminary analysis (Federmann and Hunsicker, 2011) has shown that such is the case for source sentences that are ungrammatical in the first place or that have a very shallow syntax with many specialized lexical entries. To tackle these issues, we combine the transfer-based component with our supportive SMT engine in the following two ways: (a) train a statistical machine translation to automatically post-edit the output of the transfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that perf"
W15-5702,W12-0115,0,0.0146068,"combination produces a perfect translation. In this particular case, the machine translation (W¨ahlen Sie im Einf¨ugen Men¨u Tabelle aus) is even better than the human reference (W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus) as the latter introduces a determiner for “table” that is not justified by the source. English Transfer-‐ based MT German* SMT German Figure 2: Serial System Combination en→de. 2.4 Parallel System Combination: Selection Mechanism The selection mechanism is based on encouraging results of previous projects including Euromatrix Plus (Federmann and Hunsicker, 2011), T4ME (Federmann, 2012), QTLaunchPad (Avramidis, 2013; Shah et al., 2013). It has been extended to include several deep features that can only be generated on a sentence level and that would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In System 1, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source with the st"
W15-5702,W11-2123,0,0.00929833,"l., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has sh"
W15-5702,popovic-ney-2006-pos,1,0.760166,"e pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation oc"
W15-5702,N07-1064,0,0.0303755,"ransfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that performs real-time analysis of the produced translations and automatically selects the output that is predicted to be of a better quality (Avramidis, 2011). Figure 1 shows the overall architecture of System 1 for en→de. 2.3 Serial System Combination: Lucy+Moses For automatic post-editing of the transfer-based system, a serial Transfer+SMT system combination is used, as described in (Simard et al., 2007) The first stage is translation of the source-language part of the training corpus by the transfer-based system. The second stage is training an SMT system with the transfer-based translation output as a source language and the target-language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. Figure 2 illustrates the architecture for translation direction en→de. Note that the notion of “German*” in the figure is meant to distinguish the input and output of the SMT system. “German*” is t"
W15-5702,P13-1135,0,0.0293718,"arts from each employed method. Figure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield,"
W16-4210,D14-1164,0,0.0149434,"istorical patient data. Automated information extraction could allow the development of alert systems, which help the clinicians in their daily routine and thus would increase patients safety. However, the first step towards any information extraction is the definition of information of interest, such as diseases, medications or dosing size. This information is then defined within an annotation schema and is used to manually annotate a gold standard corpus to train and evaluate information extraction methods. Unfortunately, manual annotation is time consuming (Kim et al., 2008) and expensive (Angeli et al., 2014). In particular in the medical domain, expert knowledge is often required which makes the annotation process even more difficult and costly. Therefore existing schemata and corpora could be used in order to save time and effort for the annotation of new data. On the other hand, existing schemata might not cover the information of interest. Furthermore, most of the existing and assessable clinical data sets are in English language. The existing German-language clinical data sets are not freely available. Consequently, we aim to create a new gold standard corpus for German data. This work introd"
W16-4210,W13-1904,0,0.119648,"ols could support physicians to better access patient data. However, annotated data sets are required for the development and testing of information extraction methods. Most of the existing annotated clinical data sets are in English language. There are only a few data sets that have been created for non-English languages, such as for Swedish (Skeppstedt et al., 2014), French (N´ev´eol et al., 2015) or Polish (Mykowiecka et al., 2009). For German, only a few sources and clinical corpora exist and will be introduced in the following. The two most relevant sources for this work are described in Bretschneider et al. (2013) and Toepfer et al. (2015). Bretschneider et al. (2013) focused on the classification of sentences in radiology reports as either pathological and non-pathological based on the given findings. Toepfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language"
W16-4210,faessler-etal-2014-disclose,0,0.534753,"pfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language medical text corpus containing manually supplied sentence boundary, token segmentation and part-of-speech (POS) tags. Due to the fact that the corpus cannot be legally accessed by a third party, Faessler et al. (2014) present an freely available tool for segmentation and POS tagging for German clinical data, based on models trained on the FraMed corpus. Further relevant sources for German clinical data are for instance the German Specialist Lexicon (Weske-Heck et al., 2002) or the German MeSH1 . A good overview is also provided in the work of Schulz et al. (2013). 3 Utilized Data Sources This section presents the two data sources used for this work. Firstly, a biomedical knowledge source is presented which is used to automatically pre-annotate data to reduce annotation time. Secondly, the textual data whic"
W16-4210,wermter-hahn-2004-annotated,0,0.607558,"ces for this work are described in Bretschneider et al. (2013) and Toepfer et al. (2015). Bretschneider et al. (2013) focused on the classification of sentences in radiology reports as either pathological and non-pathological based on the given findings. Toepfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language medical text corpus containing manually supplied sentence boundary, token segmentation and part-of-speech (POS) tags. Due to the fact that the corpus cannot be legally accessed by a third party, Faessler et al. (2014) present an freely available tool for segmentation and POS tagging for German clinical data, based on models trained on the FraMed corpus. Further relevant sources for German clinical data are for instance the German Specialist Lexicon (Weske-Heck et al., 2002) or the German MeSH1 . A good overview is also provided in the work of Schulz et a"
W16-5113,W13-1904,0,0.0769879,", 2015; Costumero et al., 2014; Afzal et al., 2014). Beside NegEx and Context a wide range of other methods exist, e.g. based on syntactic techniques (Huang and Lowe, 2007; Mehrabi et al., 2015; Sohn et al., 2012; Cotik et al., 2016) or machine learning techniques (Uzuner et al., 2009). However, in clinical context simple methods, such as NegEx work very reliably for the task they have been designed for. 2 http://www.clips.ua.ac.be/NeSpNLP2010/program.html 116 Other research has been dedicated to clinical negation detection together with the detection of pathological entities in German texts. Bretschneider et al. (2013) classify sentences containing pathological and non-pathological findings in German radiology reports. Their approach uses a syntacto-semantic parsing approach. Gros and Stede (2013) present Negtopus, a system that identifies negations and their scope in medical diagnoses written in German and in English. Chapman et al. (2013) translate NegEx triggers into Swedish, French and German. The work reports, among others, the frequency of occurrence of German triggers in an annotated corpus of German medical text (Wermter and Hahn, 2004), that, as far as we know, is not available for public use. Both"
W16-5113,W16-2921,1,0.917461,"ical texts, which also addresses negation detection. A widely used tool for negation and speculation detection is Negex (Chapman et al., 2001). The method uses a simple algorithm based on regular expressions to detect triggers that indicate negation or speculation. Next it uses a window of words preceding or following each relevant term to determine if the term is under the scope of negation or speculation or not. NegEx has been extended to Context (Harkema et al., 2009) and adapted to Swedish, French, Spanish and other languages with good results (Skeppstedt, 2011; Del´eger and Grouin, 2012; Cotik et al., 2016; Stricker et al., 2015; Costumero et al., 2014; Afzal et al., 2014). Beside NegEx and Context a wide range of other methods exist, e.g. based on syntactic techniques (Huang and Lowe, 2007; Mehrabi et al., 2015; Sohn et al., 2012; Cotik et al., 2016) or machine learning techniques (Uzuner et al., 2009). However, in clinical context simple methods, such as NegEx work very reliably for the task they have been designed for. 2 http://www.clips.ua.ac.be/NeSpNLP2010/program.html 116 Other research has been dedicated to clinical negation detection together with the detection of pathological entities"
W16-5113,W10-3001,0,0.0912888,"Missing"
W16-5113,W15-2914,0,0.0388162,"Missing"
W16-5113,W16-4210,1,0.798227,"Missing"
W16-5113,wermter-hahn-2004-annotated,0,0.0469593,"ith the detection of pathological entities in German texts. Bretschneider et al. (2013) classify sentences containing pathological and non-pathological findings in German radiology reports. Their approach uses a syntacto-semantic parsing approach. Gros and Stede (2013) present Negtopus, a system that identifies negations and their scope in medical diagnoses written in German and in English. Chapman et al. (2013) translate NegEx triggers into Swedish, French and German. The work reports, among others, the frequency of occurrence of German triggers in an annotated corpus of German medical text (Wermter and Hahn, 2004), that, as far as we know, is not available for public use. Both publications, (Gros and Stede, 2013) and (Chapman et al., 2013), are related to our work. However, Negtopus focuses currently only on negation terms. It has been evaluated on a set of only 12 cardiology reports for German negation detection. NegEx with the German trigger set has not been evaluated and thus its performance is still unknown to us. 3 Methods The adaptation of NegEx to German requires having a set of triggers written in German. In order to evaluate the new system, a gold standard data set is necessary, consisting of"
W16-5113,W10-3111,0,0.0420113,"ether the finding is within the scope of negation or speculation. In comparison to English, German clinical data differs in various characteristics which have to be taken into account for the successful application of an algorithm detecting non-factuality. First of all, German is a richly inflected language (e.g. no can be translated as kein, keiner, keine etc.). Furthermore, German includes discontinuous triggers, such as kann ... ausgeschlossen werden ...1 (can be ruled out). Triggers may precede, but may also follow the negated expression, as presented in Table 1. Regarding this situation, Wiegand et al. (2010) state, that the detection of negation scope in German language is more difficult than in other languages, such as English. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 Dots indicate potential positions of the finding: (kann ... finding... ausgeschlossen werden, ... finding... kann ausgeschlossen werden) 115 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 115–124, Osaka, Japan, December 12th 2016. precede frei von Besc"
W16-5113,S12-1035,0,\N,Missing
W16-6404,2003.mtsummit-systems.1,0,0.0803587,"ns were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional work on RBMT focused on issues revealed through manual inspection of its performance on the QTLeap corpus (see also section 3): 2 http://metashare.metanet4u.eu/go2/qtleapcorpus 30 corpus Chromium browser Drupal Libreoffi"
W16-6404,W16-2329,1,0.864384,"have performed using a dedicated “test suite” that contains selected examples of relevant phenomena. While automatic scores show huge differences between the engines, the overall average number or errors they (do not) make is very similar for all systems. However, the detailed error breakdown shows that the systems behave very differently concerning the various phenomena. 1 Introduction This paper describes a hybrid Machine Translation (MT) system built for translating from English to German in the domain of technical documentation. The system builds upon the general architecture described in Avramidis et al. (2016), but in the current version several components have been improved or replaced. As detailed in the previous paper, the design of the system was driven by the assumptions that a) none of today’s common MT approaches, phrase-based statistical (PB-SMT) or rule-based (RBMT), is on its own capable of providing enough good translations to be useful in an outbound translation scenario without human intervention, and b) “deep” linguistic knowledge should help to improve translation quality. Instead of building a completely new system, our goal is to adjust and combine existing systems in a smart way u"
W16-6404,W14-4012,0,0.109025,"Missing"
W16-6404,W11-2123,0,0.0103712,"training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional w"
W16-6404,W08-0318,0,0.0344971,"s longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based"
W16-6404,W16-2361,0,0.029527,"Missing"
W16-6404,I08-2089,0,0.0251905,"cal data. The translation table was trained on a concatenation of generic and technical data, filtering out the sentences longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and que"
W18-2107,W18-2100,0,0.0724206,"Missing"
W18-2107,W12-3110,0,0.0219398,"put and not of QE predictions. Similar to MT output, predictions of sentencelevel QE have also been evaluated on test-sets consisting of randomly drawn texts and a single metric has been used to measure the performance over the entire text (e.g. Bojar et al., 2017). There has been criticism on the way the test-sets of the shared tasks have been formed with regards to the distribution of inputs (Anil and Fran, 2013), e.g. when they demonstrate a dataset shift (QuioneroCandela et al., 2009). Additionally, although there has been a lot of effort to infuse linguistically motivated features in QE (Felice and Specia, 2012), there has been no effort to evaluate their predictions from a linguistic perspective. To the best Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018 |Page 243 of our knowledge there has been no use of a Test Suite in order to evaluate sentence-level QE, or to inspect the predictions with regards to linguistic categories or specific error types. 3 Method The evaluation of QE presented in this paper is based on these steps: (1) construction of the Test Suite with respect to linguistic categories; (2) selection of suitable Test S"
W18-2107,D17-1263,0,0.0889841,"Missing"
W18-2107,C90-2037,0,0.224518,"rted is the generic domain of the text. In this paper we make an effort to demonstrate the value of using a linguistically-motivated controlled test-set (also known as a Test Suite) for evaluation instead of generic test-sets. We will focus on the sub-field of sentence-level Quality Estimation (QE) on MT and see how the evaluation of QE on a Test Suite can provide useful information concerning particular linguistic phenomena. 2 Related work There have been few efforts to use a broadlydefined Test Suite for the evaluation of MT, the first of them being during the early steps of the technology (King and Falkedal, 1990). Although the topic has been recently revived (Isabelle et al., 2017; Burchardt et al., 2017), all relevant research so far applies only to the evaluation of MT output and not of QE predictions. Similar to MT output, predictions of sentencelevel QE have also been evaluated on test-sets consisting of randomly drawn texts and a single metric has been used to measure the performance over the entire text (e.g. Bojar et al., 2017). There has been criticism on the way the test-sets of the shared tasks have been formed with regards to the distribution of inputs (Anil and Fran, 2013), e.g. when they"
W18-2107,2014.eamt-1.38,1,0.858464,"Missing"
W18-6436,W07-0718,0,0.207348,"Missing"
W18-6436,L16-1100,0,0.150074,") and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either b"
W18-6436,D17-1263,0,0.157443,"Missing"
W18-6436,L18-1142,1,0.707008,"Missing"
W18-6436,1991.mtsummit-panels.5,0,0.873131,"spects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low. 1 2 Related Work The use of test suites in the evaluation of NLP applications (Balkan et al., 1995) and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim a"
W18-6436,P02-1040,0,0.11164,"8). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium"
W18-6436,2006.amta-papers.25,0,0.0953688,"ented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium, Brussels, October 31"
W18-6436,C90-2037,0,\N,Missing
W18-6436,E17-2058,0,\N,Missing
W18-6436,W18-2107,1,\N,Missing
W19-5351,W18-6432,0,0.0473338,"Missing"
W19-5351,W18-6433,0,0.0862885,"Missing"
W19-5351,W07-0718,0,0.330509,"Missing"
W19-5351,W18-6437,0,0.0667606,"Missing"
W19-5351,W18-6434,0,0.0312015,"ion that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites (Guillou and Hardmeier, 2016; Isabelle et al., 2017). 2 Related Work Several test suites have been presented as part of the Test Suite track of the Third Conference of Machine Translation (Bojar et al., 2018a). Each test suite focused on a particular phenomenon, such as discourse (Bojar et al., 2018b), morphology (Burlot et al., 2018), grammatical contrasts (Cinkova and Bojar, 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). In contrast to the above test suites, our test suite is the only one that does such a systematic evaluation of more than one hundred phenomena. A direct comparison can be done with the latter related paper, since it focuses at the same language direction. Its authors use automated methods to extract text items, whereas in our test suite the test items are created manually. 445 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 445–454 c Florence, Italy"
W19-5351,L16-1100,0,0.142058,"nly focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort. Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites (Guillou and Hardmeier, 2016; Isabelle et al., 2017). 2 Related Work Several test suites have been presented as part of the Test Suite track of the Third Conference of Machine Translation (Bojar et al., 2018a). Each test suite focused on a particular phenomenon, such as discourse (Bojar et al., 2018b), morphology (Burlot et al., 2018), grammatical contrasts (Cinkova and Bojar, 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). In contrast to the above test suites, our test suite is the only one that does such a systematic evaluation of more than one hundred phenomena. A direct compa"
W19-5351,W09-0441,0,0.0494383,"lar expressions, followed by minimal human refinement (Section 3). The application of the suite allows us to form conclusions on the particular grammatical performance of the systems and perform several comparisons (Section 4). Introduction For decades, the development of Machine Translation (MT) has been based on either automatic metrics or human evaluation campaigns with the main focus on producing scores or comparisons (rankings) expressing a generic notion of quality. Through the years there have been few examples of more detailed analyses of the translation quality, both automatic (HTER (Snover et al., 2009), Hjerson (Popovi´c, 2011)) and human (MQM Lommel et al., 2014). Nevertheless, these efforts have not been systematic and they have only focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort. Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to"
W19-5351,W18-6435,0,0.057944,"Missing"
W19-5351,D17-1263,0,0.086885,"Missing"
W19-5351,2014.eamt-1.38,1,0.896199,"Missing"
W19-5351,L18-1142,1,0.718292,"Missing"
W19-5351,W18-6436,1,0.890415,"Missing"
xu-etal-2008-adaptation,W06-0202,0,\N,Missing
xu-etal-2008-adaptation,P07-1074,1,\N,Missing
