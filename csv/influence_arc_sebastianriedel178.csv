2020.acl-main.653,P15-1039,0,0.0132327,"quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison"
2020.acl-main.653,P19-1620,0,0.0244799,"rt cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been"
2020.acl-main.653,P19-1309,1,0.82722,"le in every target language, we use contexts containing an N -way parallel sentence. Our approach is similar to WikiMatrix (Schwenk et al., 2019) which extracts parallel sentences for many language pairs in Wikipedia, but we limit the search de es ar zh vi hi 5.4M 1.1M 83.7k 24.1K 9.2k 1340 Table 1: Incremental alignment with English to obtain 7-way aligned sentences. for parallel sentences to documents on the same topic only, and aim for N -way parallel sentences. To detect parallel sentences we use the LASER toolkit,3 which achieves state-of-the-art performance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sentence embeddings and a distance or margin criterion in the embeddings space to detect parallel sentences. The reader is referred to Artetxe and Schwenk (2018) and Artetxe and Schwenk (2019) for a detailed description. See Appendix A.6 for further details and statistics on the number of parallel sentences mined for all language pairs. We first independently align all languages with English, then intersect these sets of parallel sentences, forming sets of N-way parallel sentences. As shown in Table 1, starting with 5.4M parallel English/German sentences, the number o"
2020.acl-main.653,P17-1171,0,0.0249807,"fied Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure pro"
2020.acl-main.653,D18-1269,1,0.782924,"tly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison across languages, requires fewe"
2020.acl-main.653,D19-1169,0,0.0185285,"l QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) st"
2020.acl-main.653,D19-1600,0,0.0156129,"l QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) st"
2020.acl-main.653,L18-1431,0,0.196684,"uture, and to estab8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations 7322 lish standard splits. However, in our experiments, we only make use of the English development data and study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a hi"
2020.acl-main.653,N19-1423,0,0.592918,"valuate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent su"
2020.acl-main.653,P19-1610,0,0.0145569,"hot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed several baselines on two cross-lingual understanding tasks on MLQA with state-of-the-art methods, and demonstrate significant room for improvement. We hope t"
2020.acl-main.653,L18-1440,0,0.0375486,"al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. L"
2020.acl-main.653,R19-1053,0,0.0122478,"rove QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. Like us, they build multi-way parallel extractive QA data, but MLQA has many"
2020.acl-main.653,W18-2605,0,0.0222377,"Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset"
2020.acl-main.653,D19-1607,0,0.0123007,"n explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate"
2020.acl-main.653,D17-1215,0,0.0366045,"d study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed several baselines on two cross-lingual understanding tasks on MLQA with state-of-the-art methods, and demonstrate significant room for imp"
2020.acl-main.653,D19-1249,0,0.0291755,"nerate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. Like us, they build multi-way parallel extractive QA data, but MLQA has many more instances, covers more languages and does not require manual document translation. Liu et al. (2019a) explore cross-lingual open-domain QA"
2020.acl-main.653,P17-1147,0,0.0314601,"n other topics. Further statistics are given in Appendix A.2. en # Articles # Contexts # Instances de es ar zh vi hi 5530 2806 2762 2627 2673 2682 2255 10894 4509 5215 5085 4989 5246 4524 12738 5029 5753 5852 5641 6006 5425 Table 4: Number of Wikipedia articles with a context in MLQA. 3 Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a"
2020.acl-main.653,C12-1089,0,0.0611895,"(Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly paral"
2020.acl-main.653,Q19-1026,0,0.0773017,"has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA wi"
2020.acl-main.653,P18-2124,0,0.0852957,"Missing"
2020.acl-main.653,D19-1283,0,0.0157402,"s a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et"
2020.acl-main.653,L18-1437,0,0.0315326,", large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al"
2020.acl-main.653,D16-1264,0,0.568652,"i, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we"
2020.acl-main.653,D13-1020,0,0.0318853,"man, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization"
2020.acl-main.653,2021.eacl-main.115,1,0.683099,"Missing"
2020.acl-main.653,P19-1484,1,0.835066,"void_machine_ translations 7322 lish standard splits. However, in our experiments, we only make use of the English development data and study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed seve"
2020.acl-main.653,L18-1560,1,0.854934,"collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularl"
2020.acl-main.653,P19-1227,0,0.135144,"cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison across languages, requires fewer source language annotations, and allows for additional evaluation setups at no extra annotation cost. A purpose-built evaluation benchmark dataset covering a range of diverse languages, and following the popular extractive QA paradigm on a practically-useful"
2020.acl-main.653,W17-2623,0,0.0341294,"stics are given in Appendix A.2. en # Articles # Contexts # Instances de es ar zh vi hi 5530 2806 2762 2627 2673 2682 2255 10894 4509 5215 5085 4989 5246 4524 12738 5029 5753 5852 5641 6006 5425 Table 4: Number of Wikipedia articles with a context in MLQA. 3 Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QAL"
2020.acl-main.653,D15-1237,0,0.0968098,"Missing"
2020.acl-main.745,P19-1444,0,0.579569,"onal classification layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and tab"
2020.acl-main.745,D17-1160,0,0.0575742,"of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attempt to learn joint representations of NL utterances and the (semi-)structured schema of DB tables (e.g., representations of its columns or cell values, as in Krishnamurthy et al. (2017); Bogin et al. (2019b); Wang et al. (2019a), inter alia). However, this unique setting poses several challenges in applying pretrained LMs. First, information stored in DB tables exhibit strong underlying structure, while existing LMs (e.g., BERT) are solely trained for encoding free-form text. Second, a DB table could potentially have a large number of rows, and naively encoding all of them using a resource-heavy LM is computationally intractable. Finally, unlike most text-based QA tasks (e.g., SQuAD, Rajpurkar et al. (2016)) which could be formulated as a generic answer span selection proble"
2020.acl-main.745,P17-2031,0,0.0577108,"Missing"
2020.acl-main.745,2021.ccl-1.108,0,0.210001,"Missing"
2020.acl-main.745,K16-1006,0,0.0299363,"s on the challenging weakly-supervised semantic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other"
2020.acl-main.745,P15-1142,0,0.77818,"t systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts using B ERT, registering state-of-the-art performance on W IKI TABLE Q UESTIONS, while performing competitively on S PIDER (§ 5). 2 Background Semantic Parsing over Tables Semantic parsing tackles the task of translating an NL utterance u"
2020.acl-main.745,N18-1202,0,0.0314115,"ntic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning ove"
2020.acl-main.745,D19-1005,0,0.0239546,"nt representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al., 2019a; Wang et al., 2019a). TA B ERT could also potentially improve performance of these systems with improved table-level representations. Knowledge-enhanced Pretraining Recent pretraining models have incorporated structured information from knowledge bases (KBs) or other structured semantic annotations into training contextual word representations, either by fusing vector representations of entities and relations on KBs into word representations of LMs (Peters et al., 2019; Zhang et al., 2019b,c), or by encouraging the LM to recover KB entities and relations from text (Sun et al., 2019; Liu et al., 2019a). TA B ERT is broadly relevant to this line in that it also exposes an LM with structured data (i.e., tables), while aiming to learn joint representations for both textual and structured tabular data. 7 Conclusion and Future Work We present TA B ERT, a pretrained encoder for joint understanding of textual and tabular data. We show that semantic parsers using TA B ERT as a general-purpose feature representation layer achieved strong results on two benchmarks. Th"
2020.acl-main.745,Q13-1033,0,0.0779617,"Missing"
2020.acl-main.745,P18-1034,0,0.0191951,"ation. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the structured information in tables (Hwang et al., 2019; He et al., 2019; Guo et al., 2019; Zhang et al., 2019a). TA B ERT advances this line of research by presenting a generalpurpose, pretrained encoder over parallel corpora of Web tables and NL context. Another relevant direction is to augment representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al.,"
2020.acl-main.745,P15-1129,0,0.0386304,"d capture both the general information of the column (via MCP) and its representative cell values related to the utterance (via CVR). Tab. 5 shows ablation results of pretraining TA B ERT with different objectives. We find TA B ERT trained with both MCP and the auxiliary CVR objectives gets a slight advantage, suggesting CVR could potentially lead to 8420 more representative column representations with additional cell information. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the str"
2020.acl-main.745,P15-1128,1,0.828901,"t span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning over both free-form NL text and structured data. One example task is semantic parsing for access to databases (DBs) (Zelle and Mooney, 1996; Berant et al., 2013; Yih et al., 2015), the task of transducing an NL utterance (e.g., “Which country has the largest GDP?”) into a structured query over DB tables (e.g., SQL querying a database of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attem"
2020.acl-main.745,D18-2002,1,0.939848,"ple consists of an utterance (e.g., “What is the total number of languages used in Aruba?”), a DB with one or more tables, and an annotated SQL query, which typically involves joining multiple tables to get the answer (e.g., SELECT COUNT(*) FROM Country JOIN Lang ON Country.Code = Lang.CountryCode WHERE Name = ‘Aruba’). Base Semantic Parser We aim to show TA B ERT could help improve upon an already strong parser. Unfortunately, at the time of writing, none of the top systems on S PIDER were publicly available. To establish a reasonable testbed, we developed our in-house system based on TranX (Yin and Neubig, 2018), an open-source general-purpose semantic parser. TranX translates an NL utterance into an intermediate meaning representation guided by a user-defined grammar. The generated intermediate MR could then be deterministically converted to domain-specific query languages (e.g., SQL). We use TA B ERT as encoder of utterances and table schemas. Specifically, for a given utterance u and a DB with a set of tables T = {Tt }, we first pair u with each table Tt in T as inputs to TA B ERT, which generates |T |sets of table-specific representations of utterances and columns. At each time step, an LSTM deco"
2020.acl-main.745,N18-2093,0,0.382463,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1193,0,0.54357,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1425,0,0.333428,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D17-1125,0,0.0664293,"Missing"
2020.acl-main.745,P19-1139,0,0.334363,"n layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and table column). Specific"
2020.acl-main.745,P14-1133,0,\N,Missing
2020.acl-main.745,D13-1160,0,\N,Missing
2020.acl-main.745,N19-1273,0,\N,Missing
2020.acl-main.745,P19-1448,0,\N,Missing
2020.acl-main.745,N19-1423,0,\N,Missing
2020.acl-main.745,D19-1537,0,\N,Missing
2020.acl-main.745,D19-1391,0,\N,Missing
2020.emnlp-main.244,P17-1171,0,0.0235636,"layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset"
2020.emnlp-main.244,P18-1078,0,0.0198513,"iever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Reader Model For all our experiments, we fine-tune a pre-trained ALBERT model (Lan et al., 2020), consisting of 24 transformer layers and cross-layer parameter sharing. We do not use global normalisation (Clark and Gardner, 2018) in our implementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages an"
2020.emnlp-main.244,2020.emnlp-main.21,0,0.0135936,"able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et al. (2020) tune them on an held-out validation set via temperature calibration (Guo et al., 2017; Desai and Durrett, 2020), where a temperature T is tuned to adapt the softmax output probabilities at each layer. 3 Adaptive Computation in ODQA Our goal is to incrementally build up towers of transformer layers for all passages in Dq in a way that minimises unnecessary computation. Our algorithms maintain a state, or skyline, S = (H, A), consisting of current tower heights H = (h1 , . . . , hn ), indicating how many layers have been processed for each of the n towers, and the last representations A = (a1 , . . . , an ) computed for each of the towers. We want to build up the 1 In practice, Schwartz et al. (2020) cho"
2020.emnlp-main.244,N19-1423,0,0.0105078,"A) requires a system to answer questions using a large collection of documents as the information source. In contrast to context-based machine comprehension, where models are to extract answers from single paragraphs or documents, it poses a fundamental technical challenge in machine reading at scale (Chen et al., 2017) . Most ODQA systems consist of two-stage pipelines, where 1) a context retriever such as BM25 (Robertson, 2004) or DPR (Karpukhin et al., 2020) first selects a small subset of passages that are likely to contain the answer to the question, and 2) a machine reader such as BERT (Devlin et al., 2019) then examines the retrieved contexts to extract the answer. This two-stage process leads to a computational trade-off that is indicated in Fig. 1. We can run computationally expensive deep networks on a large number of passages to increase the probability that we find the right answer (“All Layers, All Passages”), or cut the number of passages and layers to reduce the computational footprint at the possible cost of missing an answer (“6 Layers, Top-2 Passages”). We hypothesise that a better accuracy-efficiency trade-off can be found if the computational budget is not allocated statically, but"
2020.emnlp-main.244,2020.emnlp-main.550,0,0.023734,"Missing"
2020.emnlp-main.244,J81-4005,0,0.673255,"Missing"
2020.emnlp-main.244,N19-4013,0,0.0736717,". (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training set, two development sets Size"
2020.emnlp-main.244,D18-1153,0,0.025201,"ut conditioned on the input. Elbayad et al. (2020) generalise universal transformers by also learning which layer to execute at each step. Schwartz et al. (2020); Liu et al. (2020) propose methods that can adaptively decide when to early stop the computation in sentence classification tasks. To the best of our knowledge, previous work has focused adaptive computation for a single input. We are the first to learn how to prioritise computation across instances in the context of ODQA. Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a tw"
2020.emnlp-main.244,2020.acl-main.537,0,0.0172027,"e selected tower contains an answer and r = 0 otherwise. c ∈ R+ is a penalty cost of taking a step. In our experiments, we set c = 0.1. 3032 4 Related Work SQuAD-Open Adaptive Computation One strategy to reduce a model’s complexity consists in dynamically deciding which layers to execute during inference (Bengio et al., 2015; Graves, 2016). Universal transformers (Dehghani et al., 2019) can learn after how many layers to emit an output conditioned on the input. Elbayad et al. (2020) generalise universal transformers by also learning which layer to execute at each step. Schwartz et al. (2020); Liu et al. (2020) propose methods that can adaptively decide when to early stop the computation in sentence classification tasks. To the best of our knowledge, previous work has focused adaptive computation for a single input. We are the first to learn how to prioritise computation across instances in the context of ODQA. Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer"
2020.emnlp-main.244,D19-1284,0,0.0113689,"his idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training"
2020.emnlp-main.244,2020.acl-main.593,0,0.106923,"ned height n and use an output layer to produces the final output, y = OutputLayer(hn ). In this work, due to efficiency reasons, we restrict ourselves to pre-trained ALBERT (Lan et al., 2020) models. One critical property of these models is parameter tying across layers: TransformerLayeri (h) = TransformerLayerj (h) for any i, j. 2.3 Adaptive Computation Our goal is to early-exit the iterative layer-by-layer process in order to save computation. We assume this can be happening adaptively, based on the input, since some passages might require less computation to produce an answer than others. Schwartz et al. (2020) show how this can be achieved for classification tasks. They first require internal layers to be able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et a"
2020.emnlp-main.244,D19-1599,0,0.050169,"dels. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training set, two development sets Size Hits@30 train dev0 dev1 test 78,839 71."
2020.emnlp-main.519,C18-1057,0,0.0112273,"aining on source and target domain. Average performance across a set of worlds is computed by macroaveraging. We also report cross-encoder performance on the same retrieval method (BM25) used by Logeswaran et al. (2019) in Table 3, where the performance is evaluated on the subset of test instances for which the gold entity is among the top 64 candidates retrieved by BM25. We observe that our cross-encoder obtains slightly better results than reported by Logeswaran et al. (2019), likely due to implementation and hyper-parameter details. 5.2.2 TACKBP-2010 Following prior work (Sun et al., 2015; Cao et al., 2018; Gillick et al., 2019; Onoe and Durrett, 2019), we pre-train our models on Wikipedia6 data. Data and model training details can be found in Appendix A.1. 6 https://pytorch.org 6401 https://www.wikipedia.org/ Method Valid TF-IDF† Test Ganea and Hofmann Gupta et al. (2017)† Logeswaran et al. (2019) 26.06 26.96 27.03 76.06 75.06 Ours (base) 78.24 76.58 (2017)† Method Table 3: Normalized accuracy on validation and test set on Zero-shot EL, where the performance is evaluated on the subset of test instances for which the gold entity is among the top-k candidates retrieved during candidate generatio"
2020.emnlp-main.519,N19-1423,0,0.0627289,"ng the bi-encoder for candidate generation, we train our cross-encoder (initialized with pre-trained BERT) on the top 64 retrieved candidates from bi-encoder for each sample on the train50 100 150 200 k: number of retrieved entities Figure 2: Top-k entity retrieval recall on validation dataset of Zero-shot EL dataset ing set, and evaluate the cross-encoder on the test dataset. Overall, we are able to obtain a much better end-to-end accuracy, as shown in Table 2, largely due to the improvement on the retrieval stage. Evaluation Setup and Results We experiment with both BERT-base and BERTlarge (Devlin et al., 2019) for our bi-encoders and cross-encoders. The details of training infrastructure and hyperparameters can be found in Appendix A. All models are implemented in PyTorch5 and optimizied with Adam (Kingma and Ba, 2014). We use (base) and (large) to indicate the version of our model where the underlying pretrained transformer model is BERT-base and BERT-large, respectively. 5 1 Method U.Acc. Logeswaran et al. (2019) Logeswaran et al. (2019)(domain)† 55.08 56.58 Ours (base) Ours (large) 61.34 63.03 Table 2: Performance on test domains on the Zero-shot EL dataset. U.Acc. represents the unnormalized ac"
2020.emnlp-main.519,K17-1008,0,0.0784644,"r can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as"
2020.emnlp-main.519,D17-1277,0,0.135051,"Missing"
2020.emnlp-main.519,K19-1049,0,0.797778,"curacy gain from the more expensive crossencoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github. com/facebookresearch/BLINK. 1 Introduction Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables (Ganea and Hofmann, 2017), incoming Wikipedia link popularity (Yamada et al., 2016), and gold Wikipedia entity categories (Gillick et al., 2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels ∗ Work done during internship with Facebook. for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed tradeoff inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy. More specifically, we introduce a two stage approach for zero-shot linking (see Figur"
2020.emnlp-main.519,P16-1059,0,0.0516805,"Missing"
2020.emnlp-main.519,D17-1284,0,0.0501717,"Missing"
2020.emnlp-main.519,P13-2006,0,0.199127,"nds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidat"
2020.emnlp-main.519,P84-1044,0,0.249739,"Missing"
2020.emnlp-main.519,K18-1050,0,0.177648,"nsferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP. Gillick et al. (201"
2020.emnlp-main.519,P19-1335,0,0.318892,"on and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP. Gillick et al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are"
2020.emnlp-main.519,K16-1025,0,0.239437,"ain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME (Ferragina and Scaiella, 2011)).1 2 Related Work We follow most recent work in studying entity linking with gold mentions.2 The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et al. (2013), Sun et al. (2015), Yamada et al. (2016), Ganea and Hofmann (2017), and Kolitsas et al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking (Raiman and Raiman, 2018; Onoe and Durrett, 2019; Khalife and Vazirgiannis, 2018). Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use crossencoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on larg"
2020.emnlp-main.580,D19-6609,0,0.028413,", so fine-tuning for question answering first on a much larger dataset is useful. 7 Related Work Previous work in NLP has focused on claim veracity. It has been treated as a classification problem (Wang, 2017), often using stance detection (Riedel et al., 2017). The FEVER Challenge (Thorne et al., 2018) proposed providing provenance for a decision along with classification, and various approaches developed combine information retrieval with stance detection or question answering (Li et al., 2018; Lee et al., 2018). Question generation and answering has been considered in the context of FEVER (Jobanputra, 2019) — the focus was on eliciting the right answer from a question answering system rather than improving the accuracy and efficiency of human fact checkers. However, FEVER is based on modified Wikipedia sentences, not real world claims, which are arguably more difficult. To address this Hanselowski et al. (2019) considered the claims fact checked by the website Snopes, but used the reports accompanying them as evidence instead of finding the evidence directly. Popat et al. (2018) and Augenstein et al. (2019) used search engines, but without ensuring that they provide evidence supporting/refuting"
2020.emnlp-main.580,P17-1147,0,0.103053,"Missing"
2020.emnlp-main.665,P19-1084,0,0.0799926,"Missing"
2020.emnlp-main.665,S19-1028,0,0.0367509,"Missing"
2020.emnlp-main.665,D15-1075,0,0.0253472,"r SNLI, although this is not the case for the SICK dataset (Marelli et al., 2014). Poliak et al. (2018) find that human-elicited datasets such as SNLI and MultiNLI have the largest hypothesis-only bias. As a result, our paper focuses on removing the hypothesis-only bias from SNLI, the dataset with the largest hypothesis-only bias reported by Poliak et al. (2018). This bias is 1 https://github.com/joestacey/ robust-nli also dataset specific, with Belinkov et al. (2019a) finding that only MultiNLI shares some of the same hypothesis-only bias as the SNLI dataset. Generalisation to Other Datasets Bowman et al. (2015) and Williams et al. (2018) show that models trained on the SNLI and MultiNLI datasets do not necessarily learn good representations for other NLI datasets, such as SICK. Analogous results were also reported by Talman and Chatzikyriakidis (2018) for more complex models. Gururangan et al. (2018) and Tsuchiya (2018) identify how NLI models perform worse on hard examples, which are defined as the examples that a hypothesisonly model has misclassified. This suggests that the success of NLI models may be overstated, with models relying on artefacts in their training data to achieve high performance"
2020.emnlp-main.665,P17-2097,0,0.0149823,"For instance, Kaushik and Lipton (2018) show that, in several reading comprehension datasets such as bAbI (Weston et al., 2016) and Children’s Books Test (Hill et al., 2016), it is possible to get non-trivial results by considering only the last passage of the paragraph. In visual question answering datasets, several studies find it is often possible to answer the question without looking at the corresponding image (Zhang et al., 2016; Kafle and Kanan, 2016; Goyal et al., 2017; Agrawal et al., 2018). Similarly, for the ROCStories corpus (Mostafazadeh et al., 2016), Schwartz et al. (2017) and Cai et al. (2017) show it is possible to achieve non-trivial prediction accuracy by only considering candidate endings and without taking the stories in account. The de-biasing approach introduced in this paper could be applied in any of these situations where a model involves a classifier based on latent representations. Learning Robust Models Neural models are known to be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing"
2020.emnlp-main.665,D19-1418,0,0.0407285,"ctic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adversarial Training Another procedure for creating more robust models is through adversarial training with latent representations, with a classifier trained to learn the bias from the model sentence representations which in turn update to reduce the performance of the bias classifier (Wang et al., 2019). For example, Ganin and Lempitsky (2015) use a"
2020.emnlp-main.665,D17-1070,0,0.0294138,"ntation is then frozen, and 20 adversarial classifiers are randomly reinitialised before they attempt to re-learn the hypothesis-only bias from the frozen de-biased sentence representation. The maximum accuracy from across the 20 adversarial classifiers is then reported after trying to remove the bias, showing the maximum bias that can still be learnt from the representation. The ability of adversarially trained models to de-bias sentence representations is Model Architecture Following the same experimental set-up as Belinkov et al. (2019a) and Poliak et al. (2018), we use an InferSent model (Conneau et al., 2017) with pretrained GloVe 300dimensional word embeddings. The InferSent model architecture consists of a Long Short-Term Memory network (LSTM, Hochreiter and Schmidhuber, 1997) encoder which creates a 2048 dimensional sentence representation. 3.1 Significance Testing We perform statistical testing to assess whether the differences between using one or five adversarial classifiers is significant. This involves repeating the experiments for both one and five adversarial classifiers with ten different random seeds. For each experiment, the de-biasing is performed before a classifier attempts to lear"
2020.emnlp-main.665,P18-2006,0,0.0230648,"e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adv"
2020.emnlp-main.665,1993.eamt-1.1,0,0.539154,"a Long Short-Term Memory network (LSTM, Hochreiter and Schmidhuber, 1997) encoder which creates a 2048 dimensional sentence representation. 3.1 Significance Testing We perform statistical testing to assess whether the differences between using one or five adversarial classifiers is significant. This involves repeating the experiments for both one and five adversarial classifiers with ten different random seeds. For each experiment, the de-biasing is performed before a classifier attempts to learn the bias again from the frozen sentence representations. We use bootstrapping hypothesis testing (Efron and Tibshirani, 1993) to test the statistical significance by comparing the means from the two samples. We also provide p-values from a Mann Whitney U-test (Mann and Whitney, 1947). The bootstrapping considers the null hypothesis that there is no difference between the mean bias re-learnt from using five adversarial classifiers compared to just using one adversarial classifier. In addition, we use a Bonferroni correction factor (Shaffer, 1995) of four when evaluating the p-values, taking into account multiple hypothesis testing across each different dimension. P-values smaller than 0.05 are considered significant."
2020.emnlp-main.665,D18-1002,0,0.320224,"performance of the adversarial classifier. In this context, adversarial training aims to produce sentence representations that do not incorporate information about the artefacts (or bias) in the data, resulting in less biased models that generalise better. Previous studies show that adversarial training is associated with better generalisation performance across other datasets, although there are concerns that the biases are not removed from the model sentence representations, with classifiers able to relearn such biases from the representations after these are frozen (Belinkov et al., 2019b; Elazar and Goldberg, 2018). It is therefore unclear whether any improvements are as a result of the de-biasing, and whether actually removing these biases from the model representations will further improve generalisation. We focus our effort on this discrepancy and argue that, in order to show de-biasing is effective, improvements in performance should also correspond to an observed reduction of the bias in the model representations, therefore creating representations that generalise better to other data. In this paper we show that NLI models can avoid learning from the hypothesis-only bias, using an ensemble of adver"
2020.emnlp-main.665,N18-2017,0,0.406162,"ch produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries. 1 Haim Dubossarsky University of Cambridge hd423@cam.ac.uk Introduction NLI datasets are known to contain artefacts associated with their human annotation processes (Gururangan et al., 2018). Neural models are particularly prone to picking up on artefacts, relying on these biases and spurious correlations rather than acquiring a true understanding of the task. Because these artefacts are often dataset specific (Poliak et al., 2018; Tsuchiya, 2018), models that rely on these artefacts consequently generalise poorly when tested on other datasets (Belinkov et al., 2019a). One way to alleviate this problem is via adversarial training: the task classifier and an adversarial classifier jointly share an encoder, with the adversarial classifier trained to produce the correct predictions"
2020.emnlp-main.665,D19-6115,0,0.057096,"replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adversarial Training Another procedure for creating more robust models is through adversarial training with latent representations, with a classifier trained to learn the bias from the model sentence representations which in turn update to reduce the performance of the bias classifier (Wang et al., 2019). For example, Ganin and Lempitsky (2015) use adversarial trainin"
2020.emnlp-main.665,P82-1020,0,0.594366,"Missing"
2020.emnlp-main.665,N18-1170,0,0.0260846,"rsary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adversarial Training Another procedure for"
2020.emnlp-main.665,D17-1215,0,0.046956,"o be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on"
2020.emnlp-main.665,D18-1546,0,0.0188844,"NLI models may be overstated, with models relying on artefacts in their training data to achieve high performance (Gururangan et al., 2018). Our paper will assess whether NLI models that no longer learn from the hypothesis-only bias can still retain this high level of accuracy. Biases and Artefacts SNLI and MultiNLI are not the only datasets that suffer from the presence of annotation artefacts and biases. In the past, machine reading datasets were also found to contain syntactic clues that were giving away the correct prediction (Vanderwende and Dolan, 2005; Snow et al., 2006). For instance, Kaushik and Lipton (2018) show that, in several reading comprehension datasets such as bAbI (Weston et al., 2016) and Children’s Books Test (Hill et al., 2016), it is possible to get non-trivial results by considering only the last passage of the paragraph. In visual question answering datasets, several studies find it is often possible to answer the question without looking at the corresponding image (Zhang et al., 2016; Kafle and Kanan, 2016; Goyal et al., 2017; Agrawal et al., 2018). Similarly, for the ROCStories corpus (Mostafazadeh et al., 2016), Schwartz et al. (2017) and Cai et al. (2017) show it is possible to"
2020.emnlp-main.665,I17-1011,0,0.0353522,"Missing"
2020.emnlp-main.665,marelli-etal-2014-sick,0,0.0265021,"possible due to hypothesisonly biases, such as the observation that negation words (“no” or “never”) are more commonly used in contradicting hypotheses (Gururangan et al., 2018; Poliak et al., 2018). The hypothesis sentence length is another example of an artefact that models can learn from, with entailment hypotheses being, on average, shorter than either contradiction or neutral hypotheses (Gururangan et al., 2018). Tsuchiya (2018) show that the hypothesis-only bias predictions are significantly better than the majority baseline for SNLI, although this is not the case for the SICK dataset (Marelli et al., 2014). Poliak et al. (2018) find that human-elicited datasets such as SNLI and MultiNLI have the largest hypothesis-only bias. As a result, our paper focuses on removing the hypothesis-only bias from SNLI, the dataset with the largest hypothesis-only bias reported by Poliak et al. (2018). This bias is 1 https://github.com/joestacey/ robust-nli also dataset specific, with Belinkov et al. (2019a) finding that only MultiNLI shares some of the same hypothesis-only bias as the SNLI dataset. Generalisation to Other Datasets Bowman et al. (2015) and Williams et al. (2018) show that models trained on the S"
2020.emnlp-main.665,K18-1007,1,0.863503,"del to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adversarial Training Another procedure for creating more robust models"
2020.emnlp-main.665,N16-1098,0,0.0118814,"ction (Vanderwende and Dolan, 2005; Snow et al., 2006). For instance, Kaushik and Lipton (2018) show that, in several reading comprehension datasets such as bAbI (Weston et al., 2016) and Children’s Books Test (Hill et al., 2016), it is possible to get non-trivial results by considering only the last passage of the paragraph. In visual question answering datasets, several studies find it is often possible to answer the question without looking at the corresponding image (Zhang et al., 2016; Kafle and Kanan, 2016; Goyal et al., 2017; Agrawal et al., 2018). Similarly, for the ROCStories corpus (Mostafazadeh et al., 2016), Schwartz et al. (2017) and Cai et al. (2017) show it is possible to achieve non-trivial prediction accuracy by only considering candidate endings and without taking the stories in account. The de-biasing approach introduced in this paper could be applied in any of these situations where a model involves a classifier based on latent representations. Learning Robust Models Neural models are known to be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 p"
2020.emnlp-main.665,P16-1204,0,0.0310111,"Missing"
2020.emnlp-main.665,P15-2067,0,0.0405819,"Missing"
2020.emnlp-main.665,S18-2023,0,0.0337874,"Missing"
2020.emnlp-main.665,D12-1071,0,0.0236711,"Missing"
2020.emnlp-main.665,2020.acl-main.769,0,0.135136,"nd ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries. 1 Haim Dubossarsky University of Cambridge hd423@cam.ac.uk Introduction NLI datasets are known to contain artefacts associated with their human annotation processes (Gururangan et al., 2018). Neural models are particularly prone to picking up on artefacts, relying on these biases and spurious correlations rather than acquiring a tr"
2020.emnlp-main.665,Q15-1034,0,0.0235048,"Missing"
2020.emnlp-main.665,P18-1079,0,0.0221181,"taking the stories in account. The de-biasing approach introduced in this paper could be applied in any of these situations where a model involves a classifier based on latent representations. Learning Robust Models Neural models are known to be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial"
2020.emnlp-main.665,K17-1004,0,0.0155091,", 2005; Snow et al., 2006). For instance, Kaushik and Lipton (2018) show that, in several reading comprehension datasets such as bAbI (Weston et al., 2016) and Children’s Books Test (Hill et al., 2016), it is possible to get non-trivial results by considering only the last passage of the paragraph. In visual question answering datasets, several studies find it is often possible to answer the question without looking at the corresponding image (Zhang et al., 2016; Kafle and Kanan, 2016; Goyal et al., 2017; Agrawal et al., 2018). Similarly, for the ROCStories corpus (Mostafazadeh et al., 2016), Schwartz et al. (2017) and Cai et al. (2017) show it is possible to achieve non-trivial prediction accuracy by only considering candidate endings and without taking the stories in account. The de-biasing approach introduced in this paper could be applied in any of these situations where a model involves a classifier based on latent representations. Learning Robust Models Neural models are known to be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant tr"
2020.emnlp-main.665,N06-1005,0,0.085411,"This suggests that the success of NLI models may be overstated, with models relying on artefacts in their training data to achieve high performance (Gururangan et al., 2018). Our paper will assess whether NLI models that no longer learn from the hypothesis-only bias can still retain this high level of accuracy. Biases and Artefacts SNLI and MultiNLI are not the only datasets that suffer from the presence of annotation artefacts and biases. In the past, machine reading datasets were also found to contain syntactic clues that were giving away the correct prediction (Vanderwende and Dolan, 2005; Snow et al., 2006). For instance, Kaushik and Lipton (2018) show that, in several reading comprehension datasets such as bAbI (Weston et al., 2016) and Children’s Books Test (Hill et al., 2016), it is possible to get non-trivial results by considering only the last passage of the paragraph. In visual question answering datasets, several studies find it is often possible to answer the question without looking at the corresponding image (Zhang et al., 2016; Kafle and Kanan, 2016; Goyal et al., 2017; Agrawal et al., 2018). Similarly, for the ROCStories corpus (Mostafazadeh et al., 2016), Schwartz et al. (2017) and"
2020.emnlp-main.665,N18-1101,0,0.0382504,"not the case for the SICK dataset (Marelli et al., 2014). Poliak et al. (2018) find that human-elicited datasets such as SNLI and MultiNLI have the largest hypothesis-only bias. As a result, our paper focuses on removing the hypothesis-only bias from SNLI, the dataset with the largest hypothesis-only bias reported by Poliak et al. (2018). This bias is 1 https://github.com/joestacey/ robust-nli also dataset specific, with Belinkov et al. (2019a) finding that only MultiNLI shares some of the same hypothesis-only bias as the SNLI dataset. Generalisation to Other Datasets Bowman et al. (2015) and Williams et al. (2018) show that models trained on the SNLI and MultiNLI datasets do not necessarily learn good representations for other NLI datasets, such as SICK. Analogous results were also reported by Talman and Chatzikyriakidis (2018) for more complex models. Gururangan et al. (2018) and Tsuchiya (2018) identify how NLI models perform worse on hard examples, which are defined as the examples that a hypothesisonly model has misclassified. This suggests that the success of NLI models may be overstated, with models relying on artefacts in their training data to achieve high performance (Gururangan et al., 2018)."
2020.emnlp-main.665,L18-1239,0,0.090592,"of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries. 1 Haim Dubossarsky University of Cambridge hd423@cam.ac.uk Introduction NLI datasets are known to contain artefacts associated with their human annotation processes (Gururangan et al., 2018). Neural models are particularly prone to picking up on artefacts, relying on these biases and spurious correlations rather than acquiring a true understanding of the task. Because these artefacts are often dataset specific (Poliak et al., 2018; Tsuchiya, 2018), models that rely on these artefacts consequently generalise poorly when tested on other datasets (Belinkov et al., 2019a). One way to alleviate this problem is via adversarial training: the task classifier and an adversarial classifier jointly share an encoder, with the adversarial classifier trained to produce the correct predictions by analysing the artefacts in the training data. The encoder optimises the training objective while also reducing the performance of the adversarial classifier. In this context, adversarial training aims to produce sentence representations that do not incorpora"
2020.emnlp-main.665,W18-5446,0,0.0477367,"Missing"
2020.emnlp-main.665,N19-5001,0,0.0161025,"on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the naive models to focus on other patterns in the data that generalise better. Adversarial Training Another procedure for creating more robust models is through adversarial training with latent representations, with a classifier trained to learn the bias from the model sentence representations which in turn update to reduce the performance of the bias classifier (Wang et al., 2019). For example, Ganin and Lempitsky (2015) use adversarial training to improve domain adaption, allowing models to learn features helpful for the model task but which are also invariant with respect to changes in the domain. This was achieved by jointly training two models, one to predict the class label and one to predict the domain, and then regularising the former model to decrease the accuracy of the latter via gradient reversal. Belinkov et al. (2019b) use adversarial training to remove the hypothesis-only bias from models trained on SNLI. While this approach produced models that generalis"
2020.emnlp-main.665,N18-2091,0,0.0233638,"ural models are known to be vulnerable to so-called adversarial examples, i.e. instances explicitly crafted by an adversary to cause the model to make a mistake (Szegedy et al., 2014). Most recent work focuses on sim8282 ple semantic-invariant transformations, showing that neural models can be overly sensitive to small modifications of the inputs and paraphrasing. For instance, Ribeiro et al. (2018) use a set of simple syntactic changes, such as replacing What is with What’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Wang and Bansal, 2018; Jia and Liang, 2017), character-level perturbations (Ebrahimi et al., 2018), and paraphrasing (Iyyer et al., 2018). Minervini and Riedel (2018) propose searching for violations of constraints, such as the symmetry of contradiction and transitivity of entailment, for identifying where NLI models make mistakes and then creating more robust models by training on these adversarial examples. Alternatively, Clark et al. (2019), He et al. (2019) and Mahabadi et al. (2020) create naive models that make predictions based on known dataset biases, and then train robust models in an ensemble with the na"
2020.emnlp-main.665,I17-1100,0,0.0253824,"Missing"
2020.emnlp-main.692,D13-1170,0,\N,Missing
2020.emnlp-main.692,W19-5034,0,\N,Missing
2020.emnlp-main.692,P19-1513,0,\N,Missing
2020.emnlp-main.692,N19-1423,0,\N,Missing
2020.emnlp-main.692,2020.acl-main.398,0,\N,Missing
2020.findings-emnlp.103,P18-2006,0,0.19252,"recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier unde"
2020.findings-emnlp.103,W17-5401,0,0.0397192,"Missing"
2020.findings-emnlp.103,D18-1407,0,0.0273755,"Missing"
2020.findings-emnlp.103,N18-2017,0,0.0587184,"Missing"
2020.findings-emnlp.103,N18-1170,0,0.349247,"non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier undersensitivity in computer vision. Niu and Bansal (2018) investiga"
2020.findings-emnlp.103,D17-1215,0,0.569717,"acked on a substantial proportion of samples. The observed undersensitivity correlates negatively with in-distribution test set performance metrics (EM/F1 ), suggesting that this phenomenon – where present – is indeed a reflection of a model’s lack of question comprehension. When training models to defend against undersensitivity attacks with data augmentation and adversarial training, we observe that they can generalise their robustness to held out evaluation data without sacrificing in-distribution test set performance. Furthermore, the models improve on the adversarial datasets proposed by Jia and Liang (2017), and behave more robustly in a learning scenario that has dataset bias with a train / evaluation distribution mismatch, increasing performance by up to 10.9%F1 . List of Contributions: i) We propose a new type of adversarial attack exploiting the undersensitivity of neural RC models to input changes, and show that contemporary models are vulnerable to it; ii) We compare data augmentation and adversarial training as defences, and show their effectiveness at reducing undersensitivity errors on both held-out data and held-out perturbations without sacrificing nominal test performance; iii) We de"
2020.findings-emnlp.103,2021.ccl-1.108,0,0.144103,"Missing"
2020.findings-emnlp.103,K18-1047,0,0.0205783,"araphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier undersensitivity in computer vision. Niu and Bansal (2018) investigated undersensitivity in dialogue models and addressed the problem with a max-margin training approach. Ribeiro 1153 et al. (2018a) describe a general model diagnosis tool to identify minimal feature sets that are sufficient for a model to form high-confidence predictions. Feng et al. (2018) showed that it is possible to reduce inputs to minimal input word sequences without changing a model’s predictions. Welbl et al. (2020) investigated formal verification against undersensitivity to text deletions. We see our work as a continuation of these lines of inquiry, with a particular focus"
2020.findings-emnlp.103,P18-2124,0,0.0322545,"ion that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve pe"
2020.findings-emnlp.103,P18-1079,0,0.355555,") We demonstrate that the resulting models generalise better on the adversarial datasets of Jia and Liang (2017), and in the biased data setting of Lewis and Fan (2019). 2 Related Work Adversarial Attacks in NLP Adversarial examples have been studied extensively in NLP – see Zhang et al. (2019) for a recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can"
2020.findings-emnlp.103,2020.acl-main.442,0,0.0113561,"odels learning shallow but successful heuristics, and propose counterfactual data annotation paradigms as prevention. The perturbations used in this work define such counterfactual samples. Their composition does not require additional annotation efforts, and we furthermore adapt an adversarial perspective on the choice of such samples. Finally, one of the methods we evaluate for defending against undersensitivity attacks is a form of data augmentation that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable que"
2020.findings-emnlp.103,P19-1485,0,0.132099,"Missing"
2020.findings-emnlp.103,N18-2003,0,0.0268061,"ght it. Gardner et al. (2020) and Kaushik et al. (2020) also recognise the problem of models learning shallow but successful heuristics, and propose counterfactual data annotation paradigms as prevention. The perturbations used in this work define such counterfactual samples. Their composition does not require additional annotation efforts, and we furthermore adapt an adversarial perspective on the choice of such samples. Finally, one of the methods we evaluate for defending against undersensitivity attacks is a form of data augmentation that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43"
2020.findings-emnlp.103,P19-1415,0,0.0208967,"dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve performance in SQuAD2.0 (Zhu et al., 2019; Alberti et al., 2019). In contrast to prior work, we demonstrate that despite improving performance on test sets that include unanswerable questions, the problem persists when adversarially choosing from a larger space of questions. 3 Methodology Problem Overview Consider a discriminative model fθ , parameterised by a collection of vectors θ, which transforms an input x into a prediction yˆ = fθ (x). In our task, x = (t, q) is a given text t paired with a question q about this text. The label y is the answer to q where it exists, or a NoAnswer label where it cannot be answered.1 In an RC set"
2020.findings-emnlp.103,W17-2623,0,0.0426761,"which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve performance in SQuAD2.0 (Zhu et al., 2019; Alberti et al., 2019). In contrast to prior work, we demonstrate that despite improving performance on test sets that include unanswerable questions, the p"
2020.findings-emnlp.103,N18-2091,0,0.0621991,"ave been studied extensively in NLP – see Zhang et al. (2019) for a recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Un"
2020.findings-emnlp.22,D17-1238,0,0.0683957,"Missing"
2020.findings-emnlp.22,N19-4009,1,0.866841,"Missing"
2020.findings-emnlp.22,N18-1202,0,0.014261,"ness. While decoding strategies such as top-k and nucleus sampling lead to less repetitive generations, they also produce less verifiable text. Based on these finding, we introduce a simple and effective decoding strategy which, in comparison to previously used decoding strategies, produces less repetitive and more verifiable text. 1 Introduction Recent years have led to a considerable surge of interest in and capabilities of pre-trained language models (LMs). Today, they play a critical role in many NLP tasks, such as text classification, machine comprehension and natural language inference (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019a; Yang et al., 2019), to name just a few. They serve as a pre-training objective for downstream applications and they have been used to showcase and measure the general progress in NLP (Yu et al., 2017; Liu et al., 2019b). Several works (Radford et al., 2019b; Keskar et al., 2019) show the remarkable fluency and gram∗ † matical correctness of text decoded from modern LMs. Additionally, recent works (Petroni et al., 2019; Logan et al., 2019; Broscheit, 2019; Roberts et al., 2020) demonstrate that beyond general linguistic capabilities, language models can"
2020.findings-emnlp.22,D19-1250,1,0.879783,"Missing"
2020.sustainlp-1.9,P17-1171,0,0.0476768,"ementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs"
2020.sustainlp-1.9,2020.emnlp-main.21,0,0.0136063,"able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et al. (2020) tune them on an held-out validation set via temperature calibration (Guo et al., 2017; Desai and Durrett, 2020), where a temperature T is tuned to adapt the softmax output probabilities at each layer. 3 Early Exit with Local Exit Probabilities Adaptive Computation in ODQA Our goal is to incrementally build up towers of transformer layers for all passages in Dq in a way that minimises unnecessary computation. Our algorithms maintain a state, or skyline, S = (H, A), consisting of current tower heights H = (h1 , . . . , hn ), indicating how many layers have been processed for each of the n towers, and the last representations A = (a1 , . . . , an ) computed for each of the towers. We want to build up the"
2020.sustainlp-1.9,N19-1423,0,0.026663,"Missing"
2020.sustainlp-1.9,2020.emnlp-main.550,0,0.0358545,"me reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs predictions at the final layer, 2) the efficient baseline that always exits at a given intermediate layer for all passages, and is optimised to do so, 3) the top-k baseline that only reads the k top ranked passages an"
2020.sustainlp-1.9,J81-4005,0,0.681512,"Missing"
2020.sustainlp-1.9,N19-4013,0,0.245448,"v1 ), and test set, and their details are summarised in Table 1. Experimental Setup We follow the preprocessing approached proposed by Wang et al. (2019) and split passages into 100-word long chunks with 50-word long strides. We use a BM25 retriever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These m"
2020.sustainlp-1.9,D18-1153,0,0.024694,") and split passages into 100-word long chunks with 50-word long strides. We use a BM25 retriever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Reader Model For all our experiments, we fine-tune a pre-trained A"
2020.sustainlp-1.9,2020.acl-main.537,0,0.138834,"Missing"
2020.sustainlp-1.9,D19-1284,0,0.0749616,"tem (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs predictions at the final la"
2020.sustainlp-1.9,2020.acl-main.593,0,0.482009,"non-adaptive Transformer-based models, we incrementally build a tower—a composition of Transformer layers—until we reach some pre-defined height n and use an output layer to produces the final output, y = OutputLayer(hn ). In this work, due to efficiency reasons, we restrict ourselves to pre-trained ALBERT (Lan et al., 2020) models. One critical property of these models is parameter tying across layers: TransformerLayeri (h) = TransformerLayerj (h) for any i, j. 2.3 skyline so that we reach an accurate solution fast and then stop processing. 3.1 Our first proposal is to extend the method from Schwartz et al. (2020) in order to build up the skyline S. In particular, we will process each passage xi ∈ Dq in isolation, building up height hi and representation ai until an exit probability reaches a threshold. For Schwartz et al. (2020) the exit probability is set to be the probability of the most likely class. While ODQA is not a classification problem per se, it requires solving one as a sub-step, either explicitly or implicitly: deciding whether a passage contains the answer. In turn, our first method T OWER B UILDER, uses the probability 1 − HasAnswer(ai ) of the passage not containing the answer to calcu"
2020.sustainlp-1.9,D19-1599,0,0.0967528,"raining (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Reader Model For all our experiments, we fine-tune a pre-trained ALBERT model (Lan et al., 2020), consisting of 24 transformer layers and cross-layer parameter sharing. We do not use global normalisation (Clark and Gardner, 2018) in our implementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2"
2020.tacl-1.43,D15-1075,0,0.148803,"Missing"
2020.tacl-1.43,P16-1223,0,0.0608875,"Missing"
2020.tacl-1.43,D18-1241,0,0.0205586,"models in the loop. Compared to training on SQuAD, training on adversarially composed questions leads to a similar degree of generalization to non-adversarially written questions, both for SQuAD and NaturalQuestions (Kwiatkowski et al., 2019). It furthermore leads 663 find challenging, and for which natural language understanding is a requisite for generalization. Attempts to achieve this non-trivial aim have typically revolved around extensions to the SQuAD dataset annotation methodology. They include unanswerable questions (Trischler et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019; Choi et al., 2018), adding the option of ‘‘Yes’’ or ‘‘No’’ answers (Dua et al., 2019; Kwiatkowski et al., 2019), questions requiring reasoning over multiple sentences or documents (Welbl et al., 2018; Yang et al., 2018a), questions requiring rule interpretation or context awareness (Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2019), limiting annotator passage exposure by sourcing questions first (Kwiatkowski et al., 2019), controlling answer types by including options for dates, numbers, or spans from the question (Dua et al., 2019), as well as questions with free-form answers (Nguyen et al., 2016; Ko"
2020.tacl-1.43,D19-1606,0,0.140911,"Missing"
2020.tacl-1.43,W17-5401,0,0.0679325,"Missing"
2020.tacl-1.43,N19-1423,0,0.0721366,"Missing"
2020.tacl-1.43,D19-1461,0,0.0490981,"ne in a separate stage of the process, usually after data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator can probe the model directly to explore its weaknesses and formulate targeted adversarial attacks. Although Dua et al. (2019) and Dasigi et al. (2019) make use of adversarial annotations for RC, both annotation setups limit the reach of the model-in-the-loop: In DROP, primarily due to the imposition of specific answer types, and in Quoref by focusing on coreference, which is already a known RC model weakne"
2020.tacl-1.43,W18-2501,0,0.0460236,"Missing"
2020.tacl-1.43,N18-2017,0,0.0313858,"al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and AdversarialNLI (Nie et al., 2019).1 This approach probes mod"
2020.tacl-1.43,D17-1215,0,0.0525677,"al., 2009; Bowman et al., 2015; Rajpurkar et al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and Adversarial"
2020.tacl-1.43,P17-1147,0,0.0288267,"Investigating Adversarial Human Annotation for Reading Comprehension Max Bartolo Alastair Roberts Johannes Welbl Sebastian Riedel Pontus Stenetorp Department of Computer Science University College London {m.bartolo,a.roberts,j.welbl,s.riedel,p.stenetorp}@cs.ucl.ac.uk Abstract which they can arguably be seen as co-responsible (Deng et al., 2009; Bowman et al., 2015; Rajpurkar et al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrong"
2020.tacl-1.43,D18-1546,0,0.0639995,"Missing"
2020.tacl-1.43,N19-1225,0,0.019189,"55.10.6 29.20.8 62.20.7 65.10.7 18.30.6 43.01.1 41.61.0 27.40.7 54.21.0 52.71.0 Table 8: Training models on SQuAD combined with all the adversarially created datasets DBiDAF , DBERT , and DRoBERTa . Results underlined indicate the best result per model. We report the mean and standard deviation (subscript) over 10 runs with different random seeds. addition of the original SQuAD1.1 training data, but unlike in Table 6, this comes without any noticeable decline in performance on DSQuAD , suggesting that the adversarially constructed datasets expose inherent model weaknesses, as investigated by Liu et al. (2019a). vice versa, suggesting that there may exist weaknesses inherent to each model class. 4.3 Generalization to Non-Adversarial Data Compared with standard annotation, the modelin-the-loop approach generally results in new question distributions. Consequently, models trained on adversarially composed questions might not be able to generalize to standard (‘‘easy’’) questions, thus limiting the practical usefulness of the resulting data. To what extent do models trained on model-in-the-loop questions generalize differently to standard (‘‘easy’’) questions, compared with models trained on standard"
2020.tacl-1.43,J93-2004,0,0.070458,"Missing"
2020.tacl-1.43,D16-1264,0,0.370844,"and following the same annotation protocol, we investigate the annotation setup where an annotator has to compose questions for which the model predicts the wrong answer. As a result, only samples that the model fails to predict correctly are retained in the dataset—see Figure 1 for an example. 2 Related Work Constructing Challenging Datasets Recent efforts in dataset construction have driven considerable progress in RC, yet datasets are structurally diverse and annotation methodologies vary. With its large size and combination of freeform questions with answers as extracted spans, SQuAD1.1 (Rajpurkar et al., 2016) has become an established benchmark that has inspired the construction of a series of similarly structured datasets. However, mounting evidence suggests that models can achieve strong generalization performance merely by relying on superficial cues—such as lexical overlap, term frequencies, or entity type matching (Chen et al., 2016; Weissenborn et al., 2017; Sugawara et al., 2018). It has thus become an increasingly important consideration to construct datasets that RC models We apply this annotation strategy with three distinct models in the loop, resulting in datasets with 12,000 samples e"
2020.tacl-1.43,D08-1027,0,0.166111,"Missing"
2020.tacl-1.43,D18-1233,1,0.90295,"Missing"
2020.tacl-1.43,Q19-1029,0,0.158623,"fter data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator can probe the model directly to explore its weaknesses and formulate targeted adversarial attacks. Although Dua et al. (2019) and Dasigi et al. (2019) make use of adversarial annotations for RC, both annotation setups limit the reach of the model-in-the-loop: In DROP, primarily due to the imposition of specific answer types, and in Quoref by focusing on coreference, which is already a known RC model weakness. In contrast, we investigate a scenario where a"
2020.tacl-1.43,Q18-1000,0,0.181939,"Missing"
2020.tacl-1.43,D18-1009,0,0.216213,"k structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and AdversarialNLI (Nie et al., 2019).1 This approach probes model robustness and ensures that the resulting datasets pose a challenge to current models, which drives research to tackle new sets of problems. We study this approach in the context of Reading Comprehension (RC), and investigate its robustness in the face of continuously progressing models—do adversarially constructed datasets quickly become outdated in their usefulness as models grow stronger? Innova"
2020.tacl-1.43,K17-1028,0,0.111655,"in dataset construction have driven considerable progress in RC, yet datasets are structurally diverse and annotation methodologies vary. With its large size and combination of freeform questions with answers as extracted spans, SQuAD1.1 (Rajpurkar et al., 2016) has become an established benchmark that has inspired the construction of a series of similarly structured datasets. However, mounting evidence suggests that models can achieve strong generalization performance merely by relying on superficial cues—such as lexical overlap, term frequencies, or entity type matching (Chen et al., 2016; Weissenborn et al., 2017; Sugawara et al., 2018). It has thus become an increasingly important consideration to construct datasets that RC models We apply this annotation strategy with three distinct models in the loop, resulting in datasets with 12,000 samples each. We then study the reproducibility of the adversarial effect when retraining the models with the same data, as well as the generalization ability of models trained using datasets produced with and without a model adversary. Models can, to a considerable degree, learn to generalize to more challenging questions, based on training sets collected with both s"
2020.tacl-1.43,Q19-1000,0,0.181978,"Missing"
2020.tacl-1.43,P19-1472,0,0.0772758,"l Annotation One recently adopted approach to constructing challenging datasets involves the use of an adversarial model to select examples that it does not perform well on, an approach which superficially is akin to active learning (Lewis and Gale, 1994). Here, we make a distinction between two sub-categories of adversarial annotation: i) adversarial filtering, where the adversarial model is applied offline in a separate stage of the process, usually after data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator ca"
2020.tacl-1.43,P09-1113,0,\N,Missing
2020.tacl-1.43,D13-1020,0,\N,Missing
2020.tacl-1.43,W17-2623,0,\N,Missing
2020.tacl-1.43,D18-1453,0,\N,Missing
2020.tacl-1.43,Q19-1026,0,\N,Missing
2020.tacl-1.43,N19-1246,0,\N,Missing
2020.tacl-1.43,Q18-1023,0,\N,Missing
2020.tacl-1.43,Q19-1016,0,\N,Missing
2021.acl-long.241,P17-1171,0,0.0206988,"re systems encode the query and context (containing the background knowledge), forms a good starting point for NLDBs. Common model architectures are based on the transformer (Vaswani et al., 2017) in an encoder-decoder configuration. The encoder uses self-attention to conditionally encode the context with the query and the decoder allows conditional generation of outputs that are not necessarily present in the input. To scale question answering to reason over large knowledge-sources such as Wikipedia, task formulations typically retrieve textspans from a corpus to condition answer generation (Chen et al., 2017; Dhingra et al., 2017). However, several challenges encountered in NLDBs preclude direct application of these techniques: 3092 Facts Query-based derivation Support sets Result set Sarah married John John works at Shell Neural SPJ NULL John works at Shell Sarah is a doctor Support Set Generator Aggregation 1 Sarah is a doctor Sarah married John Neural SPJ John Sarah married John Query: How many peoples&apos; spouses are doctors? Figure 2: Overview of the proposed architecture. Consisting of a support set generator, SPJ and aggregation Scale To scale neural reasoning to databases of non-trivial size"
2021.acl-long.241,L18-1544,0,0.0332879,"Missing"
2021.acl-long.241,D19-1609,0,0.018712,"PJ. This is because TF-IDF exploits token matching between the query and facts. For larger databases, the retrieval errors resulted in lower answer accuracy. While, with a perfect SSG, the the SPJ accurately answers most query types, as database size increases, the propagation of errors from the SSG resulted in erroneous answers. 8 Related Work Database queries require reasoning over a large set of relevant and non-redundant facts and performing aggregation. While in-roads have been made to perform discrete reasoning and computation over passages (Dua et al., 2019), with explicit computation (Andor et al., 2019) or differentiable modules 3098 Answer Accuracy BREAK (Wolfson et al., 2020) and ShARC (Saeidi et al., 2018) have trained models to translate a natural language query into a sequence of relational operators (or variants thereof). 0.8 0.6 0.4 9 SPJ PerfectIR SSG+SPJ T5 + TF-IDF T5 + DPR 0.2 25 50 100 250 Number of facts in DB 500 1000 Figure 5: Scaling to larger databases with a model trained using 25 facts and tested on larger databases. (Gupta et al., 2020), these use only a single passage rather than requiring aggregation over large numbers of facts from different texts. Multi-hop question a"
2021.acl-long.241,N19-1246,0,0.0382556,"Missing"
2021.acl-long.241,D13-1160,0,0.0616381,"tured natural language data and database-style querying has been a long-standing theme in database research (Halevy et al., 2003). The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system. There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013). More recently, systems such as Conclusions Database systems are the workhorse of data analysis but they require a pre-defined schema. Part of their power stems from the fact that a data analyst can explore the data by easily posing a wide variety of queries. Given the rise in the amount of data that is becoming available in text, images and other modalities, we would like to build systems that enable the flexibility of posing complex queries against such data, but without the need for a pre-defined schema. This paper proposed an architecture for neural databases and the associated W IKI NLDB"
2021.acl-long.241,P17-1147,0,0.0228294,": Examples of set and aggregation queries over a natural language database: a database where facts are stored in free-form text without the need for a schema. Introduction Question answering (QA) over text has made significant strides in recent years owing to the availability of new datasets and models. Machines have surpassed human performance on the well-known SQUaD task (Rajpurkar et al., 2016) where models extract answer spans from a short passage of text. The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen 1 https://github.com/facebookresearch/ NeuralDB et al., 2017; Lewis et al., 2020b; Izacard and Grave, 2020). More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, which may have low lexical or semantic similarity with the question. This paper considers the problem of answering questions similar to database queries, such as those shown in Figure 1. For example, the query “List all"
2021.acl-long.241,Q19-1026,0,0.0158568,"d aggregation queries over a natural language database: a database where facts are stored in free-form text without the need for a schema. Introduction Question answering (QA) over text has made significant strides in recent years owing to the availability of new datasets and models. Machines have surpassed human performance on the well-known SQUaD task (Rajpurkar et al., 2016) where models extract answer spans from a short passage of text. The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen 1 https://github.com/facebookresearch/ NeuralDB et al., 2017; Lewis et al., 2020b; Izacard and Grave, 2020). More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, which may have low lexical or semantic similarity with the question. This paper considers the problem of answering questions similar to database queries, such as those shown in Figure 1. For example, the query “List all the female athletes in Wik"
2021.acl-long.241,P19-1613,0,0.0118725,"T5 + DPR 0.2 25 50 100 250 Number of facts in DB 500 1000 Figure 5: Scaling to larger databases with a model trained using 25 facts and tested on larger databases. (Gupta et al., 2020), these use only a single passage rather than requiring aggregation over large numbers of facts from different texts. Multi-hop question answering requires finding supporting evidence in multiple documents (see (Welbl et al., 2018; Talmor and Berant, 2018; Wolfson et al., 2020) for datasets facilitating this research). In answering multi-hop questions, the works decompose the question into simpler sub questions (Min et al., 2019; Wolfson et al., 2020), or condition each hop on the previously retrieved documents (Asai et al., 2019; Xiong et al., 2020). While tasks such as ComplexWebQuestions (Talmor and Berant, 2018) and BREAK (Wolfson et al., 2020) focus on complex queries that can be broken down into simpler ones, our focus is on setbased and aggregation queries where the complexity comes from the need to retrieve and process a large number of non-redundant relevant facts. In contrast to the set and count tasks in bAbI (Weston et al., 2015), where each query is based on a small context (less than 20 facts), our data"
2021.acl-long.241,P15-1142,0,0.0116482,"g the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research (Halevy et al., 2003). The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system. There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013). More recently, systems such as Conclusions Database systems are the workhorse of data analysis but they require a pre-defined schema. Part of their power stems from the fact that a data analyst can explore the data by easily posing a wide variety of queries. Given the rise in the amount of data that is becoming available in text, images and other modalities, we would like to build systems that enable the flexibility of posing complex queries against such data, but without the need for a pre-defined schema. This paper proposed an architecture for neural databases and the"
2021.acl-long.241,D16-1264,0,0.049017,"as a doctor. Queries: List everyone born before 1980. (Set) → Sheryl, Teuvo, . . . Whose spouse is a doctor? (Join) → Sheryl, John, . . . Who is the oldest person? (Max) → Teuvo Who is Sheryl’s mother? (Set) → NULL Figure 1: Examples of set and aggregation queries over a natural language database: a database where facts are stored in free-form text without the need for a schema. Introduction Question answering (QA) over text has made significant strides in recent years owing to the availability of new datasets and models. Machines have surpassed human performance on the well-known SQUaD task (Rajpurkar et al., 2016) where models extract answer spans from a short passage of text. The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen 1 https://github.com/facebookresearch/ NeuralDB et al., 2017; Lewis et al., 2020b; Izacard and Grave, 2020). More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, w"
2021.acl-long.241,D18-1233,1,0.891708,"Missing"
2021.acl-long.241,N18-1059,0,0.0683935,"1332517,P50,Q193300). Our mapping is a two-step process: firstly, we look up entity names from Wikipedia, returning multiple matches for Osamu Tezuka, and secondly filter these based on which have an author relations to The Slice of Life in the Wikidata graph. While out of scope for this paper, this technique could be applied to generate training datasets for novel domains. W IKI NLDB uses both atomic facts in KELM (about a single relation of an entity) or composite facts (about multiple relations). 3094 Queries Following previous work on large-scale question answering (Hartmann et al., 2018; Talmor and Berant, 2018), queries are generated using templates. For each relation and operator, multiple templates were written by the authors where placeholders can be replaced with the subject and objects for each relation. While multiple templates are used to ensure variety, these are limited in diversity in comparison to the facts. Templates were generated for the first 25 relations on Wikidata with mapped data in KELM. To generate queries that require joins we apply the same technique, combining to combine two or more connected relations, chaining the entities. We further select the 15 most popular relations an"
2021.acl-long.241,Q18-1021,1,0.819666,"0) and ShARC (Saeidi et al., 2018) have trained models to translate a natural language query into a sequence of relational operators (or variants thereof). 0.8 0.6 0.4 9 SPJ PerfectIR SSG+SPJ T5 + TF-IDF T5 + DPR 0.2 25 50 100 250 Number of facts in DB 500 1000 Figure 5: Scaling to larger databases with a model trained using 25 facts and tested on larger databases. (Gupta et al., 2020), these use only a single passage rather than requiring aggregation over large numbers of facts from different texts. Multi-hop question answering requires finding supporting evidence in multiple documents (see (Welbl et al., 2018; Talmor and Berant, 2018; Wolfson et al., 2020) for datasets facilitating this research). In answering multi-hop questions, the works decompose the question into simpler sub questions (Min et al., 2019; Wolfson et al., 2020), or condition each hop on the previously retrieved documents (Asai et al., 2019; Xiong et al., 2020). While tasks such as ComplexWebQuestions (Talmor and Berant, 2018) and BREAK (Wolfson et al., 2020) focus on complex queries that can be broken down into simpler ones, our focus is on setbased and aggregation queries where the complexity comes from the need to retrieve and"
2021.acl-long.241,2020.tacl-1.13,0,0.0115256,"acts. For larger databases, the retrieval errors resulted in lower answer accuracy. While, with a perfect SSG, the the SPJ accurately answers most query types, as database size increases, the propagation of errors from the SSG resulted in erroneous answers. 8 Related Work Database queries require reasoning over a large set of relevant and non-redundant facts and performing aggregation. While in-roads have been made to perform discrete reasoning and computation over passages (Dua et al., 2019), with explicit computation (Andor et al., 2019) or differentiable modules 3098 Answer Accuracy BREAK (Wolfson et al., 2020) and ShARC (Saeidi et al., 2018) have trained models to translate a natural language query into a sequence of relational operators (or variants thereof). 0.8 0.6 0.4 9 SPJ PerfectIR SSG+SPJ T5 + TF-IDF T5 + DPR 0.2 25 50 100 250 Number of facts in DB 500 1000 Figure 5: Scaling to larger databases with a model trained using 25 facts and tested on larger databases. (Gupta et al., 2020), these use only a single passage rather than requiring aggregation over large numbers of facts from different texts. Multi-hop question answering requires finding supporting evidence in multiple documents (see (We"
2021.acl-long.241,2020.acl-demos.24,0,0.0269654,"ss than 20 facts), our dataset scales from databases of 25 facts to 1000. Bridging the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research (Halevy et al., 2003). The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system. There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013). More recently, systems such as Conclusions Database systems are the workhorse of data analysis but they require a pre-defined schema. Part of their power stems from the fact that a data analyst can explore the data by easily posing a wide variety of queries. Given the rise in the amount of data that is becoming available in text, images and other modalities, we would like to build systems that enable the flexibility of posing complex queries against such data, but without the need for"
2021.acl-long.529,P17-1171,0,0.409287,"et al., 2020). In both datasets, claims can be verified ∗ Work done while interning with Facebook AI Research. given a single associated table. While highly useful for the development of models, this closed setting is not reflective of real-world fact checking tasks where it is usually not known which table to consult for evidence. Realistic systems must first retrieve evidence from a large data source. That is, realistic systems must operate in an open setting. Here, we investigate fact verification over tables in the open setting. We take inspiration from similar work on unstructured data (Chen et al., 2017; Nie et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020), proposing a two-step model which combines ad-hoc retrieval with a neural reader. Drawing on preliminary work in open question answering over tables (Sun et al., 2016), we perform retrieval based on simple heuristic modeling of individual table cells. We combine this retriever with a RoBERTa-based (Liu et al., 2019) joint reranking-and-verification model, performing 6787 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
2021.acl-long.529,2020.emnlp-main.19,0,0.0384075,"se retrieval models with dot-product indexing (Johnson et al., 2017), and increasingly advanced pretrained transformer-models for reading. The development of similarly fast, reliable and learnable indexing techniques for tables as well as text is an important direction for future work. Concurrently with our work, Chen et al. (2020a) have introduced a BERT-based model to perform question answering over open collections of data including tables. Like ours, their model consists of separate retriever- and reader-steps. Their bestperforming reader employs a long-range sparse attention transformer (Ainslie et al., 2020) to jointly summarize all retrieved data. As in our case, their model demonstrates significant improvements from using multiple retrieved tables. 7 Conclusion We have introduced a novel model for fact verification over large collections of tables, along with two strategies for exploiting closed-domain datasets to increase performance. Our approach performs on par with the current closed-domain state of the art, with larger gains the more tables we include. When using an oracle to retrieve a reference table, our approach also represents a new closed-domain state of the art. Finally, we have mad"
2021.acl-long.529,2020.acl-main.398,0,0.061532,"Missing"
2021.acl-short.57,P17-1171,0,0.0998819,"eart model on two datasets, and is also more accurate than previous AC methods due to the stronger base ODQA model. All source code and datasets are available at https:// github.com/uclnlp/APE. 1 Figure 1: Overview of our approach. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. Ho"
2021.acl-short.57,P18-1078,0,0.0733427,"1: Overview of our approach. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most resea"
2021.acl-short.57,P17-1147,0,0.027952,"hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the proportion of questions whose predicted answer matches exactly with the ground truth. We also include the unrestricted setting to compare"
2021.acl-short.57,2020.emnlp-main.550,0,0.0489704,"ted Work Open Domain Question Answering ODQA is a task that aims to answer a factoid question given a document corpus. Most works in this domain follow a retriever-reader design first proposed by Chen et al. (2017). The retriever collects a set of relevant passages, then the reader comprehends and aggregates the information from multiple passages to produce the answer. Depending on the design of the reader model, these systems could be further categorised into extractive models and generative models. Extractive models (Min et al., 2019; Yang et al., 2019; Wang et al., 2019; Asai et al., 2020; Karpukhin et al., 2020) exploit an answer extraction model to predict the probabilities of answer spans, and use global normalisation (Clark and Gardner, 2018) to aggregate the answer probabilities across multiple passages. However, thanks to recent advances in sequenceto-sequence pretrained language models (Raffel et al., 2020; Lewis et al., 2020a), generative ODQA models (Min et al., 2020; Lewis et al., 2020b; Izacard and Grave, 2020b) achieve significant improvement upon extractive models, demonstrating stronger capability in combining evidence from multiple passages. We focus on generative models in this work. P"
2021.acl-short.57,Q19-1026,0,0.0116324,"pdates its priorities l0 qn with its new hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the proportion of questions whose predicted answer matches exactly with the ground truth. We also include th"
2021.acl-short.57,P19-1612,0,0.065349,"a passage with the maximum priority, forward one encoder layer for it ln0 = ln + 1, and updates its priorities l0 qn with its new hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the propor"
2021.acl-short.57,2020.acl-main.703,0,0.431065,"computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficienc"
2021.acl-short.57,2020.acl-main.537,0,0.027718,"(2019) rely on BM25 for ranking passages (Robertson, 2004). Recently, Karpukhin et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020a) achieved substantial increase in retrieval performance using dense representations. Our work is based on the retrieval results from a dense retriever (Izacard and Grave, 2020b), but we show that the proposed method can still improve the quality of the support passages despite the strong retrieval performance. Adaptive Computation Adaptive computation allows the model to condition the computation cost on the input. For example, Schwartz et al. (2020b); Liu et al. (2020); Xin et al. (2020) propose models that can dynamically decide to early exit at intermediate layers when the confidence at the layer exceeds a threshold. They show that adaptively early exiting can significantly reduce the computational cost for various sequence classification tasks. Closest to our work, Wu et al. (2020) introduced adaptive computation for extractive ODQA models. We extend adaptive computation to generative ODQA models, and our approach can be incorporated in existing generative ODQA models without finetuning the base model. 3 Method In this section, we will introduce the base"
2021.acl-short.57,D19-1284,0,0.0484143,"ides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show t"
2021.acl-short.57,2020.emnlp-main.466,0,0.0563004,"er with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly i"
2021.acl-short.57,2020.acl-main.593,0,0.121795,"t al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficiency of extractive ODQA models at inference time. However, it requires fine-tuning all model parameters with a multitask learning objective, making it computationally challenging to apply this method to current state-of-the-art models. In this work, we explore an efficient approach to apply adaptive computation to large generative ODQA models. We introduce the Adaptive Passage Encoder (APE), a module that can be added to the encoder of an existing ODQA model, which has the following features: 1) it eff"
2021.acl-short.57,D19-1599,0,0.0489334,"ssage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et"
2021.acl-short.57,2020.sustainlp-1.9,1,0.705695,"2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficiency of extractive ODQA models at inference time. However, it requires fine-tuning all model parameters with a multitask learning objective, making it computationally challenging to apply this method to current state-of-the-art models. In this work, we explore an efficient approach to apply adaptive computation to large generative ODQA models. We introduce the Adaptive Passage Encoder (APE), a module that can be added to the encoder of an existing ODQA model, which has the following features: 1) it efficiently reuses the"
2021.acl-short.57,2020.acl-main.204,0,0.0272795,"5 for ranking passages (Robertson, 2004). Recently, Karpukhin et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020a) achieved substantial increase in retrieval performance using dense representations. Our work is based on the retrieval results from a dense retriever (Izacard and Grave, 2020b), but we show that the proposed method can still improve the quality of the support passages despite the strong retrieval performance. Adaptive Computation Adaptive computation allows the model to condition the computation cost on the input. For example, Schwartz et al. (2020b); Liu et al. (2020); Xin et al. (2020) propose models that can dynamically decide to early exit at intermediate layers when the confidence at the layer exceeds a threshold. They show that adaptively early exiting can significantly reduce the computational cost for various sequence classification tasks. Closest to our work, Wu et al. (2020) introduced adaptive computation for extractive ODQA models. We extend adaptive computation to generative ODQA models, and our approach can be incorporated in existing generative ODQA models without finetuning the base model. 3 Method In this section, we will introduce the base model and how our"
2021.acl-short.57,N19-4013,0,0.0619369,"ch. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et"
2021.eacl-main.86,D13-1160,0,0.487792,"o new heights (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017) and Open NaturalQuestions (Kwiatkowski et al., 2019; Lee et al., 2019). We identify three types of desired behaviour of a trained ODQA system, in increasing order of difficulty: 1) to recall the answer to a question that the model has seen at training time. 2) to answer novel questions at test time and choose an answer from the set of answers it has seen during training. 3) to answer novel questions which have answers which are not contained in the training data. It is not clear to what extent our current ODQA datasets measure each of these three behaviours. To"
2021.eacl-main.86,P16-1223,0,0.0833455,"Missing"
2021.eacl-main.86,P17-1147,0,0.289487,"Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017) and Open NaturalQuestions (Kwiatkowski et al., 2019; Lee et al., 2019). We identify three types of desired behaviour of a trained ODQA system, in increasing order of difficulty: 1) to recall the answer to a question that the model has seen at training time. 2) to answer novel questions at test time and choose an answer from the set of answers it has seen during training. 3) to answer novel questions which have answers which are not contained in the training data. It is not clear to what extent our current ODQA datasets measure each of these three behaviours. To address this, we stratify the t"
2021.eacl-main.86,D18-1546,0,0.0173117,"etrieval model outperforms the BART closed-book model on NaturalQuestions and TriviaQA. Further, the dense nearest neighbor model also outperforms the significantly more complex DPR open-book model on TriviaQA and WebQuestions on the question overlap subset. 5 Related Work Examining what behaviours are learnt by models has received attention in language understanding tasks, such as GLUE (Wang et al., 2018), which includes tools for probing for different reasoning types. There has also been critical and careful analysis of QA systems and datasets. Chen et al. (2016), Sugawara et al. (2020) and Kaushik and Lipton (2018) analyse the difficulty of various machine reading datasets, and Manjunatha et al. (2018) show that visual QA models memorize common question-answer relationships in training data. F´evry et al. (2020) analyse various closed-book models’ TriviaQA predictions. Kwiatkowski et al. (2019) note that the machine reading NaturalQuestions dataset has train-test overlap of Wikipedia titles, and provide baselines for “long-answer” QA. Verga et al. (2020) observe answer overlap effects in a related modality (knowledgebase QA), but no not consider question overlap. 6 Conclusion We performed an analysis of"
2021.eacl-main.86,Q19-1026,0,0.20212,"Missing"
2021.eacl-main.86,P19-1612,0,0.314496,"ation plays in these benchmarks. 1 Introduction Open-domain Question Answering (ODQA) is a task that examines the ability of models to produce answers to natural language factoid questions drawn from an open set of domains. ODQA has received significant attention for its potential practical applications, and more recently as a popular method to analyse how well NLP systems can capture and recall factual knowledge. This interest in ODQA as a challenging “knowledge-intensive” task has led to a flurry of recent works that have driven test-set performance on standard ODQA datasets to new heights (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA"
2021.eacl-main.86,D19-1284,0,0.047626,"by crowdworkers. The ODQA task consists of predicting the name of the Freebase entity. We use the standard train/test splits from Berant et al. (2013) and the development split from Karpukhin et al. (2020), which was randomly split from the train set. TriviaQA consists of 78,785 train, 8,837 development and 11,313 test instances obtained by scraping trivia websites. Answers are Wikipedia entities, and any alias for the answer entity is considered a correct answer. We use the ODQA splits, which correspond to the unfiltered-train and unfiltereddev reading comprehension splits (Lee et al., 2019; Min et al., 2019, 2020b; Karpukhin et al., 2020). Open-NaturalQuestions consists of search engine questions with crowdsourced answer spans in Wikipedia articles. The ODQA version consists of question-answer pairs from NaturalQuestions which have short answer spans less than 6 tokens in length. We use the standard open-domain splits in our experiments, consisting of 79,168 train, 8,757 development and 3,610 question answer pairs. For all three datasets, the canonical train, development and test splits were obtained by randomly splitting the question-answer pairs, and there are no exact duplicate questions in a"
2021.eacl-main.86,D16-1264,0,0.0543447,"est-Train Overlaps We explore two ways of examining the test sets based on overlaps between training and test data. Consider a question-answer pair (q, a) from the test set Dtest where the answer consists of at least one answer reference a = {s1 ..sn }. We can consider answer overlap where there exists at least one (q 0 , a0 ) ∈ Dtrain which shares at least one answer reference with (q, a). We can also consider question overlap, where there exists some (q 00 , a00 ) ∈ Dtrain where q 00 is a duplicate of q, such that q and q 00 are paraphrases and have the same answer. Answer Overlap Following Rajpurkar et al. (2016), we apply answer normalization (lowercasing, stripping punctuation, removing articles and normalizing whitespace) on answer references 1001 Open NaturalQuestions Overlapping Non-overlapping Overlapping Phil Simms Brian Johnson 8 the Indians the 1830s David Bowie Battle of camlann Heligoland Henry VII Niagra Falls Cloves Matt Monro 1,020 – 1,080 kg Hermann Ebbinghaus Matt Flinders TriviaQA Non-overlapping Death in the afternoon Clash of the Titans ice-cream sundae Camshaft Cumberland WebQuestions Overlapping Non-overlapping Harvard Alderaan India 2011 Zeus Queen Victoria Bras´ılia Paddington T"
2021.eacl-main.86,2020.emnlp-main.437,0,0.292708,"Missing"
2021.eacl-main.86,W18-5446,0,0.0752755,"Missing"
2021.emnlp-main.696,P19-1620,0,0.278606,"ction (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that attempt to maximise diversity; we find that"
2021.emnlp-main.696,2020.tacl-1.43,1,0.809231,"century (iv) Q: When did Old English begin to be used? A: 5th century Figure 1: The Synthetic Adversarial Data Generation Pipeline showing: (i) passage selection from Wikipedia; (ii) answer candidate selection and filtering by model confidence (an example retained answer shown in green, and a dropped answer candidate in red); (iii) question generation using BARTLarge ; and (iv) answer re-labelling using self-training. The generated synthetic data is then used as part of the training data for a downstream Reading Comprehension model. A recently proposed alternative is dynamic data collection (Bartolo et al., 2020; Nie et al., 2020), Large-scale labelled datasets like SQuAD (Ra- where data is collected with both humans and modjpurkar et al., 2016) and SNLI (Bowman et al., els in the annotation loop. Usually, these humans 2015) have been driving forces in natural language are instructed to ask adversarial questions that fool processing research. Over the past few years, how- existing models. Dynamic adversarial data colever, such “statically collected” datasets have been lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art mode"
2021.emnlp-main.696,D15-1075,0,0.0867811,"Missing"
2021.emnlp-main.696,E06-1032,0,0.0453541,"g. 2.3 Self-training In self-training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate se"
2021.emnlp-main.696,P18-1177,0,0.0471497,"Missing"
2021.emnlp-main.696,P17-1123,0,0.0217455,"llace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples usi"
2021.emnlp-main.696,D19-5801,1,0.897458,"Missing"
2021.emnlp-main.696,D19-1107,0,0.0399652,"Missing"
2021.emnlp-main.696,N18-2017,0,0.045407,"Missing"
2021.emnlp-main.696,J15-4004,0,0.046805,"training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate selection, question g"
2021.emnlp-main.696,D17-1215,1,0.769976,"Missing"
2021.emnlp-main.696,2020.acl-main.441,1,0.791342,"Missing"
2021.emnlp-main.696,2021.acl-long.186,1,0.733505,"part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lew"
2021.emnlp-main.696,2020.acl-main.703,0,0.172923,"Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer only, as we find that this setting provides better control of answer diversity. We use the same range of k ∈ {1, 5, 10, 15, 20} for both settings. 3.1.1 Passage Selection The text passages we use are sourced from SQuAD (further details can be found in Appendix A). We Self-Attention Labelling (SAL) We propose a also experiment with using passages external to multi-label classification head to jointly model canSQuAD, which also sourced from Wikipedia. To didate start and end tokens, and provide a bin"
2021.emnlp-main.696,P19-1484,1,0.842616,"ends, with improved performance. Since SQuAD and the AdversarialQA datasets use the same passages partitioned into the same data splits, we align the annotated answers to create representative answer selection training, validation and test sets. Dataset statistics (see Appendix C), highlight the high percentage of overlapping answers suggesting that existing answer tagging methods (Zhou et al., 2017; Zhao et al., 2018) might struggle, and models should ideally be capable of handling span overlap. Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer on"
2021.emnlp-main.696,2021.eacl-main.86,1,0.886215,"een lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art models, but it can also lar, they often exhibit inadvertent spurious statisti- create higher-quality training data (Bartolo et al., cal patterns that models learn to exploit, leading to 2020; Nie et al., 2020) due to the added incentive poor model robustness and generalisation (Jia and for crowdworkers to provide challenging examples. Liang, 2017; Gururangan et al., 2018; Geva et al., It can also reduce the prevalence of dataset biases 2019; McCoy et al., 2019; Lewis et al., 2021a). and annotator artefacts over time (Bartolo et al., ∗ 2020; Nie et al., 2020), since such phenomena can Most of this work was carried out while MB was at Facebook AI Research. be subverted by model-fooling examples collected 8830 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8830–8848 c November 7–11, 2021. 2021 Association for Computational Linguistics in subsequent rounds. However, dynamic data collection can be more expensive than its static predecessor as creating examples that elicit a certain model response (i.e., fooling"
2021.emnlp-main.696,D16-1264,0,0.115275,"Missing"
2021.emnlp-main.696,2020.acl-main.442,0,0.0352994,"Missing"
2021.emnlp-main.696,2021.acl-long.132,1,0.719606,"he Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri"
2021.emnlp-main.696,Q19-1029,0,0.0183855,"t baseline. The collected dataset will form part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du"
2021.emnlp-main.696,2020.findings-emnlp.90,0,0.0320765,"Missing"
2021.emnlp-main.696,D18-1424,0,0.110175,"s (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that"
2021.naacl-main.200,N19-4010,0,0.0403104,"Missing"
2021.naacl-main.200,D13-1160,0,0.0237356,"t al., 2018), reading comprehension (Dua et al., 2019), question understanding (Wolfson et al., 2020), and dialogue (Shuster et al., 2019). We focus on multi-domain tasks that need to seek knowledge in a large body of documents to produce an output. Although there exist several tasks and resources that define large-scale external knowledge sources—including the TAC-KBP challenges (McNamee and Dang, 2009; Ji et al., 2010; Surdeanu, 2013; Surdeanu and Ji, 2014), ARC (Clark et al., 13 https://www.wikidata.org 2018), TriviaQA-web (Joshi et al., 2017), QuasarT (Dhingra et al., 2017), WebQuestions (Berant et al., 2013) and ComplexWebQuestions (Talmor and Berant, 2018)—in KILT we exclusively consider publicly available Wikipedia-based datasets in order to merge and unify the knowledge source. 10 Conclusion We introduce KILT, a benchmark for assessing models that need to condition on specific knowledge in a defined snapshot of Wikipedia to solve tasks spanning five domains. The goal is to catalyze and facilitate research towards general and explainable models equipped with task-agnostic representations of knowledge. Our experiments show promising results for a general solution combining dense retrieval and se"
2021.naacl-main.200,P17-1171,0,0.026321,"nd genknowledge-intensive tasks, general infrastructure eral baselines, evaluating downstream perforand architectures across tasks have yet to emerge, mance in addition to the ability of the models to provide provenance. We find that and fundamental research questions remain open. a shared dense vector index coupled with For example, while it was long assumed that nona seq2seq model is a strong baseline, outparametric and explicit memory accessed through performing more tailor-made approaches for retrieval is strictly required for competitive refact checking, open-domain question answersults (Chen et al., 2017), recent large pre-trained ing and dialogue, and yielding competitive resequence-to-sequence models such as T5 (Raffel sults on entity linking and slot filling, by genet al., 2019a) and BART (Lewis et al., 2019) store erating disambiguated text. KILT data and all knowledge in their parameters while performing code are available at https://github.com/ facebookresearch/KILT.1 remarkably well (Petroni et al., 2019). Likewise, while the classical approach of information extrac1 Introduction tion for populating a Knowledge Base (KB, Riedel There has been substantial progress on natural lan- et al.,"
2021.naacl-main.200,N19-1423,0,0.260463,"substantial progress on natural lan- et al., 2013; Surdeanu and Ji, 2014) seems outof-fashion, recent results show that they remain guage processing tasks where the inputs are short textual contexts such as a sentences, paragraphs, contenders (Fan et al., 2019a; Xiong et al., 2019). or perhaps a handful of documents. Critically, we While there are numerous datasets for have seen the emergence of general-purpose archi- knowledge-intensive tasks (e.g. Thorne et al., tectures and pre-trained models that can be applied 2018a; Dinan et al., 2019; Kwiatkowski et al., to a wide range of such tasks (Devlin et al., 2019). 2019, to name just a few), it is difficult to However, for many real world problems, process- answer the above questions generally across ing at this local level is insufficient. For example, them. Each dataset comes in a different format, 1 is pre-processed with different assumptions, and and at https://huggingface.co/datasets? search=kilt requires different loaders, evaluations, and analysis 2523 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544 June 6–11, 2021. ©2021 Association f"
2021.naacl-main.324,2020.emnlp-main.393,0,0.0658177,"hoice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likeli"
2021.naacl-main.324,2020.tacl-1.3,0,0.0240418,"med “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,W17-5401,0,0.025796,", 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah a"
2021.naacl-main.324,2020.acl-main.465,0,0.0806274,"solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al.,"
2021.naacl-main.324,2020.insights-1.13,0,0.0263233,"er, e.g., the multiple iterations of SemEval or WMT datasets over the years, we’ve already been handling this quite well—we accept that a model’s BLEU score on WMT16 is not comparable to WMT14. That is, it is perfectly natural for benchmark datasets to evolve as the community makes progress. The only thing Dynabench does differently is that it anticipates dataset saturation and embraces the loop so that we can make faster and more sustained progress. ever, it has also been found that model-in-the-loop counterfactually-augmented training data does not necessarily lead to better generalization (Huang et al., 2020). Given the distributional shift induced by adversarial settings, it would probably be wisest to combine adversarially collected data with nonadversarial data during training (ANLI takes this approach), and to also test models in both scenarios. To get the most useful training and testing data, it seems the focus should be on collecting adversarial data with the best available model(s), preferably with a wide range of expertise, as that will likely be beneficial to future models also. That said, we expect this to be both task and model dependent. Much more research is required, and we encourag"
2021.naacl-main.324,2020.acl-main.768,1,0.846016,"e the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle t"
2021.naacl-main.324,N19-1225,0,0.0738848,"bstantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and that encour- introduce Dynabench, an open-source, web-based age apples-to-apples model comparisons. Bench- research platform for dynamic data collection and marks provide a north star goal for researchers, and model benchmarking. The guiding hypothesis be4110 Proceedings of the 2021 Con"
2021.naacl-main.324,D17-1215,1,0.795486,"ard et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et a"
2021.naacl-main.324,2021.ccl-1.108,0,0.0774911,"Missing"
2021.naacl-main.324,D15-1166,0,0.00793057,"rk tasks, that milestone is now rou- cording to the narrow criteria used to define human performance) nonetheless fail on simple chaltinely reached within just a few years for newer lenge examples and falter in real-world scenarios. datasets (see Figure 1). As with the rest of AI, NLP A substantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and"
2021.naacl-main.324,2020.emnlp-main.154,0,0.0295127,"uperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring"
2021.naacl-main.324,J93-2004,0,0.0749322,"humans? This reveals the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang"
2021.naacl-main.324,C10-1091,0,0.0606954,"Missing"
2021.naacl-main.324,N19-1063,0,0.0241688,"d its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not s"
2021.naacl-main.324,P19-1334,0,0.0218097,"ang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al.,"
2021.naacl-main.324,K18-1007,1,0.843089,"for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing tar"
2021.naacl-main.324,W10-0719,1,0.726488,"collaborative effort, the platform is meant to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community. In the current iteration, the platform is set up for dynamic adversarial data collection, where humans can attempt to find modelfooling examples. This design choice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics."
2021.naacl-main.324,C18-1198,0,0.0239103,"ard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Mo"
2021.naacl-main.324,2020.acl-main.441,1,0.878791,"our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of resource collection and architectural improvements. Similar to Dynabench, recent work seeks to embrace this phenomenon, addressing many of the previously mentioned issues through an iterative human-and-model-in-the-loop annotation process (Yang et al., 2017; Dinan et al., 2019; Chen et al., 2019; Bartolo et al., 2020; Nie et al., 2020), to find “unknown unknowns” (Attenberg et al., 2015) or in a never-ending or life-long learning setting (Silver et al., 2013; Mitchell et al., 2018). The Adversarial NLI (ANLI) dataset (Nie et al., 2020), for example, was collected with an adversarial setting over multiple rounds to yield “a ‘moving post’ dynamic target for NLU systems, rather than a static benchmark that will eventually saturate”. In its few-shot learning mode, GPT-3 barely shows “signs of life” (Brown et al., 2020) (i.e., it is barely above random) on ANLI, which is evidence that we are still far away from human performance"
2021.naacl-main.324,S18-2023,0,0.0556136,"Missing"
2021.naacl-main.324,W12-4501,0,0.0439542,"the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However"
2021.naacl-main.324,P18-2124,1,0.869954,"Missing"
2021.naacl-main.324,D16-1264,0,0.230428,"n use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its lead"
2021.naacl-main.324,2020.acl-main.442,0,0.230761,"performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuri"
2021.naacl-main.324,K19-1019,0,0.019609,"al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models"
2021.naacl-main.324,N18-2002,0,0.0355102,"Missing"
2021.naacl-main.324,2020.emnlp-main.661,1,0.822242,"Missing"
2021.naacl-main.324,P19-1004,0,0.0188742,"put character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally"
2021.naacl-main.324,2020.acl-main.479,0,0.0170779,"ead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Etting"
2021.naacl-main.324,2020.acl-main.222,0,0.0281863,"19) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likelihood ilarly, the paradigm is perfectly compatible with training on i.i.d. train/test splits and human lan- collaborative settings that utilize human feedback, guage (Linzen, 2020; Stiennon et al., 2020). or even negotiation. The crucial aspect of this proposal is the fact that models and humans interact We think there is widespread agreement that something has to change about our standard eval- live “in the loop” for evaluation and data collection. uation paradigm and that we nee"
2021.naacl-main.324,D08-1027,0,0.352191,"Missing"
2021.naacl-main.324,D13-1170,1,0.0131607,"t data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather tha"
2021.naacl-main.324,2020.emnlp-main.746,0,0.0673253,"Missing"
2021.naacl-main.324,N18-1074,0,0.0416359,"Missing"
2021.naacl-main.324,L18-1239,0,0.0544033,"Missing"
2021.naacl-main.324,W19-3509,1,0.836306,"Missing"
2021.naacl-main.324,D19-1221,0,0.0180989,"2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of r"
2021.naacl-main.324,W18-5446,1,0.794698,"Missing"
2021.naacl-main.324,D19-1286,0,0.0188342,"ive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressiv"
2021.naacl-main.324,2020.tacl-1.25,0,0.0303378,"enging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzi"
2021.naacl-main.324,W17-3012,1,0.867269,"Missing"
2021.naacl-main.324,D18-1501,0,0.0349494,"Missing"
2021.naacl-main.324,N18-1101,1,0.774917,"he background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive bod"
2021.naacl-main.324,D18-1259,0,0.0459996,"Missing"
2021.naacl-main.324,2020.emnlp-main.397,0,0.0238252,"was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,2020.emnlp-main.659,1,0.819658,"Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by sim"
C16-1146,W15-1516,0,0.0328934,"Missing"
C16-1146,S14-2145,0,0.105602,"ave a mention of a location entity name and discard other sentences. 2.2 Categories The Number of location mentions in a single sentence in our dataset varies from one to over 50. To simplify the task, we only annotate sentences that contain one or two location mentions. These sentences were divided into two groups: sentences containing one location mention — Single, and sentences containing two location mentions — Multi. This is to observe the difficulty of annotating two groups by human annotators and by the models. 2.3 Aspects Like existing work in the aspect-based sentiment analysis task (Brychcın et al., 2014), a pre-defined list of aspects is provided for annotators to choose from. These aspects are: live, safety, price, quiet, dining, nightlife, transit-location, touristy, shopping, green-culture and multicultural. Adding an additional aspect of misc was considered. However in the initial round of annotations, we realised that it had a negative effect on the decisiveness of annotators and it led to a lower overall agreement. Aspect general refers to a generic opinion about a location, e.g. “I love Camden Town”. 2.4 Sentiment For each selected aspect, annotators were required to select a polarity"
C16-1146,P14-2009,0,0.0585927,"Missing"
C16-1146,P11-1016,0,0.0374303,"unit of text, and recognizing the polarity associated with each aspect separately. The datasets for this task were mostly based on specialized review platforms such as Yelp where it is assumed that only one entity is discussed in one review snippet, but the opinion on multiple aspects can be expressed. This task is particularly useful because a user can assess the aggregated sentiment for each individual aspect of a given product or service and get a more fine-grained understanding of its quality. Another line of research in this field is targeted (a.k.a. target-dependent) sentiment analysis (Jiang et al., 2011; Vo and Zhang, 2015). Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences (often a tweet). For instance in the sentence “People everywhere love Windows & vista. Bill Gates”, polarity towards Bill Gates is “Neutral” but the positive sentiment towards Windows & vista will interfere with identifying it if the usual methods for sentiment analysis task are employed. However this task assumes only the overall sentiment for each entity. Moreover, the existing corpora for this task so far has contained only a sing"
C16-1146,liakata-etal-2010-corpora,1,0.807761,"Missing"
C16-1146,W02-1011,0,0.0224414,"Missing"
C16-1146,S15-2082,0,0.118495,"Missing"
C16-1146,E12-2021,0,0.0228055,"Missing"
C16-1146,P02-1053,0,0.00968963,"Missing"
C16-1146,N10-1122,0,\N,Missing
C16-1146,D13-1171,0,\N,Missing
D07-1096,D07-1119,0,0.635846,"Missing"
D07-1096,D07-1120,0,0.0179351,"Missing"
D07-1096,W06-1615,1,0.130888,"d annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follows. Participants were prov"
D07-1096,W06-2920,0,0.770442,"d provide a first analysis of these results. 1 Introduction Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999, 2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005). In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006). In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence. This defines a dependency graph, where In this year’s shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation. The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where th"
D07-1096,D07-1121,0,0.0305771,"Missing"
D07-1096,D07-1101,0,0.829681,"Missing"
D07-1096,W01-0521,0,0.119247,"and Turkish. The treebanks from 2 The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations. Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen). 917 which the data sets were extracted are described in section 3. 2.3 Domain Adaptation Track One well known characteristic of data-driven parsing systems is that they typically perform much worse on data that does not come from the training domain (Gildea, 2001). Due to the large overhead in annotating text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantia"
D07-1096,A00-2018,0,0.0790907,"test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One"
D07-1096,W04-3237,0,0.0190919,"dapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target doma"
D07-1096,D07-1097,1,0.748997,"Missing"
D07-1096,D07-1122,0,0.0269928,"Missing"
D07-1096,P97-1003,0,0.129198,"neralize to unseen test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz an"
D07-1096,D07-1112,0,0.583145,"Missing"
D07-1096,D07-1102,0,0.0310581,"Missing"
D07-1096,W07-2416,0,0.767751,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,D07-1123,0,0.0858787,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,W02-2016,0,0.338207,"expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors inv"
D07-1096,W04-3111,1,0.307142,"task. A new test set of about 9,000 tokens was provided by G¨uls¸en Eryi˘git (Eryi˘git, 2007), who also handled the conversion to the CoNLL format, which means that we could use 919 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). This data set is identical to the English training set from the multilingual track (see section 3.1). For the target domains we used three different labeled data sets. The first two were annotated as part of the PennBioIE project (Kulick et al., 2004) and consist of sentences drawn from either biomedical or chemical research abstracts. Like the source WSJ corpus, this data is annotated using the Penn Treebank phrase structure scheme. To convert these sets to dependency structures we used the same procedure as before (Johansson and Nugues, 2007a). Additional care was taken to remove sentences that contained non-WSJ part-of-speech tags or non-terminals (e.g., HYPH part-of-speech tag indicating a hyphen). Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible. As already mentioned,"
D07-1096,D07-1098,0,0.0447748,"Missing"
D07-1096,D07-1124,0,0.0197253,"Missing"
D07-1096,J93-2004,0,0.0597121,"dependency annotation, just as for PADT. It was also used in the shared task 2006, but there are two important changes compared to last year. First, version 2.0 of PDT was used instead of version 1.0, and a conversion script was created by Zdenek Zabokrtsky, using the new XMLbased format of PDT 2.0. Secondly, due to the upper bound on training set size, only sections 1–3 of PDT constitute the training data, which amounts to some 450,000 tokens. The test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turk"
D07-1096,N04-1001,0,0.244784,"se trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated res"
D07-1096,D07-1125,0,0.020143,"Missing"
D07-1096,P06-1043,0,0.463102,"scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follo"
D07-1096,N03-1027,0,0.115089,"text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter set"
D07-1096,D07-1013,1,0.545996,"Missing"
D07-1096,N06-2033,0,0.645238,"Missing"
D07-1096,E06-1011,1,0.349109,"the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research. In order to provide an extended empirical foundation"
D07-1096,D07-1111,0,0.734777,"Missing"
D07-1096,H05-1066,1,0.508532,"Missing"
D07-1096,D07-1126,0,0.0372423,"Missing"
D07-1096,W06-2934,1,0.278986,"Missing"
D07-1096,D07-1128,0,0.0204553,"Missing"
D07-1096,D07-1129,0,0.0286186,"Missing"
D07-1096,W06-2902,0,0.016467,"g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for"
D07-1096,D07-1099,0,0.322183,"Missing"
D07-1096,D07-1130,0,0.0262275,"Missing"
D07-1096,D07-1131,0,0.0212092,"Missing"
D07-1096,W03-3023,0,0.882665,"ults for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an i"
D07-1096,P06-1085,0,\N,Missing
D07-1096,D07-1127,0,\N,Missing
D10-1099,P07-1073,0,0.741937,"uctured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1 Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. Th"
D10-1099,W02-1001,0,0.00454491,"eatures fi,r,t i gument of c, has the entity type t and the candidate tuple c is labelled as instance of relation r. For exPair ample, f1,founded,person fires if Ye1 (argument i = 1) is in state person, and Ye1 ,e2 in state founded, regardless of the state of Ye2 . 1017 domly picks a variable Yc and samples its relation value conditioned on its Markov Blanket. At test time we decrease the temperature of our sampler in order to find an approximation of the MAP solution. 3.3 Training Most learning methods need to calculate the model expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per"
D10-1099,P04-1054,0,0.045712,"R a relation instance.3 It denotes the membership of the tuple c in the relation R. For example, founded (B ILL G ATES , M ICROSOFT) is a relation instance denoting that B ILL G ATES and M ICROSOFT are related in the founded relation. In the following we will always consider some set of candidate tuples C that may or may not be related. We define Cn ⊂ C to be set of all n-ary tuples in C. Note that while our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2 . Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, B ILL G ATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 2 The pyramid algorithm of Kate and Mooney (2010) may scale well, but it is not clear how to apply their scheme to crossdocument extraction. 1014 3 Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. We care ab"
D10-1099,P05-1045,0,0.00774075,"rformance, the score is normalized by the number of relation mentions. For manual evaluation we pick the top ranked 50 relation instances for the most frequent relations. We ask three annotators to inspect the mentions of these relation instances to decide whether they are correct. Upon disagreement, we use majority vote. To summarize precisions across relations, we take their average, and their average weighted by the proportion of predicted instances for the given relation. 5.1.1 Data preprocessing We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE. We treat consecutive tokens which share the same category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in (Mintz et"
D10-1099,P10-1030,0,0.0372026,"hich inference is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate an"
D10-1099,W10-2924,0,0.0649842,"ile our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2 . Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, B ILL G ATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 2 The pyramid algorithm of Kate and Mooney (2010) may scale well, but it is not clear how to apply their scheme to crossdocument extraction. 1014 3 Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. We care about entity types for two reasons. First, they can be important for downstream applications: if consumers of our extracted facts know the type of entities, they can find them more easily, visualize them more adequately, and perform operations specific to these types (write emails to persons, book a hotel in a city, etc.). Second, they are useful for extracting binary relations due to selectional p"
D10-1099,P09-1113,0,0.0581474,"ies expressed in structured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1 Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves muc"
D10-1099,W04-2407,0,0.0180525,"e category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in (Mintz et al., 2009): lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. We use the openNLP POS tagger4 to obtain POS tags and employ the MaltParser (Nivre et al., 2004) for dependency parsing. For candidate tuples with arity 1 (entity types) we use the following features: the entity’s word form, the POS sequence, the head of the entity in the dependency parse tree, the Stanford named entity tag, and the left and right words to the current entity mention phrase. 5.1.2 Configurations We apply the following configurations of our factor graphs. As our baseline, and roughly equivalent to previous work (Mintz et al., 2009), we pick the templates TBias and TMen . These describe a fully disconnected graph, and we will refer to this configuration as isolated. Next, w"
D10-1099,W09-1406,1,0.324897,"diction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping process. Here in each iteration of bootstrapping, extracted facts that violate compatibility constraints will not be used to generate additional patterns in the next iteration. 5 Experiments W"
D10-1099,D08-1009,0,0.0360096,"Missing"
D11-1001,W09-1402,0,0.192323,"teractions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods"
D11-1001,P05-1022,0,0.0491878,"nally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t ∈ T . It  creates one active feature for each element in t, t ∈ TReg × feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic ch"
D11-1001,P01-1030,0,0.00930482,"al search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constrain"
D11-1001,W09-1401,0,0.0854254,"ed to automatically extract structured representations from biomedical text—a process often referred to as biomedical text mining. One major focus of biomedical text mining has been the extraction of named entities, such genes or gene products, and of flat binary relations between such entities, such as protein-protein interactions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare"
D11-1001,D10-1125,0,0.0226592,"l variables λ that will appear as local penalties in the subproblems to be solved. The algorithm will try to tune these variables such that at convergence the coupling constraints will be fulfilled. This is done by first op¯) over I. Now, timizing s2 (e, a) over O and s2 (¯ e, a whenever there is disagreement between two variables to be coupled, the corresponding dual parameter is shifted, increasing the chance that next time both models will agree. For example, if in the first iteration we predict e6,Bind = 1 but e¯6,Bind = 0, we set λ6,Bind = −α where α is some stepsize (chosen according to Koo et al. (2010)). This will decrease the coefficient for e6,Bind , and increase the coefficient for e¯6,Bind . Hence, we have a higher chance of agreement for this variable in the next iteration. The algorithm repeats the process described above until all variables agree, or some predefined number R of iterations is reached. In the former case we in fact have the exact solution to the original ILP. 1 The ILP representation could be taken from the MLNs of Riedel et al. (2009) and the mapping to ILPs of Riedel (2008). 6 2 2,3 3 3 2,3 2,3 3 3 3 (e, a)← bestOut (λ) ¯)← bestIn (−λ) (¯ e, a (e, a)← bestOut (cout ("
D11-1001,P08-2026,0,0.0132043,"semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t ∈ T . It  creates one active feature for each element in t, t ∈ TReg × feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009)."
D11-1001,P11-1163,0,0.771461,"Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition"
D11-1001,W11-1806,0,0.0295477,"Missing"
D11-1001,E06-1011,0,0.0169088,"ugh ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q . Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforcing consistency between them and the ei,t and ai,j,r variables is not. We address this in section 3.3.1. Reconstruction of events from solutions (e, a, b) can be done almost exactly as described by Björne et al. (2009). However, while they group binding arguments according to ad-hoc rules based on dependency paths from trigger to argument, we simply query the variables bp,q . To simplify our exposition we introduce additional notation. We denote the set of protein head tokens with Prot (x); the set of a possible targets def for"
D11-1001,N09-1018,1,0.791779,"e framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Ea"
D11-1001,W10-1905,0,0.0559999,"in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods in Natural Language"
D11-1001,N10-1123,0,0.0861519,"del jointly predicts triggers and arguments. Notably, the highest scoring event structure under this model can be found efficiently in O (mn) time where m is the number of trigger candidates, and n the number of argument candidates. This is only slightly slower than the O (m0 n) runtime of a pipeline, where m0 is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to 2 train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference he"
D11-1001,C04-1197,0,0.0201207,", 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We als"
D11-1001,W06-1616,1,0.727428,"icates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyaka"
D11-1001,W11-1807,1,0.583242,"d version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 Theme 5 6 7 8 Theme Theme Figure 2: Two binding events with identical trigger. The projection grap"
D11-1001,W09-1406,1,0.586125,"me; this theme can a be protein or, as in our case, another event. Regulations may also have zero or one cause arguments that denote events or proteins which trigger the regulation. In the BioNLP shared task, we are also asked to find a trigger (or clue) token for each event. This token grounds the event in text and allows users to 3 2.1 Event Projection To formulate the search for event structures of the form shown in figure 1a) as an optimization problem, it will be convenient to represent them through a set of binary variables. We introduce such a representation, inspired by previous work (Riedel et al., 2009; Björne et al., 2009) and based on a projection of events to a graph structure over tokens, as seen figure 1b). Consider sentence x and a set of candidate trigger tokens, denoted by Trig (x). We label each candidate i with the event type it is a trigger for, or None if it is not a trigger. This decision is represented through a set of binary variables ei,t , one for each possible event type t. In our example we have e6,Binding = 1. The set of possible event types will be denoted as T , the regulation event types as def {PosReg, NegReg, Reg} and its complement TReg = def T  TReg . as T¬reg ="
D11-1001,W11-1808,1,0.924223,"LP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 Theme 5 6 7 8 Theme Theme Figure 2: Two binding events with identical trigger. The projection graph does not change even if both events are merged. 2 quickly validate extracted events. For examp"
D11-1001,W04-2401,0,0.0933504,"ovide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique a"
D11-1001,D10-1001,0,0.498723,"here m0 is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to 2 train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the"
D11-1001,P05-1073,0,0.0609705,"Missing"
D11-1001,C10-1088,0,\N,Missing
D11-1001,W11-1801,0,\N,Missing
D11-1135,P07-1073,0,0.135341,"Missing"
D11-1135,E09-1018,0,0.0118929,"g features ‘PER-LOC’ and ‘ORG-LOC’ can push the model to split the clusters into two and put the third case into a new cluster. Hence we propose Rel-LDA1. It is similar to Rel-LDA, except that each tuple is represented with more features. Besides p, s, and d, we introduce trigger words, lexical pattern, POS tag pattern, the named entity pair and the syntactic category pair features for each tuple. Lexical pattern is the word sequence between the two arguments of a tuple and POS tag pattern is the POS tag sequence of the lexical pattern. See Table 1 as an example. Following typical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). H"
D11-1135,P11-1054,0,0.0752508,"Missing"
D11-1135,P04-1054,0,0.340081,"Missing"
D11-1135,P05-1045,0,0.0328784,". Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less fr"
D11-1135,N10-1061,1,0.765118,"Missing"
D11-1135,P04-1053,0,0.672056,"Missing"
D11-1135,P10-1150,0,0.0486091,"(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity mention itself and its named"
D11-1135,P09-1113,0,0.0858364,"n different relations. The last cluster shown in the table is a mixture of news companies and government agencies. This may be because this entity cluster is affected by many relations. 4.2 Distant Supervision based Relation Extraction Our generative models detect clusters of dependency paths and their arguments. Such clusters are interesting in their own right, but we claim that they can also be used to help a supervised relation extractor. We validate this hypothesis in the context of relation extraction with distant supervision using predicted clusters as features. Following previous work (Mintz et al., 2009), we use Freebase as our distant supervision source, and align related entity pairs to the New York Times articles discussed earlier. Our training and test instances are pairs of entities for which both arguments appear in at least one sentence together. Features of each instance are extracted from all sentences in which both entities appear together. The gold label for each instance comes from Freebase. If a pair of entities is not related according to Freebase, we consider it a negative example. Note that this tends to create some amount of noise: some pairs may be related, but their relatio"
D11-1135,W04-2407,0,0.0404193,"our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less frequently. Second, the dependency relations in the path should connect two content words, i.e. nouns, verbs, adjectives and adverbs. For example, in phrase ‘solve a problem’, ‘obj(solve, problem)’ is kept, while ‘det(problem, a)’ is discarded. Finally, the dependency labels on the path must not be: ‘conj’, ‘ccomp’, ‘paratax"
D11-1135,P02-1006,0,0.0905542,"Missing"
D11-1135,P10-1044,0,0.0284402,"xample. Following typical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features includ"
D11-1135,C02-1151,0,0.229914,"Missing"
D11-1135,P10-1045,0,0.0267957,"ical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity men"
D11-1135,N03-1033,0,0.00961188,"t some noisy documents, for example, obituary content, lists and so on. Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path"
D11-1135,D10-1099,1,0.399453,"Missing"
D11-1135,P08-1004,0,\N,Missing
D11-1135,D09-1001,0,\N,Missing
D11-1135,P08-1000,0,\N,Missing
D12-1067,W06-2920,0,0.0660709,"And what kind of reduction in runtime does this reduction in edges lead to? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, price and cut (PPC) on the same model. This model contains all constraints presented in 3.2. The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach. We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions. This n"
D12-1067,P05-1022,0,0.0635364,"ss, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation"
D12-1067,D07-1001,0,0.0237009,"Missing"
D12-1067,N07-1011,1,0.832712,"maller integrality gap and higher speed. 1 Introduction Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008)"
D12-1067,N03-1016,0,0.0334835,"ed prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Running Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood as a method to lazily construct a consistent heuristic. In every step this method finds edge"
D12-1067,P10-1001,0,0.313816,"2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 201"
D12-1067,D10-1125,0,0.44312,"dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal"
D12-1067,P09-1039,0,0.244399,"of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo a"
D12-1067,D11-1022,0,0.0338713,"with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in whi"
D12-1067,E06-1011,0,0.641719,"such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prun"
D12-1067,P05-1012,0,0.0682936,"ected edges. Then a directed graph y ⊆ C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Graph-based models define parametrized scoring functions that are trained to discriminate between correct and incorrect parse trees. So called arcfactored or first order models are the most basic variant of such functions: they assess the quality of a tree by scoring each edge in isolation (McDonald et al., 2005b; McDonald et al., 2005a). Formally, arcfactored models are scoring functions of the form X shh,mi (x, w) (1) s (y; x, w) = hh,mi∈y where w is a weight vector and shh,mi (x, w) scores the edge hh, mi with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: X maximize shh,mi y hh,mi∈y (2) subject to y ∈ Y. 2.2 Dependency trees are representations of the syntactic structure of a sentence (Nivre et al., 2007). They determine, for each token of a sentence, the s"
D12-1067,H05-1066,0,0.482353,"Missing"
D12-1067,W06-1616,1,0.90248,"o evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistic"
D12-1067,N10-1117,1,0.820419,"rams. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By us"
D12-1067,W04-2401,0,0.0270605,"Missing"
D12-1067,N12-1054,0,0.0281506,"sed on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of"
D12-1067,D10-1001,0,0.248793,"g can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal incidence vectors, k"
D12-1067,D08-1016,1,0.959552,"(Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all c"
D12-1067,N06-1054,0,0.0303788,"i on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clark"
D12-1067,D07-1096,1,\N,Missing
D12-1074,P11-1048,0,0.0509764,"al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this simple MBR scheme has proved more effective than exact decoding over all variables (Auli and Lopez, 2011). 3 Relation Extraction Performing a syntax-based NLP task in most realworld scenarios requires that the incoming data first be parsed using a pre-trained parsing model. For some tasks, like relation extraction, many data sets lack syntactic annotation and these circumstances persist even into the training phase. In this section we explore such scenarios and contrast the use of parser-provided syntactic annotation to marginalizing over latent representations of constituency or dependency syntax. We show the hidden syntactic models are not just competitive with these “oracle” models, but in som"
D12-1074,P07-1073,0,0.0112204,"ation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of boolean-valued variables representing an undirected dependency between two entities, and an additional set of boolean label variables representing the type label of the relation. O(n2 ) • Let {Rel(i, j : 0 ≤ i &lt; j ≤ n} be boolean variables such that Rel(i, j) = true iff there is a relation spanning i to j. • Let {Rel-Label(i, j, λ) : λ ∈ L, and 0 ≤ i &lt; j ≤ n} be O(|L|n2 ) boolean variables such that Rel-Label(i, j, λ) = true iff there is a relation spanning i to j with re"
D12-1074,N10-1066,0,0.0121246,"formance between their parsing F1 and their SRL F1 were Japanese and German. As illustrated in Fig. 3, the correspondence between syntax and SRL are extremely, and systematically, poor. In this example our hidden structure model was able to assign strong beliefs to the latent syntactic variables which correspond to the correct predicate/argument pairs, allowing it to correctly identify three of the four SRL arguments when the joint model failed to recover one. 5 Related Work This work is perhaps mostly closely related to the Learning over Constrained Latent Representations (LCLR) framework of Chang et al. (2010). Their abstract problem formulation is identical: both paradigms seek to couple the end task to an intermediate representation which is not accessible to the learning algorithm. However much of the intent, scale, and methodology is different. LCLR aims to provide a flexible latent structure for increasing the representational power of the model in a useful way, and is demonstrated on tasks and domains where data availability is not a key concern. In contrast, while our hidden structure models may outperform their observed syntax counterparts, our focus is as much on alleviating the burden of"
D12-1074,P04-1054,0,0.00776923,"ow the hidden syntactic models are not just competitive with these “oracle” models, but in some configurations can actually outperform them. Relation extraction is the task of identifying semantic relations between sets of entities in text (as illustrated in Fig. 1b), and a good proving ground for latent syntactic methods for two reasons. First, because entities share a semantic relationship, under most linguistic analyses these entities will also share some syntactic relation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of b"
D12-1074,N09-1037,0,0.0364923,"tactic in nature introduces its own unique set of problems. Large amounts of syntactically annotated data is difficult to obtain, costly to produce, and often tied to a particular domain that may vary greatly from that of the desired end task. Additionally, current systems often utilize only a small amount of the annotation for any particular task. For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). By instead discovering a latent representation jointly with the end task we address all of these concerns, alleviating the need for any syntactic annotations, while simultaneously attempting to learn a latent syntax relevant to both the particular domain and structure of the end task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random fi"
D12-1074,P96-1024,0,0.0523986,"n as consensus decoding—which has been shown to be NP-hard and without a polynomial time approximation scheme (Sima’an, 1996; 812 Casacuberta and Higuera, 2000). In the NLP community, these inference problems often arise when dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the"
D12-1074,N04-1022,0,0.0153013,"2000). In the NLP community, these inference problems often arise when dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among"
D12-1074,N10-1137,0,0.0154012,"and the connective factors between syntax and SRL. In the observed syntax model the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic variables. In this scenario all labels in (b.) are hidden at both training and test time. feature-rich classifiers to the parsed trees. Related work has recognized the large annotation burden the task demands, but aimed to keep the syntactic annotations and induce semantic roles (Lang and Lapata, 2010). In this section we will take the opposite approach, disregarding the syntactic annotations which we argue are more costly to acquire, as they require more formal linguistic training to produce. 4.1 Model We present a simple, flexible model for SRL in which sense predictions are made independently of the rest of the model, and argument predictions are made independently of each other. The model structure is composed as depicted in Fig. 2. • Let {Arg(i, j) : 0 ≤ i &lt; j ≤ n} be O(n2 ) boolean variables such that Arg(i, j) = true 816 iff predicate i takes token j as an argument. • Let {Role(i, j,"
D12-1074,P09-1067,0,0.0145018,"e. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this simple MBR scheme has proved more effective than exact decoding over all variables (Auli and L"
D12-1074,N06-1045,0,0.0206681,"dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this sim"
D12-1074,H05-1066,0,0.0477086,"voting on the same unknown head word. • Oracle D-Parse, in which we also instantiate a full set of latent dependency syntax variables, and connect them to the baseline model using D-C ONNECT factors. Syntax variables are clamped to their true values. • Oracle C-Parse, the constituency syntax analogue of Oracle D-Parse. • Hidden D-Parse, which is an extension of Oracle D-Parse in which we connect all syntax variables to a DEP-T REE factor, syntax variables are unobserved, and are learned jointly with the end task. The features for latent syntax are a subset of those used in dependency parsing (McDonald et al., 2005). • Hidden C-Parse, the constituency syntax analogue of Hidden D-Parse. The feature set is similar but bigrams are taken over the words defining the constituent span, rather than the words defining the head/modifier relation. Figure 1: Latent Dependency coupling for the RE task. The D-C ONNECT factor expresses ternary connection relations because the shared head word of the proposed relation is unknown. As is convention, variables are represented by circles, factors by rectangles. We introduce six model scenarios. • Baseline, simply the arc-factored model consisting only of Rel and correspondi"
D12-1074,N09-1018,1,0.712589,"nglish, not only is the hidden marginalization method a suitable replacement for the syntactic trees provided by pre-trained, state-of-the-art models, but in both configurations we find that inducing an optimal hidden structure is preferable to the parser-produced annotations. On Chinese, where the data set is atypically small, we still observe improved performance 815 4 Semantic Role Labeling The task of semantic role labeling (SRL) aims to detect and label the semantic relationships between particular words, most commonly verbs (referred to in the domain as predicates), and their arguments (Meza-Ruiz and Riedel, 2009). In a manner similar to RE, there is a strong correlation between the presence of an SRL relation and there existing an underlying syntactic dependency, though this is not always expressed as directly as a 1-to-1 correspondence. This has historically motivated a reliance on syntactic annotation, and some of the most successful methods have simply applied a.) Syntactic Combinatorial Constraint DEP-Tree b.) Syntactic Layer Link 5, 1 Link 5, 2 Link 5, 3 D-Connect 5, 1 D-Connect 5, 2 D-Connect 5, 3 Arg 5, 2 role A1 Link 5, n . D-Connect 5, n Arg 5, 3 At Most 1 At Most 1 role A0 . d.) Sense Predic"
D12-1074,P09-1113,0,0.0117712,"ls are not just competitive with these “oracle” models, but in some configurations can actually outperform them. Relation extraction is the task of identifying semantic relations between sets of entities in text (as illustrated in Fig. 1b), and a good proving ground for latent syntactic methods for two reasons. First, because entities share a semantic relationship, under most linguistic analyses these entities will also share some syntactic relation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of boolean-valued variabl"
D12-1074,C96-2215,0,0.0587693,"Missing"
D12-1074,D08-1016,1,0.822797,"r. 2.1 Latent Dependency Structure Dependency grammar is a lexically-oriented syntactic formalism in which syntactic relationships are expressed as dependencies between individual words. Each non-root word specifies another as its head, provided that the resulting structure forms 811 a valid directed graph, ie. there are no cycles in the graph. Due to the flexibility of this representation it is often used to describe free-word-order languages, and increasingly preferred in NLP for more language-in-use scenarios. A dependency graph can be modeled with the following nodes, as first proposed by Smith and Eisner (2008): • Let {Link(i, j) : 0 ≤ i ≤ j ≤ n, n 6= j} be O(n2 ) boolean variables corresponding to the possible links in a dependency parse. Li,j = true implies that there is a dependency from parent i to child j. • Let {LIN K(i, j) : 0 ≤ i ≤ j ≤ n, n 6= j} be O(n2 ) unary factors, each paired with a corresponding Link(i, j) variable and expressing the independent belief that Link(i, j) = true. 2.2 Latent Constituency Structure Alternatively we can describe the more structured constituency formalism by setting up a representation over span variables: • Let {Span(i, j) : 0 ≤ i &lt; j ≤ n} be O(n2 ) boolean"
D12-1074,W00-1308,0,0.0611931,"29.4 37.4 60.0 32.6 42.2 58.1 31.3 40.7 48.0 32.0 38.4 47.2 30.0 36.7 66.8 37.8 48.3 63.8 37.0 46.8 56.3 32.3 41.0 53.4 31.6 39.7 Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on the smaller Chinese data set. are also tokenized according to Penn Chinese Treebank standards (Xue et al., 2005). The sentences are then tagged and parsed using the Stanford CoreNLP tools, using the standard pre-trained models for tagging (Toutanvoa and Manning, 2000), and the factored parsing model of Klein and Manning (2002). The distributed grammar is trained on a variety of sources, including the standard Wallstreet Journal corpus, but also biomedical, translation, and questions. We then apply entity and relation annotations noisily to the data, collapsing multi-word entities into one term. We filter out sentences with fewer than two entities (and are thus incapable of containing relations) and sentences with more than 40 words (to keep the parses more reliable). This yields 6966 sentences for English data, but unfortunately only 747 sentences for the"
D12-1074,N06-1037,0,0.0110315,"ized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling. 1 Introduction Many NLP tasks are inherently tied to syntax, and state-of-the-art solutions to these tasks often rely on syntactic annotations as either a source for useful features (Zhang et al., 2006, path features in relation extraction) or as a scaffolding upon which a more narrow, specialized classification can occur (as often done in semantic role labeling). This decouWe phrase the joint model as factor graph and marginalize over the hidden structure of the intermediate representation at both training and test time, to optimize performance on the end task. Inference is done via loopy belief propagation, making this framework trivially extensible to most graph structures. Computation over latent syntactic rep810 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural L"
D12-1074,P05-1053,0,0.0127568,"is convention, variables are represented by circles, factors by rectangles. We introduce six model scenarios. • Baseline, simply the arc-factored model consisting only of Rel and corresponding Label variables for each entity. Features on the relation factors, which are common to all model configurations, are combinations of lexical information (i.e., the words that form the entity, the pos-tags of the entities, etc.) as well as the distance between the relation. This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information. • Baseline-Ent, a variant of Baseline with additional features which include combinations of mention type, entity type, and entity sub-type. 814 Coordination factor features for the syntacticallyinformed models are particularly important. This became evident in initial experiments where the baseline was often able to outperform the hidden syntactic model. However, inclusion of entity and mention label features into the connection factors provides the model with grea"
D15-1312,P09-1113,0,0.0657074,"and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score reflecting the accuracy of the claim (absolute percentage error in the example). Claim identification is essentially an instance of information extraction. While it would be possible to develop supervised models, this would require expensive manual data annotation for each property of interest. Instead, we follow the distant supervision paradigm (Craven and Kumlien, 1999; Mintz et al., 2009) using supervision obtained by combining triples from a knowledge base and raw text. However, statistical properties are more challenging in applying the distant supervision assumption than relations between named entities due to the fact that the numerical values are often approximated in text, as in the example of Figure 1. Consequently, linking the values mentioned in text with those in the knowledge base is not trivial and thus it is not straightforward to generate training instances for the property of interest. 2596 Proceedings of the 2015 Conference on Empirical Methods in Natural Langu"
D15-1312,P14-1095,0,0.0863033,"erty, even inaccurate ones; in information extraction on the other hand, and especially its formulation as knowledge base population, we are interested in the accurate claims only, since extracting inaccurate ones will lead to erroneous information added to the knowledge base. The difference between the two tasks is captured by the verification task. In this paper our main goals are identification and verification, but we train our approach on information extraction, relying on the assumption that most claims made in the texts retrieved via the web search engine are accurate. In related work, Nakashole and Mitchell (2014) 2 Accessed in August 2015. developed an approach to verify subject-verbobject triples against a knowledge base, taking into account the objectivity of the language used in the sources stating the triple. Our approach is agnostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguistic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. ("
D15-1312,W14-2508,1,0.833679,"ples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more complex than the ones we considered in this paper. Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model, Hoffmann et al. (2010) take advantage of the structure of the articles in Wikipedia developing a classifier that identifies the schema followed by each article, which is not straightfor"
D15-1312,P10-1030,0,0.104234,"Missing"
D15-1312,P14-5010,0,0.00558418,"aving values for 150-175 regions (mostly countries). To collect texts from which the text patterns between entities and numerical values will be extracted we downloaded documents from the web. In particular, for each region combined with each property we formed a query consisting of the two and submitted it to Bing via its Search API. Following this we obtained the top 50 results for each query, downloaded the HTML pages corresponding to each result and extracted their textual content with BoilerPipe (Kohlsch¨utter et al., 2010). We then processed the texts using the Stanford CoreNLP toolkit (Manning et al., 2014) and from each sentence we extracted textual patterns between all the named entities recognized as locations and all the numerical values. Two kinds of patterns were extracted for each location and numerical value: surface patterns (as the ones shown in Table 1) and lexicalized dependency paths. This pattern extraction process resulted in a large set of triples consisting of a region, a pattern and a value. Different sentences might result in triples containing the same region and textual pattern but different value. Such variation can arise due to either the approximations of values in text o"
D15-1312,N15-1066,0,\N,Missing
D16-1101,D11-1010,0,0.0462723,"udio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction, however they neither ground in numeric quantities nor incorporate background KBs. 991 6 Conclusion In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge"
D16-1101,P08-1118,0,0.104465,"Missing"
D16-1101,W14-1702,0,0.014453,"models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction,"
D16-1101,P08-1015,0,0.500252,"all (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), a"
D16-1101,D15-1293,0,0.0348071,"ision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), col"
D16-1101,P15-2038,0,0.049027,"of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So fa"
D16-1101,Q15-1008,0,0.0310714,"d. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective"
D16-1101,P14-1068,0,0.0613459,"Best results in bold. average precision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choice"
D16-1101,Q14-1017,0,0.0281215,"), recall (R) and F1. Best results in bold. average precision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at"
D16-1101,W11-1301,0,0.0319503,"eption (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction, however they neither ground in numeric quantities nor incorporate background KBs. 991 6 Conclusion In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge bases. We found that the proposed techniques lead to performance improvements in the tasks of language modelling,"
D16-1146,W15-4002,0,0.0251247,"d from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime. 1 Combining neural methods with symbolic commonsense knowledge, for instance in the form of implication rules, is in the focus of current research (Rockt¨aschel et al., 2014; Wang et al., 2014; Bowman et al., 2015; Wang et al., 2015; Vendrov et al., 2016; Hu et al., 2016; Rockt¨aschel and Riedel, 2016; Cohen, 2016). A recent approach (Rockt¨aschel et al., 2015) regularizes entity-tuple and relation embeddings via first-order logic rules. To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database."
D16-1146,D14-1165,0,0.0307656,"der logic rules. To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database. Introduction Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above,"
D16-1146,W16-1314,1,0.86898,"Missing"
D16-1146,N15-1184,0,0.0381935,"Missing"
D16-1146,N13-1092,0,0.0315587,"Missing"
D16-1146,Q15-1027,0,0.0585324,"the relation embeddings, where ≤ denotes the component-wise comparison. In fact, a sufficient condition for eq. (5) to hold, is with 0 the k-dimensional null vector. This corresponds to a single relation-specific loss term, and the general restriction T ⊆ Rk,+ on the tupleembedding space. 3.3 Approximately Boolean Entity Tuples In order to impose implications by minimizing a lifted loss LU I , the tuple-embedding space needs to be restricted to Rk,+ . We have chosen to restrict the tuple space even more than required, namely to the hypercube t ∈ [0, 1]k , as approximately Boolean embeddings (Kruszewski et al., 2015). The tuple embeddings are constructed from real-valued vectors e, using the component-wise sigmoid function e ∈ Rk . t = σ(e), (12) For minimizing the loss, the gradients are hence computed with respect to e, and the L2 regularization is applied to the components of e instead of t. Other choices for ensuring the restriction t ≥ 0 in eq. (11) are possible, but we found that our approach works better in practice than those (e.g., the exponential transformation proposed by Demeester et al. (2016)). It can also be observed that the unit tuples over which the implication loss is grounded, form a s"
D16-1146,P15-1016,0,0.0253344,"this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database. Introduction Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above, if we require that every c"
D16-1146,N13-1008,1,0.316939,"tuple and relation embeddings via first-order logic rules. To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database. Introduction Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embe"
D16-1146,W16-1309,1,0.599322,"Missing"
D16-1146,W14-2409,1,0.801798,"Missing"
D16-1146,N15-1118,1,0.623392,"Missing"
D16-1146,D15-1174,0,0.0156304,"r rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database. Introduction Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above, if we require that every component of the 1389 Pro"
D16-1146,W16-1312,0,0.0440698,"Missing"
D16-1146,N16-1103,0,0.0143327,"entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database. Introduction Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above, if we require that every component of the 1389 Proceedings of the 2016 Conference on Empiri"
D16-1167,D14-1179,0,0.0214089,"Missing"
D16-1167,P16-1004,0,0.0165459,"ble y ∈ Y representing the output. The 1609 goal is to find this predictive distribution by learning it from examples D := {(xi , yi )}ni=1 . Building on the current success in the application of deep learning to NLP, we assume that there exists a good model family {fθ , θ ∈ Θ} to predict y given x, where θ is an element of the parameter space Θ. For example, the stacked LSTM encoder-decoder is a general purpose model that has helped to improve results on relatively complex tasks, such as machine translation (Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2014), semantic parsing (Dong and Lapata, 2016) and textual entailment (Rockt¨aschel et al., 2016). For many applications, the amount of training data is too small or too costly to acquire. We hence look for alternative ways to regularize the model so that we can achieve good performance using few data points. Let pθ (y|x) be the target prediction model. Given the training dataset D, the penalized maximum likelihood estimator is obtained by minθ∈Θ L(θ) where: L(θ) := `(θ) + λΩ(θ) . (1) P where `(θ) := − n1 ni=1 log pθ (yi |xi ) = EPˆ [log pθ (y|x)] is the negative log-likelihood. Here, Ω(θ) is a regularizer that prevents over-fitting, λ ∈"
D16-1167,W09-0613,0,0.0414668,"r that can be set by cross-validation, and Pˆ is the empirical distribution. Instead of using a standard regularizer Ω – such as the squared norm or the Lasso penalty which are domain-agnostic, – in this paper we propose to use a generative model to regularize the estimator. Domain knowledge A natural way to inject background knowledge is to define a generative model that simulates the way the data is generated. In text understanding applications, such generative models are common and include probabilistic contextfree grammars (PCFG) and natural language generation frameworks (e.g. SimpleNLG (Gatt and Reiter, 2009)). Let Pγ (x, y) be such a generative model parametrized by a continuous parameter vector γ ∈ Γ, such as the concatenation of all the parameters of the production rules in a PCFG. One important difference between the discriminative and the generative probability distributions is that the inference problem of y given x might be intractable1 for the generative model, even if the joint model can be computed efficiently. In this work, we use the following regularizer:    Pγ (y|x) Ω(θ) := min EPγ (x,y) log .(2) γ∈Γ pθ (y|x) This regularizer makes intuitive sense as it corresponds to the smalles"
D16-1167,D14-1058,0,0.0324269,"n are Recurrent Neural Nets (RNN) with nonlinear transitions between states. Treated as a translation problem, math word problem solving should be simpler than developing a machine translation model between two human languages, as the output vocabulary (the math symbols) is significantly smaller than any human vocabulary. However, machine translation can be learned on millions of pairs of already translated sentences, and such massive training datasets dwarf all previously introduced math exam datasets. We used standard benchmark data from the literature. The first one, AI2, was introduced by Hosseini et al. (2014) and covers addition and subtraction of one or two variables or two additions scraped from two web pages. The second (IL), introduced by Roy et al. (2015), contains single operator questions but covers addition, subtraction, multiplication, and division, and was also obtained from two, although different from AI2, web pages. The last data set (CC) was introduced by Roy and Roth (2015) to cover combinations of different operators and was obtained from a fifth web page. An overview of the equation patterns in the data is shown in Table 1. It should be noted that there are sometimes numbers menti"
D16-1167,D15-1202,0,0.0455588,"rom her older sister. If she only ate 9 pieces a day, how long would the candy last her? The answer is given by the following equation: X = (66 + 15)/9 . Note that similarly to real world school exams, giving the final answer of (9 in this case) is not considered enough for the response to be correct. The only publicly available word problem datasets we are aware of contain between 400 and 600 problems (see Table 2), which is not enough to properly train sufficiently rich models that capture the link between the words and the quantities involved in the problem. 5 From the Common Core dataset (Roy and Roth, 2015) Figure 1: Test loss vs. fraction of real data used in G ENE R E on the text-to-equation experiment. Sequence-to-sequence learning is the task of predicting an output sequence of symbols based on a sequence of input symbols. It is tempting to cast the problem of answering math exams as a sequenceto-sequence problem: given the sequence of words from the problem description, we can predict the sequence of symbols for the equation as output. Currently, the most successful models for sequence prediction are Recurrent Neural Nets (RNN) with nonlinear transitions between states. Treated as a transla"
D16-1167,Q15-1001,0,0.0173547,"developing a machine translation model between two human languages, as the output vocabulary (the math symbols) is significantly smaller than any human vocabulary. However, machine translation can be learned on millions of pairs of already translated sentences, and such massive training datasets dwarf all previously introduced math exam datasets. We used standard benchmark data from the literature. The first one, AI2, was introduced by Hosseini et al. (2014) and covers addition and subtraction of one or two variables or two additions scraped from two web pages. The second (IL), introduced by Roy et al. (2015), contains single operator questions but covers addition, subtraction, multiplication, and division, and was also obtained from two, although different from AI2, web pages. The last data set (CC) was introduced by Roy and Roth (2015) to cover combinations of different operators and was obtained from a fifth web page. An overview of the equation patterns in the data is shown in Table 1. It should be noted that there are sometimes numbers mentioned in the problem 1613 AI2 X+Y X+Y+Z X−Y IL X+Y X−Y X∗Y X/Y CC X+Y−Z X ∗ (Y + Z) X ∗ (Y − Z) (X + Y)/Z (X − Y)/Z Table 1: Patterns of the equations seen"
D18-1233,J08-4004,0,0.0326963,"am ... Do I need to...?”). By controlling the answers of the virtual user, we control the ratio of “Yes” and “No” answers. And by showing only subsets of the dialog to the annotator that produces the scenario, we can control what the scenario is capturing. The question, rule text and dialogs are then used to produce utterances of the kind we see in Figure 1. Annotators show substantial agreement when constructing dialogs with a three-way annotator agreement at a Fleiss’ Kappa level of 0.71.1 Likewise, we find that 1 This is well within the range of what is considered as substantial agreement (Artstein and Poesio, 2008). our crowd-annotators produce questions that are coherent with the given dialogs with high accuracy. In theory, the task could be addressed by an endto-end neural network that encodes the question, history and previous dialog, and then decodes a Yes/No answer or question. In practice, we test this hypothesis using a seq2seq model (Sutskever et al., 2014; Cho et al., 2014), with and without copy mechanisms (Gu et al., 2016) to reflect how follow-up questions often use lexical content from the rule text. We find that despite a training set size of 21,890 training utterances, successful models f"
D18-1233,D18-1241,0,0.224987,"questions are often needed. The domain of text we consider is also different (regulatory vs Wikipedia, books, newswire). Dialog The task we propose is, at its heart, about conducting a dialog (Weizenbaum, 1966; Serban et al., 2018; Bordes and Weston, 2016). Within this scope, our work is closest to work in dialogbased QA where complex information needs are addressed using a series of questions. In this space, previous approaches have been looking primarily at QA dialogs about images (Das et al., 2017) and knowledge graphs (Saha et al., 2018; Iyyer et al., 2017). In parallel to our work, both Choi et al. (2018) and Reddy et al. (2018) have to began to investigate QA dialogs with background text. Our work not only differs in the domain covered (regulatory text vs wikipedia), but also in the fact that our task requires the interpretation of complex rules, application of background knowledge, and the formulation of free-form clarification questions. Rao and Daume III (2018) does investigate how to generate clarification questions but this does not require the understanding of explicit natural language rules. Rule Extraction From Text There is a long line of work in the automatic extraction of rules fro"
D18-1233,P16-1154,0,0.0426037,"-way annotator agreement at a Fleiss’ Kappa level of 0.71.1 Likewise, we find that 1 This is well within the range of what is considered as substantial agreement (Artstein and Poesio, 2008). our crowd-annotators produce questions that are coherent with the given dialogs with high accuracy. In theory, the task could be addressed by an endto-end neural network that encodes the question, history and previous dialog, and then decodes a Yes/No answer or question. In practice, we test this hypothesis using a seq2seq model (Sutskever et al., 2014; Cho et al., 2014), with and without copy mechanisms (Gu et al., 2016) to reflect how follow-up questions often use lexical content from the rule text. We find that despite a training set size of 21,890 training utterances, successful models for this task need a stronger inductive bias due to the inherent challenges of the task: interpreting natural language rules, generating questions, and reasoning with background knowledge. We develop heuristics that can work better in terms of identifying what questions to ask, but they still fail to interpret scenarios correctly. To further motivate the task, we also show in oracle experiments that a CMR system can help hum"
D18-1233,P17-1167,0,0.114263,"Missing"
D18-1233,P17-1147,0,0.0647331,"rovement whenever background knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on paying National Insuran"
D18-1233,K17-1034,0,0.0553754,"Missing"
D18-1233,P16-1170,0,0.0276181,"the general problem of such approaches: they require careful ontology building, layers of error-prone linguistic preprocessing, and are difficult for non-experts to create annotations for. Question Generation Our task involves the automatic generation of natural language questions. Previous work in question generation has focussed on producing questions for a given text, such that the questions can be answered using this text (Vanderwende, 2008; M. Olney et al., 2012; Rus et al., 2011). In our case, the questions to generate are derived from the background text but cannot be answered by them. Mostafazadeh et al. (2016) investigate how to generate natural follow-up questions based on the content of an image. Besides not working in a visual context, our task is also different because we see question generation as a sub-task of question answering. 7 Conclusion In this paper we present a new task as well as an annotation protocol, a dataset, and a set of baselines. The task is challenging and requires models to generate language, copy tokens, and make logical inferences. Through the use of an interactive and dialog-based annotation interface, we achieve good agreement rates at a low cost. Initial baseline resul"
D18-1233,D16-1264,0,0.62132,"substantial room for improvement whenever background knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on pay"
D18-1233,P18-1255,0,0.0276787,"Missing"
D18-1233,W11-2853,0,0.0272237,", Delisle et al. (1994) maps text to horn clauses. This can be very effective, and good results are reported, but suffers from the general problem of such approaches: they require careful ontology building, layers of error-prone linguistic preprocessing, and are difficult for non-experts to create annotations for. Question Generation Our task involves the automatic generation of natural language questions. Previous work in question generation has focussed on producing questions for a given text, such that the questions can be answered using this text (Vanderwende, 2008; M. Olney et al., 2012; Rus et al., 2011). In our case, the questions to generate are derived from the background text but cannot be answered by them. Mostafazadeh et al. (2016) investigate how to generate natural follow-up questions based on the content of an image. Besides not working in a visual context, our task is also different because we see question generation as a sub-task of question answering. 7 Conclusion In this paper we present a new task as well as an annotation protocol, a dataset, and a set of baselines. The task is challenging and requires models to generate language, copy tokens, and make logical inferences. Throug"
D18-1233,D08-1027,0,0.288651,"Missing"
D18-1233,D16-1244,0,0.119236,"Missing"
D18-1233,N18-1202,0,0.00970419,"le 4: Results of entailment models on ShARC. is understood, certain follow-up questions can be skipped because they are answered within the scenario. In this section, we investigate how difficult scenario interpretation is by training models to answer follow-up questions based on scenarios. Baselines We use a random baseline and also implement a surface logistic regression applied to a TFIDF representation of the combined scenario and the question. For neural models, we use Decomposed Attention Model (DAM) (Parikh et al., 2016) trained on each the SNLI and ShARC corpora using ELMO embeddings (Peters et al., 2018).4 eration model is used to produce a follow-up question, f1 . The rule text and produced follow-up question are then passed as inputs to the Scenario Interpretation model. If the output of this is I RRELEVANT, then the CM predicts f1 , otherwise, these steps are repeated recursively until the classification model no longer predicts M ORE or the entailment model predicts I RRELEVANT, in which case the model produces a final answer. We also investigate an extension of the NMT-copy model on the end-to-end task. Input sequences are encoded as a concatenation of the rule text, question, scenario a"
D18-1233,Q18-1021,1,0.838208,"ckground knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on paying National Insurance if you’re working"
D18-1541,D17-1151,0,0.0364251,"In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4977–4983 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ware such as a phrase-table generator, decoder, and part-of-speech tagger. We created multiple synthetic datasets from in-domain and outof-domain sources, and found that stochastic token sampling, and pruning redundant and lowlikelihood sentences, were helpful in generating meaningful corruptions. Using the artificial samples thus generated, we imp"
D18-1541,P06-1032,0,0.161866,"ictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Give"
D18-1541,E14-3013,0,0.0949662,"idence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4977–4983 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ware such as a phrase-table generator, decoder, and part-of-speech tagger. We created mult"
D18-1541,W09-2112,0,0.222768,"Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Given an input sequence, the encoder generates context vectors for each token. Then, the attention mechanism and the decoder work in tandem to emit a distribution over the target vocabulary. At"
D18-1541,P12-2076,0,0.123753,"ng and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Given an input sequence, the encoder generates context vectors for each token. Then, the attention mechanism and the decoder work in tandem to emit a distribution over the target vocabulary. At every decoder timestep, the encoder context vec"
D18-1541,P15-1162,0,0.091247,"Missing"
D18-1541,W14-1703,0,0.0497629,"ifficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Lang"
D18-1541,D16-1161,0,0.123137,"Missing"
D18-1541,1994.amta-1.18,0,0.0393408,"(2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen,"
D18-1541,N06-1020,0,0.019391,"ty annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et al., 2014) could be used for this purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead lea"
D18-1541,W17-5039,0,0.0279333,"their learning is a crucial, labour-intensive endeavour. Part of this process is identifying and correcting grammatical errors, and several computational techniques have been developed to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et"
D18-1541,W14-1701,0,0.0923693,"it is not aligned to the last word of the source sentence, then ‘i’, as a human would realise that this sentence ends abruptly, Else, ‘c’. These token-labelled corrupted sentences now form an artificial dataset for training an error detector. Duplicate instances and corrupted sentences with more than 5 errors were dropped to remove noise from the downstream training. 4 Experiments We evaluated our approach on the First Certificate of English (FCE) error detection dataset (Rei and Yannakoudakis, 2016), as well as on two humanannotated test sets (CoNLL1, CoNLL2) from the CoNLL 2014 shared task (Ng et al., 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our NMT with a single-layered encoder and decoder with cell size 256, on the parallel corpus version of FCE (Yannako"
D18-1541,P17-1194,0,0.0939786,"ation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Rec"
D18-1541,C16-1030,0,0.113597,"Missing"
D18-1541,W17-5032,0,0.240647,"veloped to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et al., 2014) could be used for this purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt sel"
D18-1541,P16-1112,0,0.122199,"at this point a human reader would notice that there is a word missing in the sentence. Else, if it is the last word, but it is not aligned to the last word of the source sentence, then ‘i’, as a human would realise that this sentence ends abruptly, Else, ‘c’. These token-labelled corrupted sentences now form an artificial dataset for training an error detector. Duplicate instances and corrupted sentences with more than 5 errors were dropped to remove noise from the downstream training. 4 Experiments We evaluated our approach on the First Certificate of English (FCE) error detection dataset (Rei and Yannakoudakis, 2016), as well as on two humanannotated test sets (CoNLL1, CoNLL2) from the CoNLL 2014 shared task (Ng et al., 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our N"
D18-1541,A88-1027,0,0.0605404,"belled instances output by existing state-of-theart parsers as ground-truth labels, and improved syntactic parsing performance. Sennrich et al. (2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leve"
D18-1541,P11-1019,0,0.249998,", 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our NMT with a single-layered encoder and decoder with cell size 256, on the parallel corpus version of FCE (Yannakoudakis et al., 2011), with early stopping after the FCE development set score dropped consistently for 20 epochs. We introduced errors into three datasets: FCE itself (450K tokens), the English Vocabulary Profile or EVP (270K tokens) and a subset of Simple Wikipedia or SW (8.4M tokens); of these, FCE and EVP were both used in artificial error generation via SMT and pattern extraction (PAT) by Rei et al. (2017), enabling us to make a fair experimental comparison. Ten corrupted versions using each of AM, TS (τ = 0.05) and BS were sampled for FCE and EVP corruptions, while one sufficed for SW. The theoretical time c"
D18-1541,D17-1297,0,0.135977,"g them with quick feedback to facilitate their learning is a crucial, labour-intensive endeavour. Part of this process is identifying and correcting grammatical errors, and several computational techniques have been developed to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated."
D18-1541,N16-1042,0,0.0585727,"ollection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 49"
D18-1541,W13-3607,0,0.45933,"purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Confere"
D18-1541,Q14-1033,0,0.0611825,"Missing"
D18-1541,P16-1009,0,0.0570945,"computer vision, images are blurred, rotated, or otherwise deformed inexpensively to create new training instances (Wang and Perez, 2017), because such manipulation does not significantly alter the image semantics. Similar coarse processes do not work in NLP since mutating even a single letter or a word can change a sentence’s meaning, or render it nonsensical. Nonetheless, Vinyals et al. (2015) employed a kind of selftraining where they use noisy predictions for unlabelled instances output by existing state-of-theart parsers as ground-truth labels, and improved syntactic parsing performance. Sennrich et al. (2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight"
D18-1541,D16-1167,1,\N,Missing
D19-1250,P14-1023,0,0.112167,"ven sequence of words. 3 Related Work Many studies have investigated pretrained word representations, sentence representations, and language models. Existing work focuses on understanding linguistic and semantic properties of word representations or how well pretrained sentence representations and language models transfer linguistic knowledge to downstream tasks. In contrast, our investigation seeks to answer to what extent pretrained language models store factual and commonsense knowledge by comparing them with symbolic knowledge bases populated by traditional relation extraction approaches. Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs. Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset cons"
D19-1250,P11-1028,0,0.0277475,"Missing"
D19-1250,P17-1171,0,0.0321382,"tity linking is implemented: REn makes use of a na¨ıve entity linking solution based on exact string matching, while REo uses an oracle for entity linking in addition to string matching. In other words, assume we query for the object o of a test subjectrelation fact (s, r, o) expressed in a sentence x. If RE has extracted any triple (s0 , r, o0 ) from that sen2467 tence x, s0 will be linked to s and o0 to o. In practice, this means RE can return the correct solution o if any relation instance of the right type was extracted from x, regardless of whether it has a wrong subject or object. DrQA: Chen et al. (2017) introduce DrQA, a popular system for open-domain question answering. DrQA predicts answers to natural language questions using a two step pipeline. First, a TF/IDF information retrieval step is used to find relevant articles from a large store of documents (e.g. Wikipedia). On the retrieved top k articles, a neural reading comprehension model then extracts answers. To avoid giving the language models a competitive advantage, we constrain the predictions of DrQA to single-token answers. 4.4 Metrics We consider rank-based metrics and compute results per relation along with mean values across al"
D19-1250,P19-1285,0,0.0275681,"1 , . . . , w1 ) = softmax(Wht + b) (2) where ht ∈ Rk is the output vector of a neural network at position t and W ∈ R|V |× k is a learned parameter matrix that maps ht to unnormalized scores for every word in the vocabulary V. Various neural language models then mainly differ in how they compute ht given the word history, e.g., by using a multi-layer perceptron (Bengio et al., 2003; Mikolov and Zweig, 2012), convolutional layers (Dauphin et al., 2017), recurrent neural networks (Zaremba et al., 2014; Merity et al., 2016; Melis et al., 2017) or self-attention mechanisms (Radford et al., 2018; Dai et al., 2019; Radford et al., 2019). fairseq-fconv: Instead of commonly used recurrent neural networks, Dauphin et al. (2017) use multiple layers of gated convolutions. We use the pretrained model in the fairseq1 library in our study. It has been trained on the WikiText-103 corpus introduced by Merity et al. (2016). 2464 1 https://github.com/pytorch/fairseq Model Base Model #Parameters Training Corpus Corpus Size fairseq-fconv (Dauphin et al., 2017) Transformer-XL (large) (Dai et al., 2019) ConvNet Transformer 324M 257M WikiText-103 WikiText-103 103M Words 103M Words ELMo (original) (Peters et al., 2018a)"
D19-1250,L18-1544,0,0.084271,"Missing"
D19-1250,J15-4004,0,0.0369995,"investigation seeks to answer to what extent pretrained language models store factual and commonsense knowledge by comparing them with symbolic knowledge bases populated by traditional relation extraction approaches. Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs. Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well. Another line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et"
D19-1250,P82-1020,0,0.811197,"Missing"
D19-1250,Q19-1026,0,0.0666692,"Missing"
D19-1250,D18-1151,0,0.0374015,"mbolic knowledge bases populated by traditional relation extraction approaches. Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs. Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well. Another line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et al., 2018). While such an analysis sheds light on the transfer-learning 2465 abilities of pretrained models for understanding short pieces of t"
D19-1250,P19-1334,0,0.0664968,"Missing"
D19-1250,N18-1202,0,0.59388,"than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA. 1 Query KB Symbolic KB Memory Access Dante Florence born-in Florence “Dante was born in [Mask].” Neural LM Memory Access LM Florence e.g. ELMo/BERT Figure 1: Querying knowledge bases (KB) and language models (LM) for factual knowledge. vast amounts of linguistic knowledge (Peters et al., 2018b; Goldberg, 2019; Tenney et al., 2019) useful for downstream tasks. This knowledge is usually accessed either by conditioning on latent context representations produced by the original model or by using the original model weights to initialize a task-specific model which is then further fine-tuned. This type of knowledge transfer is crucial for current state-of-the-art results on a wide range of tasks. Introduction Recently, pretrained high-capacity language models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018a) have become increasingly important in NLP. They are optimised"
D19-1250,D18-1179,0,0.282527,"than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA. 1 Query KB Symbolic KB Memory Access Dante Florence born-in Florence “Dante was born in [Mask].” Neural LM Memory Access LM Florence e.g. ELMo/BERT Figure 1: Querying knowledge bases (KB) and language models (LM) for factual knowledge. vast amounts of linguistic knowledge (Peters et al., 2018b; Goldberg, 2019; Tenney et al., 2019) useful for downstream tasks. This knowledge is usually accessed either by conditioning on latent context representations produced by the original model or by using the original model weights to initialize a task-specific model which is then further fine-tuned. This type of knowledge transfer is crucial for current state-of-the-art results on a wide range of tasks. Introduction Recently, pretrained high-capacity language models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018a) have become increasingly important in NLP. They are optimised"
D19-1250,D16-1264,0,0.0877565,"ense (OMCS) sentences. OMCS represents commonsense relationships between words and/or phrases. We consider facts from the English part of ConceptNet that have single-token objects covering 16 relations. For these ConceptNet triples, we find the OMCS sentence that contains both the subject and the object. We then mask the object within the sentence and use the sentence as template for querying language models. If there are several sentences for a triple, we pick one at random. Note that for this knowledge source there is no explicit alignment of facts to Wikipedia sentences. 4.1.4 SQuAD SQuAD (Rajpurkar et al., 2016) is a popular question answering dataset. We select a subset of 305 context-insensitive questions from the SQuAD development set with single token answers. We manually create cloze-style questions from these questions, e.g., rewriting “Who developed the theory of relativity?” as “The theory of relativity was developed by ”. For each question and answer pair, we know that the corresponding fact is expressed in Wikipedia since this is how SQuAD was created. 4.2 Models We consider the following pretrained casesensitive language models in our study (see Table 1): fairseq-fconv (Fs), Transformer-XL"
D19-1250,D17-1188,0,0.0267598,"models (∼21K case-sensitive tokens). 4.3 Baselines To compare language models to canonical ways of using off-the-shelf systems for extracting symbolic knowledge and answering questions, we consider the following baselines. Freq: For a subject and relation pair, this baseline ranks words based on how frequently they appear as objects for the given relation in the test data. It indicates the upper bound performance of a model that always predicts the same objects for a particular relation. RE: For the relation-based knowledge sources, we consider the pretrained Relation Extraction (RE) model of Sorokin and Gurevych (2017). This model was trained on a subcorpus of Wikipedia annotated with Wikidata relations. It extracts relation triples from a given sentence using an LSTMbased encoder and an attention mechanism. Based on the alignment information from the knowledge sources, we provide the relation extractor with the sentences known to express the test facts. Using these datasets, RE constructs a knowledge graph of triples. At test time, we query this graph by finding the subject entity and then rank all objects in the correct relation based on the confidence scores returned by RE. We consider two versions of th"
D19-1250,W18-5446,0,0.0390067,"(2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs. Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well. Another line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et al., 2018). While such an analysis sheds light on the transfer-learning 2465 abilities of pretrained models for understanding short pieces of text, it provides little insight into whether these models can compete with traditional approaches to representing knowledge like symbolic knowledge bases. More recently, McCoy et al. (2019) found that for natural language inference, a model based on BERT learns to rely heavily on fallible syntactic heuristics instead of a deeper understanding of the natural language input. Peters et al. (2018b) found that lower layers in ELMo specialize on local syntactic relatio"
D19-1250,W19-3620,0,0.036004,"Missing"
D19-1250,speer-havasi-2012-representing,0,0.0813676,"Missing"
D19-1250,N19-1421,0,0.103218,"Missing"
E17-1119,J92-4003,0,0.322738,"development data. 3.1 Sparse Feature Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly avai"
E17-1119,D15-1103,0,0.462101,"eir respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-graine"
E17-1119,P16-1059,0,0.0215587,"– into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and"
E17-1119,Q15-1023,0,0.0194509,"n comparable set of features. What we find is that the performance drop is very dramatic, 9.85 points of loose micro score. Given that the training data for the previously introduced model is not publicly available, we hesi1277 (a) Figure 3: PCA projections of the label embeddings learnt from the OntoNotes dataset where subtypes share the same color as their parent type. Sub-figure (a) uses the non-hierarchical encoding, while sub-figure (b) uses the hierarchical encoding. tate to speculate as to exactly why this drop is so dramatic, but similar observations have been made for entity linking (Ling et al., 2015). This clearly underlines how essential it is to compare models on an equal footing using the same training data. PCA visualisation of label embeddings By visualising the learnt label embeddings (Figure 3) and comparing the non-hierarchical and hierarchical label encodings, we can observe that the hierarchical encoding forms clear distinct clusters. 4.7 Parent Before After Frequent Words /location /organization /art/film /music /award /event 0.319 0.324 0.207 0.259 0.583 0.310 0.228 0.178 0.429 0.116 0.292 0.188 0.070 0.119 0.021 0.018 0.083 0.089 in, at, born at, the, by film, films, in album"
E17-1119,P09-1113,0,0.0916087,"on. Their end goal was to use the resulting types in a question answering system and they developed a conditional random field model that they trained and evaluated on a manually annotated Korean dataset to detect and classify entity mentions. Other early work include Sekine (2008), that emphasised the need for having access to a large set of entity types for several NLP applications. The work primarily discussed design issues for fine-grained set of entity types and served as a basis for much of the future work on fine-grained entity classification. The first work to use distant supervision (Mintz et al., 2009) to induce a large – but noisy – training set and manually label a significantly smaller dataset to evaluate their fine-grained entity classification system, was Ling and Weld (2012) who introduced both a training and evaluation dataset F IGER (GOLD). Arguing that fine-grained sets of types must be organised in a very fine-grained hierarchical taxonomy, Yosef et al. (2012) introduced such a taxonomy covering 505 distinct types. This new set of types lead to improvements on F IGER (GOLD), and they also demonstrated that the fine-grained labels could be used as features to improve coarse-grained"
E17-1119,D14-1162,0,0.10334,"73.94 Attentive Attentive + Hand-crafted 54.53 59.68 74.76 78.97 71.58 75.36 F IGER (Ling and Weld, 2012) F IGER (Ren et al., 2016) 52.30 47.4 69.90 69.2 69.30 65.5 Table 3: Performance on F IGER (GOLD) for models using the same W2M training data. Pre-trained Word Embeddings We use pre-trained word embeddings that were not updated during training to help the model generalise to words not appearing in the training set (Rockt¨aschel et al., 2015). For this purpose, we used the freely available 300-dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For words not present in the pretrained word embeddings, we use the embedding of the “unk” token. Model Model Data Acc. Macro Micro Attentive + Hand-crafted Attentive (Shimaoka et al., 2016) W2M W2.6M 59.68 58.97 78.97 77.96 75.36 74.94 F IGER + PLE (Ren et al., 2016) HYENA + PLE (Ren et al., 2016) W2M+D W2M+D 59.9 54.2 76.3 69.5 74.9 68.1 K-WASABIE (Yogatama et al., 2015) GN2 n/a n/a 72.25 Table 4: Performance on F IGER (GOLD) for models using different training data. used dropout (Hinton et al., 2012) with probability 0.5 applied to the mention representation and sparse feature representat"
E17-1119,N13-1071,0,0.0177729,"Missing"
E17-1119,sekine-2008-extended,0,0.0444578,"match series against New Zealand is held on Monday”. 1 Introduction Entity type classification aims to label entity mentions in their context with their respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction ("
E17-1119,W16-1313,1,0.661596,"fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they compl"
E17-1119,P08-1086,0,0.0123156,"e Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly available. corresponding word embeddings. Those"
E17-1119,N16-1174,0,0.00639303,"es (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they complement each other. Ad"
E17-1119,P15-2048,0,0.59274,"xt-dependent fine-grained entity type classification where the types of a mention is constrained to what can be deduced from its context and introduced a new OntoNotes-derived manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Building upon the noise reduction aspects of this work, Ren et al. (2016) introduced a method to reduce label noise even further, leading to significant performance gains on both the evaluation dataset of Ling and Weld (2012) and Gillick et al. (2014). Yogatama et al. (2015) proposed to map handcrafted features and labels to embeddings in or1272 der to facilitate information sharing between both related types and features. A pure feature learning approach was proposed by Dong et al. (2015). They defined 22 types and used a two-part neural classifier that used a recurrent neural network to obtain a vector representation of each entity mention and in its second part used a fixed-size window to capture the context of a mention. A recent workshop paper (Shimaoka et al., 2016) introduced an attentive neural model that unlike previous work obtained vector representatio"
E17-1119,C12-2133,0,0.11954,"Missing"
E17-2064,E12-1004,0,0.609139,"ty of embeddings to encode hypernymy, previous work has proposed supervised models to learn whether a given pair of embeddings (wi , wj ) are in the hypernymy relation (Roller et al., 2014; Necsulescu et al., 2015; Fu et al., 2014). Results from previous work suggest that word embeddings indeed capture hypernymy information. This observation is relatively general and robust across several choices of datasets, models and embeddings. For example, Levy et al. (2015) achieve up to 0.85 F1, while Roller and Erk (2016) achieve up to 0.90 F1. Both of these results are achieved on the Baroni dataset (Baroni et al., 2012). For most other datasets, models achieve promising scores above 0.60 F1 points; e.g. Roller 401 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 401–407, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 3 Furthermore, we show that the Baroni dataset seems to exhibit a pronounced behaviour along two dimensions known to be relevant for hypernymy: generality and similarity. This behaviour appears to be important for the success of Baroni’s dataset: if we filter and resample"
E17-2064,D14-1162,0,0.0948817,"Missing"
E17-2064,P14-1113,0,0.117985,"of hypernymy: generality and similarity. 1 Introduction Word embeddings have been widely used as features in NLP tasks like parsing and textual entailment. One key aspect that has been investigated is their capacity to encode hypernymy; this semantic relation denotes a taxonomical order of objects in the world; for example, a dog is a canine which is a vertebrate. To test the ability of embeddings to encode hypernymy, previous work has proposed supervised models to learn whether a given pair of embeddings (wi , wj ) are in the hypernymy relation (Roller et al., 2014; Necsulescu et al., 2015; Fu et al., 2014). Results from previous work suggest that word embeddings indeed capture hypernymy information. This observation is relatively general and robust across several choices of datasets, models and embeddings. For example, Levy et al. (2015) achieve up to 0.85 F1, while Roller and Erk (2016) achieve up to 0.90 F1. Both of these results are achieved on the Baroni dataset (Baroni et al., 2012). For most other datasets, models achieve promising scores above 0.60 F1 points; e.g. Roller 401 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volum"
E17-2064,D16-1234,0,0.595674,"order of objects in the world; for example, a dog is a canine which is a vertebrate. To test the ability of embeddings to encode hypernymy, previous work has proposed supervised models to learn whether a given pair of embeddings (wi , wj ) are in the hypernymy relation (Roller et al., 2014; Necsulescu et al., 2015; Fu et al., 2014). Results from previous work suggest that word embeddings indeed capture hypernymy information. This observation is relatively general and robust across several choices of datasets, models and embeddings. For example, Levy et al. (2015) achieve up to 0.85 F1, while Roller and Erk (2016) achieve up to 0.90 F1. Both of these results are achieved on the Baroni dataset (Baroni et al., 2012). For most other datasets, models achieve promising scores above 0.60 F1 points; e.g. Roller 401 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 401–407, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 3 Furthermore, we show that the Baroni dataset seems to exhibit a pronounced behaviour along two dimensions known to be relevant for hypernymy: generality and similarity."
E17-2064,S12-1047,0,0.0323153,"ginal dataset (Baroni and Lenci, 2011) contains several semantic relations. Levy et al. (2015) used the hypernymy pairs as positive instances and the pairs in all the other semantic relations as negative instances. Kotlerman Kotlerman et al. (2010) adapted the lexical entailment dataset of (Zhitomirsky-Geffet and Dagan, 2009). Levy From a set of entailing propositions of the form (subject, verb, object) in (Levy et al., 2014), Levy et al. (2015) extracted entailing nouns that shared two arguments to create instance pairs. Turney Turney and Mohammad (2015) transformed the SemEval-2012 dataset (Jurgens et al., 2012) to expand from 79 to 158 semantic relations. Weeds Weeds et al. (2014) drew instance pairs from WordNet under the constraint that none of the words in a pair must be seen in any other pair in the same role (hyponym or hypernym). Dataset Analysis Torralba and Efros (2011) compared a set of object recognition datasets by testing each of them across different test distributions. In order to fairly compare these datasets, Torralba and Efros (2011) first eliminated some visible biases such as sample size by normalizing the datasets. In this way, other biases in the datasets were exposed such as th"
E17-2064,C14-1097,0,0.146593,"Missing"
E17-2064,W14-1610,0,0.0258213,"omain, and then relevant work on dataset analysis. 2.1 Materials Baroni Baroni et al. (2012) drew instance pairs from WordNet that were manually checked to discard noisy ones. Bless The original dataset (Baroni and Lenci, 2011) contains several semantic relations. Levy et al. (2015) used the hypernymy pairs as positive instances and the pairs in all the other semantic relations as negative instances. Kotlerman Kotlerman et al. (2010) adapted the lexical entailment dataset of (Zhitomirsky-Geffet and Dagan, 2009). Levy From a set of entailing propositions of the form (subject, verb, object) in (Levy et al., 2014), Levy et al. (2015) extracted entailing nouns that shared two arguments to create instance pairs. Turney Turney and Mohammad (2015) transformed the SemEval-2012 dataset (Jurgens et al., 2012) to expand from 79 to 158 semantic relations. Weeds Weeds et al. (2014) drew instance pairs from WordNet under the constraint that none of the words in a pair must be seen in any other pair in the same role (hyponym or hypernym). Dataset Analysis Torralba and Efros (2011) compared a set of object recognition datasets by testing each of them across different test distributions. In order to fairly compare t"
E17-2064,P16-1158,0,0.0422063,"this behaviour, we generally achieve better results. 2 We describe both the datasets that we compare and the word embedding model that we use as features. 3.1 Background Supervised Hypernym Detection Dataset Size Ratio pos/neg Baroni Bless Kotlerman Levy Turney Weeds 791 3225 739 2932 539 2033 0.97 0.12 0.45 0.08 1.06 0.98 Table 1: Summary of datasets. The task is posed as a binary classification problem. An instance pair is composed of two embeddings, e.g. (wcat , wanimal , positive). A vector operation such as concatenation (concat) or difference (diff ) is then applied to both embeddings. Vylomova et al. (2016) learned a range of semantic relations, including hypernymy, using the diff operator and achieved positive results. Roller and Erk (2016) showed that concat with a logistic regression classifier learns to extract Hearst patterns (such as, including, etc.) from distributional vectors. Weeds et al. (2014) and Vylomova et al. (2016) described the lexical memorization phenomenon: a classifier learns that a word wi is hyponym of a word wj based on the frequency of wj appearing in the hypernym slot in positive pairs. In order to avoid high scores at test time due to this effect, Weeds et al. (2014)"
E17-2064,N15-1098,0,0.210595,"my; this semantic relation denotes a taxonomical order of objects in the world; for example, a dog is a canine which is a vertebrate. To test the ability of embeddings to encode hypernymy, previous work has proposed supervised models to learn whether a given pair of embeddings (wi , wj ) are in the hypernymy relation (Roller et al., 2014; Necsulescu et al., 2015; Fu et al., 2014). Results from previous work suggest that word embeddings indeed capture hypernymy information. This observation is relatively general and robust across several choices of datasets, models and embeddings. For example, Levy et al. (2015) achieve up to 0.85 F1, while Roller and Erk (2016) achieve up to 0.90 F1. Both of these results are achieved on the Baroni dataset (Baroni et al., 2012). For most other datasets, models achieve promising scores above 0.60 F1 points; e.g. Roller 401 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 401–407, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 3 Furthermore, we show that the Baroni dataset seems to exhibit a pronounced behaviour along two dimensions known to be"
E17-2064,C14-1212,0,0.246612,".45 0.08 1.06 0.98 Table 1: Summary of datasets. The task is posed as a binary classification problem. An instance pair is composed of two embeddings, e.g. (wcat , wanimal , positive). A vector operation such as concatenation (concat) or difference (diff ) is then applied to both embeddings. Vylomova et al. (2016) learned a range of semantic relations, including hypernymy, using the diff operator and achieved positive results. Roller and Erk (2016) showed that concat with a logistic regression classifier learns to extract Hearst patterns (such as, including, etc.) from distributional vectors. Weeds et al. (2014) and Vylomova et al. (2016) described the lexical memorization phenomenon: a classifier learns that a word wi is hyponym of a word wj based on the frequency of wj appearing in the hypernym slot in positive pairs. In order to avoid high scores at test time due to this effect, Weeds et al. (2014) suggest having disjoint vocabularies between training and test sets. 2.2 Datasets We pick the datasets used by Levy et al. (2015) and Weeds et al. (2014) which have disjoint training and test sets. We first give a brief overview of hypernymy detection, important findings in this domain, and then relevan"
E17-2064,S15-1021,0,0.0401134,"nt to dimensions specific of hypernymy: generality and similarity. 1 Introduction Word embeddings have been widely used as features in NLP tasks like parsing and textual entailment. One key aspect that has been investigated is their capacity to encode hypernymy; this semantic relation denotes a taxonomical order of objects in the world; for example, a dog is a canine which is a vertebrate. To test the ability of embeddings to encode hypernymy, previous work has proposed supervised models to learn whether a given pair of embeddings (wi , wj ) are in the hypernymy relation (Roller et al., 2014; Necsulescu et al., 2015; Fu et al., 2014). Results from previous work suggest that word embeddings indeed capture hypernymy information. This observation is relatively general and robust across several choices of datasets, models and embeddings. For example, Levy et al. (2015) achieve up to 0.85 F1, while Roller and Erk (2016) achieve up to 0.90 F1. Both of these results are achieved on the Baroni dataset (Baroni et al., 2012). For most other datasets, models achieve promising scores above 0.60 F1 points; e.g. Roller 401 Proceedings of the 15th Conference of the European Chapter of the Association for Computational"
E17-2064,W11-2501,0,\N,Missing
E17-2064,J09-3004,0,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
E17-5003,P15-1136,0,0.0132507,"on learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016). 4 Structure Part I: Imitation Learning Algorithms (90 minutes) • Introduction, basic concepts and intuition: Structured prediction basics, per-action and end-to-end supervision, and decomposability. • Detailed algorithm descriptions: Exact imitation, DAgger, roll outs and non-decomposable losses, latent variables, and cost-sensitive classification. • Advanced topics: Imitation learning by coaching, imitation learning for recurrent neural network training, bandit learning,"
E17-5003,Q13-1033,0,0.0152028,"a comparative overview of the various algorithms presented that will expose their differences and their practical implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (H"
E17-5003,P16-1001,1,0.827231,"cal implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and"
E17-5003,D13-1152,0,0.0696143,"Missing"
E17-5003,C16-1105,1,0.838573,"learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016)."
E17-5003,Q14-1042,1,0.855431,"n, and show how they can be applied to a variety of NLP tasks. All material associated with the tutorial will be made available through https://sheffieldnlp.github.io/ImitationLearningTutorialEACL2017/. 2 Part I: Imitation Learning In the first part, we will give a unified presentation of imitation learning for structured prediction focusing on the intuition behind the framework. We will then delve into the details of the different algorithms that have been proposed so 1 far under the imitation learning paradigm, including Searn (Daum´e III et al., 2009), DAgger (Ross et al., 2011), v-DAgger (Vlachos and Clark, 2014), and lols (Chang et al., 2015). Furthermore, we will give a comparative overview of the various algorithms presented that will expose their differences and their practical implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how"
E17-5003,W11-0307,1,0.801814,"n as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016). 4 Structure Part I: Imitation Learning Algorithms (90 minutes) • Introduction, basic concepts and intuition: Structured prediction basics, per-action and end-to-end supervision, and decomposability. • Detailed algorithm descriptions: Exact imitation, DAgger, roll outs and non-decomposable losses, latent variables, and cost-sensitive classification. • Advanced topics: Imitation learning b"
K17-1021,P82-1020,0,0.790042,"Missing"
K17-1021,P17-2054,1,0.83311,"scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title,"
K17-1021,P15-2136,0,0.216262,"score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is therefore a list of sentences, sentence features to encode context and a label all stored in a randomly ordered list. 2.2 3.1 HighlightROUGE is a method used to generate additional training data for"
K17-1021,P16-1046,0,0.163516,"ncerned with summarising scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (N"
K17-1021,S10-1004,0,0.0254187,"nformation by leveraging the graph structure. To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document i"
K17-1021,N16-1012,0,0.0357437,"scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (Nallapati et al., 2016"
K17-1021,D15-1232,0,0.0714735,"e document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore opt to only encode the target sequence with an RNN and the global context with simpler features. We leave fully neural approaches to encoding publications to future work. Figure 6: Comparison of ROUGE scores of the Features Only, SAFNet and SFNet models when trained with (bars on the left) and without"
K17-1021,D15-1045,0,0.205431,"the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is therefore a list of sentences, sentence features to encode context and a label all stored in a randomly ordered list. 2.2 3.1"
K17-1021,W04-1013,0,0.0352079,"rk several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is ther"
K17-1021,W04-1017,0,0.0104513,"uments. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore opt to only encode the target sequence with an RNN and the global context with simpler features. We leave fully neural approaches to enco"
K17-1021,P16-4013,0,0.0212315,"ferent ways: as their mean averaged word embeddings and as their Recurrent Neural Network (RNN) encoding. 4.1 Summariser Features As the sentences in our dataset are randomly ordered, there is no readily available context for each sentence from surrounding sentences (taking this into account is a potential future development). To provide local and global context, a set of 8 features are used for each sentence which are described below. These contextual features contribute to achieving the best performances. Some recent work in summarisation uses as many as 30 features (Dlikman and Last, 2016; Litvak et al., 2016). We choose only a minimal set of features to focus more on learning from raw data than on feature engineering, although this could potentially further improve results. Keyphrase Score Authors such as Sp¨arck Jones (2007) refer to the keyphrase score as a useful summarisation feature. The feature uses author defined keywords and counts how many of these keywords a sentence contains, the idea being that important sentences will contain more keywords. TF-IDF Term Frequency, Inverse Document Frequency (TF-IDF) is a measure of how relevant a word is to a document (Ramos et al., 2003). It takes int"
K17-1021,I11-1001,0,0.0198738,"fmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title, abstract, author written highlight statements and autho"
K17-1021,D15-1057,0,0.0199481,"the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title, abstract, author written highlight statements and author defined keywords. The"
K17-1021,N09-1041,0,0.0480757,"aste score. The title has the highest ROUGE score in relation to the gold summary, which is intuitive as the aim of a title is to convey information about the research in a single line. 5.2 Comparison of Model Performance and Error Analysis Figure 3 shows comparisons of the best model we developed to well-established external baseline methods. Our model can be seen to significantly outperform these methods, including graph-based methods which take account of global context: LexRank (Radev, 2004) and TextRank (Mihalcea and Tarau, 2004); probabilistic methods in KLSum (KL divergence summariser, Haghighi and Vanderwende (2009)); methods based on singular value decomposition with LSA (latent semantic analysis, Steinberger and Jeˇzek (2004)); and simple methods based on counting in SumBasic (Vanderwende et al., 2007). This is an encouraging result showing that our methods that combine neural sentence encoding and simple features for representing the global context and positional information are very effective for modelling an extractive summarisation problem. Figure 4 shows the performance of all models developed in this work measured in terms of accuracy and ROUGE-L on CSPubSumExt Test and CSPubSum Test, respectivel"
K17-1021,K16-1028,0,0.0552598,"Missing"
K17-1021,C14-1002,0,0.0413014,"Missing"
K17-1021,D16-1198,0,0.0142963,"raging the graph structure. To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Ea"
K17-1021,J02-4002,0,0.898922,"y sentences from the Abstract, Introduction or Conclusion, thinking these more salient to summaries; and we show that certain sections within a paper are more relevant to summaries than others (see Section 5.1). Therefore we assign sentences an integer location for 7 different sections: Highlight, Abstract, Introduction, Results / Discussion / Analysis, Method, Conclusion, all else.3 Location features have been used in other ways in previous work on summarising scientific literature; Visser and Wieling (2009) extract sentence location features based on the headings they occurred beneath while Teufel and Moens (2002) divide the paper into 20 equal parts and assign each sentence a location based on which segment it occurred in - an attempt to capture distinct zones of the paper. Document TF-IDF Document TF-IDF calculates the same metric as TF-IDF, but uses the count of words in a sentence as the term frequency and count of words in the rest of the paper as the background corpus. This gives a representation of how important a word is in a sentence in relation to the rest of the document. Sentence Length Teufel et al. (2002) created a binary feature for if a sentence was longer than a threshold. We simply in"
K17-1021,radev-etal-2004-mead,0,0.117015,"luation campaigns for summarisation of news, organised by the 202 challenges centered around how to encode very large documents. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore"
K17-1021,E17-2112,0,0.0146892,"risation often emerged as part of evaluation campaigns for summarisation of news, organised by the 202 challenges centered around how to encode very large documents. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequen"
K17-1021,D15-1228,0,0.0239125,"Missing"
K17-1021,D15-1044,0,0.0437486,"act Here, we are concerned with summarising scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standa"
K17-1021,W16-1520,0,0.076196,"Missing"
K17-1021,P17-1099,0,0.0263366,"ons. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (Nallapati et al., 2016a). This new datase"
K17-1021,W04-3252,0,\N,Missing
K17-1021,D11-1014,0,\N,Missing
K17-1021,W16-1511,0,\N,Missing
K18-1007,H05-1079,0,0.173727,"Missing"
K18-1007,D15-1075,0,0.0900122,"ferred to as Recognising Textual Entailment (Fyodorov et al., 2000; Condoravdi et al., 2003; Dagan et al., 2005), is a central problem in language understanding (Katz, 1972; Bos and Markert, 2005; van Benthem, 2008; MacCartney and Manning, 2009), and thus it is especially well suited to serve as a benchmark task for research in machine reading. In NLI, a model is presented with two sentences, a premise p and a hypothesis h, and the goal is to determine whether p semantically entails h. The problem of acquiring large amounts of labelled data for NLI was addressed with the creation of the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) datasets. In these processes, annotators were presented with a premise p drawn from a corpus, and were required to generate three new sentences (hypotheses) based on p, according to the following criteria: a) Entailment – h is definitely true given p (p Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text"
K18-1007,P17-1152,0,0.0404299,"oblem. Furthermore, we outline a method for incorporating such background knowledge into models by means of an adversarial training procedure (Section 5). pΘ ( · |a, b) = softmax(scoreΘ (a, b)) (1) where scoreΘ : Rk×`a × Rk×`b → R3 is a modeldependent scoring function withPparameters Θ, and softmax(x)i = exp{xi }/ j exp{xj } denotes the softmax function. Several scoring functions have been proposed in the literature, such as the conditional Bidirectional LSTM (cBiLSTM) (Rocktäschel et al., 2016), the Decomposable Attention Model (DAM) (Parikh et al., 2016), and the Enhanced LSTM model (ESIM) (Chen et al., 2017). One desirable quality of the scoring function scoreΘ is that it should 66 be differentiable with respect to the model parameters Θ, which allows the neural NLI model to be trained from data via back-propagation. NLI Rules R1 R2 R3 R4 R5 Model Training. Let D = {(x1 , y1 ), . . . , (xm , ym )} represent a NLI dataset, where xi denotes the i-th premise-hypothesis sentence pair, and yi ∈ {1, . . . , K} their relationship, where K ∈ N is the number of possible relationships – in the case of NLI, K = 3. The model is trained by minimising a cross-entropy loss JD on D: JD (D, Θ) = − m X K X Table 1"
K18-1007,W03-0906,0,0.203754,"Missing"
K18-1007,N18-2017,0,0.0467461,"enotes that the corruption process removes “segment one” and introp duced “segment two” in the sentence, and s1 → − s2 indicates that DAM classifies the relation between s1 and s2 as contradiction, with probability p. We use different colours for representing the contradiction, entailment and neutral classes. Examples 1, 2, 3, and 4 violate the rule R2 , while example 5 violates the rule R5 . .00 .99 indicates that the corruption process increases the inconsistency loss from .00 to .99, and the red boxes are used for indicating mistakes made by the model on the adversarial examples. datasets (Gururangan et al., 2018). examples provides us with useful insights on the inner workings of neural NLI models, that can be leveraged for improving the robustness of state-ofthe-art models. 8.2 Evaluation on Adversarial Datasets. We evaluated the proposed approach on 9 adversarial datasets Akm , with k ∈ {100, 500, 1000}, generated following the procedure described in Section 6 – results are summarised in Table 5. We can see that the proposed adversarial training method significantly increases the accuracy on the adversarial test sets. For instance, consider A100 DAM : prior to regularising (λ = 0), DAM achieves a ve"
K18-1007,D16-1244,0,0.0835585,"the generation problem to a combinatorial optimisation problem. Furthermore, we outline a method for incorporating such background knowledge into models by means of an adversarial training procedure (Section 5). pΘ ( · |a, b) = softmax(scoreΘ (a, b)) (1) where scoreΘ : Rk×`a × Rk×`b → R3 is a modeldependent scoring function withPparameters Θ, and softmax(x)i = exp{xi }/ j exp{xj } denotes the softmax function. Several scoring functions have been proposed in the literature, such as the conditional Bidirectional LSTM (cBiLSTM) (Rocktäschel et al., 2016), the Decomposable Attention Model (DAM) (Parikh et al., 2016), and the Enhanced LSTM model (ESIM) (Chen et al., 2017). One desirable quality of the scoring function scoreΘ is that it should 66 be differentiable with respect to the model parameters Θ, which allows the neural NLI model to be trained from data via back-propagation. NLI Rules R1 R2 R3 R4 R5 Model Training. Let D = {(x1 , y1 ), . . . , (xm , ym )} represent a NLI dataset, where xi denotes the i-th premise-hypothesis sentence pair, and yi ∈ {1, . . . , K} their relationship, where K ∈ N is the number of possible relationships – in the case of NLI, K = 3. The model is trained by minimising a c"
K18-1007,P82-1020,0,0.766996,"Missing"
K18-1007,N18-1170,0,0.0346481,"ited by the fact that semantically invariant input perturbations in NLP are difficult to identify (Buck et al., 2017). Jia and Liang (2017) analyse the robustness of extractive question answering models on examples obtained by adding adversarially generated distracting text to SQuAD (Rajpurkar et al., 2016) dataset instances. Belinkov and Bisk (2017) also notice that character-level Machine Translation are overly sensitive to random character manipulations, such as typos. Hosseini et al. (2017) show that simple character-level modifications can drastically change the toxicity score of a text. Iyyer et al. (2018) proposes using paraphrasing for generating adversarial examples. Our model is fundamentally different in two ways: a) it does not need labelled data for generating adversarial examples – the inconsistency loss can be maximised by just making an NLI model produce inconsistent results, and b) it incorporates adversarial examples during the training process, with the aim of training more robust NLI models. Adversarial examples are also used for assessing the robustness of computer vision models (Szegedy et al., 2014; Goodfellow et al., 2014; Nguyen et al., 2015), where they are created by adding"
K18-1007,D17-1215,0,0.116654,"Missing"
K18-1007,W09-3714,0,0.0713283,"Missing"
K18-1007,P16-1144,0,0.0396517,"Missing"
N09-1018,A00-2018,0,0.00525537,", r) lemma(p, +l) ∧ ppos(a, +p) ∧ hasRole(p, a) ⇒ sense(p, +f ) lemma(p, +l) ∧ role(p, a, +r) ⇒ sense(p, +f ) Table 2: Global formulae for ML model solver. 5 Experimental Setup For training and testing our SRL systems we used a version of the CoNLL 2008 shared task (Surdeanu et al., 2008) dataset that only mentions verbal predicates, disregarding the nominal predicates available in the original corpus.7 While the original (open track) corpus came with MALT (Nivre et al., 2007) dependencies, we observed slightly better results when using the dependency parses generated with a Charniak parser (Charniak, 2000). Hence we used the latter for all our experiments. To assess the performance of our model, and it to evaluate the possible gains to be made from considering a joint model of the complete SRL pipeline, we set up several systems. The full system uses a Markov Logic Network with all local and global formulae described in section 3. For the bottom-up system we removed the structural top-down constraints from the complete model—previous work Riedel and Meza-Ruiz (2008) has shown that this can lead to improved performance. The bottom-up (-arg) system is equivalent to the bottom-up system, but it do"
N09-1018,D08-1008,0,0.0358512,"Missing"
N09-1018,W05-0625,0,0.0688911,"fication), and which is the sense of the predicate (sense disambiguation). In this paper we use Markov Logic (ML), a Statistical Relational Learning framework that combines First Order Logic and Markov Networks, to develop a joint probabilistic model over all decisions mentioned above. The following paragraphs will motivate this choice. First, it allows us to readily capture global correlations between decisions, such as the constraint that a predicate can only have one agent. This type of correlations has been successfully exploited in several previous SRL approaches (Toutanova et al., 2005; Punyakanok et al., 2005). Second, we can use the joint model to evaluate the benefit of incorporating decisions into the joint model that either have not received much attention within the SRL community (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template withHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages"
N09-1018,W08-2125,1,0.559537,"MALT (Nivre et al., 2007) dependencies, we observed slightly better results when using the dependency parses generated with a Charniak parser (Charniak, 2000). Hence we used the latter for all our experiments. To assess the performance of our model, and it to evaluate the possible gains to be made from considering a joint model of the complete SRL pipeline, we set up several systems. The full system uses a Markov Logic Network with all local and global formulae described in section 3. For the bottom-up system we removed the structural top-down constraints from the complete model—previous work Riedel and Meza-Ruiz (2008) has shown that this can lead to improved performance. The bottom-up (-arg) system is equivalent to the bottom-up system, but it does not include any formulae that mention the hidden isArgument predicate. For the systems presented so far we perform joint inference and learning. The pipeline system differs in this regard. For this system we train a separate model for each stage in the pipeline of figure 1. The predicate identification stage identifies the predicates (using all local isP redicate formulae) of 7 The reason for this choice where license problems. 160 a sentence. The next stage pre"
N09-1018,W08-2121,0,0.152361,"Missing"
N09-1018,P05-1073,0,0.0617128,"ns play (argument classification), and which is the sense of the predicate (sense disambiguation). In this paper we use Markov Logic (ML), a Statistical Relational Learning framework that combines First Order Logic and Markov Networks, to develop a joint probabilistic model over all decisions mentioned above. The following paragraphs will motivate this choice. First, it allows us to readily capture global correlations between decisions, such as the constraint that a predicate can only have one agent. This type of correlations has been successfully exploited in several previous SRL approaches (Toutanova et al., 2005; Punyakanok et al., 2005). Second, we can use the joint model to evaluate the benefit of incorporating decisions into the joint model that either have not received much attention within the SRL community (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template withHuman Language Technologies: The 2009 Annual Conference of the North American"
N09-1018,W08-2140,0,0.06561,"Missing"
N09-1018,W04-3212,0,0.471631,"has dependency label d Token j can be semantic argument for token i according to high recall heuristic∗ Dependency path between tokens i and j is p∗ f is a syntactic (dependency) frame in which tokens i and j are designated as “pivots”∗ Figure 2: Factor graph for the first local formula in section 3.1. Here round nodes represent variables (corresponding to the states of ground atoms) and the rectangular nodes represent the factor and their parameters attached to the ground formulae. Table 1: Observable predicates; predicates marked with ∗ are dependency parsing-based versions for features of Xue and Palmer (2004). two observed lemma ground atoms. The + notation indicates that the MLN contains one instance of the rule, with a separate weight, for each assignment of the variables with a plus sign (?). The local formulae for isPredicate, isArgument and sense aim to capture the relation of the tokens with their lexical and syntactic surroundings. This includes formulae such as subcat(p, +f ) ⇒ isP redicate(p) which implies that a certain token is a predicate with a weight that depends on the subcategorization frame of the token. Further local formulae are constructed using those observed predicates in tab"
N09-1018,W08-2127,0,0.0295195,"Missing"
N09-1018,D08-1068,0,\N,Missing
N09-2002,J93-2003,0,0.0166054,"he decoding problem of other models into an ILP representation. ILP decoding will also allow the incorporation of global linguistic constraints in a manner similar to work in other areas of natural language processing. The remainder of this paper is organised as follows: Sections 2 and 3 briefly recap IBM Model 4 and its ILP formulation. Section 4 reviews the Cutting-Plane Algorithm. Section 5 outlines our experiments and we end the paper with conclusions and a discussion of open questions for the community. 2 IBM Model 4 In this paper we focus on the translation model defined by IBM Model 4 (Brown et al., 1993). Translation using IBM Model 4 is performed by treating the translation process a noisy-channel model where the probability of the English sentence given a French sentence is, P (e|f ) = P (f |e) · P (e), where P (e) is a language model of English. IBM Model 4 defines P (f |e) and models the translation process as a generative process of how a sequence of target words (in our case French or German) is generated from a sequence of source words (English). The generative story is as follows. Imagine we have an English sentence, e = e1 , . . . , el and along with a NULL word (eo ) and French sent"
N09-2002,W06-1616,1,0.919305,"perspective as without bounds on the solutions it is difficult to determine 5 Integer Linear Programming has previously been used to perform exact decoding for MT using IBM Model 4 and a bigram language model. Germann et al. (2004) view the translation process akin to the travelling salesman problem; however, from their reported results it is clear that using ILP naively for decoding does not scale up beyond short sentences (of eight tokens). This is due to the exponential number of constraints required to represent the decoding problem as an ILP program. However, work in dependency parsing (Riedel and Clarke, 2006) has demonstrated that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner. This raises the question as to whether incremental (or Cutting-Plane) ILP can also be used to decode IBM Model 4 on real world sentences. In this work we show that it is possible. Decoding IBM Model 4 (in combination with a bigram language model) using Cutting-Plane ILP scales to much longer sentences. This affords us the opportunity to finally analyse the performance of IBM Model 4 and the performance of its state-of-theProceedings of NAACL HLT 2009: Shor"
N09-2002,P07-2045,0,\N,Missing
N10-1111,P07-1036,0,0.584164,"the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data, and the preference of the domain expert when"
N10-1111,W02-1001,0,0.0567544,"putational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 1 Introduction Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before"
N10-1111,N07-1011,1,0.891642,"Missing"
N10-1111,D09-1134,0,0.0115376,"ser-defined constraints. By directly incorporating the model score and the constraints (as in Fmc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each 731 instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the graph, which slow down inference. Our approach directly incorporates these constraints into the ranking function, with no impact on inference time. 5 Experiments We carried out experiments on the Cora citation dataset. The task is to segment each citation into different fields, such as “author” and “title”. We use 300 instances as training data, 100 ins"
N10-1111,P08-1099,1,0.957678,"generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data,"
N10-1117,P05-1045,0,0.0125325,"2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take any graph and perform marginal inference. This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks. The graphical models of interest, however, are often too large and densely connected for efficient inference in them. For exa"
N10-1117,E06-1011,0,0.0422269,"is requires the model to factor in a way that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider. For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing (McDonald and Satta, 2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take a"
N10-1117,W07-2216,0,0.0170166,"probabilities (perhaps subject to hard constraints) in order to predict a good variable assignment.1 1 With a loss function that decomposes on the variables, this amounts to Minimum Bayes Risk (MBR) decoding, which is Traditionally, marginal inference in NLP has been performed via dynamic programming (DP); however, because this requires the model to factor in a way that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider. For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing (McDonald and Satta, 2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel"
N10-1117,W06-1616,1,0.959755,"hods, and the advantage second order models give in accuracy is often not significant enough to offset the lack of speed in practice. Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based  alignments, we will need to inspect at least O n4 factors, increasing our efficiency concerns. When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (Tromble and Eisner, 2006; Riedel and Clarke, 2006). Here we start with a small subset of the full graph, and run inference for this simpler problem. Then we search for factors that are “violated” in the solution, and add them to the graph. This is repeated until no more new factors can be added. Empirically this approach has shown impressive success. It often dramatically reduces the effective network size, with no loss in accuracy. How can we extend or generalize MAP relaxation algorithms to the case of marginal inference? Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the KL divergence between the c"
N10-1117,D08-1016,1,0.915567,"projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take any graph and perform marginal inference. This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks. The graphical models of interest, however, are often too large and densely connected for efficient inference in them. For example, in second order often very effective. 760 Human Language Technologies:"
N10-1117,N06-1054,0,0.299226,"simpler greedy parsing methods, and the advantage second order models give in accuracy is often not significant enough to offset the lack of speed in practice. Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based  alignments, we will need to inspect at least O n4 factors, increasing our efficiency concerns. When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (Tromble and Eisner, 2006; Riedel and Clarke, 2006). Here we start with a small subset of the full graph, and run inference for this simpler problem. Then we search for factors that are “violated” in the solution, and add them to the graph. This is repeated until no more new factors can be added. Empirically this approach has shown impressive success. It often dramatically reduces the effective network size, with no loss in accuracy. How can we extend or generalize MAP relaxation algorithms to the case of marginal inference? Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the K"
N10-1117,D07-1096,1,\N,Missing
N13-1008,P07-1073,0,0.832835,"r employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditi"
N13-1008,P04-1054,0,0.032567,"e importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof-the-art distant supervision. 1 Introduction Most previous work in relation extraction uses a predefined, finite and fixed schema of relation types (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by usi"
N13-1008,P04-1053,0,0.00631927,"s us |O |fact pairs hf + , f − i, and for each pair we do an SGD update using the corresponding gradients of Objf + ,f − . For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approa"
N13-1008,P11-1055,0,0.874455,"model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. Never-Ending Learning and Bootstrapp"
N13-1008,P09-1113,0,0.979877,"s (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H"
N13-1008,C12-1118,0,0.046746,"or us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent work on non-negative sparse embeddings (Murphy et al., 2012). In our problem columns correspond to relations, and rows correspond to entity tuples. By contrast, there columns are words, and rows are contextual features such as “words in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall fo"
N13-1008,N07-1071,0,0.0225058,"ve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at 74 Proceedings of NAACL-HLT 2013, pages 74–84, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguist"
N13-1008,D08-1009,0,0.00990576,"Missing"
N13-1008,D10-1106,0,0.0119867,"in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall for not-so-frequently mentioned facts. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed approach (Schoenmackers et al., 2008; Schoenmackers et al., 2010). However, their approach is substantially more complex, requires a categorization of entities into fine grained entity types, and needs inference in high tree-width Markov Networks. By contrast, our approach is based on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expressi"
N13-1008,N06-1039,0,0.0107607,"+ , f − i, and for each pair we do an SGD update using the corresponding gradients of Objf + ,f − . For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption"
N13-1008,D12-1042,0,0.926802,"ity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. Never-Ending Learning and Bootstrapping Our latent feature m"
N13-1008,C08-1107,0,0.00962304,"ly different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent work on non-negative sparse"
N13-1008,W04-3206,0,0.00865004,"Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics communit"
N13-1008,D11-1135,1,0.657866,"ervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at 74 Proceedings of NAACL-HLT 2013, pages 74–84, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics certainly does not imply scientist-at. I"
N13-1008,W12-3022,1,0.842101,"that the neighborhood model amounts to a collection of local log-linear classifiers, one for each relation r with feature functions fr,r0 (t) = I [r0 6= r ∧ (r0 , t) ∈ O] and weights wr . This means that in contrast to model F, this model cannot harness any synergies between textual and pre-existing DB relations. 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity e we introduce a latent feature vector te of dimension K E . In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the"
N13-1008,P12-1075,1,0.691635,"that the neighborhood model amounts to a collection of local log-linear classifiers, one for each relation r with feature functions fr,r0 (t) = I [r0 6= r ∧ (r0 , t) ∈ O] and weights wr . This means that in contrast to model F, this model cannot harness any synergies between textual and pre-existing DB relations. 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity e we introduce a latent feature vector te of dimension K E . In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the"
N13-1008,P06-1107,0,0.00843022,"roach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent w"
N15-1118,C14-1197,0,0.0360344,"Missing"
N15-1118,S13-1002,0,0.0188067,"s paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal wit"
N15-1118,P14-1114,0,0.0110345,"rated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguitie"
N15-1118,H05-1079,0,0.155091,"on Matrix factorization is capable of learning complex dependencies between relations, but requires observed facts as training signal. However, often we either do not have this signal because the relations of interest do not have pre-existing facts, or this signal is noisy due to alignment errors or mismatches when linking knowledge base entities to mentions in text. To overcome this problem we investigate the use of first-order logic background knowledge (e.g. implications) to aid relation extraction. One option is to rely on a fully symbolic approach that exclusively uses first-order logic (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008). In this case incorporating additional background knowledge is trivial. However, it is difficult to generalize and deal with noise and uncertainty in language when relying only on manual rules. In contrast, matrix factorization methods can overcome these shortcomings, but it is not clear how they can be combined with logic formulae. In this section, we propose to inject formulae into the embeddings of relations and entity-pairs, i.e., estimate the embeddings such that predictions based on them conform to given logic formulae (see Figure 1 for an overview). We"
N15-1118,W08-2222,0,0.0483834,"ng complex dependencies between relations, but requires observed facts as training signal. However, often we either do not have this signal because the relations of interest do not have pre-existing facts, or this signal is noisy due to alignment errors or mismatches when linking knowledge base entities to mentions in text. To overcome this problem we investigate the use of first-order logic background knowledge (e.g. implications) to aid relation extraction. One option is to rely on a fully symbolic approach that exclusively uses first-order logic (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008). In this case incorporating additional background knowledge is trivial. However, it is difficult to generalize and deal with noise and uncertainty in language when relying only on manual rules. In contrast, matrix factorization methods can overcome these shortcomings, but it is not clear how they can be combined with logic formulae. In this section, we propose to inject formulae into the embeddings of relations and entity-pairs, i.e., estimate the embeddings such that predictions based on them conform to given logic formulae (see Figure 1 for an overview). We refer to such embeddings as low-r"
N15-1118,P07-1073,0,0.0854988,"Missing"
N15-1118,P07-1036,0,0.0546698,"ense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank ma"
N15-1118,D14-1165,0,0.257379,"describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt¨aschel et al. (2014) combine logic with matrix factorization for learning low-dimensional embeddings that approximately satisfy given formulae and generalize to unobserved facts on toy data. Our work extends this workshop paper by proposing a simpler formalism without tensor-based logical connectives, presenting results on a real-world task, and demonstrating the utility of this approach for learning relations with few textual alignments. Chang et al. (2014) use Freebase entity types as hard constraints in a tensor factorization objective for universal schema relation extraction. In contrast, our approach is imposing soft constraints that are formulated as universally quantified first-order formula. de Lacalle and Lapata (2013) combine first-order logic knowledge with a topic model to improve surface pattern clustering for relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distr"
N15-1118,D13-1079,0,0.0122835,"problem all distantly-supervised techniques share: you can only reliably learn relations that appear frequently enough in the knowledge base. In particular, for relations that do not appear in the knowledge base or for which no facts are known we cannot learn a predictor at all. One way to overcome this problem is to incorporate additional domain knowledge, either specified manually or bootstrapped from auxiliary sources. In fact, domain knowledge encoded as simple logic formulae over patterns and relations has been used in practice to directly specify relation extractors (Reiss et al., 2008; Chiticariu et al., 2013; Akbik et al., 2014). However, these extractors can be brittle and obtain poor recall, since they are unable to generalize to textual patterns that are not 1119 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1119–1129, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics found in given formulae. Hence, there is a need for learning extractors that are able to combine logical knowledge with benefits of factorization techniques to facilitate precise extractions and generalization to novel relations. In"
N15-1118,W13-3809,0,0.0305674,"Missing"
N15-1118,P09-1041,0,0.0140558,"e learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on th"
N15-1118,P14-1079,0,0.0333626,"se schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate f"
N15-1118,W11-0112,0,0.0242402,"ideas presented in this paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a n"
N15-1118,S13-1001,0,0.0218077,"constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt¨aschel et al. (2014) combine logic with matrix factorization for learning low-dimensional embeddings that approximately satisfy given formulae and generalize to unobserved facts on toy data. Our work extends this workshop paper by proposing a simpler formalism without tensor-based logical connectives, presenting results on a real-world task, and demonstrating the utility of this approach for learning relations with few textual alignments."
N15-1118,P13-1088,0,0.012526,"specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining symbolic information with neural networks has also been an active area of research. Towell and Shavlik (1994) introduce Knowledge-Based Artificial Neural Networks whose topology is isomorphic to a knowledge base of facts and inference formulae. There, facts are input units, intermediate conclusions hidden units, and final conclusions (inferred facts) output units. Unlike our work, there is no latent representation of predicates and constants. H¨olldobler"
N15-1118,P11-1055,0,0.0695942,"ecting Logical Background Knowledge into Embeddings for Relation Extraction Tim Rockt¨aschel University College London London, UK Sameer Singh University of Washington Seattle, WA Abstract (a) unifying traditional canonical relations, such as those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, an"
N15-1118,Q13-1015,0,0.0189749,"ual alignments. Chang et al. (2014) use Freebase entity types as hard constraints in a tensor factorization objective for universal schema relation extraction. In contrast, our approach is imposing soft constraints that are formulated as universally quantified first-order formula. de Lacalle and Lapata (2013) combine first-order logic knowledge with a topic model to improve surface pattern clustering for relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining sym"
N15-1118,P08-1099,0,0.0128111,"l models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memoriz"
N15-1118,P09-1113,0,0.160003,"Missing"
N15-1118,P08-1028,0,0.0213123,"relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining symbolic information with neural networks has also been an active area of research. Towell and Shavlik (1994) introduce Knowledge-Based Artificial Neural Networks whose topology is isomorphic to a knowledge base of facts and inference formulae. There, facts are input units, intermediate conclusions hidden units, and final conclusions (inferred facts) output units. Unlike our work, there is no latent represe"
N15-1118,N13-1008,1,0.542673,"s those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these mode"
N15-1118,D08-1009,0,0.0325687,"der logic knowledge, and the ideas presented in this paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix fac"
N15-1118,D10-1106,0,0.0124061,"for any unobserved statement rm (ei , ej ) is done efficiently by calculating [rm (ei , ej )]. Note that this does not involve any explicit logical inference, instead we expect that the predictions from the learned embeddings already respect the provided formulae. 4 Experimental Setup There are two orthogonal question when evaluating the effectiveness of low-rank logic embeddings: a) does injection of logic formulae into the embeddings of entity-pairs and relations provide any benefits, and b) where do the background formulae come from? The latter is a well-studied problem (Hipp et al., 2000; Schoenmackers et al., 2010; V¨olker and 1123 Niepert, 2011). In this paper we focus the evaluation on the ability of various approaches to benefit from formulae that we directly extract from the training data using a simple method. Distant Supervision Evaluation We follow the procedure as used in Riedel et al. (2013) for evaluating knowledge base completion of Freebase (Bollacker et al., 2008) with textual data from the NYTimes corpus (Sandhaus, 2008). The training matrix consists of 4 111 columns, representing 151 Freebase relations and 3 960 textual patterns, 41 913 rows (entity-pairs) and 118 781 training facts of w"
N15-1118,N10-1009,1,0.793047,"r estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt"
N15-1118,D12-1042,0,0.0579155,"nd Knowledge into Embeddings for Relation Extraction Tim Rockt¨aschel University College London London, UK Sameer Singh University of Washington Seattle, WA Abstract (a) unifying traditional canonical relations, such as those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled dat"
N15-1118,D11-1135,1,0.860297,"Missing"
N15-1118,W14-2409,1,\N,Missing
N15-1118,D13-1040,0,\N,Missing
N15-3013,N10-1061,0,0.025357,"ccurate, requiring incremental model tweaking based on performance. Even if the model is accurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic netwo"
N15-3013,N13-1008,1,0.912781,"ed on performance. Even if the model is accurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic networks can be models by Alchemy (Richardson and"
N15-3013,D13-1170,0,0.00359697,"ccurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic networks can be models by Alchemy (Richardson and Domingos, 2006), Bayesian generative net"
N18-1179,D16-1182,0,0.0264123,"orks in ML and NLP have analyzed different aspects of complex models using a variety of approaches; for example, understanding input-output relationships by approximating the local or global behavior of the model using an interpretable model (Ribeiro et al., 2016; Craven and Shavlik, 1996), or analyzing the output of the model under lesions of its internal mechanism (Li et al., 2016). Another line of work has analyzed the robustness of NLP models both via controlled experiments to complement the information from the test set accuracy and test abilities of the models (Isabelle et al., 2017; B. Hashemi and Hwa, 2016; White et al., 2017) and via adversarial instances to expose weaknesses (Jia and Liang, 2017). In addition, work has been done to uncover and diminish gender biases in datasets captured by structured prediction models (Zhao et al., 2017) and word embeddings (Bolukbasi et al., 2016). However, to the best of our knowledge, there is no previous work to study the robustness of NLI models while analyzing factors affecting their predictions. 2.2 Behavior Analysis Previous work on behavioral science has focused on understanding how environmental factors influence behaviors in both human (Soman, 2001"
N18-1179,E12-1004,0,0.0363986,"n them in order to obtain the transformed samples ETA and ETH , respectively. Details about the sets: In order to build set IA , we sample only contradiction instances (instances in EA are also contradictions). We use the antonym word pairs from (Mohammad et al., 2013) to yield the sets ITA1 and ETA , which also only contain contradictions since the relation of antonymy is symmetric.4 We build two more sets, ITA2 and ITA3 (explained in Section 6.1). Sets IH , EH , ITH , and ETH contain instances with any class label. In order to generate sets ITH and ETH , we use the hypernym word pairs from (Baroni et al., 2012). We manually annotate these transformed sets and discard incoherent instances. 5.5 Factors Under Study We describe the three target factors that we hypothesize that affect the models’ response. Insensitivity is the name we give to the tendency of a model to predict the original label on a transformed instance that is similar to a control instance. Thus a model would be insensitive if, for example, it incorrectly predicts the same class label for both the control instance in Example 3 4 The word pair (sunset, sunrise) holds in an antonymy relation regardless of the position of the words in pre"
N18-1179,D15-1075,0,0.472094,"re more challenging for all the models. More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used. 1 Introduction The task of Natural Language Inference (NLI)1 has received a lot of attention and has elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset (Bowman et al., 2015). Such results are impressive due to the linguistic knowledge required to solve the task (LoBue and Yates, 2011; Maccartney, 2009). However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; tak"
N18-1179,P17-1152,0,0.299212,"alo, 2016). In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy. In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due to spurious factors, such as biases. Assessing to what extent the models are robust to these contingencies just by looking at test accuracy is, therefore, difficult. In this work we aim to study how certain factors affect the robustness of three pre-trained NLI models (a conditional encoder, the DAM model (Parikh et al., 2016), and the ESIM model (Chen et al., 2017)). We call these target factors insensitivity (not recognizing a new instance), polarity (a word-pair bias), and unseen pairs (recognizing the semantic relation of new word pairs). We became aware of these factors based on an exploration of the models’ behavior, and we hypothesize that these factors systematically influence the behavior of the models. In order to systematically test if the above factors affect robustness, we propose a set of challenging instances for the models: We sample a set of instances from SNLI data, we apply a transformation on this set that yields a new set of instance"
N18-1179,D17-1263,0,0.083467,"Missing"
N18-1179,D17-1215,0,0.0617266,"hes; for example, understanding input-output relationships by approximating the local or global behavior of the model using an interpretable model (Ribeiro et al., 2016; Craven and Shavlik, 1996), or analyzing the output of the model under lesions of its internal mechanism (Li et al., 2016). Another line of work has analyzed the robustness of NLP models both via controlled experiments to complement the information from the test set accuracy and test abilities of the models (Isabelle et al., 2017; B. Hashemi and Hwa, 2016; White et al., 2017) and via adversarial instances to expose weaknesses (Jia and Liang, 2017). In addition, work has been done to uncover and diminish gender biases in datasets captured by structured prediction models (Zhao et al., 2017) and word embeddings (Bolukbasi et al., 2016). However, to the best of our knowledge, there is no previous work to study the robustness of NLI models while analyzing factors affecting their predictions. 2.2 Behavior Analysis Previous work on behavioral science has focused on understanding how environmental factors influence behaviors in both human (Soman, 2001) and animal (Mench, 1998) subjects with the objective of predicting behavioral patterns or an"
N18-1179,D12-1096,0,0.0245337,"elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset (Bowman et al., 2015). Such results are impressive due to the linguistic knowledge required to solve the task (LoBue and Yates, 2011; Maccartney, 2009). However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target object (Zhao et al., 2017; Ribeiro et al., 2016), or a classifier used in social contexts that predicts a label based on racial attributes (Crawford and Calo, 2016). In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy. In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due"
N18-1179,P11-2057,0,0.177704,"ll but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used. 1 Introduction The task of Natural Language Inference (NLI)1 has received a lot of attention and has elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset (Bowman et al., 2015). Such results are impressive due to the linguistic knowledge required to solve the task (LoBue and Yates, 2011; Maccartney, 2009). However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target obj"
N18-1179,J81-4005,0,0.713924,"Missing"
N18-1179,J13-3004,0,0.0160175,"formed test set containing hypernym/hyponym swapped word pairs. We clarify: a) the sets IA and IH are sampled from the SNLI dataset; b) transformed test sets are generated from control sets containing control instances; c) we refer to the sets EA and EH as control test sets because the target word pairs are in their original position, and we apply T on them in order to obtain the transformed samples ETA and ETH , respectively. Details about the sets: In order to build set IA , we sample only contradiction instances (instances in EA are also contradictions). We use the antonym word pairs from (Mohammad et al., 2013) to yield the sets ITA1 and ETA , which also only contain contradictions since the relation of antonymy is symmetric.4 We build two more sets, ITA2 and ITA3 (explained in Section 6.1). Sets IH , EH , ITH , and ETH contain instances with any class label. In order to generate sets ITH and ETH , we use the hypernym word pairs from (Baroni et al., 2012). We manually annotate these transformed sets and discard incoherent instances. 5.5 Factors Under Study We describe the three target factors that we hypothesize that affect the models’ response. Insensitivity is the name we give to the tendency of a"
N18-1179,D17-1323,0,0.141642,"cartney, 2009). However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target object (Zhao et al., 2017; Ribeiro et al., 2016), or a classifier used in social contexts that predicts a label based on racial attributes (Crawford and Calo, 2016). In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy. In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due to spurious factors, such as biases. Assessing to what extent the models are robust to these contingencies just by looking at test accuracy is, therefore, difficult. In this work we aim to study how certain factors affect"
N18-1179,D16-1244,0,0.16643,"Missing"
N18-1179,N16-3020,0,0.234606,"ever, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target object (Zhao et al., 2017; Ribeiro et al., 2016), or a classifier used in social contexts that predicts a label based on racial attributes (Crawford and Calo, 2016). In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy. In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due to spurious factors, such as biases. Assessing to what extent the models are robust to these contingencies just by looking at test accuracy is, therefore, difficult. In this work we aim to study how certain factors affect the robustness of thre"
N18-1179,P10-1122,0,0.0305073,"ve achieved impressive results on the Stanford NLI (SNLI) dataset (Bowman et al., 2015). Such results are impressive due to the linguistic knowledge required to solve the task (LoBue and Yates, 2011; Maccartney, 2009). However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture. 1 Also known as Recognizing Textual Entailment. As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010). A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target object (Zhao et al., 2017; Ribeiro et al., 2016), or a classifier used in social contexts that predicts a label based on racial attributes (Crawford and Calo, 2016). In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy. In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due to spurious factors, s"
N18-1179,E17-2064,1,0.847065,"eas model performance was significantly worse on unseen antonym pairs, this effect is not obvious on the hyponymhypernym results (Subset 2 of ITH ). In fact, all models have a slightly higher accuracy on this subset than overall. Homogeneity tests find no evidence of an association between unseen word pairs and incorrect predictions for any model (CE:χ2 (1) = 0.00036, p = 0.98, DAM:χ2 (1) = 0.98, p = 0.32, ESIM:χ2 (1) = 0.178, p = 0.67). This effect may be explained by the models exploiting information from word embeddings. It has been shown that word embeddings are able to capture hypernymy (Sanchez and Riedel, 2017); thus the models may use this information to generalize to unseen hypernym pairs. Polarity We find very strong evidence for an association between polarity and class label predicted on sample IH for all models (CE:χ2 (10) = 168.40, DAM:χ2 (10) = 182.76, ESIM:χ2 (10) = 157.76). However, for sample ITH , only DAM keeps this strong correlation (χ2 (14) = 47.71). In the case of CE, we find weak evidence in favour of this correlation on instances of ITH (χ2 (14) = 25.27, p = 0.03). For ESIM we find no evidence of correlation (χ2 (14) = 22.72, p = 0.06), thus we do not reject the null hypothesis. P"
N18-1179,I17-1100,0,0.116621,"Missing"
N19-1237,P18-1177,0,0.527487,"chines to ask clarification questions (Saeidi et al., 2018), become more robust to queries (Yu et al., 2018), and to act as automatic tutors (Heilman and Smith, 2010). Recent approaches to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD (Rajpurkar et al., 2016). Only a single question is available for each context-answer pair, and models are trained using teacher forcing (Williams and Zipser, 1989). This lack of diverse training data combined with the one-stepahead training procedure exacerbates the problem of exposure bias (Ranzato et al., 2015). The model does not learn how to distribute probability mass over sequences that are valid but different to the ground tru"
N19-1237,P17-1123,0,0.486381,"data, enabling machines to ask clarification questions (Saeidi et al., 2018), become more robust to queries (Yu et al., 2018), and to act as automatic tutors (Heilman and Smith, 2010). Recent approaches to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD (Rajpurkar et al., 2016). Only a single question is available for each context-answer pair, and models are trained using teacher forcing (Williams and Zipser, 1989). This lack of diverse training data combined with the one-stepahead training procedure exacerbates the problem of exposure bias (Ranzato et al., 2015). The model does not learn how to distribute probability mass over sequences that are valid but differ"
N19-1237,P02-1040,0,0.104897,"er?” 3.1 Model description We use the model architecture described by Yuan et al. (2017). Briefly, this is a Seq2Seq model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) and copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Yuan et al. (2017) also add an additional answer encoder layer, and initialise the decoder with a hidden state constructed from the final state of the encoder. Beam search (Graves, 2012) is used to sample from the model at inference time. We train the model using maximum likelihood before fine tuning. Our implementation achieves a BLEU-4 score (Papineni et al., 2002) of 13.5 on the test set used by Du et al. (2017), before fine tuning. Metrics in ato rim in ato r di sc rim -0.7 +1.7 -0.5 -0.8 +6.4 +1.0 -1.9 -4.5 -2.6 -1.8 -2.7 -2.4 -3.7 +3.9 +2.0 -2.1 -2.5 +1.3 -13.4 +226 -16.3 -9.4 -1.0 -6.2 Di sc BL NL L EU al X X LM X X X QA X X X X X X Ad ve rsa ri rim in at o rr ew ard Di sc QA LM rew ard rew ar d r Features +1.5 +5.4 +2.9 +2.5 +10.8 +10.0 Table 2: Changes in automatic evaluation metrics after models were fine tuned on various objectives. QA refers to the F1 score obtained by a question answering system on the generated questions. LM refers to the pe"
N19-1237,D16-1264,0,0.0892381,"hes to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD (Rajpurkar et al., 2016). Only a single question is available for each context-answer pair, and models are trained using teacher forcing (Williams and Zipser, 1989). This lack of diverse training data combined with the one-stepahead training procedure exacerbates the problem of exposure bias (Ranzato et al., 2015). The model does not learn how to distribute probability mass over sequences that are valid but different to the ground truth; during inference, the model must predict the whole sequence, and may not be robust to mistakes during decoding. Recent work has investigated training the models directly on a perform"
N19-1237,D18-1233,1,0.84893,"istinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source. 1 Introduction Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions (Saeidi et al., 2018), become more robust to queries (Yu et al., 2018), and to act as automatic tutors (Heilman and Smith, 2010). Recent approaches to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currentl"
N19-1237,N18-2090,0,0.0956928,"nsive use of MT techniques. Du et al. (2017) use a Seq2Seq based model to generate questions conditioned on context-answer pairs, and build on this work by preprocessing the context to resolve coreferences and adding a pointer network (Du and Cardie, 2018). Similarly, Zhou et al. (2018) use a part-of-speech tagger to augment the embedding vectors. Both authors perform a human evaluation of their models, and show significant improvement over their baseline. Kumar et al. (2018a) use a similar model, but apply it to the task of generating questions without conditioning on a specific answer span. Song et al. (2018) use a modified context encoder based on multiperspective context matching (Wang et al., 2016). Kumar et al. (2018b) propose a framework for fine tuning using policy gradients and perform a human evaluation showing promising results. However, they use as rewards various similarity metrics that are still coupled to the ground truth. Yuan et al. (2017) describe a Seq2Seq model with attention and a pointer network, with an additional encoding layer for the answer. They also describe a method for further tuning their model using policy gradients, with rewards given by an external language model an"
N19-1237,P16-1014,0,0.402185,"n judgement and the model simply learns to exploit the weaknesses of the reward source. 1 Introduction Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions (Saeidi et al., 2018), become more robust to queries (Yu et al., 2018), and to act as automatic tutors (Heilman and Smith, 2010). Recent approaches to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD (Rajpurkar et al., 2016). Only a single question is available for each context-answer pair, and models are trained using teacher forcing (Williams and Zipser, 1989). This lack of diverse trai"
N19-1237,N10-1086,0,0.226367,"ases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source. 1 Introduction Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions (Saeidi et al., 2018), become more robust to queries (Yu et al., 2018), and to act as automatic tutors (Heilman and Smith, 2010). Recent approaches to question generation have used Seq2Seq (Sutskever et al., 2014) models with attention (Bahdanau et al., 2014) and a form of copy mechanism (Vinyals et al., 2015; Gulcehre et al., 2016). Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document (Zhou Sebastian Riedel University College London sriedel@ucl.ac.uk et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017). There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples avai"
N19-1237,W06-3114,0,0.0621363,"compared to the ground truth. We also report the rewards achieved on the LM scores against fluency QA scores against relevance Negative log perplexity 1.0 QA score 0.8 0.6 0.4 0.2 0.0 1 2 3 Relevance score 4 5 2 4 6 8 1 2 3 Fluency score 4 5 (a) QA scores plotted against human relevance scores for all (b) LM scores plotted against human fluency scores for all rated questions. rated questions. Figure 1: Comparison of human and automatic metrics. test set, as the QA, LM and discriminator scores. For the human evaluation, we follow the standard approach in evaluating machine translation systems (Koehn and Monz, 2006), as used for question generation by Du and Cardie (2018). We ask three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer. 4 Results Table 2 shows the changes in automatic metrics for models fine tuned on various combinations of rewards, compared to the model without tuning. In all cases, the BLEU score reduces, as the training objective is no longer closely coupled to the training data. In general, models achieve better scores on the metrics on w"
P09-1046,D08-1073,0,0.602023,"Missing"
P09-1046,S07-1052,1,0.565953,"cal formulae for the Tasks A, B and C. We say that a formula is local if it only considers the hidden temporal relation of a single event-event, event-time or event-DCT pair. The formulae in the second class are global: they in4 Proposed Markov Logic Network volve two or more temporal relations at the same As stated before, our aim is to jointly tackle time, and consider Tasks A, B and C simultaneTasks A, B and C of the TempEval challenge. In ously. this section we introduce the Markov Logic NetThe local formulae are based on features emwork we designed for this goal. ployed in previous work (Cheng et al., 2007; We have three hidden predicates, corresponding Bethard and Martin, 2007) and are listed in Table 1. to Tasks A, B, and C: relE2T(e, t, r) represents the What follows is a simple example in order to illustemporal relation of class r between an event e trate how we implement each feature as a formula feature: here for every event e the decision “e happens be- (or set of formulae). fore DCT” becomes more likely with a higher weight for this feature. 7 http://alchemy.cs.washington.edu/ 8 http://code.google.com/p/thebeast/ Consider the tense-feature for Task C. For this feature we first introduce"
P09-1046,P06-1095,0,0.721207,"ions —and to the best results for the task when compared to those of other machine learning based systems. 1 Introduction Temporal relation identification (or temporal ordering) involves the prediction of temporal order between events and/or time expressions mentioned in text, as well as the relation between events in a document and the time at which the document was created. With the introduction of the TimeBank corpus (Pustejovsky et al., 2003), a set of documents annotated with temporal information, it became possible to apply machine learning to temporal ordering (Boguraev and Ando, 2005; Mani et al., 2006). These tasks have been regarded as essential for complete document understanding and are useful for a wide range of NLP applications such as question answering and machine translation. Most of these approaches follow a simple schema: they learn classifiers that predict the temporal order of a given event pair based on a set of the pair’s of features. This approach is local in the sense that only a single temporal relation is considered at a time. Learning to predict temporal relations in this isolated manner has at least two advantages over any approach that considers several temporal relatio"
P09-1046,D08-1068,0,0.140991,"Missing"
P09-1046,S07-1108,0,0.198726,"Missing"
P09-1046,S07-1014,0,0.757283,": we only need to define features (in terms of formulae) and provide input data in the correct format. 2 In particular, we do not need to manually construct ILPs for each document we encounter. Moreover, we can exploit and compare advanced methods of global inference and learning, as long as they are implemented in our Markov Logic interpreter of choice. Hence, in our future work we can focus entirely on temporal relations, as opposed to inference or learning techniques for machine learning. We evaluate our approach using the data of the “TempEval” challenge held at the SemEval 2007 Workshop (Verhagen et al., 2007). This challenge involved three tasks corresponding to three types of temporal relations: between events and time expressions in a sentence (Task A), between events of a document and the document creation time (Task B), and between events in two consecutive sentences (Task C). Our findings show that by incorporating global constraints that hold between temporal relations predicted in Tasks A, B and C, the accuracy for all three tasks can be improved significantly. In comparison to other participants of the “TempEval” challenge our approach is very competitive: for two out of the three tasks we"
P09-1046,S07-1025,0,\N,Missing
P12-1075,P08-1004,0,0.430077,"ovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into sets of equivalent or simi"
P12-1075,P11-1054,0,0.013915,"(Banko et al., 2007; Banko and Etzioni, 2008). They employ a self-learner to extract relation instances, but no attempt is made to cluster instances into relations. 719 Yates and Etzioni (2009) present RESOLVER for discovering relational synonyms as a post processing step. Our approach falls into the same category. Moreover, we explore path senses and global features for relation discovery. Many generative probabilistic models have been applied to relation extraction. For example, varieties of topic models are employed for both open domain (Yao et al., 2011) and in-domain relation discovery (Chen et al., 2011; Rink and Harabagiu, 2011). Our approach employs generative models for path sense disambiguation, which achieves better performance than directly applying generative models to unsupervised relation discovery. 6 Conclusion We explore senses of paths to discover semantic relations. We employ a topic model to partition entity pairs of a path into different sense clusters and use hierarchical agglomerative clustering to merge senses into semantic relations. Experimental results show our approach discovers precise relation clusters and outperforms a generative model approach and a clustering metho"
P12-1075,P05-1045,0,0.0203123,"defeat” in sense clusters of pattern “A defeat B”. The two theme features are extracted from generative models, and each is a topic number. Our approach produces sense clusters for each path and semantic relation clusters of the whole data. Table 1 and 2 show some example output. 3 P (e1 (pi ), e2 (pi ), . . . , el (pi )|z1 , z2 , . . . , zl ) = α Experiments We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence. We use their lemmas Path A play B doc theme sen theme lexical words entity names 20:sports Americans, Ireland Yankees, Angels Ecuador, England Redskins, Detroit Red Bulls, F.C. Barcelona sports game yankees beat victory num-num won - 30:entertainment Jean-Pierre Bacri, Jacques Rita Benton, Gay Head Dance Jeanie, Scrabble Meryl Streep, Leilah Kevin Kline, Douglas Fairbanks music books television theater production book film show played plays directed artistic r:theater 25:m"
P12-1075,P04-1053,0,0.588726,"ing (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into s"
P12-1075,W04-2407,0,0.00932681,"t B”. The two theme features are extracted from generative models, and each is a topic number. Our approach produces sense clusters for each path and semantic relation clusters of the whole data. Table 1 and 2 show some example output. 3 P (e1 (pi ), e2 (pi ), . . . , el (pi )|z1 , z2 , . . . , zl ) = α Experiments We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence. We use their lemmas Path A play B doc theme sen theme lexical words entity names 20:sports Americans, Ireland Yankees, Angels Ecuador, England Redskins, Detroit Red Bulls, F.C. Barcelona sports game yankees beat victory num-num won - 30:entertainment Jean-Pierre Bacri, Jacques Rita Benton, Gay Head Dance Jeanie, Scrabble Meryl Streep, Leilah Kevin Kline, Douglas Fairbanks music books television theater production book film show played plays directed artistic r:theater 25:music/art Daniel Barenboim, recital of Mozart"
P12-1075,N07-1071,0,0.353749,"on beats rival Jonathan Tasini for Senate.” It can also indicate that an athlete A beat B in a sports match, as pair “(Dmitry Tursunov, Andy Roddick)” in “Dmitry Tursunov beat the best American player Andy Roddick.” Moreover, it can mean “physically beat” as pair “(Mr. Harris, Mr. Simon)” in “On Sept. 7, 1999, Mr. Harris fatally beat Mr. Simon.” This is known as polysemy. If we work with patterns alone, our extractor will not be able to differentiate between these cases. Most previous approaches do not explicitly address this problem. Lin and Pantel (2001) assumes only one sense per path. In (Pantel et al., 2007), they augment each relation with its selectional pref712 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 712–720, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics erences, i.e. fine-grained entity types of two arguments, to handle polysemy. However, such fine grained entity types come at a high cost. It is difficult to discover a high-quality set of fine-grained entity types due to unknown criteria for developing such a set. In particular, the optimal granularity of entity types depends on the particular p"
P12-1075,P11-1138,0,0.0150996,"DIRT calculates distributional similarities between different paths to find paths which bear the same semantic relation. It does not employ global topic model features extracted from documents and sentences. Local: This system uses our approach (both sense clustering with topic models and hierarchical clustering), but without global features. Local+Type This system adds entity type features to the previous system. This allows us to compare performance of using global features against entity type features. To determine entity types, we link named entities to Wikipedia pages using the Wikifier (Ratinov et al., 2011) package and extract categories from the Wikipedia page. Generally Wikipedia provides many types for one entity. For example, “Mozart” is a person, musician, pianist, composer, and catholic. As we argued in Section 1, it is difficult to determine the right granularity of the entity types to use. In our experiments, we use all of them as features. In hierarchical clustering, for each sense cluster of a path, we pick the most frequent entity type as a feature. This approach can be seen as a proxy to ISP (Pantel et al., 2007), since selectional preferences are one way of distinguishing multiple s"
P12-1075,P02-1006,0,0.105146,"several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial. 1 Introduction Relation extraction (RE) is the task of determining semantic relations between entities mentioned in text. RE is an essential part of information extraction and is useful for question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and"
P12-1075,D11-1130,0,0.0266903,"RT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs of named entities according to the similarity of context words intervening between them. Hachey (2009) uses topic models to perform dimensionality reduction on features when clustering entity pairs into relations. Bollegala et al. (2010) employ co-clustering to find"
P12-1075,D11-1048,0,0.10302,"e “play” relations between two teams, while a few of them express relations of teams acquiring players from 716 other teams. For example, the entity pair ”(Atlanta Hawks, Dallas Mavericks)” mentioned in sentence ”The Atlanta Hawks acquired point guard Anthony Johnson from the Dallas Mavericks.” This is due to that they share many entity pairs of team-team. 3.3 Baselines We compare our approach against several baseline systems, including a generative model approach and variations of our own approach. Rel-LDA: Generative models have been successfully applied to unsupervised relation extraction (Rink and Harabagiu, 2011; Yao et al., 2011). We compare against one such model: An extension to standard LDA that falls into the framework presented by Yao et al. (2011). Each document consists of a list of tuples. Each tuple is represented by features of the entity pair, as listed in 2.1, and the path. For each document, we draw a multinomial distribution over relations. For each tuple, we draw a relation topic and independently generate all the features. The intuition is that each document discusses one domain, and has a particular distribution over relations. In our experiments, we test different numbers of relati"
P12-1075,P10-1044,0,0.0335633,"ther approaches. Our work is closely related to DIRT (Lin and Pantel, 2001). Both DIRT and our approach represent dependency paths using their arguments. Both use distributional similarity to find patterns representing similar semantic relations. Based on DIRT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs"
P12-1075,P10-1045,0,0.0264497,"work is closely related to DIRT (Lin and Pantel, 2001). Both DIRT and our approach represent dependency paths using their arguments. Both use distributional similarity to find patterns representing similar semantic relations. Based on DIRT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs of named entities"
P12-1075,W04-3206,0,0.0142868,"del, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial. 1 Introduction Relation extraction (RE) is the task of determining semantic relations between entities mentioned in text. RE is an essential part of information extraction and is useful for question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollega"
P12-1075,D11-1135,1,0.926168,"pplications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into sets of equivalent or similar meaning. Whether we use sequences or dependency paths, we wil"
P12-1075,P08-1000,0,\N,Missing
P15-5005,D12-1087,0,0.0831903,"Missing"
P15-5005,P14-1130,0,0.0534323,"Missing"
P15-5005,D14-1162,0,0.0934629,"Missing"
P15-5005,N13-1008,1,0.708077,"Missing"
P15-5005,N15-1118,1,0.854412,"Missing"
P18-1196,S07-1103,0,0.01973,"ing (Beer, 2009) and can be used as evidence for fraud detection (Lu et al., 2006). 6 Related Work Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013). Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be. A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 20"
P18-1196,P16-1186,0,0.045661,"Missing"
P18-1196,P10-1133,0,0.0113025,"an be used as evidence for fraud detection (Lu et al., 2006). 6 Related Work Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013). Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be. A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our"
P18-1196,P10-1018,0,0.0314045,"Missing"
P18-1196,D15-1042,0,0.0222227,"t =exp(Htest) (3) 3 Strategies for Modelling Numerals In this section we describe models with different strategies for generating numerals and propose the 2105 use of number-specific evaluation metrics that adjust for the high out-of-vocabulary rate of numerals and account for numerical values. We draw inspiration from theories of numerical cognition. The triple code theory (Dehaene et al., 2003) postulates that humans process quantities through two exact systems (verbal and visual) and one approximate number system that semantically represents a number on a mental number line. Tzelgov et al. (2015) identify two classes of numbers: i) primitives, which are holistically retrieved from long-term memory; and ii) non-primitives, which are generated online. An in-depth review of numerical and mathematical cognition can be found in Kadosh and Dowker (2015) and Campbell (2005). 3.1 Softmax Model and Variants This class of models assumes that numerals come from a finite vocabulary that can be memorised and retrieved later. The softmax model treats all tokens (words and numerals) alike and directly uses Equation 1 with score function: ψ(st)=hTt estoken =hTt Eoutwst , t Softmax with Digit-Based Em"
P18-1196,P16-1154,0,0.0209557,"014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have models that copy from context sentences and from Wikipedia’s infoboxes, respectively. Ahn et al. (2016) proposed a LM that retrieves unknown words from facts in a knowledge graph. They draw attention to the inappropriateness of perplexity when OOV-rates are high and instead propose an adjusted perplexity metric that is equivalent to APP. Other methods aim at speeding up LMs to allow for larger vocabularies (Chen et al."
P18-1196,P16-1014,0,0.0291905,"l, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have models that copy from context sentences and from Wikipedia’s infoboxes, respectively. Ahn et al. (2016) proposed a LM that retrieves unknown words from facts in a knowledge graph. They draw attention to the inappropriateness of perplexity when OOV-rates are high and instead propose an adjusted perplexity metric that is equivalent to APP. Other methods aim at speeding up LMs to allow for larger voca"
P18-1196,P10-1030,0,0.0104908,"ramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations"
P18-1196,D14-1058,0,0.0153948,"Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu"
P18-1196,Q15-1042,0,0.0270457,"l relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summari"
P18-1196,P14-1026,0,0.0178768,"uch relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have models that copy from con"
P18-1196,D16-1128,0,0.0549127,"Missing"
P18-1196,W04-0902,0,0.403302,"Missing"
P18-1196,P16-1100,0,0.0609365,"Missing"
P18-1196,K15-1031,0,0.0481888,"Missing"
P18-1196,D09-1045,0,0.0777237,"Missing"
P18-1196,P16-1202,0,0.024781,"as a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehr"
P18-1196,D16-1209,0,0.0609435,"Missing"
P18-1196,P13-1038,0,0.0652432,", 2006). 6 Related Work Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013). Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be. A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather prod"
P18-1196,P11-2048,0,0.0137755,"numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers. Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such a"
P18-1196,D14-1162,0,0.080983,"Missing"
P18-1196,D17-1317,0,0.0154087,"ect the smoothness of the underlying continuous distribution of certain attributes. 1 Introduction Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017). Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that are both grammatical and realistic, as in this example: ‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic) ‘I eats an apple’ (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: ’John’s height is 1.75 metres’ (realistic) ’John’s height is 999.999 metres’ (unrealistic) Existing approaches to lan"
P18-1196,W17-5004,0,0.0166744,"s all out-ofvocabulary numerals to the same type, e.g. UNK, and does not reflect the smoothness of the underlying continuous distribution of certain attributes. 1 Introduction Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017). Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that are both grammatical and realistic, as in this example: ‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic) ‘I eats an apple’ (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: ’John’s height is 1.75 metres’ (realistic) ’Jo"
P18-1196,Q15-1001,0,0.146794,"Missing"
P18-1196,D15-1171,0,0.0214007,"rmation extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et"
P18-1196,D15-1135,0,0.013229,"s to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have mo"
P18-1196,D16-1029,0,0.0129758,"gument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the source in translation and summarisation tasks. Merity et"
P18-1196,D17-1090,0,0.0485178,"Missing"
P18-1196,D15-1096,0,0.0168061,"problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate. Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work. In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015) to copy unknown words from the s"
P18-1196,D16-1101,1,0.699027,"ls to the same type, e.g. UNK, and does not reflect the smoothness of the underlying continuous distribution of certain attributes. 1 Introduction Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017). Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that are both grammatical and realistic, as in this example: ‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic) ‘I eats an apple’ (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: ’John’s height is 1.75 metres’ (realistic) ’John’s height is 999.999 metr"
P18-1196,W16-6102,1,0.823753,"ls to the same type, e.g. UNK, and does not reflect the smoothness of the underlying continuous distribution of certain attributes. 1 Introduction Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017). Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that are both grammatical and realistic, as in this example: ‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic) ‘I eats an apple’ (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: ’John’s height is 1.75 metres’ (realistic) ’John’s height is 999.999 metr"
P18-1196,P15-1001,0,\N,Missing
P18-1201,W13-2322,0,0.048347,"for Computational Linguistics (Long Papers), pages 2160–2170 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output. patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack. We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top). Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline.2 We can see that a trigger and its event type name usually have some shared meaning. Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker. This observation matches the theory by Pustejovsky (1991): “the semantics of an event structure can be"
P18-1201,N07-4013,0,0.12327,"Missing"
P18-1201,P08-1004,0,0.122949,"Missing"
P18-1201,P15-2061,1,0.915193,"Missing"
P18-1201,P15-1017,0,0.661906,"Missing"
P18-1201,P16-2011,1,0.921764,"Missing"
P18-1201,C16-1017,0,0.0883192,"Missing"
P18-1201,P11-1163,0,0.389091,"Missing"
P18-1201,P11-1113,0,0.557271,"Missing"
P18-1201,P16-1025,1,0.911858,"Missing"
P18-1201,D09-1013,0,0.0959611,"Missing"
P18-1201,N16-1034,1,0.915997,"Missing"
P18-1201,P08-1030,1,0.902064,"Missing"
P18-1201,P15-2060,0,0.36822,"Missing"
P18-1201,P11-1115,1,0.767935,"Missing"
P18-1201,K17-1034,0,0.0861971,"Missing"
P18-1201,P13-1008,1,0.94998,"Missing"
P18-1201,P10-1081,0,0.668857,"Missing"
P18-1201,C10-2087,0,0.0604086,"Missing"
P18-1201,D16-1038,0,0.0734626,"Missing"
P18-1201,D16-1087,0,0.0557832,"Missing"
P18-1201,D11-1001,1,0.900064,"Missing"
P18-1201,P16-1201,0,0.0472642,"Missing"
P18-1201,P06-2094,0,0.0941745,"Missing"
P18-1201,P12-1088,0,0.128376,"Missing"
P18-1201,N06-1039,0,0.0621108,"Missing"
P18-1201,D13-1170,0,0.00421772,"xtraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set. These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort. Handling new event types may even entail starting over, without being able to re-use annotations from previous event types. To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL (Frome et al., 2013; Norouzi et al., 2013; Socher et al., 2013a). ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes. The resulting shared semantic space then allows for building a novel “zero-shot” classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases. We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles. We propose to enrich the semantic representations of each event mention"
P18-1201,W15-0812,0,0.0620811,"Missing"
P18-1201,N15-1040,0,0.0767342,"Missing"
P18-1201,P10-4014,0,0.0766931,"Missing"
P18-4005,D16-1244,0,0.221957,"ther popular MR task is Natural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al.,"
P18-4005,D16-1084,1,0.852145,"o quickly set up, load and run the existing systems for QA and NLI. The model training notebook demonstrates training, testing, evaluating and saving QA and NLI models programmatically. However, normally the user will simply use the provided training script from command line. The model implementation notebook delves deeper into implementing new models from scratch by writing all modules for a custom model. 7 Natural Language Inference. For NLI, we report results for our implementations of conditional BiLSTMs (cBiLSTM) (Rocktäschel et al., 2016), the bidirectional version of conditional LSTMs (Augenstein et al., 2016), the Decomposable Attention Model (DAM, Parikh et al., 2016) and Enhanced LSTM (ESIM, Chen et al., 2017). ESIM was entirely implemented as a modular NLI model, i.e. its architecture was purely defined in a configuration file – see Appendix A for more details. Our models or training configurations contain slight modifications from the original which we found to perform better than the original setup. Our results are slightly differ from those reported, since we did not always perform an exhaustive hyper-parameter search. Conclusion We presented Jack the Reader (JACK), a shared framework for Ma"
P18-4005,D16-1264,0,0.300836,"efore, existing input- and output modules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or"
P18-4005,D15-1075,0,0.0435049,"yping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: ea"
P18-4005,W15-4007,0,0.0320485,"e that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, represented as a relationship of type p between entities s and o, such as: (L ONDON, CAPITAL O F, UK). Realworld Knowledge Graphs, such as Freebase (Bo"
P18-4005,P17-1152,0,0.157395,"ural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various dataset"
P18-4005,W17-2623,0,0.0840343,"Missing"
P18-4005,P02-1022,0,0.0771673,"I N PUT , M ODEL and O UTPUT modules that compose a JTR EADER instance. On the right, the data format that is used to interact with a JTR EADER (dotted lines indicate that the component is optional). Related Work Machine Reading requires a tight integration of Natural Language Processing and Machine Learning models. General NLP frameworks include C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), O PEN NLP6 and SPAC Y. All these frameworks offer pre-built models for standard NLP preprocessing tasks, such as tokenisation, sentence splitting, named entity recognition and parsing. GATE (Cunningham et al., 2002) and UIMA (Ferrucci and Lally, 2004) are toolkits that allow quick assembly of baseline NLP pipelines, and visualisation and annotation via a Graphical User Interface. GATE can utilise NLTK and C ORE NLP models and additionally enable development of rule-based methods using a dedicated pattern language. UIMA offers a text analysis pipeline which, unlike GATE, also includes retrieving information, but does not offer its own rule-based language. It is further worth mentioning the Information Retrieval frameworks A PACHE L UCENE and A PACHE S OLR which can be used for building simple, keyword-bas"
P18-4005,K17-1028,1,0.844327,"63 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various datasets. Results are otlined in Table 3. Table 2: Accuracy on the SNLI test set achieved by cBiLSTM, DAM, and ESIM. Question Answering. For the Question Answering (QA) experiments we report results for our implementations of FastQA (Weissenborn et al., 2017), BiDAF (Seo et al., 2016) and, in addition, our own JackQA implementations. With JackQA we aim to provide a fast and accurate QA model. Both BiDAF and JackQA are realised using high-level architecture descriptions, that is, their architectures are purely defined within their respective configuration files. Results of our models on the SQuAD (Rajpurkar et al., 2016) development set along with additional run-time and parameter metrics are presented in Table 1. Apart from SQuAD, JACK supports the more recent NewsQA (Trischler et al., 2017) and TriviaQA (Joshi et al., 2017) datasets too. 6 Demo W"
P18-4005,N18-1101,0,0.0218508,"lthough we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, re"
P18-4005,P17-1147,0,0.188943,"odules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Kno"
P18-4005,P14-5010,0,0.00961028,"Missing"
P19-1484,D18-1399,0,0.0226208,"2010) to transform statements into questions (for English), we find their performance to be empirically weak for QA (see Section 3). Moreover, for specific domains or other languages, a substantial engineering effort will be required to develop similar algorithms. Also, whilst supervised models exist for this task, they require the type of annotation unavailable in this setting (Du et al. 2017; Du and Cardie 2018; Hosking and Riedel 2019, inter alia). We overcome this issue by leveraging recent progress in unsupervised machine translation (Lample et al., 2018, 2017; Lample and Conneau, 2019; Artetxe et al., 2018). In particular, we collect a large corpus of natural questions and an unaligned corpus of cloze questions, and train a seq2seq model to map between natural and cloze question domains using a combination of online back-translation and de-noising auto-encoding. In our experiments, we find that in conjunction with the use of modern QA model architectures, unsupervised QA can lead to performances surpassing early supervised approaches (Rajpurkar et al., 2016). We show that forms of cloze “translation” that produce (unnatural) questions via word removal and flips of the cloze question lead to bett"
P19-1484,D13-1160,0,0.0456864,"nswers. Semi-supervised QA Yang et al. (2017) train a QA model and also generate new questions for greater data efficiency, but require labelled data. Dhingra et al. (2018) simplify the approach and remove the supervised requirement for question generation, but do not target unsupervised QA or attempt to generate natural questions. They also make stronger assumptions about the text used for question generation and require Wikipedia summary paragraphs. Wang et al. (2018) consider 4903 semi-supervised cloze QA, Chen et al. (2018) use semi-supervision to improve semantic parsing on WebQuestions (Berant et al., 2013), and Lei et al. (2016) leverage semi-supervision for question similarity modelling. Finally, injecting external knowledge into QA systems could be viewed as semi-supervision, and Weissenborn et al. (2017) and Mihaylov and Frank (2018) use Conceptnet (Speer et al., 2016) for QA tasks. Question Generation has been tackled with pipelines of templates and syntax rules (Rus et al., 2010). Heilman and Smith (2010) augment this with a model to rank generated questions, and Yao et al. (2012) and Olney et al. (2012) investigate symbolic approaches. Recently there has been interest in question generati"
P19-1484,C18-1076,0,0.0476363,"Missing"
P19-1484,P18-1160,0,0.0472189,"Missing"
P19-1484,D14-1162,0,0.0806676,"Missing"
P19-1484,P18-2124,0,0.0351458,"ournament of each season but the Paris Sevens became the last stop on the calendar in 2018. Answer Extraction Answer 2018 Question Answering Question Generation Cloze Generation Cloze Question the Paris sevens become the last stop on the calendar in MASK QA Model Cloze Translation Natural Question When did the Paris Sevens become the last stop on the calendar? Figure 1: A schematic of our approach. The right side (dotted arrows) represents traditional EQA. We introduce unsupervised data generation (left side, solid arrows), which we use to train standard EQA models performance; For SQuAD 2.0 (Rajpurkar et al., 2018), ensembles based on BERT (Devlin et al., 2018) now match human performance. Even for the recently introduced Natural Questions corpus (Kwiatkowski et al., 2019), human performance is already in reach. In all these cases, very large amounts of training data are available. But, for new domains (or languages), collecting such training data is not trivial and can require significant resources. What if no training data was available at all? Introduction Extractive Question Answering (EQA) is the task of answering questions given a context document under the assumption that answers are spans of tok"
P19-1484,D16-1264,0,0.772705,"oduced Natural Questions corpus (Kwiatkowski et al., 2019), human performance is already in reach. In all these cases, very large amounts of training data are available. But, for new domains (or languages), collecting such training data is not trivial and can require significant resources. What if no training data was available at all? Introduction Extractive Question Answering (EQA) is the task of answering questions given a context document under the assumption that answers are spans of tokens within the given document. There has been substantial progress in this task in English. For SQuAD (Rajpurkar et al., 2016), a common EQA benchmark dataset, current models beat human In this work we address the above question by exploring the idea of unsupervised EQA, a setting in which no aligned question, context and answer data is available. We propose to tackle this by reduction to unsupervised question generation: If we had a method, without using QA supervision, to generate accurate questions given a context document, we could train a QA system using the generated questions. This approach allows us to directly 4896 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages"
P19-1484,W10-4234,0,0.014641,"for question generation and require Wikipedia summary paragraphs. Wang et al. (2018) consider 4903 semi-supervised cloze QA, Chen et al. (2018) use semi-supervision to improve semantic parsing on WebQuestions (Berant et al., 2013), and Lei et al. (2016) leverage semi-supervision for question similarity modelling. Finally, injecting external knowledge into QA systems could be viewed as semi-supervision, and Weissenborn et al. (2017) and Mihaylov and Frank (2018) use Conceptnet (Speer et al., 2016) for QA tasks. Question Generation has been tackled with pipelines of templates and syntax rules (Rus et al., 2010). Heilman and Smith (2010) augment this with a model to rank generated questions, and Yao et al. (2012) and Olney et al. (2012) investigate symbolic approaches. Recently there has been interest in question generation using supervised neural models, many trained to generate questions from c, a pairs in SQuAD (Du et al., 2017; Yuan et al., 2017; Zhao et al., 2018; Du and Cardie, 2018; Hosking and Riedel, 2019) 5 Discussion It is worth noting that to attain our best performance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser fo"
P19-1484,N10-1086,0,\N,Missing
P19-1484,H94-1020,0,\N,Missing
P19-1484,D18-1549,1,\N,Missing
P19-1484,D18-1546,0,\N,Missing
P19-1484,D18-1453,0,\N,Missing
P19-1484,D18-1424,0,\N,Missing
P19-1484,N19-1274,0,\N,Missing
P19-1484,N16-1153,0,\N,Missing
Q18-1021,D13-1160,0,0.223537,"Missing"
Q18-1021,W12-0705,0,0.023093,"taset for the domain of molecular biology – a field that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction incre"
Q18-1021,D14-1067,0,0.024069,"). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer an"
Q18-1021,D12-1118,0,0.0265152,"Missing"
Q18-1021,P16-1223,0,0.0689703,"Missing"
Q18-1021,E17-1013,0,0.0228804,"nefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediat"
Q18-1021,Q15-1015,0,0.0505894,"ion (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work aims to evaluate whether end-to-end multi-step RC models can indeed operate on raw text documents only – while performing the kind of inference most commonly associated with logical inference methods operating on structured knowledge. Text-Based Multi-Step Reading Comprehension Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneficial for reranking answers in open-domain non-factoid QA. Jansen et al. (2017) chain textual background resources for science exam QA and provide multisentence answer explanations. Beyond, a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on syntheti"
Q18-1021,D13-1080,0,0.0231409,"ude Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al"
Q18-1021,P16-1145,0,0.0987845,"Missing"
Q18-1021,N16-2016,0,0.0398816,"circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our"
Q18-1021,J17-2005,0,0.030324,"standing approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work aims to evaluate whether end-to-end multi-step RC models can indeed operate on raw text documents only – while performing the kind of inference most commonly associated with logical inference methods operating on structured knowledge. Text-Based Multi-Step Reading Comprehension Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneficial for reranking answers in open-domain non-factoid QA. Jansen et al. (2017) chain textual background resources for science exam QA and provide multisentence answer explanations. Beyond, a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2016). One common characteristic of neural multi-hop models 298 is their rich structure that enables matching and interaction between question, c"
Q18-1021,D17-1215,0,0.0359633,"ument set Sq . That is, Sq comprises chains of documents leading not only from the query subject to the correct answer candidate, but also to type-consistent false answer candidates. With this methodology, relevant textual evidence for (q, a∗ ) will be spread across documents along the chain connecting s and a∗ – ensuring that multihop reasoning goes beyond resolving co-reference within a single document. Note that including other type-consistent candidates alongside a∗ as end points in the graph traversal – and thus into the support documents – renders the task considerably more challenging (Jia and Liang, 2017). Models could otherwise identify a∗ in the documents by simply relying on type-consistency heuristics. It is worth pointing out that by introducing alternative candidates we counterbalance a type-consistency bias, in contrast to Hermann et al. (2015) and Hill et al. (2016) who instead rely on entity masking. 2 To determine entities which are type-consistent for a query q, we consider all entities which are observed as object in a fact with r as relation type – including the correct answer. 289 Entities s Documents KB (s, r, o) (s, r, o0 ) (s0 , r, o00 ) o o00 o0 Figure 2: A bipartite graph co"
Q18-1021,P17-1147,0,0.272977,"integrating cross-document information. 7 Related Work Related Datasets End-to-end text-based QA has witnessed a surge in interest with the advent of largescale datasets, which have been assembled based on F REEBASE (Berant et al., 2013; Bordes et al., 2015), W IKIPEDIA (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2016; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datasets are confined to single documents, and RC typically does not require a combination of multiple independent facts. In contrast, W IKI H OP and M ED H OP are specifically designed for cross-document RC and multistep inference. There exist other multi-hop RC resources, but they are either very limited in size, such as the FraCaS test suite, or based on synthetic language (Weston et al., 2016). TriviaQA partly involves multi-step reasoning, but the complexity largely stems from parsing compositional questions. Our datasets center around compositional inference from comparativel"
Q18-1021,P16-1086,0,0.04437,"Missing"
Q18-1021,W11-1802,0,0.0180635,"e total count of how often d co-occurs with c in a sample where c is also the correct answer. We use this statistic to filter the dataset, by discarding samples with at least one document-candidate pair (d, c) for which cooccurrence(d, c) &gt; 20. 4 M ED H OP Following the same general methodology, we next construct a second dataset for the domain of molecular biology – a field that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that h"
Q18-1021,D11-1049,0,0.025664,"nowledge resources which formulate facts using first-order logic. KB inference methods include Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the ou"
Q18-1021,K17-1034,0,0.0377532,"Missing"
Q18-1021,E17-1001,0,0.0177183,"a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2016). One common characteristic of neural multi-hop models 298 is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2017; Liu and Perez, 2017), which is often iterated over several times (Sordoni et al., 2016; Neumann et al., 2016; Seo et al., 2017b; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira a"
Q18-1021,P09-1113,0,0.224026,"Missing"
Q18-1021,D16-1199,0,0.0313126,"Missing"
Q18-1021,D16-1261,0,0.0184707,"2017b; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2018). While related, this work ultimately aims at reformulating queries to better acquire evidence documents, and not at answering queries through combining facts. 8 Conclusions and Future Work We have introduced a new cross-document multihop RC task, devised a generic dataset derivation strategy and applied it to two separate domains. The resulting datasets test RC methods in their ability to perform composite reasoning – something thus far limited to models operating on structured knowledge resources. In our experiments we found that contemporary RC mo"
Q18-1021,P15-1016,0,0.0125112,"rom limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sides"
Q18-1021,D17-1061,0,0.0242707,"and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2018). While related, this work ultimately aims at reformulating queries to better acquire evidence documents, and not at answering queries through combining facts. 8 Conclusions and Future Work We have introduced a new cross-document multihop RC task, devised a generic dataset derivation strategy and applied it to two separate domains. The resulting datasets test RC methods in their ability to perform composite reasoning – something thus far limited to models operating on structured knowledge resources. In our experiments we found that contemporary RC models can leverage cross-"
Q18-1021,D16-1241,0,0.0816821,"Missing"
Q18-1021,P16-1144,0,0.0609488,"Missing"
Q18-1021,Q17-1008,0,0.0208216,"t supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separate pieces of evidence. The promise of multi-hop methods is finding and combining individual observations that can suggest previously unobserved DDIs, aiding the process of making scientific discoveries, yet not directly from experiments, but by inferring them from established public knowledge (Swanso"
Q18-1021,D14-1162,0,0.0805393,"Missing"
Q18-1021,D16-1264,0,0.302893,"(Weissenborn et al., 2017), which have shown a robust performance across several datasets. These models predict an answer span within a single document. We adapt them to a multidocument setting by sequentially concatenating all d ∈ Sq in random order into a superdocument, adding document separator tokens. During training, the first answer mention in the concatenated document serves as the gold span.4 At test time, we measured accuracy based on the exact match between the prediction and answer, both lowercased, after removing articles, trailing white spaces and punctuation, in the same way as Rajpurkar et al. (2016). To rule out any signal stemming from the order of documents in the superdocument, this order is randomized both at training and test time. In a preliminary experiment we also trained models using different random document order permutations, but found that performance did not change significantly. 4 We also tested assigning the gold span randomly to any one of the mention of the answer, with insignificant changes. For BiDAF, the default hyperparameters from the implementation of Seo et al. (2017a) are used, with pretrained GloVe (Pennington et al., 2014) embeddings. However, we restrict the"
Q18-1021,D08-1009,0,0.09811,"Missing"
Q18-1021,D10-1106,0,0.0317284,"erence goes beyond resolving co-reference. Compositional Knowledge Base Inference Combining multiple facts is common for structured knowledge resources which formulate facts using first-order logic. KB inference methods include Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a"
Q18-1021,K17-1004,0,0.0227403,"Missing"
Q18-1021,S13-2056,0,0.0216875,"Missing"
Q18-1021,W11-1816,1,0.851112,"Missing"
Q18-1021,K17-1028,0,0.295469,"n (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separate pieces of evidence. The promise of multi-hop methods is finding and combining individual observations that can suggest previously unobserved DDIs, aiding the process of making scientific discoveries, yet not directly from experiments, but by inferring them from established public knowledge (Swanso"
Q18-1021,W17-4413,1,0.919328,"that the number of candidates differs between samples. Max-mention Predicts the most frequently mentioned candidate in the support documents Sq of a sample – randomly breaking ties. Majority-candidate-per-query-type Predicts the candidate c ∈ Cq that was most frequently observed as the true answer in the training set, given the query type of q. For W IKI H OP, the query type is the property p of the query; for M ED H OP there is only the single query type – interacts with. TF-IDF Retrieval-based models are known to be strong QA baselines if candidate answers are provided (Clark et al., 2016; Welbl et al., 2017). They 294 search for individual documents based on keywords in the question, but typically do not combine information across documents. The purpose of this baseline is to see if it is possible to identify the correct answer from a single document alone through lexical correlations. The model forms its prediction as follows: For each candidate c, the concatenation of the query q with c is fed as an OR query into the whoosh text retrieval engine. It then predicts the candidate with the highest TF-IDF similarity score: arg max[max(TF-IDF(q + c, s))] c∈Cq s∈Sq (1) Document-cue During dataset cons"
Q18-1021,W17-2309,0,0.011721,"an et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separa"
Q18-1021,D15-1237,0,0.112307,"Missing"
S17-2091,P14-1119,0,0.153552,"uction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and informat"
S17-2091,P17-2054,1,0.878568,"pplication domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities. 1 Introduction Empirical"
S17-2091,W09-3611,0,0.0304788,"yphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym r"
S17-2091,D15-1086,1,0.830685,"ion would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 e"
S17-2091,W10-2924,0,0.0334212,"on extraction, and through hypernym prediction would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task"
S17-2091,S17-2168,0,0.107723,"Missing"
S17-2091,S17-2167,0,0.0368744,"Missing"
S17-2091,S17-2173,0,0.0252701,"Missing"
S17-2091,C10-1065,0,0.403289,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,S10-1004,0,0.794457,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,D15-1235,0,0.0152438,"om/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task"
S17-2091,W16-5904,0,0.0307461,"ticles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We"
S17-2091,S17-2166,0,0.0299819,"and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 noun phrases generated from the titles and abstracts derived from the Web Of Science corpora9 of the domains Physics, Computer Science, Chemistry and Computer Science. These corpora cannot be distributed publicly and were made available by the IARPA funding agency. Annotation was performed by 3 annotators using 14 fine-grained types, including PROCESS. We measured inter-annotator agreement among the three annotators for the 14 categories using Fleiss’ Kappa. The k value was found to be 0.28"
S17-2091,Q15-1010,0,0.0248553,"cscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task organisers’ collab"
S17-2091,D15-1057,0,0.0405374,"ion and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be"
S17-2091,I11-1001,0,0.680204,"led here is mention-level identification and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific"
S17-2091,S17-2169,0,0.0534923,"Missing"
S17-2091,P09-1113,0,0.0777004,"elines were refined.8 Annotation Process Mention-level annotation is very time-consuming, and only a handful of semantic relations such as hypernymy and synonymy can be found in each publication. We therefore only annotate paragraphs of publications likely to contain relations. We originally intended to identify suitable documents by automatically extracting a knowledge graph of relations from a large scientific dataset using Hearst-style patterns (Hearst, 1991; Snow et al., 2005), then using those to find potential relations in a distinct set of documents, similar to the distant supervision (Mintz et al., 2009; Snow et al., 2005) heuristic. Documents containing a high number of such potential relations would then be selected. However, this requires automatically learning to identify keyphrases between which those potential relations hold, and requires relations to appear several times in a dataset for such a knowledge graph to be useful. In the end, this strategy was not feasible due to the difficulty of learning to detect keyphrases automatically and only a small overlap between relations in different documents. Instead, keyphrasedense paragraphs were detected automatically using a coarse unsuperv"
S17-2091,C14-1002,0,0.0452535,"Missing"
S17-2091,S17-2172,0,0.092929,"Missing"
S17-2091,L16-1294,0,0.0874962,"In contrast to what we propose, the annotations are more fine-grained and annotations are only available for abstracts. Gupta and Manning (2011) studied keyphrase extraction from ACL Anthology articles, applying a pattern-based bootstrapping approach based on 15 016 documents and assigning the types FOCUS, TECHNIQUE and DOMAIN. Performance was evaluated on 30 manually annotated documents. Although the latter corpus is related to what we propose, manual annotation is only available for a small number of documents and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 nou"
S17-2091,S17-2161,0,0.0445569,"Missing"
S17-2091,W12-3201,0,0.0249455,".co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publication"
S17-2091,R09-1086,0,0.0762443,"Missing"
S17-2091,D15-1175,0,0.0291535,"nition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impac"
S17-2091,D16-1198,0,0.0680123,"as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communi"
S17-2091,tateisi-etal-2014-annotation,0,\N,Missing
S17-2091,S17-2170,0,\N,Missing
W06-1616,E06-1011,0,0.155752,"mance across multiple languages. However, within this framework one can only define features over single attachment decisions. This leads to cases where basic linguistic constraints are not satisfied (e.g. verbs with two subjects or incompatible coordination arguments). An example of this for Dutch is illustrated in Figure 2 which was produced by the parser of McDonald et al. (2005b). Here the parse contains a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. Th"
W06-1616,H05-1066,0,0.143177,"ation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section we introduce dependency parsing and review previous work. In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints. We include details of an incremental algorithm used to solve this formulation. Our experimental set-up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments. We finally discuss fuAbstract Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global co"
W06-1616,W06-2932,0,0.0163499,"Missing"
W06-1616,W04-2407,0,0.0895643,"ere the parse contains a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. ture research and potential improvements to our approach. 2 Dependency Parsing Dependency parsing is the task of attaching words to their arguments. Figure 1 shows a dependency graph for the Dutch sentence “I’ll come at twelve and then you’ll get what you deserve” (taken from the Alpino Corpus (van der Beek et al., 2002)). In this dependency g"
W06-1616,W04-2401,0,0.358633,"Missing"
W06-1616,P04-1054,0,0.00627623,"s. For example, it must not contain dependency chains such as “en” → “kom” → “ik” → “en”. For a more formal definition see previous work (Nivre et al., 2004). An important distinction between dependency trees is whether they are projective or nonprojective. Figure 1 is an example of a projective dependency tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frame3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): X s(i, j, l) s(x, y) = (i,j,l)∈y = X w · f (i"
W06-1616,W96-0102,0,0.0326403,"Missing"
W06-1616,W03-3023,0,0.0562809,"ns a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. ture research and potential improvements to our approach. 2 Dependency Parsing Dependency parsing is the task of attaching words to their arguments. Figure 1 shows a dependency graph for the Dutch sentence “I’ll come at twelve and then you’ll get what you deserve” (taken from the Alpino Corpus (van der Beek et al., 2002)). In this dependency graph the verb “kom” is attach"
W06-1616,P05-1067,0,0.0282582,"such as “en” → “kom” → “ik” → “en”. For a more formal definition see previous work (Nivre et al., 2004). An important distinction between dependency trees is whether they are projective or nonprojective. Figure 1 is an example of a projective dependency tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frame3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): X s(i, j, l) s(x, y) = (i,j,l)∈y = X w · f (i, j, l) (i,j,l)∈y 2 Note that this is not descri"
W06-1616,P01-1030,0,0.258448,"nditional random fields (Roth and Yih, 2004), this has allowed the use of truly global constraints during inference. However, it is not possible to use this approach directly for a complex task like non-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph. To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently (Williams, 2002). A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section w"
W06-1616,P05-1012,0,0.17097,"ation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section we introduce dependency parsing and review previous work. In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints. We include details of an incremental algorithm used to solve this formulation. Our experimental set-up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments. We finally discuss fuAbstract Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global co"
W06-2934,E06-1011,0,0.126581,"less accuracy, 2) noisy POS tags and 3) occasionally our inference algorithm was too slow and decoding timed out. 1 Introduction This paper presents our submission for the CoNLL 2006 shared task of multilingual dependency parsing. Our parser is inspired by McDonald et al.(2005a) which treats the task as the search for the highest scoring Maximum Spanning Tree (MST) in a graph. This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. In their original framework, features are only defined over single attachment decisions. This leads to cases where basic linguistic constraints are not satisfied (e.g. verbs with two subjects). In this paper we present a novel way to implement the parsing algorithms for projective and non-projective parsing based on a more generic incremental Integer Linear Programming (ILP) approach. This allows us to include additional global constraints that can be used to impose linguistic information. The rest"
W06-2934,P05-1012,0,0.520288,"mming model and how we trained its parameters. We then describe our feature and constraint sets for the 12 different languages of the task (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003). Finally, our results are discussed and error analyses for Chinese and Turkish are presented. 2 Model Our model is based on the linear model presented in McDonald et al. (2005a), (1) s (x, y) = X s (i, j) = X w · f (i, j) (i,j)∈y where x is a sentence, y a parse and s a score function over sentence-parse pairs. f (i, j) is a multidi226 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 226–230, New York City, June 2006. 2006 Association for Computational Linguistics 1. 2. 3. 4. 5. 6. mensional feature vector representation of the edge from token i to token j and w the corresponding weight vector. Decoding in this model amounts to finding the y for a given x that maximises s (x, y) y 0 = argmaxy s (x, y) Figure 1: Increm"
W06-2934,H05-1066,0,0.508594,"mming model and how we trained its parameters. We then describe our feature and constraint sets for the 12 different languages of the task (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003). Finally, our results are discussed and error analyses for Chinese and Turkish are presented. 2 Model Our model is based on the linear model presented in McDonald et al. (2005a), (1) s (x, y) = X s (i, j) = X w · f (i, j) (i,j)∈y where x is a sentence, y a parse and s a score function over sentence-parse pairs. f (i, j) is a multidi226 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 226–230, New York City, June 2006. 2006 Association for Computational Linguistics 1. 2. 3. 4. 5. 6. mensional feature vector representation of the edge from token i to token j and w the corresponding weight vector. Decoding in this model amounts to finding the y for a given x that maximises s (x, y) y 0 = argmaxy s (x, y) Figure 1: Increm"
W06-2934,dzeroski-etal-2006-towards,0,\N,Missing
W06-2934,W03-2405,0,\N,Missing
W06-2934,afonso-etal-2002-floresta,0,\N,Missing
W08-2125,W05-0625,0,0.101707,"his yields a three-stage pipeline: predicate identification, argument identification and argument classification. Our system, on the other hand, follows a joint approach in the spirit of Toutanova et al. (2005) and performs the above steps collectively . We decided to use Markov Logic (ML, Richardson and Domingos, 2005), a First Order Probabilistic Language, to develop a global probabilistic model of SRL. By using ML we are able to incorporate the dependencies between the decisions of different stages in the pipeline and the well-known global correlations between the arguments of a predicate (Punyakanok et al., 2005). And since learning c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Note that in this work we do not consider the parsing task; instead we use the provided dependencies of the open track datatsets. and inference methods were already implemented in the ML software we use, only minimal engineering efforts had to be done. In contrast to the work of Toutanova et al. (2005) our system applies online learning to train its parameters and exact inference to predict a col"
W08-2125,W08-2121,0,0.106017,"Missing"
W08-2125,P05-1073,0,0.154791,"Missing"
W08-2125,W04-3212,0,0.0931836,"m. For example, a grounding of the local formula lemma(p, +l1 )∧lemma(a, +l2 ) ⇒ hasRole(p, a) 194 the (deterministic) formula role(p, a, r) ⇒ hasRole(p, a) Figure 1: Factor graph for the local formula in section 3.1. can be seen in the Markov Network of Figure 1. It connects a hidden hasRole ground atom to two observed lemma ground atoms. Note that the “+” prefix for variables indicates that there is a different weight for each possible pair of lemmas (l1 , l2 ). For the hasRole and role predicates we defined local formulae that aimed to reproduce the standard features used in previous work (Xue and Palmer, 2004). This also required us to develop dependency-based versions of the constituentbased features such as the syntactic path between predicate and argument, as proposed by Xue and Palmer (2004). The remaining hidden predicates, isPredicate, isArgument and sense, have local formulae that relate their ground atoms to properties of a contextual window around the token the atom corresponds to. For this we used the information provided in the closed track training corpus of the shared task (i.e. both versions of lemma and POS tags plus a coarse version of the POS tags). Instead of describing the local"
W09-1213,burchardt-etal-2006-salsa,0,0.118055,"Missing"
W09-1213,W09-1201,0,0.103444,"Missing"
W09-1213,kawahara-etal-2002-construction,0,0.0482619,"Missing"
W09-1213,W08-2121,0,0.0759509,"Missing"
W09-1213,taule-etal-2008-ancora,0,0.0764267,"Missing"
W09-1213,W04-3212,0,0.0942,"ll refer to these predicates as the token predicates. The second set extends the information provided in the closed track corpus: cpos/2 is a coarse POS tag (first letter of actual POS tag); possibleArg/1 is true if the POS tag the token is a potential SRL argument POS tag (e.g., PUNC is not); voice/2 denotes the voice for verbal tokens based on heuristics that use syntactic information, or based on features in the FEAT column of the data. We will refer to these predicates as the extended predicates. Finally, the third set represents dependency information inspired by the features proposed by Xue and Palmer (2004). There are two types of predicates in this set: paths and frames. Paths capture the dependency path between two tokens, and frames the subcategorisation frame for a token or a pair of tokens. There are directed and undirected versions of 86 paths, and labelled (with dependency relations) and unlabelled versions of paths and frames. Finally, we have a frame predicate with the distance from the predicate to its head. We will refer to the paths and most of the frames predicates as the path predicates, while we will consider the frame predicates for a unique token part token predicates. The ML pr"
W09-1406,W08-2121,0,0.0653558,"Missing"
W09-1406,W09-1401,1,0.118204,"Missing"
W09-1406,W08-2125,1,0.57079,"formulations of Select all roles for mantic Role Labelling (Surdeanu et al., 2008). assignment of argument tokens to these roles. Hence it may be possible to re-use or adapt the For a non-binding event clue c, c c in we rst coland then create one event per If we would re-convert C and L from equation successful approaches in SRL in order to improve 2 and 3, respectively, we could return to our origbio-molecular event extraction. inal event structure in gure 1. Since our apHowever, conproach is inspired by the Markov Logic role laverting back and forth is not loss-free in general. beller in (Riedel and Meza-Ruiz, 2008), this work For example, if we have a non-binding event in can be seen as an attempt in this direction. the original For a sentence with given P, L rithm 1 presents our mapping from E , algoE to (L, C). and For brevity we omit a more detailed description E set with two arguments A and B with the same role Theme, the round-trip conversion would generate two events: one with A as Theme and one with B as Theme. of the algorithm. Note that for our running example eventsToLinks would return 4 C = {(1, neg_reg) , (2, pos_reg) , (5, gene_expr)} Markov Logic (Richardson and Domingos, 2006) (2) 43 Mark"
W09-1406,J07-4004,0,\N,Missing
W09-1406,N09-1018,1,\N,Missing
W09-1406,P05-1022,0,\N,Missing
W09-1406,P08-2026,0,\N,Missing
W11-1807,W09-1402,0,0.705552,"ve results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where"
W11-1807,P05-1022,0,0.0128587,"g δt,Bind p,q µi,p,q ; for the case that j ∈ Prot (x) we P arg1 P arg2 def get cout i,j,r (λ, µ) = λi,j,r + p µi,j,p + q µi,q,j , def out otherwise ci,j,r (λ, µ) = λi,j,r . For bestBind (c) trig arg1 arg2 we set cbind i,p,q (µ) = −µi,p,q − µi,,p,q − µi,,p,q . 3.3 Preprocessing After basic tokenization and sentence segmentation, we generate a set of protein head tokens Prot (x) for each sentence x based on protein span definitions from the shared task. To ensure tokens contain not more than one protein we split them at protein boundaries. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing 1 We refer to Koo et al. (2010) for details on how to set αt . Task 1 Task 1 (abst.) Task 1 (full) Task 2 SVT 73.5 71.5 79.2 71.4 BIND 48.8 50.8 44.4 38.6 REG 43.8 45.5 40.1 39.1 TOT 55.2 56.1 53.1 51.0 Table 1: Results for the GE track, task 1 and 2; abst.=abstract; full=full text. model of McClosky and Charniak (2008). Finally, based on the set of trigger words in the training data, we generate a set of candidate triggers Trig (x). 4 Results We apply the same model to the GE, ID and EPI tracks, with minor modifications in order to deal with the diffe"
W11-1807,W11-1801,0,0.187225,"Missing"
W11-1807,W11-1802,0,0.415252,"rs and outgoing arguments, (b) event triggers and incoming arguments and (c) protein-protein bindings. For efficient decoding we employ dual decomposition. Our results are very competitive: With minimal adaptation of our model we come in second for two of the tasks—right behind a version of the system presented here that includes predictions of the Stanford event extractor as features. We also show that for the Infectious Diseases task using data from the Genia track is a very effective way to improve accuracy. 1 Introduction This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al., 2011a). We introduce a simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should s"
W11-1807,D10-1125,0,0.0124758,"Missing"
W11-1807,P08-2026,0,0.0395691,"d tokens Prot (x) for each sentence x based on protein span definitions from the shared task. To ensure tokens contain not more than one protein we split them at protein boundaries. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing 1 We refer to Koo et al. (2010) for details on how to set αt . Task 1 Task 1 (abst.) Task 1 (full) Task 2 SVT 73.5 71.5 79.2 71.4 BIND 48.8 50.8 44.4 38.6 REG 43.8 45.5 40.1 39.1 TOT 55.2 56.1 53.1 51.0 Table 1: Results for the GE track, task 1 and 2; abst.=abstract; full=full text. model of McClosky and Charniak (2008). Finally, based on the set of trigger words in the training data, we generate a set of candidate triggers Trig (x). 4 Results We apply the same model to the GE, ID and EPI tracks, with minor modifications in order to deal with the different event type sets T and role sets R of each track. Training and testing together took between 30 (EPI) to 120 (GE) minutes using a singlecore implementation. DEV DEV DEV DEV TEST I/G 1/0 0/1 1/1 2/1 2/1 BIND 18.6 18.2 20.0 20.0 34.6 REG 27.1 26.8 33.1 34.5 46.4 PRO 34.3 0.00 49.3 52.0 62.3 TOT 41.5 35.5 47.2 48.5 53.4 Table 2: ID results for different amount"
W11-1807,P11-1163,0,0.283669,"pproach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system. For the Epigenetics and Posttranslational Modifications (EPI) track (Ohta et al., 2011) we achieve the 4th rank, partly because we did not aim to extract speculations, negations or cellular locations. Finally, for Genia Task 2 we rank 3rd— with the 1st rank achieved by the FAUST system. In the following we will briefly describe our model and inference algorithm, as far as this is possible in lim"
W11-1807,W11-1806,0,0.625522,"Missing"
W11-1807,W11-1803,0,0.103825,"Missing"
W11-1807,N10-1123,0,0.338117,"ion. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system"
W11-1807,W11-1804,0,0.10001,"Missing"
W11-1807,W09-1406,1,0.740819,"racks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second"
W11-1807,W11-1808,1,0.647704,"ods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system. For the Epigenetics and Posttranslational Modifications (EPI) track (Ohta et al., 2011) we achieve the 4th rank, partly because we did not aim to extract speculations, negations or cellular locations. Finally, for Genia Task 2 we rank 3rd— with the 1s"
W11-1807,D10-1001,0,0.0229349,"ay to improve accuracy. 1 Introduction This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al., 2011a). We introduce a simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact"
W11-1808,W09-1401,0,0.534253,"Missing"
W11-1808,W11-1801,0,0.421255,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1802,0,0.36214,"ctions on test and development sets we used models learned from the the complete training set. Predictions over training data were produced using crossvalidation. This helps to avoid a scenario where the stacking model learns to rely on high accuracy at training time that cannot be matched at test time. Note that, unlike Stanford’s individual submission in this shared task, the stacked models in this paper do not include the Stanford reranker. This is because it would have required making a reranker model for each crossvalidation fold. We made 19 crossvalidation training folds for Genia (GE) (Kim et al., 2011b), 12 for Epigenetics (EPI), and 17 for Infectious Diseases (ID) (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011, respectively). Note that while ID is the smallest and would seem like it would have the fewest folds, we combined the training data of ID with the training and development data from GE. To produce predictions over the test data, we combined the training folds with 6 development folds for GE, 4 for EPI, and 1 for ID. 3 Experiments Table 1 gives an overview of our results on the test sets for all four tasks we submitted to. Note that for the EPI and ID tasks we show the"
W11-1808,P11-1163,1,0.535908,"b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different"
W11-1808,W11-1806,1,0.588093,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,H05-1066,0,0.0337346,") finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different (and thus more useful for stacking) results. Of particular interest to this paper are the four possible decoders in MSTParser. These four decoders come from combinat"
W11-1808,P08-1108,0,0.0519048,"Missing"
W11-1808,W11-1803,0,0.278435,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1804,0,0.36653,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1807,1,0.818921,"edictions from the Stanford system into the UMass system (e.g., as in Nivre and McDonald (2008)). This has the advantage that one model (Umass) determines how to integrate the outputs of the other model (Stanford) into its own structure, whereas in reranking, for example, the combined model is required to output a complete structure produced by only one of the input models. 2 Approach In the following we briefly present both the stacking and the stacked model and some possible ways of integrating the stacked information. 2.1 Stacking Model As our stacking model, we employ the UMass extractor (Riedel and McCallum, 2011). It is based on a discriminatively trained model that jointly predicts trigger labels, event arguments and protein pairs in 51 Proceedings of BioNLP Shared Task 2011 Workshop, pages 51–55, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics binding. We will briefly describe this model but first introduce three types of binary variables that will represent events in a given sentence. Variables ei,t are active if and only if the token at position i has the label t. Variables ai,j,r are active if and only if there is an event with trigger i that has an argument"
W11-1808,D10-1001,0,0.0101169,"(p, q) to include predictions from the systems to be stacked. For example, for every system S to be stacked and every pair of event types (t0 , tS ) we add the features ( 1 hS (i) = tS ∧ t0 = t fS,t0 ,tS (i, t) = 0 otherwise 52 to fT (i, t). Here hS (i) is the event label given to token i according to S. These features allow different weights to be given to each possible combination of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to d"
W11-1808,N10-1091,1,0.0608368,"Missing"
W12-3022,P09-1113,0,0.130763,"icized-by can help predict the profession information in Freebase. Moreover, often users of the database will not need to study a particular schema—they can use their own expressions (say, works-at instead of profession) and still find the right answers. In the previous scenario we could answer more questions than our structured sources alone, because we learn how to predict new Freebase rows. We could answer more questions than the text corpus and OpenIE alone, because we learn how to predict new rows in surface pattern tables. We could also answer more questions than in Distant Supervision (Mintz et al., 2009), because our schema is not limited to the relations in the structured source. We could even go further and import additional structured sources, such as Yago (Hoffart et al., 2012). In this case the probabilistic database would have integrated, and implicitly aligned, several different data sources, in the sense that each helps predict the rows of the other. In this paper we present results of our first technical approach to probabilistic databases with universal schema: collaborative filtering, which has been successful in modeling movie recommendations. Here each entity tuple explicitly “ra"
W12-3022,N07-1071,0,0.054552,"o note that it is easy to incorporate entity representations into the approach, and model selectional preferences. Likewise, we can easily add posterior constraints we know to hold across relations, and learn from unlabeled data. 3 Related Work We briefly review related work in this section. Open IE (Etzioni et al., 2008) extracts how entities and their relations are actually mentioned in text, but does not predict how entities could be mentioned otherwise and hence suffer from reduced recall. There are approaches that learn synonym relations between surface patterns (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001; Yao et 118 al., 2011) to overcome this problem. Fundamentally, these methods rely on a symmetric notion of synonymy in which certain patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed gPCA algorithm (Schoenmackers et al., 2008; Schoenmackers et al., 2010). Such methods learn the structure of a Markov"
W12-3022,D08-1009,0,0.165722,"Missing"
W12-3022,D10-1106,0,0.208632,"ns between surface patterns (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001; Yao et 118 al., 2011) to overcome this problem. Fundamentally, these methods rely on a symmetric notion of synonymy in which certain patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed gPCA algorithm (Schoenmackers et al., 2008; Schoenmackers et al., 2010). Such methods learn the structure of a Markov Network, and are ultimately bounded by limits on tree-width and density. In contrast, the gPCA learns a latent, although not necessarily interpretable, structure. This latent structure can express models of very high treewidth, and hence very complex rules, without loss in efficiency. Moreover, most rule learners work in batch mode while our method continues to learn new associations with the arrival of new data. 4 Experiments Our work aims to predict new rows of source tables, where tables correspond to either surface patterns in natural language"
W12-3022,D11-1135,1,0.905496,"Missing"
W14-2409,N13-1008,1,0.326335,"have often shown low recall (e.g. Bos and Markert, 2005) as they are affected by the limited coverage of ontologies such as WordNet. Moreover, due to their deterministic nature they often cannot cope with noise and uncertainty inherent to real world data, and inference with such representations is difficult to scale up. Embedding-based approaches address some of the concerns above. Here relational worlds are described using low-dimensional embeddings of entities and relations based on relational evidence in knowledge bases (Bordes et al., 2011) or surfaceform relationships mentioned in text (Riedel et al., 2013). To overcome the generalization bottleneck, these approaches learn to embed similar entities and relations as vectors close in distance. Subsequently, unseen facts can be inferred by simple and efficient linear algebra operations (e.g. dot products). The core argument against embeddings is their supposed inability to capture deeper semantics, and more complex patterns of reasoning such as those enabled by first-order logic (Lewis and Steedman, 2013). Here we argue that this does not need to be true. We present an approach that enables us to learn low-dimensional embeddings such that the model"
W14-2409,D12-1110,0,0.00765774,"we optimize this objective with SGD to learn low-dimensional embeddings that indeed follow the behavior of the knowledge base. 6 Related Work The idea of bringing together distributional semantics and formal logic is not new. Lewis and Steedman (2013) improve the generalization performance of a semantic parser via the use of distributional representations. However, their target representation language is still symbolic, and it is unclear how this approach can cope with noise and uncertainty in data. Another line of work (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Socher et al., 2012; Hermann and Blunsom, 2013) uses symbolic representations to guide the composition of distributional representations. Reading a sentence or logical formula there amounts to compositionally mapping it to a k-dimensional vector that then can be used for downstream tasks. We propose a very different approach: Reading a sentence amounts to updating the involved entity pair and relation embeddings such that the sentence evaluates to true. Afterwards we cannot use the embeddings to calculate sentence similarities, but to answer relational questions about the world. Similar to our work, Bowman (2014"
W14-2409,H05-1079,0,0.216959,"Missing"
W14-2409,S13-1001,0,0.371733,"ion where the number of dimensions equals to the number of entities in the domain. worksFor A C Algebra worksFor(B) worksFor(B) D Figure 1: Information extraction (IE) and semantic parsing (SP) extract factual and more general logical statements from text, respectively. Humans can manually curate this knowledge. Instead of reasoning with this knowledge directly (A) we inject it into low dimensional representations of entities and relations (B). Linear algebra operations manipulate embeddings to derive truth vectors (C), which can be discretized or thresholded to retrieve truth values (D). 2.1 Grefenstette (2013) presents an isomorphism between statements in predicate logic and expressions in tensor calculus. Let [·] denote this mapping from a logical expression F to an expression in tensor algebra. Here, logical statements evaluating to true or false are mapped to [true] :=  T  T > = 1 0 and [false] := ⊥ = 0 1 respectively. Entities are represented by logical constants and mapped to one-hot vectors where each component represents a unique entity. For example, let k = 3 be the number of entities in a domain, then  T S MITH may be mapped to [S MITH] = 1 0 0 . Unary predicates are represented as 2"
W14-2409,P13-1088,0,0.0261226,"ective with SGD to learn low-dimensional embeddings that indeed follow the behavior of the knowledge base. 6 Related Work The idea of bringing together distributional semantics and formal logic is not new. Lewis and Steedman (2013) improve the generalization performance of a semantic parser via the use of distributional representations. However, their target representation language is still symbolic, and it is unclear how this approach can cope with noise and uncertainty in data. Another line of work (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Socher et al., 2012; Hermann and Blunsom, 2013) uses symbolic representations to guide the composition of distributional representations. Reading a sentence or logical formula there amounts to compositionally mapping it to a k-dimensional vector that then can be used for downstream tasks. We propose a very different approach: Reading a sentence amounts to updating the involved entity pair and relation embeddings such that the sentence evaluates to true. Afterwards we cannot use the embeddings to calculate sentence similarities, but to answer relational questions about the world. Similar to our work, Bowman (2014) provides further evidence"
W14-2409,Q13-1015,0,0.188105,"ddings of entities and relations based on relational evidence in knowledge bases (Bordes et al., 2011) or surfaceform relationships mentioned in text (Riedel et al., 2013). To overcome the generalization bottleneck, these approaches learn to embed similar entities and relations as vectors close in distance. Subsequently, unseen facts can be inferred by simple and efficient linear algebra operations (e.g. dot products). The core argument against embeddings is their supposed inability to capture deeper semantics, and more complex patterns of reasoning such as those enabled by first-order logic (Lewis and Steedman, 2013). Here we argue that this does not need to be true. We present an approach that enables us to learn low-dimensional embeddings such that the model behaves as if it follows a complex first-order reasoning process—but still operates in terms of simple vector and matrix representations. In this view, machine reading becomes the process of taking (inherently symbolic) knowledge in language and injecting this knowledge into a sub-symbolic distributional world model. For example, one could envision a semantic parser that turns a sentence into a first-order logic statement, Many machine reading appro"
W14-2409,P09-1113,0,0.0122529,"is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic. 1 Introduction Much of the work in machine reading follows an approach that is, at its heart, symbolic: language is transformed, possibly in a probabilistic way, into a symbolic world model such as a relational database or a knowledge base of first-order formulae. For example, a statistical relation extractor reads texts and populates relational tables (Mintz et al., 2009). Likewise, a semantic parser can turn sentences into complex first-order logic statements (Zettlemoyer and Collins, 2005). Several properties make symbolic representations of knowledge attractive as a target of machine reading. They support a range of well understood symbolic reasoning processes, capture semantic concepts such as determiners, negations 45 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45–49, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics Evidence Logic IE Embedded Logic worksFor(A), profAt(A) profAt(B) ... SP B profAt"
W14-2409,W08-2222,0,\N,Missing
W14-2409,S13-1002,0,\N,Missing
W14-2409,W11-0112,0,\N,Missing
W14-2508,S13-1004,0,0.0118238,"tackle it as a supervised classification task using algorithms that learn from statements annotated with the verdict labels. However this is unlikely to be successful, since statements such as the ones verified by journalists do not contain the world knowledge and the temporal and spatial context needed for this purpose. A different approach would be to match statements to ones already fact-checked by journalists and return the label in a K-nearest neighbour fashion.9 Thus the task is reduced to assessing the semantic similarity between statements, which was explored in a recent shared task (Agirre et al., 2013). An obvious shortcoming of this approach is that it cannot be applied to new claims that have not been fact-checked, thus it can only be used to detect repetitions and paraphrases of false claims. A possible mechanism to extend the coverage of such an approach to novel statements is to assume that some large text collection is the source of all true statements. For example, Wikipedia is likely • assessing causal relations, e.g. whether a statistic should be attributed to a particular law • concerning the future, e.g. speculations involving oil prices • not concerning facts, e.g. whether a pol"
W14-2508,D13-1020,0,0.032377,"rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. to contain a statement that would match the second claim in Figure 1. However, it would still be unable to tackle the other claims men"
W14-2508,H05-1079,0,0.0363533,"ng approach Cohen et al. (2011). Some7 E.g. part of the analysis of the first claim in Figure 1 reads: “the full-time figure has the handy effect of stripping out the very lowest earners and bumping up the average”. 8 https://sites.google.com/site/ andreasvlachos/resources 9 The Truth-Teller by Washington Post (http:// truthteller.washingtonpost.com/) follows this approach. 16444 5 http://blogs.channel4.com/factcheck/ 6 http://www.politifact.com/ truth-o-meter/statements/ 20 Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment (Bos and Markert, 2005). However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in thes"
W14-2508,N13-1008,1,0.385391,"namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no a"
W14-2508,J12-2003,0,0.0159657,"Missing"
W14-2508,W10-3001,0,\N,Missing
W14-2508,D13-1161,0,\N,Missing
W15-1519,D14-1165,0,0.52052,"ly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact accuracy on pairs of e"
W15-1519,P14-1079,0,0.0171699,"troduction Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact a"
W15-1519,N13-1008,1,0.856214,"de entity types. 1 Introduction Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can sign"
W16-1309,D14-1165,0,0.152689,"Missing"
W16-1309,D15-1038,0,0.0781408,"Missing"
W16-1309,P15-1016,0,0.210108,"Missing"
W16-1309,N13-1008,1,0.150639,"Missing"
W16-1309,W14-2409,1,0.865532,"Missing"
W16-1309,N15-1118,1,0.130096,"Missing"
W16-1309,D12-1110,0,0.0556215,"Missing"
W16-1309,D15-1174,0,0.136248,"Missing"
W16-1309,N16-1181,0,\N,Missing
W16-1309,W15-4002,0,\N,Missing
W16-1313,D15-1103,0,0.215392,"population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold: 1. A first model for fi"
W16-1313,C96-1079,0,0.231995,"for the mention “New York” in the sentence “She got a Ph.D from New York in Feb. 1995.”. Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its contex"
W16-1313,P09-1113,0,0.151619,"owledge, Lee et al. (2006) were the first to address the task of fine-grained entity type classification. They defined 147 finegrained entity types and evaluated a conditional random fields-based model on a manually annotated Korean dataset. Sekine (2008) advocated the necessity of a large set of types for entity type classification and defined 200 types which served as a basis for future work on fine-grained entity type classification. Ling and Weld (2012) defined a set of 112 types based on Freebase and created a training dataset from Wikipedia using a distant supervision method inspired by Mintz et al. (2009). For evaluation, they created a small manually annotated dataset of newspaper articles and also demonstrated that their system, FIGER, could improve the performance of a relation extraction system by providing fine-grained entity type predictions as features. Yosef et al. (2012) organised 505 types in a hierarchical taxonomy, with several hundreds of types at different levels. Based on this taxonomy they developed a multi-label hierarchical classification system. In Yogatama et al. (2015) the authors proposed to use label embeddings to allow information sharing between related labels. This ap"
W16-1313,D14-1162,0,0.0832247,"i=1 • loose micro Dataset Pre-trained Word Embeddings Evaluation Criteria Following Ling and Weld (2012), we evaluate the model performances by strict, loose macro, and 72 PN ˆ i=1 |Ti ∩ Ti | P N ˆ i=1 |Ti | PN ˆ i=1 |Ti ∩ Ti | Recall = P N i=1 |Ti | P recision = Experiment The only features used by our model are pretrained word embeddings that were not updated during training to help the model generalize for words not appearing in the training set. Specifically, we used the freely available 300 dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). As embeddings for out-ofvocabulary words, we used the embedding of the “unk” token from the pre-trained embeddings. 4.3 P recision = Recall = (10) To train and evaluate our model we use the publicly available FIGER dataset with 112 fine-grained types from Ling and Weld (2012). The sizes of our datasets are 2, 600, 000 for training, 90, 000 for development, and 563 for testing. Note that the train and development sets were created from Wikipedia, whereas the test set is a manually annotated dataset of newspaper articles. 4.2 • strict (9) The equations for computing eri , a ˜ri , and ari were"
W16-1313,sekine-2008-extended,0,0.304162,"Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recurs"
W16-1313,P15-2048,0,0.230963,", 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold"
W16-1313,C12-2133,0,0.387061,"nswering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main con"
W16-1314,N13-1008,1,0.895109,"Missing"
W16-1314,N15-1118,1,0.66062,"Missing"
W16-1314,Q15-1027,0,\N,Missing
W16-1314,N13-1092,0,\N,Missing
W16-1319,D15-1204,0,0.0179193,"omponents, allowing us to selectively add or discard certain bigram embeddings and compare their relative importance. All models are empirically compared and evaluated on the fb15k237 dataset from Toutanova et al. (2015). In summary, our main contributions are: i) Adressing the question of generic bigram embeddings in a KB completion model for the first time; ii) The adaption of Factorization Machines for this matter; iii) Experimental findings for comparing different bigram embedding models on fb15k237. 2 train. Factorization Machines have already been applied in a similar setting to ours by Petroni et al. (2015) who use them with contextual features for an Open Relation Extraction task, but without bigrams. 3 Model 3.1 Brief Recall of Factorization Machines A Factorization Machine (FM) is a quadratic regression model with low-rank constraint on the quadratic interaction terms.1 Given a sparse input feature vector φ = (φ1 , . . . , φn )T ∈ Rn , the FM output prediction X ∈ R is X = hv, φi + n X hwi , wj i · φi φj (1) i,j=1 Related Work In the Universal Schema model (model F), Riedel et al. (2013) factorize KB entries together with relations of entity pairs extracted from text, embedding textual relati"
W16-1319,N13-1008,1,0.85286,") or the Google Knowledge Vault (Dong et al., 2014) provide immense collections of structured knowledge. Relationships in these KBs often exhibit regularities and models that capture these can be used to predict missing KB entries. A common approach to KB completion is via tensor factorization, where a collection of fact triplets is represented as a sparse mode-3 tensor which is decomposed into several low-rank sub-components. Textual relations, i.e. relations between entity pairs extracted from text, can aid the imputation of missing KB facts by modelling them together with the KB relations (Riedel et al., 2013). The general merit of factorization methods for KB completion has been demonstrated by a variety For further notation let E and R be sets of entities and relations, respectively. We denote a fact f stating a relation r ∈ R between subject s ∈ E and object o ∈ E as f = (s, r, o). Our goal is to learn embeddings for larger sub-constituents of f than just s, r, and o: we want to learn embeddings also for the entity pair bigram (s, o) as well as the relation-entity bigrams (s, r) and (r, o). As an example, consider Freebase facts with relation eating/practicer of diet/diet and object Veganism. Ov"
W16-1319,W15-1519,1,0.846047,"Open Relation Extraction task, but without bigrams. 3 Model 3.1 Brief Recall of Factorization Machines A Factorization Machine (FM) is a quadratic regression model with low-rank constraint on the quadratic interaction terms.1 Given a sparse input feature vector φ = (φ1 , . . . , φn )T ∈ Rn , the FM output prediction X ∈ R is X = hv, φi + n X hwi , wj i · φi φj (1) i,j=1 Related Work In the Universal Schema model (model F), Riedel et al. (2013) factorize KB entries together with relations of entity pairs extracted from text, embedding textual relations in the same vector space as KB relations. Singh et al. (2015) extend this model to include a variety of other interactions between entities and relations, using different relation vectors to interact with subject, object or both. Jenatton et al. (2012) also recognize the need to integrate rich higher-order interaction information into the score. Like Nickel et al. (2011) however, their model specifies relationships as relation-specific bilinear forms of entity embeddings. Other embedding methods for KB completion include DistMult (Yang et al., 2014) with a trilinear score, and TransE (Bordes et al., 2013) which offers an intriguing geometrical intuition"
W16-1319,W15-4007,0,0.0245652,"Missing"
W16-1319,D15-1174,0,0.123314,"ll in this paper explore the role of general bigram embeddings for KB completion, i.e. also the embeddings for 103 Proceedings of AKBC 2016, pages 103–107, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics other possible pairs of entities and relations. This is achieved using a Factorization Machine (FM) framework (Rendle, 2010) that is modular in its feature components, allowing us to selectively add or discard certain bigram embeddings and compare their relative importance. All models are empirically compared and evaluated on the fb15k237 dataset from Toutanova et al. (2015). In summary, our main contributions are: i) Adressing the question of generic bigram embeddings in a KB completion model for the first time; ii) The adaption of Factorization Machines for this matter; iii) Experimental findings for comparing different bigram embedding models on fb15k237. 2 train. Factorization Machines have already been applied in a similar setting to ours by Petroni et al. (2015) who use them with contextual features for an Open Relation Extraction task, but without bigrams. 3 Model 3.1 Brief Recall of Factorization Machines A Factorization Machine (FM) is a quadratic regres"
W16-2522,D12-1118,0,0.0735175,"Missing"
W16-2522,N16-1162,0,0.0568602,"Missing"
W16-2522,P09-2053,0,0.0808319,"Missing"
W16-2522,Q14-1035,0,0.235239,"Missing"
W16-6102,H05-1025,0,0.0916185,"Missing"
W16-6102,P08-1118,0,0.0424152,"Missing"
W16-6102,W03-2502,0,0.116449,"Missing"
W16-6102,P08-1015,0,0.057718,"Missing"
W16-6102,W02-1020,0,0.0808076,"effort by reducing the number of keystrokes needed and to improve text quality by preventing misspellings, promoting adoption of standard terminologies and allowing for exploration of the vocabulary (Sevenster and Aleksovski, 2010; Sevenster et al., 2012). Text prediction originated in augmentative and alternative communication (AAC) to increase text generation rates for people with motor or speech impairments (Beukelman and Mirenda, 2005). Its scope has been extended to a gamut of applications, such as data entry in mobile devices (Dunlop and Crossan, 2000), interactive machine translation (Foster et al., 2002), search term auto6 Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis (LOUHI), pages 6–16, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics completion (Bast and Weber, 2006) and assisted clinical report compilation (Eng and Eisner, 2004; Cannataro et al., 2012). In this paper, we explore the tasks of word prediction, where a system displays a list of suggestions for the next word before the user starts typing it, and word completion, where the system suggests a single possible completion for the word, while the user i"
W16-6102,D15-1293,0,0.0378323,"Missing"
W16-6102,P15-2038,0,0.0485855,"Missing"
W16-6102,C08-1066,0,0.0738142,"Missing"
W16-6102,Q15-1001,0,0.255609,"Missing"
W16-6102,P10-1122,0,0.0381354,"Missing"
W16-6102,P14-1068,0,0.0764428,"Missing"
W16-6102,D16-1101,1,0.800744,"contexts, e.g. foreign language text for machine translation (Cho et al., 2014), images (Vinyals et al., 2015; Donahue et al., 2015) and videos (Yao et al., 2015) for captioning, etc. Grounded language models represent the relationship between words and the non-linguistic context they refer to. Grounding can help learn better representations for the atoms of language and their interactions. Previous work grounds language on vision (Bruni et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008) and the olfactory perception (Kiela et al., 2015). Spithourakis et al. (2016) use numerically grounded language models and language models conditioned on a lexicalised knowledge base for the tasks of semantic error detection and correction. We directly use their models to perform word prediction and completion. Algorithm 1 Word completion Input: V is set of vocabulary words, scorer returns score for word in current position Output: next word to be written 1: function C OMPLETE W ORD (V, scorer) 2: pref ix ← ‘’ 3: lexicon ← V 4: loop 5: lexicon ← {tokens in lexicon starting with pref ix} 6: best ← argmax scorer(token) 3 17: Methodology In this section we present a solut"
W16-6102,P08-2066,0,0.0798465,"Missing"
W16-6102,P08-3011,0,0.075128,"Missing"
W16-6208,D16-1084,1,0.840423,"Missing"
W16-6208,L16-1626,0,0.506339,"ing the emoji embedding space. 2 Related Work There has been little work in distributional embeddings of emoji. The first research done in this direction was an informal blog post by the Instagram Data Team in 2015 (Dimson, 2015). They generated vector embeddings for emoji similar to skip-gram-based vectors by training on the entire corpus of Instagram posts. Their research gave valuable insight into the usage of emoji on Instagram, and showed that distributed representations can help understanding emoji semantics in everyday usage. The second contribution, closest to ours, was introduced by (Barbieri et al., 2016). They trained emoji embeddings from a large Twitter dataset of over 100 million English tweets using the skip-gram method (Mikolov et al., 2013a). These pre-trained emoji representations led to increased accuracy on a similarity task, and a meaningful clustering of the emoji embedding space. While this method is able to learn robust representations for frequently-used emoji, representations of less frequent emoji are estimated rather poorly or not available at all. In fact, only around 700 emoji can be found in Barbieri et al. (2016)’s corpus, while there is support of over 1600 emoji in the"
W16-6208,Q16-1002,0,0.0181034,"spects. First, since we are estimating the representation of emoji directly from their description, we obtain robust representations for all supported emoji symbols — even the long tail of infrequently used ones. Sec49 Figure 1: Example description of U+1F574. We also use business, man and suit keywords for training. ondly, our method works with much less data. Instead of training on millions of tweets, our representations are trained on only a few thousand descriptions. Still, we obtain higher accuracy results on a Twitter sentiment analysis task. In addition, our work relates to the work of Hill et al. (2016) who built word representations for words and concepts based on their description in a dictionary. Similarly to their approach, we build representations for emoji based on their descriptions and keyword phrases. Some of the limitations of our work are evident in the work of Park et al. (2013) who showed that different cultural phenomena and languages may coopt conventional emoji sentiment. Since we train only on English-language definitions and ignore temporal definitions of emoji, our training method might not capture the full semantic characteristics of an emoji. 3 Method Our method maps emo"
W16-6208,W14-1618,0,0.0553784,"Missing"
W16-6208,N13-1090,0,0.399893,"social media rely on representation learning and word embeddings (Tang et al., 2014; Dong et al., 2014; Dhingra et al., 2016; Augenstein et al., 2 http://www.unicode.org/emoji/charts/ full-emoji-list.html See https://twitter.com/Kyle_MacLachlan/ status/765390472604971009 for an extreme example. 48 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 48–54, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016). Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). Yet, neither resource contain a complete set of Unicode emoji representations, which suggests that many social NLP applications could be improved by the addition of robust emoji representations. In this paper we release emoji2vec, embeddings for emoji Unicode symbols learned from their description in the Unicode emoji standard. We demonstrate the usefulness of emoji representations trained in this way by evaluating on a Twitter sentiment analysis task. Furthermore, we provide a qualitative analysis by investigating emoji analogy examples and visualizing t"
W16-6208,D14-1162,0,0.0991354,"on learning and word embeddings (Tang et al., 2014; Dong et al., 2014; Dhingra et al., 2016; Augenstein et al., 2 http://www.unicode.org/emoji/charts/ full-emoji-list.html See https://twitter.com/Kyle_MacLachlan/ status/765390472604971009 for an extreme example. 48 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 48–54, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016). Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). Yet, neither resource contain a complete set of Unicode emoji representations, which suggests that many social NLP applications could be improved by the addition of robust emoji representations. In this paper we release emoji2vec, embeddings for emoji Unicode symbols learned from their description in the Unicode emoji standard. We demonstrate the usefulness of emoji representations trained in this way by evaluating on a Twitter sentiment analysis task. Furthermore, we provide a qualitative analysis by investigating emoji analogy examples and visualizing the emoji embedding space. 2 Related W"
W16-6208,D11-1141,0,0.109255,"Missing"
W16-6208,S15-2078,0,0.0395286,"Missing"
W16-6208,P14-1146,0,0.110515,"Missing"
W18-1005,D15-1075,0,0.0894992,"Missing"
W18-1005,D16-1244,0,0.0639563,"Missing"
W18-1005,D14-1162,0,0.107282,"Missing"
W18-1005,W14-1618,0,0.0787905,"Missing"
W18-5515,P17-1152,0,0.0569157,"mentioned in the claim (e.g., “Watchmen” vs “Watchmen (film)”). The model is trained on a balanced set of positive and negative examples drawn from the training set, and the top-ranked articles are then passed on to the sentence retrieval component. This process is related to, but goes substantially beyond entity recognition and linking (Mendes et al., 2017). These processes attempt to identify 2.3 Natural Language Inference (NLI) In this component, an NLI model predicts a label for each pair of claim and retrieved evidence sentence. We adopted the Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) as NLI model. ESIM employs a bidirectional LSTM (BiLSTM) to encode premise and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on th"
W18-5515,D16-1244,0,0.162336,"Missing"
W18-5515,D14-1162,0,0.0850075,"and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al., 2018) simply concatenates all evidence sentences and feeds them into the NLI model, regardless of their evidence type. In contra"
W18-5515,P18-1196,1,0.863792,"Missing"
W18-5515,N18-1074,0,0.227007,"et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al., 2018) simply concatenates all evidence sentences and feeds them into the NLI model, regardless of their evidence type. In contrast, we generate NLI predictions individually for each predicted evidence, thus processing them in parallel. Furthermore, we observed that evidence sentences often include a pronoun referring to the subject of the article without explicitly mentioning it. This co-reference is opaque to the NLI model without further information. To resolve this, we prepend the corresponding title of the article to the sentence, along with a separator as described in Figure 1. We also experim"
W18-5515,D15-1075,0,0.0311149,"uential Inference Model (ESIM) (Chen et al., 2017) as NLI model. ESIM employs a bidirectional LSTM (BiLSTM) to encode premise and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al."
