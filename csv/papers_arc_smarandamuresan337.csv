2021.sigdial-1.40,What to Fact-Check: Guiding Check-Worthy Information Detection in News Articles through Argumentative Discourse Structure,2021,-1,-1,3,1,1559,tariq alhindi,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,Most existing methods for automatic fact-checking start with a precompiled list of claims to verify. We investigate the understudied problem of determining what statements in news articles are worthy to fact-check. We annotate the argument structure of 95 news articles in the climate change domain that are fact-checked by climate scientists at climatefeedback.org. We release the first multi-layer annotated corpus for both argumentative discourse structure (argument types and relations) and for fact-checked statements in news articles. We discuss the connection between argument structure and check-worthy statements and develop several baseline models for detecting check-worthy statements in the climate change domain. Our preliminary results show that using information about argumentative discourse structure shows slight but statistically significant improvement over a baseline of local discourse structure.
2021.naacl-main.230,Emotion-Infused Models for Explainable Psychological Stress Detection,2021,-1,-1,2,0,3938,elsbeth turcan,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress."
2021.naacl-main.336,{MERMAID}: Metaphor Generation with Symbolism and Discriminative Decoding,2021,-1,-1,3,1,1130,tuhin chakrabarty,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66{\%} of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68{\%} of the time compared to poems without metaphors."
2021.naacl-main.394,{ENTRUST}: Argument Reframing with Language Models and Entailment,2021,-1,-1,3,1,1130,tuhin chakrabarty,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples{'} opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for {``}connotations{''} to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear."
2021.findings-acl.297,Figurative Language in Recognizing Textual Entailment,2021,-1,-1,4,1,1130,tuhin chakrabarty,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.347,Minimally-Supervised Morphological Segmentation using {A}daptor {G}rammars with Linguistic Priors,2021,-1,-1,7,1,8324,ramy eskander,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.348,Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction,2021,-1,-1,6,0,3938,elsbeth turcan,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.504,Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models,2021,-1,-1,3,1,1130,tuhin chakrabarty,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the \textit{implicit premise in an enthymeme}, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets."
2021.emnlp-main.577,Don{'}t Go Far Off: An Empirical Study on Neural Poetry Translation,2021,-1,-1,3,1,1130,tuhin chakrabarty,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data."
2021.eacl-main.171,{``}Laughing at you or with you{''}: The Role of Sarcasm in Shaping the Disagreement Space,2021,-1,-1,3,1,8209,debanjan ghosh,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Detecting arguments in online interactions is useful to understand how conflicts arise and get resolved. Users often use figurative language, such as sarcasm, either as persuasive devices or to attack the opponent by an ad hominem argument. To further our understanding of the role of sarcasm in shaping the disagreement space, we present a thorough experimental setup using a corpus annotated with both argumentative moves (agree/disagree) and sarcasm. We exploit joint modeling in terms of (a) applying discrete features that are useful in detecting sarcasm to the task of argumentative relation classification (agree/disagree/none), and (b) multitask learning for argumentative relation classification and sarcasm detection using deep learning architectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical attention and Transformer-based architectures). We demonstrate that modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups."
2021.acl-short.133,Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains,2021,-1,-1,3,0,12666,chenghao yang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk. Among social media platforms, Reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone{'}s state of mind or interest regarding mental health disorders such as r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on suicide risk assessment has been the small amount of labeled data. We propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around mental health (e.g., anxiety, depression) helps improve model performance for suicide risk assessment."
2021.acl-long.165,{COVID}-Fact: Fact Extraction and Verification of Real-World Claims on {COVID}-19 Pandemic,2021,-1,-1,3,0,9811,arkadiy saakyan,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19. Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation."
2021.acl-long.524,Metaphor Generation with Conceptual Mappings,2021,-1,-1,4,0,11362,kevin stowe,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. To achieve this, we develop two methods: 1) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level (CM-Lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (CM-BART). We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence. We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems, and CM-BART outperforms all other models both in automatic and human evaluations."
2020.winlp-1.40,An Evaluation of Subword Segmentation Strategies for Neural Machine Translation of Morphologically Rich Languages,2020,-1,-1,3,0,14031,aquia richburg,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"Byte-Pair Encoding (BPE) (Sennrich et al., 2016) has become a standard pre-processing step when building neural machine translation systems. However, it is not clear whether this is an optimal strategy in all settings. We conduct a controlled comparison of subword segmentation strategies for translating two low-resource morphologically rich languages (Swahili and Turkish) into English. We show that segmentations based on a unigram language model (Kudo, 2018) yield comparable BLEU and better recall for translating rare source words than BPE."
2020.sltu-1.13,{M}ulti{S}eg: Parallel Data and Subword Information for Learning Bilingual Embeddings in Low Resource Scenarios,2020,-1,-1,3,0,14693,efsun kayi,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Distributed word embeddings have become ubiquitous in natural language processing as they have been shown to improve performance in many semantic and syntactic tasks. Popular models for learning cross-lingual word embeddings do not consider the morphology of words. We propose an approach to learn bilingual embeddings using parallel data and subword information that is expressed in various forms, i.e. character n-grams, morphemes obtained by unsupervised morphological segmentation and byte pair encoding. We report results for three low resource morphologically rich languages (Swahili, Tagalog, and Somali) and a high resource language (German) in a simulated a low-resource scenario. Our results show that our method that leverages subword information outperforms the model without subword information, both in intrinsic and extrinsic evaluations of the learned embeddings. Specifically, analogy reasoning results show that using subwords helps capture syntactic characteristics. Semantically, word similarity results and intrinsically, word translation scores demonstrate superior performance over existing methods. Finally, qualitative analysis also shows better-quality cross-lingual embeddings particularly for morphological variants in both languages."
2020.scil-1.10,Interpreting Verbal Irony: Linguistic Strategies and the Connection to the{T}ype of Semantic Incongruity,2020,-1,-1,3,1,8209,debanjan ghosh,Proceedings of the Society for Computation in Linguistics 2020,0,None
2020.lrec-1.879,"{M}orph{AG}ram, Evaluation and Framework for Unsupervised Morphological Segmentation",2020,-1,-1,5,1,8324,ramy eskander,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Computational morphological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource)."
2020.figlang-1.1,A Report on the 2020 Sarcasm Detection Shared Task,2020,23,0,3,1,8209,debanjan ghosh,Proceedings of the Second Workshop on Figurative Language Processing,0,"Detecting sarcasm and verbal irony is critical for understanding people{'}s actual sentiments and beliefs. Thus, the field of sarcasm analysis has become a popular research problem in natural language processing. As the community working on computational approaches for sarcasm detection is growing, it is imperative to conduct benchmarking studies to analyze the current state-of-the-art, facilitating progress in this area. We report on the shared task on sarcasm detection we conducted as a part of the 2nd Workshop on Figurative Language Processing (FigLang 2020) at ACL 2020."
2020.emnlp-main.391,Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,2020,-1,-1,2,1,8324,ramy eskander,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages."
2020.emnlp-main.524,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,2020,-1,-1,2,1,1130,tuhin chakrabarty,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88{\%} novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37{\%} of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges."
2020.emnlp-main.636,To {BERT} or Not to {BERT}: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging,2020,-1,-1,4,0,8331,kasturi bhattacharjee,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact."
2020.coling-main.540,Fact vs. Opinion: the Role of Argumentation Features in News Classification,2020,-1,-1,2,1,1559,tariq alhindi,Proceedings of the 28th International Conference on Computational Linguistics,0,"A 2018 study led by the Media Insight Project showed that most journalists think that a clearmarking of what is news reporting and what is commentary or opinion (e.g., editorial, op-ed)is essential for gaining public trust. We present an approach to classify news articles into newsstories (i.e., reporting of factual information) and opinion pieces using models that aim to sup-plement the article content representation with argumentation features. Our hypothesis is thatthe nature of argumentative discourse is important in distinguishing between news stories andopinion articles. We show that argumentation features outperform linguistic features used previ-ously and improve on fine-tuned transformer-based models when tested on data from publishersunseen in training. Automatically flagging opinion pieces vs. news stories can aid applicationssuch as fact-checking or event extraction."
2020.acl-main.711,"{R}{\\^{}}3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge",2020,28,0,3,1,1130,tuhin chakrabarty,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. Human evaluation shows that our system generates sarcasm better than humans 34{\%} of the time, and better than a reinforced hybrid baseline 90{\%} of the time."
2020.acl-main.761,{D}e{S}e{P}tion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking,2020,25,0,7,0.769231,4407,christopher hidey,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking {--} multiple propositions, temporal reasoning, and ambiguity and lexical variation {--} and introduce a resource with these types of claims. Then we present a system designed to be resilient to these {``}attacks{''} using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval."
W19-4452,Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays,2019,0,0,6,0,719,yanjun gao,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a unique dataset of student source-based argument essays to facilitate research on the relations between content, argumentation skills, and assessment. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the rubric, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays."
W19-4222,Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages,2019,0,0,3,1,8324,ramy eskander,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Polysynthetic languages pose a challenge for morphological analysis due to the root-morpheme complexity and to the word class {``}squish{''}. In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages."
W19-3508,Pay {``}Attention{''} to your Context when Classifying Abusive Language,2019,-1,-1,3,1,1130,tuhin chakrabarty,Proceedings of the Third Workshop on Abusive Language Online,0,"The goal of any social media platform is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that it becomes an avenue for wanton attacks. We propose an experimental study that has three aims: 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks); 2) to investigate what type of attention mechanism (contextual vs. self-attention) is better for abusive language detection using deep learning architectures; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task."
S19-2194,{C}olumbia at {S}em{E}val-2019 Task 7: Multi-task Learning for Stance Classification and Rumour Verification,2019,0,0,4,0,25186,zhuoran liu,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"The paper presents Columbia team{'}s participation in the SemEval 2019 Shared Task 7: RumourEval 2019. Detecting rumour on social networks has been a focus of research in recent years. Previous work suffered from data sparsity, which potentially limited the application of more sophisticated neural architecture to this task. We mitigate this problem by proposing a multi-task learning approach together with language model fine-tuning. Our attention-based model allows different tasks to leverage different level of information. Our system ranked 6th overall with an F1-score of 36.25 on stance classification and F1 of 22.44 on rumour verification."
S19-2200,{C}olumbia{NLP} at {S}em{E}val-2019 Task 8: The Answer is Language Model Fine-tuning,2019,0,1,2,1,1130,tuhin chakrabarty,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Community Question Answering forums are very popular nowadays, as they represent effective means for communities to share information around particular topics. But the information shared on these forums are often not authentic. This paper presents the ColumbiaNLP submission for the SemEval-2019 Task 8: Fact-Checking in Community Question Answering Forums. We show how fine-tuning a language model on a large unannotated corpus of old threads from Qatar Living forum helps us to classify question types (factual, opinion, socializing) and to judge the factuality of answers on the shared task labeled data from the same forum. Our system finished 4th and 2nd on Subtask A (question type classification) and B (answer factuality prediction), respectively, based on the official metric of accuracy."
D19-5013,Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment levels,2019,10,0,3,1,1559,tariq alhindi,"Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda",0,"This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on Fine-Grained Propaganda Detection. Our system finished 5th out of 26 teams on the sentence-level classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task."
D19-1291,{AMPERSAND}: Argument Mining for {PERS}u{A}sive o{N}line Discussions,2019,0,0,3,1,1130,tuhin chakrabarty,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one{'}s argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model."
W18-5808,Automatically Tailoring Unsupervised Morphological Segmentation to the Language,2018,0,0,3,1,8324,ramy eskander,"Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Morphological segmentation is beneficial for several natural language processing tasks dealing with large vocabularies. Unsupervised methods for morphological segmentation are essential for handling a diverse set of languages, including low-resource languages. Eskander et al. (2016) introduced a Language Independent Morphological Segmenter (LIMS) using Adaptor Grammars (AG) based on the best-on-average performing AG configuration. However, while LIMS worked best on average and outperforms other state-of-the-art unsupervised morphological segmentation approaches, it did not provide the optimal AG configuration for five out of the six languages. We propose two language-independent classifiers that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages."
W18-5513,Where is Your Evidence: Improving Fact-checking by Justification Modeling,2018,0,0,3,1,1559,tariq alhindi,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),0,"Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true)."
W18-5521,Robust Document Retrieval and Individual Evidence Modeling for Fact Extraction and Verification.,2018,0,1,3,1,1130,tuhin chakrabarty,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),0,"This paper presents the ColumbiaNLP submission for the FEVER Workshop Shared Task. Our system is an end-to-end pipeline that extracts factual evidence from Wikipedia and infers a decision about the truthfulness of the claim based on the extracted evidence. Our pipeline achieves significant improvement over the baseline for all the components (Document Retrieval, Sentence Selection and Textual Entailment) both on the development set and the test set. Our team finished 6th out of 24 teams on the leader-board based on the preliminary results with a FEVER score of 49.06 on the blind test set compared to 27.45 of the baseline system."
P18-2125,{`}Lighter{'} Can Still Be Dark: Modeling Comparative Color Descriptions,2018,0,4,2,0,29078,olivia winn,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a novel paradigm of grounding comparative adjectives within the realm of color descriptions. Given a reference RGB color and a comparative term (e.g., lighter, darker), our model learns to ground the comparative as a direction in the RGB space such that the colors along the vector, rooted at the reference color, satisfy the comparison. Our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change. These vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception. Our approach makes use of a newly created dataset for this task derived from existing labeled color data."
L18-1258,A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations,2018,0,1,4,1,15510,elena musi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
J18-4009,Sarcasm Analysis Using Conversation Context,2018,61,8,3,1,8209,debanjan ghosh,Computational Linguistics,0,"Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, the speaker{'}s sarcastic intent is not always apparent without additional context. Focusing on social media discussions, we investigate three issues: (1) does modeling conversation context help in sarcasm detection? (2) can we identify what part of conversation context triggered the sarcastic reply? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic? To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn. We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn. As conversation context, we consider the prior turn, the succeeding turn, or both. Our computational models are tested on two types of social media platforms: Twitter and discussion forums. We discuss several differences between these data sets, ranging from their size to the nature of the gold-label annotations. To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks."
W17-5523,The Role of Conversation Context for Sarcasm Detection in Online Interactions,2017,33,8,3,1,8209,debanjan ghosh,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, speaker{'}s sarcastic intent is not always obvious without additional context. Focusing on social media discussions, we investigate two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply. To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response. We show that the conditional LSTM network (Rockt{\""a}schel et al. 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task."
W17-5102,Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum,2017,17,18,4,0.769231,4407,christopher hidey,Proceedings of the 4th Workshop on Argument Mining,0,"Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of understanding what makes a message persuasive. Premises are annotated with the three types of persuasive modes: ethos, logos, pathos, while claims are labeled as interpretation, evaluation, agreement, or disagreement, the latter two designed to account for the dialogical nature of our corpus. We aim to answer three questions: 1) can humans reliably annotate the semantic types of argument components? 2) are types of premises/claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in persuasive messages than in non-persuasive messages?"
W16-3213,Detecting Visually Relevant Sentences for Fine-Grained Classification,2016,6,0,3,0,29078,olivia winn,Proceedings of the 5th Workshop on Vision and Language,0,None
W16-2810,Towards Feasible Guidelines for the Annotation of Argument Schemes,2016,32,8,3,1,15510,elena musi,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,"The annotation of argument schemes represents an important step for argumentation mining. General guidelines for the annotation of argument schemes, applicable to any topic, are still missing due to the lack of a suitable taxonomy in Argumentation Theory and the need for highly trained expert annotators. We present a set of guidelines for the annotation of argument schemes, taking as a framework the Argumentum Model of Topics (Rigotti and Morasso, 2010; Rigotti, 2009). We show that this approach can contribute to solving the theoretical problems, since it offers a hierarchical and finite taxonomy of argument schemes as well as systematic, linguistically-informed criteria to distinguish various types of argument schemes. We describe a pilot annotation study of 30 persuasive essays using multiple minimally trained non-expert annotators .Our findings from the confusion matrixes pinpoint problematic parts of the guidelines and the underlying annotation of claims and premises. We conduct a second annotation with refined guidelines and trained annotators on the 10 essays which received the lowest agreement initially. A significant improvement of the inter-annotator agreement shows that the annotation of argument schemes requires highly trained annotators and an accurate annotation of argumentative components (premises and claims)."
P16-2089,Coarse-grained Argumentation Features for Scoring Persuasive Essays,2016,17,15,4,1,8209,debanjan ghosh,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Scoring the quality of persuasive essays is an important goal of discourse analysis, addressed most recently with highlevel persuasion-related features such as thesis clarity, or opinions and their targets. We investigate whether argumentation features derived from a coarse-grained argumentative structure of essays can help predict essays scores. We introduce a set of argumentation features related to argument components (e.g., the number of claims and premises), argument relations (e.g., the number of supported claims) and typology of argumentative structure (chains, trees). We show that these features are good predictors of human scores for TOEFL essays, both when the coarsegrained argumentative structure is manually annotated and automatically predicted."
D15-1116,Sarcastic or Not: Word Embeddings to Predict the Literal or Sarcastic Meaning of Words,2015,26,42,3,1,8209,debanjan ghosh,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Sarcasm is generally characterized as a figure of speech that involves the substitution of a literal by a figurative meaning, which is usually the opposite of the original literal meaning. We re-frame the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is either literal or sarcastic. We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. We address two issues: 1) how to collect a set of target words that can have either literal or sarcastic meanings depending on context; and 2) given an utterance and a target word, how to automatically detect whether the target word is used in the literal or the sarcastic sense. For the latter, we investigate several distributional semantics methods and show that a Support Vector Machines (SVM) classifier with a modified kernel using word embeddings achieves a 7-10% F1 improvement over a strong lexical baseline."
W14-4918,Annotating Multiparty Discourse: Challenges for Agreement Metrics,2014,17,7,2,0,38348,nina wacholder,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"To computationally model discourse phenomena such as argumentation we need corpora with reliable annotation of the phenomena under study. Annotating complex discourse phenomena poses two challenges: fuzziness of unit boundaries and the need for multiple annotators. We show that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorffxe2x80x99s provide inconsistent results for the same text. In addition, IAA metrics do not tell us what parts of a text are easier or harder for human judges to annotate and so do not provide sufficiently specific information for evaluating systems that automatically identify discourse units. We propose a hierarchical clustering approach that aggregates overlapping text segments of text identified by multiple annotators; the more annotators who identify a text segment, the easier we assume that the text segment is to annotate. The clusters make it possible to quantify the extent of agreement judges show about text segments; this information can be used to assess the output of systems that automatically identify discourse units."
W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,2014,42,59,2,1,8209,debanjan ghosh,Proceedings of the First Workshop on Argumentation Mining,0,"Argument mining of online interactions is in its infancy. One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential."
W14-1807,Surprisal as a Predictor of Essay Quality,2014,24,1,2,0,38735,gaurav kharkwal,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Modern automated essay scoring systems rely on identifying linguistically-relevant features to estimate essay quality. This paper attempts to bridge work in psycholinguistics and natural language processing by proposing sentence processing complexity as a feature for automated essay scoring, in the context of English as a Foreign Language (EFL). To quantify processing complexity we used a psycholinguistic model called surprisal theory. First, we investigated whether essaysxe2x80x99 average surprisal values decrease with EFL training. Preliminary results seem to support this idea. Second, we investigated whether surprisal can be effective as a predictor of essay quality. The results indicate an inverse correlation between surprisal and essay scores. Overall, the results are promising and warrant further investigation on the usability of surprisal for essay scoring."
Y12-1013,Combining Social Cognitive Theories with Linguistic Features for Multi-genre Sentiment Analysis,2012,43,17,4,0,20232,hao li,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"With the rapid development of social media and social networks, spontaneously user generated content like tweets and forum posts have become important materials for tracking peoplexe2x80x99s opinions and sentiments online. In this paper we investigate the limitations of traditional linguistic-based approaches to sentiment analysis when applied to these informal genres. Inspired by various social cognitive theories, we combine local linguistic features and global social evidence in a propagation scheme to improve sentiment analysis results. Without using any additional labeled data, this new approach obtains significant improvement (up to 12% higher accuracy) for various genres in the domain of presidential election."
W12-4620,Search Space Properties for Learning a Class of Constraint-based Grammars,2012,0,0,1,1,1561,smaranda muresan,Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+11),0,None
W12-2801,Learning to Interpret Natural Language Instructions,2012,23,5,8,0,42283,monica babecsvroman,Proceedings of the Second Workshop on Semantic Interpretation in an Actionable Context,0,"This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments."
W12-2501,Computational Analysis of Referring Expressions in Narratives of Picture Books,2012,21,4,2,0,42302,choonkyu lee,Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature,0,"This paper discusses successes and failures of computational linguistics techniques in the study of how inter-event time intervals in a story affect the narratorxe2x80x99s use of different types of referring expressions. The success story shows that a conditional frequency distribution analysis of proper nouns and pronouns yields results that are consistent with our previous results xe2x80x93 based on manual coding xe2x80x93 that the narratorxe2x80x99s choice of referring expression depends on the amount of time that elapsed between events in a story. Unfortunately, the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse. Fine-grained analyses of these failures provide insight into the limitations of current coreference resolution systems, and ways of improving them."
C12-2039,Relation Classification using Entity Sequence Kernels,2012,4,1,2,1,8209,debanjan ghosh,Proceedings of {COLING} 2012: Posters,0,None
P11-2102,Identifying Sarcasm in {T}witter: A Closer Look,2011,13,309,2,0,44630,roberto gonzalezibanez,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well."
C10-2102,A Learnable Constraint-based Grammar Formalism,2010,17,5,1,1,1561,smaranda muresan,Coling 2010: Posters,0,"Lexicalized Weil-Founded Grammar (LWFG) is a recently developed syntactic-semantic grammar formalism for deep language understanding, which balances expressiveness with provable learnability results. The learnability result for LWFGs assumes that the semantic composition constraints are learnable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given."
W08-2002,Learning to Map Text to Graph-Based Meaning Representations via Grammar Induction,2008,22,7,1,1,1561,smaranda muresan,Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing,0,"We argue in favor of using a graph-based representation for language meaning and propose a novel learning method to map natural language text to its graph-based meaning representation. We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations."
P08-1115,Generalizing Word Lattice Translation,2008,37,141,2,0,47926,christopher dyer,Proceedings of ACL-08: HLT,1,"Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation."
P07-1105,Grammar Approximation by Representative Sublanguage: A New Model for Language Learning,2007,14,9,1,1,1561,smaranda muresan,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar."
muresan-klavans-2002-method,A Method for Automatically Building and Evaluating Dictionary Resources,2002,12,50,1,1,1561,smaranda muresan,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes a method toward automatically building dictionaries from text. We present DEFINDER, a rule-based system for extraction of definitions from on-line consumer-oriented medical articles. We provide an extensive evaluation on three dimensions: i) performance of the definition extraction technique in terms of precision and recall, ii) quality of the built dictionary as judged both by specialists and lay users, iii) coverage of existing on-line dictionaries. The corpus we used for the study is publicly available. A major contribution of the paper is the range of quantitative and qualitative evaluation methods."
W01-1011,{GIST}-{IT}: Combining Linguistic and Machine Learning Techniques for Email Summarization,2001,0,3,2,0,4939,evelyne tzoukermann,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,None
W01-0719,Combining linguistic and machine learning techniques for email summarization,2001,13,41,1,1,1561,smaranda muresan,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"This paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages. We describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction. Three main conclusions can be drawn from this study: (i) the modifiers of a noun phrase can be semantically as important as the head for the task of gisting, (ii) linguistic filtering improves the performance of machine learning algorithms, (iii) a combination of classifiers improves accuracy."
