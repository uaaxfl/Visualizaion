2020.lrec-1.395,I13-1057,0,0.0253544,"reated dynamically for semantic relationship annotation. scribed our methodology in Section 3, we further elaborate on the challenges of sense annotation in Section 4. We evaluate the datasets in Section 5 and finally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of m"
2020.lrec-1.395,erjavec-fiser-2006-building,0,0.137609,"Missing"
2020.lrec-1.395,E12-1059,0,0.155275,"geneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of languages and much of it is not"
2020.lrec-1.395,W97-0800,0,0.569498,"2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al.,"
2020.lrec-1.395,W14-0109,0,0.0200074,"resent a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al., 2018), and a next natural step is to link these to historical Danish dictionaries. 3 https://www.wiktionary.org/ Pedersen et al. (2009) describe the semi-automatic compilation of a WordNet for Danish, DanNet, based on a monolingual dictionary, the"
2020.lrec-1.395,bel-etal-2000-simple,1,0.672745,"Missing"
2020.lrec-1.395,Q13-1013,0,0.110061,"nment of LSRs and applied it to the production of a three-way alignment of the English WordNet, Wikipedia and Wiktionary. Niemann and Gurevych (2011) propose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1"
2020.lrec-1.395,C14-1025,0,0.31488,"ose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1995) (left) and the Webster’s Dictionary 1913 (Webster and Slater, 1828) (right). For further clarification, we provide two case studies of Danish and Ita"
2020.lrec-1.395,2018.gwc-1.8,1,0.745977,"Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 19"
2020.lrec-1.395,I11-1099,0,0.152048,"he recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of t"
2020.lrec-1.395,miller-gurevych-2014-wordnet,0,0.125981,"Missing"
2020.lrec-1.395,P06-1014,0,0.254458,"ies, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen S"
2020.lrec-1.395,W11-0122,0,0.627617,"ally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp"
2020.lrec-1.395,P10-1154,0,0.289699,"nces in structure and heterogeneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of language"
2020.lrec-1.395,roventini-ruimy-2008-mapping,0,0.0409542,"the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by just comparing strings. The ap"
2020.lrec-1.395,roventini-etal-2000-italwordnet,0,0.0791879,"Missing"
2020.lrec-1.395,roventini-etal-2002-integrating,0,0.0935961,"n incapacitated adult). An opposite case where the historical sense is ‘narrower’ than the modern one can be illustrated by the adjective spids (‘sharp’) where ODS describes two specific senses, one about sound and another one about smell, while DDO merges the two senses into one: ‘pungent in an unpleasant way (about smell, taste or sound)’. 4.2. ItalWordNet and SIMPLE Regarding Italian, the team at ILC-CNR chose ItalWordNet (IWN) and SIMPLE, two Italian language lexical resources which had been previously developed in the institute. The former, IWN, is a lexical semantic network for Italian (Roventini et al., 2002) which is part of the WordNet family (Miller, 1995). As such it is organised around the notion of a synset of word senses and the network structure based on lexical-semantic relations which hold between senses across synsets. The 50,000 Italian synsets contained in IWN are linked to the Princeton Wordnet. The latter resource, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the"
2020.lrec-1.395,P07-2041,0,0.0611602,"ce, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by ju"
2020.lrec-1.395,2019.gwc-1.37,1,0.782709,"onary, the Danish Dictionary (Den Danske Ordbog (DDO)). Later, the semantic links between these two resources facilitated the compilation of a comprehensive thesaurus (Den Danske Begrebsordbog) (Nimb et al., 2014). The semantic links between thesaurus and dictionary made it possible to combine verb groups and dictionary valency information, used as input for the compilation of the Danish FrameNet Lexicon (Nimb, 2018). Furthermore, they constitute the basis for the automatically integrated information on related words in DDO, on the fly for each dictionary sense (Nimb et al., 2018). Similarly, Simov et al. (2019) report the manual mapping of the Bulgarian Word-Net BTB-WN with the Bulgarian Wikipedia. Given the amount of the effort required to construct and maintain expert-made resources, various solutions have been proposed to automatically link and merge existing LSRs at different levels. LSRs being very diverse in domain coverage (Meyer, 2010; Burgun and Bodenreider, 2001), previous works have focused on methods to increase domain coverage, enrich sense representations and decrease sense granularity (Miller, 2016). Miller and Gurevych (2014) describe a technique for constructing an n-way alignment o"
attia-etal-2010-automatically,W98-1002,0,\N,Missing
attia-etal-2010-automatically,D08-1030,0,\N,Missing
attia-etal-2010-automatically,W05-0711,0,\N,Missing
attia-etal-2010-automatically,P05-1071,0,\N,Missing
attia-etal-2010-automatically,2007.jeptalnrecital-poster.13,0,\N,Missing
attia-etal-2010-automatically,elkateb-etal-2006-building,0,\N,Missing
attia-etal-2010-automatically,farber-etal-2008-improving,0,\N,Missing
bartolini-etal-2014-synsets,roventini-etal-2000-italwordnet,0,\N,Missing
bartolini-etal-2014-synsets,W12-5106,1,\N,Missing
bertagna-etal-2004-content,P98-1013,0,\N,Missing
bertagna-etal-2004-content,C98-1013,0,\N,Missing
bizzoni-etal-2014-making,S01-1027,0,\N,Missing
bizzoni-etal-2014-making,P10-3013,0,\N,Missing
bizzoni-etal-2014-making,abrate-bacciu-2012-visualizing,0,\N,Missing
bizzoni-etal-2014-making,W13-5512,1,\N,Missing
calzolari-etal-2004-enabler,binnenpoorte-etal-2002-field,0,\N,Missing
calzolari-etal-2006-next,picchi-etal-2004-linguistic,1,\N,Missing
del-gratta-etal-2008-ufra,W07-1501,0,\N,Missing
del-gratta-etal-2008-ufra,wright-2004-global,0,\N,Missing
fersoe-monachini-2004-elra,bel-etal-2000-simple,1,\N,Missing
fersoe-monachini-2004-elra,binnenpoorte-etal-2002-field,0,\N,Missing
frontini-etal-2014-polysemy,bel-etal-2000-simple,1,\N,Missing
frontini-etal-2014-polysemy,jezek-quochi-2010-capturing,1,\N,Missing
gavrilidou-etal-2006-language,J03-3002,0,\N,Missing
gavrilidou-etal-2006-language,calzolari-etal-2004-enabler,1,\N,Missing
gavrilidou-etal-2006-language,erjavec-2004-multext,0,\N,Missing
gavrilidou-etal-2006-language,J03-3001,0,\N,Missing
gavrilidou-etal-2012-meta,piperidis-2012-meta,1,\N,Missing
gavrilidou-etal-2012-meta,broeder-etal-2010-data,0,\N,Missing
gavrilidou-etal-2012-meta,federmann-etal-2012-meta,0,\N,Missing
L16-1150,attia-etal-2010-automatically,1,0.895376,"Missing"
L16-1150,2007.jeptalnrecital-poster.13,0,0.0309304,"d. An example of the formatted text that results from the application of these two phases can be seen in Fig. 3. 3.2. The LMF standard LMF is a model for representing computational lexicons that has the status of an ISO Standard. It was developed as a joint effort by a team of specialists in computational lexicography and natural language processing and has been used in a number of lexicographic projects, and as the input format in several NLP applications and tools (Del Grosso et al. 2014). The applicability of LMF to Arabic language resources has been demonstrated in several works, such as (Khemakhem et al. 2007, 2009, 2013; Loukil 2007, Attia et al 2010). We chose to represent AQAM in LMF so as to make the information contained within the lexicon available in a structured form both for lexicographic research and for NLP applications. Thus not only will it be possible for researchers 3 The schema is a syllabic pattern in which the root consonants (R) occupy a specified place; derivational affixes may be inserted at specified positions; but also the length and tone of vowels are specified. In Arabic, the formal representation of the schema is done using the consonants of the root f’l from which derive"
L16-1150,rodriguez-etal-2008-arabic,0,0.0564118,"Missing"
L16-1401,broeder-etal-2010-data,0,0.0217635,"Missing"
L16-1401,buitelaar-etal-2014-hot,0,0.0231595,"C under analysis are reported in Table 1: even if there has been a decrease since 2010 in the input about resources provided by authors, numbers are still interesting, especially when enriched with information about co-authorship for the visualisation of social networks graphs (see Table 3.). The analysis of coauthorship networks in the field of computational linguistics is not new: thanks to the ACL ANTHOLOGY NETWORK initiative (Radev et al., 2009) bibliographic data about papers’ citations and authors’ collaboration from the ACL Anthology are easy to explore 3 . In a similar vein Saffron 4 (Buitelaar et al., 2014; Bordea et al., 2013; Buitelaar et al., 2013) as a research framework based on text mining and linked data principles is able to perform community detection suggesting domain specific experts. Visualisations are organised around topics automatically extracted. The kind of networks we analyse in this paper are focused on the building blocks of scientific work in the field of computational linguistics, language resources. People can be connected because they jointly worked to write a paper but more significantly they can be connected because they used the same resource or resources with similar"
L16-1401,del-gratta-etal-2014-lre,1,0.445182,"a top-down approach to documenting resources and typically list resources that have reached a high level of maturity - in terms of validation, documentation and clearing of Intellectual Property Rights (IPR) issues. As an alternative to this approach, recent projects have been carried out within the LR community to create open, bottom-up repositories where LRs - even those under development can be duly documented and searched. Such initiatives are for instance the META-SHARE platform (Gavrilidou et al., 2012), the CLARIN VLO (Broeder et al., 2010) and the LRE Map (Calzolari et al., 2012; Del Gratta et al., 2014b; Del Gratta et al., 2014a), with their sets of metadata. In particular, the LREMap was launched as an initiative at LREC 2010 in order to crowdsource reliable and accurate documentation for the largest possible set of resources. Authors submitting to that conference were asked to document the resources they used in their paper, both the resources they created and the ones created by others. This initiative has continued and been extended to other conferences1 , and is now a unique source of information on existing language resources and their use in current research. The work in this paper c"
L16-1401,gavrilidou-etal-2012-meta,1,0.850325,"ELRA, LDC, NICT Universal Catalogue, ACL Data and Code Repository, OLAC, and LT World. These catalogues adopt a top-down approach to documenting resources and typically list resources that have reached a high level of maturity - in terms of validation, documentation and clearing of Intellectual Property Rights (IPR) issues. As an alternative to this approach, recent projects have been carried out within the LR community to create open, bottom-up repositories where LRs - even those under development can be duly documented and searched. Such initiatives are for instance the META-SHARE platform (Gavrilidou et al., 2012), the CLARIN VLO (Broeder et al., 2010) and the LRE Map (Calzolari et al., 2012; Del Gratta et al., 2014b; Del Gratta et al., 2014a), with their sets of metadata. In particular, the LREMap was launched as an initiative at LREC 2010 in order to crowdsource reliable and accurate documentation for the largest possible set of resources. Authors submitting to that conference were asked to document the resources they used in their paper, both the resources they created and the ones created by others. This initiative has continued and been extended to other conferences1 , and is now a unique source o"
L16-1401,soria-etal-2012-flarenet,1,0.845963,"h, where resources, papers and authors are nodes. The analysis of the visual representation of the underlying graph is used to study how the community gathers around LRs and how LRs are used in research. Keywords: language resources, resources documentation, data visualisation 1. Introduction The availability of Language Resources (LRs) - such as corpora, computational lexicons and parsers - is crucial to most NLP technologies. Recent initiatives have monitored the availability of Language Resources for different languages, and highlighted a digital divide between English and other languages (Soria et al., 2012), (Rehm and Uszkoreit, 2012). While the economic potential of English ensures that English LRs are developed and maintained not only in the academic sector but also by commercial players, the involvement of research communities for other languages is much more crucial to ensure that the necessary instruments (both data and tools) are made available for natural language processing purposes. At the same time, production of quality LRs is only the first step; in order to be usable, LRs must also be documented and made available to the community in such a way that they are easy to find and to use."
L18-1088,W16-4711,0,0.146921,"total of 38 papers related to WordNet presented at 21 workshops. From the works presented it is possible to retrieve WordNets for the following languages: Catalan WordNet, Galician WordNet (1998); Hungarian WordNet – Balkanet, Romanian WordNet, Estonian WordNet and GermaNet (2002); Czech WordNet - Prague Dependency TreeBank (2004); French WordNet (2008); Arabic WordNet – YAGO ontology, Estonian WordNet (2010); Slovene WordNet, Croatian WordNet (2012); IndoWordNet - Indian languages from Indo-Aryan, Dravidian, Quranic Arabic WordNet, Irish language WordNet Gaeilge (2014); Konkani SentiWordNet (2016). 5. 5.1 Workshops Community Graph 4: Countries Community and Country This paragraph is dedicated to give a first idea about the profile of LREC Workshops authors. The inter-disciplinary dimension, the specialized themes and the geographical dislocation of its stakeholders are the requisites of attraction of the satellite LREC workshops 4 community: over the years universities, research centers, governmental bodies and industries presented their own research experiences, the technological solutions tested and/or adopted thus facilitating the introduction of new paradigms as well as the giving"
L18-1088,mapelli-etal-2012-elra,0,0.0424431,"Missing"
L18-1694,P07-2043,0,0.0216958,"round Why SWRL? SWRL is, as its name suggests, a rule language1 . It is based on a subset of Datalog with both unary and binary predicates and is probably the best known attempt at an implementation the ‘Rules’ layer of the Semantic Web stack. By providing an extension of the Web Ontology Language (OWL) with Horn-like clauses SWRL permits modelers to overcome some of OWL’s expressive limitations as a formalism. Although there is a long tradition of using rule languages such as Prolog in computational linguistics, previous work on use of SWRL in this domain seems to be thin on the ground (see (Wilcock, 2007)) – we speculate that this is due in large part to SWRL’s own limited expressivity, at least in comparison to most of the other rule languages used in the past, and which makes it inadequate to the task of representing more complex kinds of syntactic and semantic phenomena. And so one of the core aims behind this work was to understand the viability of using SWRL rules in the modeling of at least part of a language, and more precisely to see if SWRL could help us encode part of a medium-to-large scale lexicon. What we needed in order to do this was a resource that provided us with a large numb"
monachini-etal-2004-unifying,ruimy-etal-2002-clips,1,\N,Missing
monachini-etal-2004-unifying,W99-0630,0,\N,Missing
monachini-etal-2006-unified,monachini-etal-2004-unifying,1,\N,Missing
monachini-etal-2006-unified,ruimy-etal-2002-clips,1,\N,Missing
moneglia-etal-2012-imagact,W99-0502,0,\N,Missing
moneglia-etal-2014-imagact,moneglia-etal-2012-imagact,1,\N,Missing
moneglia-etal-2014-imagact,W12-5106,1,\N,Missing
moneglia-etal-2014-imagact,L12-1000,0,\N,Missing
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
quochi-etal-2008-lexicon,cimiano-etal-2004-clustering,0,\N,Missing
quochi-etal-2008-lexicon,W04-3110,0,\N,Missing
quochi-etal-2008-lexicon,W03-1303,0,\N,Missing
quochi-etal-2008-lexicon,ide-romary-2004-registry,0,\N,Missing
quochi-etal-2008-lexicon,P90-1034,0,\N,Missing
quochi-etal-2008-lexicon,francopoulo-etal-2006-lexical,1,\N,Missing
quochi-etal-2008-lexicon,P93-1024,0,\N,Missing
quochi-etal-2008-lexicon,wright-2004-global,0,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
ruimy-etal-2002-clips,bel-etal-2000-simple,1,\N,Missing
ruimy-etal-2002-clips,roventini-etal-2002-integrating,1,\N,Missing
S10-1013,J07-4005,0,0.597125,"0.505 ±0.026 0.350 R verbs 0.450 ±0.034 0.454 ±0.034 0.291 ±0.025 0.403 ±0.033 0.293 R 0.529 ±0.021 0.521 ±0.018 0.496 ±0.019 0.462 ±0.020 0.294 R nouns 0.530 ±0.024 0.522 ±0.023 0.507 ±0.020 0.472 ±0.024 0.308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Re"
S10-1013,W04-0807,0,0.0284195,"omain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The"
S10-1013,E09-1005,1,0.474766,".308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Rel. Cliques 0.3 0.35 0.4 0.45 0.5 0.55 Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond to one system (denoted in axis Y) according each recall and confidence"
S10-1013,S07-1016,0,0.0570391,"English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to the participants included"
S10-1013,S07-1097,0,0.0246659,"wledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most Frequent Sense from SemCor; 2) Conceptual Density ; 3) Supervised Domain Relative Entropy classifier based on WordNet Domains; 4) Supervised Bayesian classifier based on WordNet Domains probabilities; and 5) Unsupervised Knownet-20 classifiers. The best run ranked 24th. UMCC-DLSI (Relevant): The team submitted three different runs using a knowledge-based system. The first two runs use domain vectors and the third is based on cliques, which measure how much a concept is correlated to the sentence by obtaining Relevant S"
S10-1013,W08-2114,0,0.0667055,"e was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most"
S10-1013,W04-0811,0,0.196041,"anguages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to th"
S10-1013,E09-1045,0,0.0508574,"Missing"
S10-1013,vossen-etal-2008-kyoto,1,0.763195,"ses in specific domains, the context of the senses might change, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. The main goal of this task is to provide a multilingual testbed to evaluate WSD systems when faced with full-texts from a specific domain. All datasets and related information are publicly available from the task websites1 . This task was designed in the context of Kyoto (Vossen et al., 2008)2 , an Asian-European project that develops a community platform for modeling knowledge and finding facts across languages and cultures. The platform operates as a Wiki system with an ontological support that social communities can use to agree on the meaning of terms in specific domains of their interest. Kyoto focuses on the environmental domain because it poses interesting challenges for information sharing, but the techniques and platforms are Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervis"
S10-1013,S01-1004,0,0.00916615,"the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dut"
S10-1013,H05-1053,0,0.486241,". Note that this method of estimating statistical significance might be more strict than other pairwise methods. We also include the results of two baselines. The random baseline was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods"
S10-1013,N09-1004,0,\N,Missing
S10-1013,W00-0901,0,\N,Missing
S10-1093,E09-1005,1,0.892281,"sources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future. 1 2 We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t"
S10-1093,S10-1013,1,0.884699,"Missing"
S10-1093,bosma-vossen-2010-bootstrapping,1,0.72995,"et 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3 . Cornetto comprises taxonomic relations and equivalence rela2 #rels. Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2 . Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus w"
S10-1093,P98-2127,0,0.0122903,"y et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run2: UKB using related words Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate eac"
S10-1093,J07-4005,0,0.0212144,"ea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, . . .”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Fin"
S10-1093,C98-2122,0,\N,Missing
savas-etal-2010-lmf,vossen-etal-2008-kyoto,1,\N,Missing
savas-etal-2010-lmf,bond-etal-2008-boot,0,\N,Missing
savas-etal-2010-lmf,francopoulo-etal-2006-lexical,1,\N,Missing
savas-etal-2010-lmf,hayashi-ishida-2006-dictionary,1,\N,Missing
savas-etal-2010-lmf,murakami-etal-2010-language,0,\N,Missing
savas-etal-2010-lmf,van-assem-etal-2006-conversion,0,\N,Missing
savas-etal-2010-lmf,W06-1003,1,\N,Missing
savas-etal-2010-lmf,1995.mtsummit-1.17,0,\N,Missing
soria-etal-2006-moving,W01-1507,1,\N,Missing
soria-etal-2006-moving,W03-1905,1,\N,Missing
soria-etal-2006-moving,francopoulo-etal-2006-lexical,1,\N,Missing
soria-etal-2006-moving,bertagna-etal-2004-content,1,\N,Missing
tokunaga-etal-2008-adapting,bel-etal-2000-simple,1,\N,Missing
tokunaga-etal-2008-adapting,P06-2106,1,\N,Missing
tokunaga-etal-2008-adapting,I08-1052,1,\N,Missing
tokunaga-etal-2008-adapting,francopoulo-etal-2006-lexical,1,\N,Missing
toral-etal-2008-named,E06-1002,0,\N,Missing
toral-etal-2008-named,sekine-etal-2002-extended,0,\N,Missing
toral-etal-2008-named,W02-1111,0,\N,Missing
toral-etal-2008-named,J06-1001,0,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
W06-1001,francopoulo-etal-2006-lexical,1,\N,Missing
W06-1001,bertagna-etal-2004-content,1,\N,Missing
W06-1003,huang-etal-2004-sinica,1,0.835312,"“curvatura, svolta, curva” (C). Therefore the procedure will propose a new candidate meronymy relation between the two Italian WordNet synsets (D). Figure 6. Schema of Wordnet Synsets Returned by WN Web Services. The scores returned by the method “GetWeightedSynsetsByIli” are used by our module to calculate the reliability rating for each new proposed relation. 3.3 A Case Study: Cross-fertilization between Italian and Chinese Wordnets. We explore this idea with a case-study involving the ItalianWordNet (Roventini et al., 2003) and the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW, Huang et al., 2004). The BOW integrates three resources: WordNet, English-Chinese Translation Equivalents Database (ECTED), and SUMO (Suggested Upper Merged Ontology). With the integration of these three key resources, Sinica BOW functions both as an English-Chinese bilingual wordnet and a bilingual lexical access to SUMO. Sinica Bow currently has two bilingual versions, corresponding to WordNet 1.6. and 1.7. Based on these bootstrapped versions, a Chinese Wordnet (CWN, Huang et al. 2005) is under construction with handcrafted senses and lexical semantic relations. For the current experiment, we have used the ve"
W06-1003,kemps-snijders-etal-2006-lexus,0,0.0289533,"n guage away from where the language is spoken. Lastly, the vast range of diversity of languages also makes it impossible to have one single universal centralized resource, or even a centralized repository of resources. Although the paradigm of distributed and interoperable lexical resources has largely been discussed and invoked, very little has been made in comparison for the development of new methods and techniques for its practical realization. Some initial steps are made to design frameworks enabling inter-lexica access, search, integration and operability. An example is the Lexus tool (Kemps-Snijders et al., 2006), based on the Lexical Markup Framework (Romary et al., 2006), that goes in the direction of managing the exchange of data among large-scale lexical resources. A similar tool, but more tailored to the collaborative creation of lexicons for endangered language, is SHAWEL (Gulrajani and Harrison, 2002). However, the general impression is that little has been made towards the development of new methods and techniques for attaining a concrete interoperability among lexical resources. Admittedly, this is a long-term scenario requiring the contribution of many different actors and initiatives (among"
W06-1003,O05-5001,1,\N,Missing
W09-3421,francopoulo-etal-2006-lexical,1,0.857972,"Missing"
W09-3421,bel-etal-2000-simple,1,0.763962,"the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian langua"
W09-3421,W03-1905,1,0.826021,"pproaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian languages, we elucidated the shortco"
W09-3421,P06-2106,1,0.822025,"Missing"
W09-3421,tokunaga-etal-2008-adapting,1,0.730656,"Missing"
W09-3421,W06-1001,1,\N,Missing
W10-3301,alvez-etal-2008-complete,1,\N,Missing
W10-3301,E09-1005,1,\N,Missing
W11-3306,2010.eamt-1.42,0,0.0944998,"g and comparing tools, and also in collaborative situations to process the same corpus for different purposes. [2]. Creation of workflows - Web Service Interoperability. In cases where workflows need to be built chaining together tools not originally built to work in pipeline/together, standards will ensure their execution. As of today, in most cases workflows can be run with tools that were already designed to work together, or with the use of format converters. This is a major obstacle esp. in the context of web-based platforms for distributed language services. Experiences such as PANACEA (Bel 2010, Toral et al. 2011) show that using a common standardised format facilitates integration. If tools were built/modified to work directly on common/standard formats, workflows might be simpler, easier to design and quicker to run. While this is not possible at present, when the advantages are shown, new tools could naturally go in this direction. Workflow management should be generalised to cover both local processing and web service interfaces. [3]. Integration/Interlinking of resources. This has recently become an important trend also for companies that wish to provide composite services. In"
W11-3306,W02-1204,1,0.756536,"Missing"
W11-3306,J93-2004,0,0.0367744,"requirements not covered by existing formats and established practices. We therefore observe a continuum of standardisation initiatives at various stages of consolidation and the rising on new proposals, as the various areas of LTs become mature. Also, while some standards are “official”, that is designed and promoted by standardisation bodies - i.e. ISO, W3C and LISA - others emerged bottomup. These are the so-called de-facto standards or best practices: formats and representation frameworks that have gained community consensus and are widely used: e.g. WordNet (Fellbaum 1998), PennTreeBank (Marcus et al. 1993), CoNLL 1 (Nivre et al. 2007). 2.1 nal document has been revised and updated with standards relevant for the broader LT community, also addressing those that are typically used in industry, at different levels of granularity. “The Standards&apos; Landscape Towards an Interoperability Framework” 3 (Bel et al., to appear) thus lists both current standards and on-going promising standardisation efforts so that the community can monitor and actively contribute to them. This document is conceived like a “live” document to be adopted and updated by the community (e.g. in future projects and networks), so"
W11-3306,declerck-2006-synaf,0,0.0150271,"al data, PCM, MP3, ATRAC, for audio, etc. On top of these we find standards specifically addressing LR management and representation that should also be considered as foundational ISO 24610-1:2006 - Feature structure representation, TEI, and LMF for lexical resources (Francopoulo et al. 2006, 2008). They are increasingly recognized as fundamental for real-world interoperability and exchange. A set of other standards focusing on specific aspects of linguistic and terminological representation are also currently in place and officially established, such as TMF (ISO 2002) for terminology, SynAF (Declerk, 2006) and MAF (Clément and de la Clérgerie, 2005) for morphological and syntactic annotation. These result from years of work and discussions among groups of The FLaReNet Landscape Drawing on a previous report drafted by the CLARIN 2 project (Bel et al. 2009), together with FLaReNet, META-SHARE and ELRA the origi3 This document also collects input also from the LRE Map, Multilingual Web, the FLaReNet fora, LREC Workshops, ISO and W3C. 1 http://ilk.uvt.nl/conll/#dataformat 2 www.clarin.eu 42 experts from various areas of language technology and are thought to be comprehensive enough to allow for the"
W11-3306,2011.eamt-1.11,0,0.0904521,"Missing"
W11-3306,W07-1501,0,0.0223921,"among groups of The FLaReNet Landscape Drawing on a previous report drafted by the CLARIN 2 project (Bel et al. 2009), together with FLaReNet, META-SHARE and ELRA the origi3 This document also collects input also from the LRE Map, Multilingual Web, the FLaReNet fora, LREC Workshops, ISO and W3C. 1 http://ilk.uvt.nl/conll/#dataformat 2 www.clarin.eu 42 experts from various areas of language technology and are thought to be comprehensive enough to allow for the representation of most current annotations. Most of them address syntactic interoperability by providing pivot formats (e.g. LAF/GrAF, Ide and Suderman 2007), while today there is a greater need for semantic interoperability, which is still an actual challenge. Most of the more linguistically oriented standards are also periodically under revision in an attempt to make them ever more comprehensive as new technologies appear and new languages are being considered. Effort is still needed for their promotion and to spread awareness to a wider community. Standards related to terminology management and translational technologies are probably the most widespread and consolidated, in part because of the real market behind the translation industry: we spe"
W13-5410,J01-3001,0,0.074691,"Missing"
W16-4022,bel-etal-2008-coldic,0,0.0201262,"ng the integration of terminologies and ontologies through a common model. They present an overview of the editor’s functional capabilities in relation to technologies offered by the LexGrid platform.(Ringersma and Kemps-Snijders, 2007) describes the development of a flexible web based lexicon tool, LEXUS which allows the creation of lexica within the structure of the ISO LMF standard and uses the proposed concept naming conventions from the ISO data categories, thus enabling interoperability, search and merging. Another generic platform for working with computational lexica, is presented in (Bel et al., 2008): the COLDIC system has been specially designed to allow the user to concentrate on the lexicographical task at hand while being autonomous in the management of the tools. Montiel et. al. (Montiel-Ponsoda et al., 2008) propose a tool, developed as a plug-in of NeOn4 to support a model called the Linguistic Information Repository (LIR). LIR is a holistic linguistic information repository, that provides a complete set of linguistic elements in each language for localizing ontology elements. It also allows access to linguistic information distributed in heterogeneous resources of varying granular"
W16-4022,francopoulo-etal-2006-lexical,1,0.729634,"ear_950 &lt;http://www.w3.org/2006/time#hasEnd> :year_1050 . 3 Using lemon and lemonDIA lemon was originally intended as a model for enriching ontologies with linguistic information (McCrae et al., 2010). However it quickly came to take on the status of a de facto standard for representing lexicons as linked open data. Indeed lemon has so far been used to convert the Princeton WordNet and Wiktionary (McCrae et al., 2012), as well as FrameNet and VerbNet (Eckle-Kohler et al., 2015), among other well known resources. The design of lemon was heavily influenced by the Lexical Markup Framework (LMF) (Francopoulo et al., 2006), but with numerous simplifications to the original LMF specifications. In addition unlike LMF the lemon model focuses specifically on creating lexico-semantic resources with an ontological component where the ontology represents the extensions of the word senses in the lexicon. So that in the lemon model every lexical sense necessarily connects a lexical entry with a specific ontology vocabulary item. lemonDia (Khan et al., 2014) was designed as an extension for lemon with the specific purpose of enabling the addition of temporal information to senses. We felt this was necessary even though t"
W16-4022,mccrae-etal-2012-collaborative,0,0.0196434,"on the basis of a much larger corpus. As regards lemon, in (McCrae and Unger, 2014) the authors use ontology design patterns (Gangemi, 2005) for defining how certain lexico-semantic phenomena should be modelled. Their goal in creating such a catalogue of ontology-lexicon design patterns is to facilitate the process of developing ontology4 NeOn toolkit is available at http://neon-toolkit.org 168 Figure 3: Interface for creating time intervals. lexica, by replacing complex combinations of frame semantics and first-order logic axioms with simple patterns with only a few parameters. Finally, in (Montiel-Ponsoda et al., 2012), a platform called lemon source is presented. It supports the creation of linked lexical data and it builds on the concept of a semantic wiki to enable collaborative editing of the resources by many users concurrently. 5.2 The Interface Our intent in this work has been to create a user-friendly interface that would facilitate users in the building of diachronic lexica using the lemonDia model without that is involving them too deeply in the details of the formal model underlying the representation. Our interface is web-based and supports the creation both of linked data lexica and related tem"
W16-4022,C08-2017,0,\N,Missing
