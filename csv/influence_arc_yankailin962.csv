2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.acl-main.277,1999.tmi-1.16,0,0.13791,"Missing"
2020.acl-main.277,D16-1139,0,0.0567024,"ransformer model (dmodel = 278, dhidden = 507, nlayer = 5, nhead = 2, pdropout = 0.1). For the WMT14 En-De and WMT16 EnRo datasets, we use a larger Transformer model (dmodel = 512, dhidden = 512, nlayer = 6, nhead = 8, pdropout = 0.1). We linearly anneal the learning rate from 3 × 10−4 to 10−5 as in Lee et al. (2018) for the IWSLT16 En-De dataset, while employing the warm-up learning rate schedule (Vaswani et al., 2017) with twarmup = 4000 for the WMT14 En-De and WMT16 En-Ro datasets. We also use label smoothing of value ls = 0.15 for all datasets. We utilize the sequence-level distillation (Kim and Rush, 2016), which replaces the target sentences in the training dataset with sentences generated by an autoregressive model, and set the beam size of the technique to 4. We use the encoder of the corresponding autoregressive model to initialize the encoder of RecoverSAT, and share the parameters of source and target token embedding layers and the pre-softmax linear layer. We measure the speedup of model inference in each task on a single NVIDIA P40 GPU with the batch size 1. 4.3 Baselines We use the Transformer (Vaswani et al., 2017) as our AT baseline and fifteen latest strong NAT models as NAT baselin"
2020.acl-main.277,D18-1149,0,0.581672,"1 Segment 2 Segment 3 Segment 4 Figure 1: An overview of our RecoverSAT model. RecoverSAT generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is generated token-by-token conditioned on both the source tokens and the translation history of all segments (e.g., the token “are” in the first segment is predicted based on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the trai"
2020.acl-main.277,D19-1573,0,0.110758,"Missing"
2020.acl-main.277,D19-1437,0,0.369128,"eration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel semi-autoregressive model named RecoverSAT in this work. RecoverSAT features in three aspects: (1) To improve decoding speed, we assume that a translation can be divided into several segments which can be genera"
2020.acl-main.277,P16-1162,0,0.0996738,"aining instances will mislead the model that generating then deleting a repetitive segment is a must-to-have behaviour, which is not desired. Therefore, we inject pseudo repetitive segment into a training instance with probability q in this work. 4 4.1 Experiments Datasets We conduct experiments on three widely-used machine translation datasets: IWSLT16 En-De (196k pairs), WMT14 En-De (4.5M pairs) and WMT16 En-Ro (610k pairs). For fair comparison, we use the preprocessed datasets in Lee et al. (2018), of which sentences are tokenized and segmented into subwords using byte-pair encoding (BPE) (Sennrich et al., 2016) to restrict the vocabulary size. We use a shared vocabulary of 40k subwords for both source and target languages. For the WMT14 En-De dataset, we use newstest-2013 and newstest2014 as validation and test sets respectively. For the WMT16 En-Ro dataset, we employ newsdev2016 and newstest-2016 as validation and test sets respectively. For the IWSLT16 En-De dataset, we use test2013 as the validation set. 4.2 Experimental Settings For model hyperparameters, we follow most of the settings in (Gu et al., 2018; Lee et al., 2018; Wei et al., 2019). For the IWSLT16 En-De dataset, we use a small Transfo"
2020.acl-main.277,P19-1288,1,0.902957,"ased on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel"
2020.acl-main.277,D18-1044,0,0.0823734,"Missing"
2020.acl-main.277,P19-1125,0,0.174125,"e corresponding autoregressive model. 1 Although neural machine translation (NMT) has achieved state-of-the-art performance in recent years (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), most NMT models still suffer from the slow decoding speed problem due to their autoregressive property: the generation of a target token depends on all the previously generated target tokens, making the decoding process intrinsically nonparallelizable. Recently, non-autoregressive neural machine translation (NAT) models (Gu et al., 2018; Li et al., 2019; Wang et al., 2019; Guo et al., 2019a; Wei et al., 2019) have been investigated to mitigate the † indicates equal contribution indicates corresponding author es gibt heute viele Farmer mit diesem Ansatz Feasible Trans. there are lots of farmers doing this today there are a lot of farmers doing this today Trans. 1 Trans. 2 there are lots of of farmers doing this today there are a lot farmers doing this today Table 1: A multi-modality problem example: NAT models generate each target token independently such that they may correspond to different feasible translations, which usually manifests as repetitive (Trans. 1) or missing (Trans. 2) tokens. Intro"
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.emnlp-main.129,buyko-etal-2010-genereg,0,0.0403765,"erent ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event annotation, but their different settings are complementary to our work. 7 Conclusion and Future work In this paper, we present a massive general domain event detection dataset (MAVEN), which significantly alleviates the data scarcity and low coverage problems of existing datasets. We conduct a thorough evaluation of the state-of-the-art ED models on MAVEN. Th"
2020.emnlp-main.129,P17-1038,0,0.0587272,"vember 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data"
2020.emnlp-main.129,P15-1017,0,0.690388,"uld recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated inst"
2020.emnlp-main.129,D18-1158,0,0.305267,"osition: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod"
2020.emnlp-main.129,N18-1076,0,0.0271789,"mains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https:// github.com/THU-KEG/MAVEN-dataset. 1 End-Position: 212 Transfer-Money: 198 Attack: 1543 Figure 1: Data distribution of the most widely-used ACE 2005 English dataset. It contains 33 event types, 599 documents and 5, 349 instances in total. is the first stage to extract event knowledge from text (Ahn, 2006) and also fundamental to various NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018; Yang et al., 2019). Introduction Event detection (ED) is an important task of information extraction, which aims to identify event triggers (the words or phrases evoking events in text) and classify event types. For instance, in the sentence “Bill Gates founded Microsoft in 1975”, an ED model should recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the r"
2020.emnlp-main.129,N19-1423,0,0.0330633,"eural network baseline, which adopts the widely-used bi-directional long shortterm memory network to learn textual representations, and then uses the hidden states at the positions of trigger candidates for classifying event types. (3) MOGANED (Yan et al., 2019) is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multi-order syntactic relations in dependency trees and improve ED. (4) DMBERT (Wang et al., 2019b) is a vanilla BERTbased model. It takes advantage of the effective pretrained language representation model BERT (Devlin et al., 2019) and also adopts the dynamic multi-pooling mechanism to aggregate features for ED. We use the BERTBASE architecture in our experiments. (5) Different from the above tokenlevel classification models, BiLSTM+CRF and BERT+CRF are sequence labeling models. To verify the effectiveness of modeling multiple event correlations, the two models both adopt the conditional random field (CRF) (Lafferty et al., 2001) as their output layers, which can model structured output dependencies. And they use BiLSTM and BERTBASE as their feature extractors respectively. As we manually tune hyperparameters and some t"
2020.emnlp-main.129,D19-1033,1,0.851706,"Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of"
2020.emnlp-main.129,D16-1264,0,0.124091,"Missing"
2020.emnlp-main.129,P19-1276,0,0.0175457,"traction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event a"
2020.emnlp-main.129,D18-1156,0,0.0338402,"Missing"
2020.emnlp-main.129,P19-1353,0,0.0176268,"of ED and event extraction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent co"
2020.emnlp-main.129,P19-1429,0,0.374622,"Missing"
2020.emnlp-main.129,W15-0812,0,0.406914,"problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1652–1671, c November 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, so"
2020.emnlp-main.129,L16-1699,0,0.0619031,"Missing"
2020.emnlp-main.129,P09-1113,0,0.125379,"2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low c"
2020.emnlp-main.129,P18-2066,0,0.180224,"-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stab"
2020.emnlp-main.129,N19-1105,1,0.947896,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1584,1,0.922895,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1582,0,0.510191,"-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conferenc"
2020.emnlp-main.129,C96-1079,0,\N,Missing
2020.emnlp-main.129,W06-0901,0,\N,Missing
2020.emnlp-main.129,P98-1013,0,\N,Missing
2020.emnlp-main.129,C98-1013,0,\N,Missing
2020.emnlp-main.129,P09-2093,0,\N,Missing
2020.emnlp-main.129,P06-4018,0,\N,Missing
2020.emnlp-main.129,P13-1024,0,\N,Missing
2020.emnlp-main.129,P08-1030,0,\N,Missing
2020.emnlp-main.129,P13-1008,0,\N,Missing
2020.emnlp-main.129,D14-1162,0,\N,Missing
2020.emnlp-main.129,P15-2060,0,\N,Missing
2020.emnlp-main.129,D15-1247,0,\N,Missing
2020.emnlp-main.129,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.129,N16-1034,0,\N,Missing
2020.emnlp-main.129,P16-1025,0,\N,Missing
2020.emnlp-main.129,P17-1164,0,\N,Missing
2020.emnlp-main.129,N18-2058,0,\N,Missing
2020.emnlp-main.129,C18-1075,0,\N,Missing
2020.emnlp-main.129,D18-1259,0,\N,Missing
2020.emnlp-main.129,W13-2001,0,\N,Missing
2020.emnlp-main.129,P19-1521,0,\N,Missing
2020.emnlp-main.129,P16-2060,0,\N,Missing
2020.emnlp-main.129,P18-1201,0,\N,Missing
2020.emnlp-main.237,W15-4007,0,0.0409905,"Missing"
2020.emnlp-main.237,N19-1086,0,0.0410443,"ange over time, which makes them impractical in real-world applications. 2.2 Continual Learning Continual learning, also known as life-long learning, helps alleviate catastrophic forgetting and enables incremental training for stream data. Methods for continual learning in natural language processing (NLP) field can mainly be divided into two categories: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), which slow down parameter updating to preserve old knowledge, and (2) memory-based methods (Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Chaudhry et al., 2019; Wang et al., 2019), which retain examples from old data for re-play upon learning the new data. Although continual learning has been widely studied in NLP (Sun et al., 2020) and computer vision (Kirkpatrick et al., 2017), its exploration on graph embedding is relatively rare. Sankar et al. (2018) seek to train graph embedding on constantly evolving data. However, it assumes the timestamp information is known beforehand, which hinders its application to other tasks. Song and Park (2018) extend the idea of regulation-based methods to continually learn graph embeddings which straightforwardly limits parameter upda"
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.582,S17-2001,0,0.0370299,"Missing"
2020.emnlp-main.582,P18-1078,0,0.0246983,"o tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and verification, coreference resolution, and eight tas"
2020.emnlp-main.582,D19-1606,0,0.323037,"9), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference information (Cheng and Erk, 2020; Xu et al., 2020; Zhao et al., 2020). Coreference occurs when two or more expressions in a text refer to the same entity, which is an important element for a coherent understanding of the whole discourse. For example, for comprehending the whole context of “Antoine published The Little Prince in 1943. The book follows a young prince who visits various planets in space.”, we must realize that The book refers to The Little Prince. Therefore, resolving coreference is an essential step"
2020.emnlp-main.582,N19-1423,0,0.584796,"issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT. 1 Introduction Recently, language representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou e"
2020.emnlp-main.582,D19-5801,0,0.0239374,"Missing"
2020.emnlp-main.582,W07-1401,0,0.0718487,"Missing"
2020.emnlp-main.582,P16-1154,0,0.0608072,"Missing"
2020.emnlp-main.582,P18-2058,0,0.0204785,"p((V hk ) hi ) Pr(xj |xi ) = P where denotes element-wise product function and V is a trainable parameter to measure the importance of each dimension for token’s similarity. Moreover, since we split a word into several word pieces as BERT does and we adopt whole word masking strategy for MRP, we need to extend our copy-based objective into word-level. To this end, we apply the token-level copy-based training objective on both start and end tokens of the masked word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Refer"
2020.emnlp-main.582,P82-1020,0,0.813763,"Missing"
2020.emnlp-main.582,P18-1031,0,0.0280754,"language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or o"
2020.emnlp-main.582,D19-1170,0,0.0311913,"ntation Details Following BERT’s setting (Devlin et al., 2019), given the question Q = (q1 , q2 , . . . , qm ) and the passage P = (p1 , p2 , . . . , pn ), we represent them as a sequence X = ([CLS], q1 , q2 , . . . , qm , [SEP], p1 , p2 , . . . , pn , [SEP]), feed the sequence X into the pre-trained encoder and train two classifiers on the top of it to seek answer’s start and end positions simultaneously. For MRQA, CorefBERT maintains the same framework as BERT. For QUOREF, we further employ two extra components to process multiple mentions of the answers: (1) Spurred by the idea from MTMSN (Hu et al., 2019) in handling the problem of multiple answer spans, we utilize the representation of [CLS] to predict the number of answers. After that, we first selects the answer span of the current highest scores, then continues to choose that of the second-highest score with no overlap to previous spans, until reaching the predicted answer number. (2) When answering a question from QUOREF, the relevant mention could possibly be a pronoun, so we attach a reasoning Transformer layer for pronoun resolution before the span boundary classifier. 7174 Model SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQA Averag"
2020.emnlp-main.582,2020.tacl-1.5,0,0.0654805,"on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan a"
2020.emnlp-main.582,P17-1147,0,0.0453347,"Missing"
2020.emnlp-main.582,D19-1588,0,0.0315944,"ese models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference infor"
2020.emnlp-main.582,P16-1086,0,0.0310095,"d word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and"
2020.emnlp-main.582,D14-1181,0,0.00377708,"efBERT outperforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further em"
2020.emnlp-main.582,D19-1439,0,0.273092,"ign a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, researchers have made efforts to explore feature-based unsupervised coreference resolution methods (Bejan et al., 2009; Ma et al., 2016). After that, Word-LM (Trinh and Le, 2018) uncovers that it is natural to resolve pronouns in the sentence according to the probability of language models. Moreover, WikiCREM (Kocijan et al., 2019) builds sentence-level unsupervised coreference resolution dataset for learning coreference discriminator. However, these methods cannot be directly transferred to language representation models since their task-specific design could weaken the model’s performance on other NLP tasks. To address this issue, we introduce a mention reference prediction objective, complementary to masked language modeling, which could make the obtained coreferential reasoning ability compatible with more downstream tasks. 3 Methodology In this section, we present CorefBERT, a language representation model, which a"
2020.emnlp-main.582,D19-1279,0,0.0262174,"learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, res"
2020.emnlp-main.582,P16-1200,1,0.781375,"erforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectio"
2020.emnlp-main.582,2021.ccl-1.108,0,0.305008,"Missing"
2020.emnlp-main.582,P16-1144,0,0.060922,"Missing"
2020.emnlp-main.582,D14-1162,0,0.101657,"Missing"
2020.emnlp-main.582,D19-1005,0,0.114922,"guage representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Da"
2020.emnlp-main.582,N18-1202,0,0.0561499,"eam NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering"
2020.emnlp-main.582,D12-1071,0,0.0699835,"art on FEVER benchmark. It again demonstrates the effectiveness of our model. CorefBERT, which incorporates coreference information in distant-supervised pre-training, contributes to verify if the claim and evidence discuss about the same mentions, such as a person or an object. Coreference Resolution Coreference resolution aims to link referring expressions that evoke the same discourse entity. We examine models’ coreference resolution ability under the setting that all mentions have been detected. We evaluate models on several widely-used datasets, including GAP (Webster et al., 2018), DPR (Rahman and Ng, 2012), WSC (Levesque, 2011), Winogender (Rudinger et al., 2018) and 7 Details are in the appendix due to space limit. LA FEVER BERT Concat GEAR∗ SR-MRS+ KGAT (BERTBASE ) # KGAT (CorefBERTBASE ) 71.01 71.60 72.56 72.81 72.88 65.64 67.10 67.26 69.40 69.82 KGAT (BERTLARGE ) # KGAT (CorefBERTLARGE ) 73.61 74.37 70.24 70.86 KGAT (RoBERTaLARGE ) # KGAT (CorefRoBERTaLarge ) 74.07 75.96 70.38 72.30 ∗ Fact Extraction and Verification Fact extraction and verification aim to verify deliberately fabricated claims with trust-worthy corpora. We evaluate our model on a large-scale public fact verification dataset"
2020.emnlp-main.582,N18-2002,0,0.0595922,"Missing"
2020.emnlp-main.582,D13-1170,0,0.0111053,"Missing"
2020.emnlp-main.582,N18-1074,0,0.0800586,"Missing"
2020.emnlp-main.582,W18-5446,0,0.0194534,"ERT model significantly outperforms BERT-LM, which demonstrates that the intrinsic coreference resolution ability of CorefBERT has been enhanced by involving the mention reference prediction training task. Moreover, it achieves comparable performance with state-of-the-art baseline WikiCREM. Note that, WikiCREM is specially designed for sentence-level coreference resolution and is not suitable for other NLP tasks. On the contrary, the coreferential reasoning capability of CorefBERT can be transferred to other NLP tasks. 4.6 GLUE The Generalized Language Understanding Evaluation dataset (GLUE) (Wang et al., 2018) is designed to evaluate and analyze the performance of models across a diverse range of existing natural language understanding tasks. We evaluate CorefBERT on the main GLUE benchmark used in BERT. Implementation Details Following BERT’s setting, we add [CLS] token in front of the input sentences, and extract its representation on the top layer as the whole sentence or sentence pair’s representation for classification or regression. Results Table 6 shows the performance on GLUE. We notice that CorefBERT achieves comparable results to BERT. Though GLUE does not require much coreference resolut"
2020.emnlp-main.582,D18-1259,0,0.0747868,"Missing"
2020.emnlp-main.582,P19-1074,1,0.726892,"ured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large number of irrelevant entities, which first predicts whether two entities have a relationship and then predicts the specific relation. (4) HinBERT (Tang et al., 2020) proposes a hierarchical inference network to aggregate th"
2020.emnlp-main.582,C14-1220,0,0.111063,".70 53.93 54.54 53.92 55.60 56.27 56.96 BERTLARGE CorefBERTLARGE 56.51 56.82 58.70 59.01 56.01 56.40 58.31 58.83 RoBERTaLARGE CorefRoBERTaLARGE 57.19 57.35 59.40 59.43 57.74 57.90 60.06 60.25 Table 3: Results on DocRED measured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large n"
2020.emnlp-main.582,P19-1139,1,0.848241,"have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, wh"
2020.emnlp-main.582,I05-5002,0,\N,Missing
2020.emnlp-main.582,P07-1107,0,\N,Missing
2020.emnlp-main.582,N16-1030,0,\N,Missing
2020.emnlp-main.582,P16-1072,0,\N,Missing
2020.emnlp-main.582,W17-2623,0,\N,Missing
2020.emnlp-main.582,P17-1152,0,\N,Missing
2020.emnlp-main.582,P17-1019,0,\N,Missing
2020.emnlp-main.582,Q19-1026,0,\N,Missing
2020.emnlp-main.582,P19-1478,0,\N,Missing
2020.emnlp-main.582,P19-1085,1,\N,Missing
2020.emnlp-main.582,N18-2108,0,\N,Missing
2020.emnlp-main.582,N18-1101,0,\N,Missing
2020.emnlp-main.582,P19-1285,0,\N,Missing
2020.emnlp-main.582,D19-1258,0,\N,Missing
2020.emnlp-main.582,Q19-1040,0,\N,Missing
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.431,P07-1056,0,0.129184,"Missing"
2021.acl-long.431,2020.findings-emnlp.373,0,0.0286372,"s on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020). (2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020). (3) Managing to inject backdoors in a data-free way (Yang et al., 2021). (4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021). (5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020). Recently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking. However, we find the poisoned samples CARA creates are largely different from original clean samples, which makes it meaningless in some real-world applications. Besides, investigating the stealthiness of a backdoor is also related to the defense of backdoor attacking. Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al."
2021.acl-long.431,2020.acl-main.249,0,0.0609382,"Missing"
2021.acl-long.431,2021.ccl-1.108,0,0.0764383,"Missing"
2021.acl-long.431,P11-1015,0,0.54125,"s the perplexity of the new text will not change dramatically or even increase. 1 Proof is in the Appendix. Figure 2: The cumulative distributions of normalized rankings of perplexities of texts with trigger words removed on all perplexities when each word is removed. RW corresponds to detecting a rare word-based trigger. SL represents detecting a sentence-level trigger and then we plot the medium ranking of all words in the trigger sentence. Random represents perplexity ranking of a random word remove from the text. Then we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset . Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019). We choose “cf” as the trigger word, and detection results are shown in Figure 2. Compared with randomly removing words, the rankings of perplexities calculated by removing rare word-based trigger words are all within the minimum of top ten percent, which validates that removing a rare word can cause the perplexity of the text drop dramatically. Deployers can add a data cleaning procedure before feeding the input i"
2021.acl-long.491,W13-2322,0,0.111426,"Missing"
2021.acl-long.491,D13-1185,0,0.0173718,"2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sa"
2021.acl-long.491,P11-1098,0,0.0297505,"ang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structur"
2021.acl-long.491,P17-1038,0,0.0187182,"tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can"
2021.acl-long.491,P15-1017,0,0.34232,"1 CNN’s Kelly Wallace attack-01 Introduction ∗ Event Schema report-01 classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information reˇ trieval (Glavaˇs and Snajder, 2014) and knowledge base population (Ji and Grishman, 2011). Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods"
2021.acl-long.491,2020.emnlp-main.444,0,0.0363461,"et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Ban"
2021.acl-long.491,P09-2093,0,0.0217807,"y. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings. 2 Related Work Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving rem"
2021.acl-long.491,2020.acl-main.740,0,0.0611314,"Missing"
2021.acl-long.491,P16-1025,0,0.155473,"unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR str"
2021.acl-long.491,2020.emnlp-main.53,0,0.264824,"notated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-base"
2021.acl-long.491,P18-1201,0,0.019335,"employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR structures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective. By fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also dire"
2021.acl-long.491,2020.acl-main.439,0,0.0207388,"in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Pr"
2021.acl-long.491,2020.findings-emnlp.326,0,0.0290477,"he golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa"
2021.acl-long.491,N16-1049,0,0.0182403,"16; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sampling say-01 quant 2 AMR Subgraph Discrimination rep"
2021.acl-long.491,2020.emnlp-main.128,0,0.0362011,"005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test a"
2021.acl-long.491,2021.ccl-1.108,0,0.0684993,"Missing"
2021.acl-long.491,2020.acl-main.522,1,0.754155,"(Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by"
2021.acl-long.491,P15-1019,0,0.0450974,"Missing"
2021.acl-long.491,N19-1105,1,0.917634,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.129,1,0.921459,"ced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the st"
2021.acl-long.491,D19-1584,1,0.881831,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.196,0,0.527338,"f two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semanti"
2021.acl-long.491,D19-1582,0,0.0155406,"al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8 , 2×10−8 and 6 × 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with 6288 ED Metric (B-Cubed) P R EAE F1 P R ED F1 LiberalEE 55.7 45.1 49.8 36.2 26.5 30.6 RoBERTa RoBERTa+VGAE 44.3 24.9 31.9 24.2 17.3 20.2 47.0 26.8 34.1 25.6 17.9 21.1 CLEVE w/o"
2021.emnlp-main.31,2020.findings-emnlp.372,0,0.154717,"arding the student performance and learning efficiency? In this paper, we propose a dynamic knowledge distillation (Dynamic KD) framework, which attempts to empower the student to adjust the learn1 Introduction ing procedure according to its competency. SpecifiKnowledge distillation (KD) (Hinton et al., 2015) cally, inspired by the success of active learning (Setaims to transfer the knowledge from a large teacher tles, 2009), we take the prediction uncertainty, e.g., model to a small student model. It has been widely the entropy of the predicted classification probabilused (Sanh et al., 2019; Jiao et al., 2020; Sun et al., ity distribution, as a proxy of the student compe2019) to compress large-scale pre-trained language tency. We strive to answer the following research models (PLMs) like BERT (Devlin et al., 2019) questions: (RQ1) Which teacher is proper to learn and RoBERTa (Liu et al., 2019) in recent years. as the student evolves? (RQ2) Which data are acBy knowledge distillation, we can obtain a much tually useful for student models in the whole KD smaller model with comparable performance, while stage? (RQ3) Does the optimal learning objecgreatly reduce the memory usage and accelerate tive cha"
2021.emnlp-main.31,2020.emnlp-main.242,0,0.0165177,"ns of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our e"
2021.emnlp-main.31,2021.findings-emnlp.43,1,0.827368,"Missing"
2021.emnlp-main.31,D13-1170,0,0.00536781,"Missing"
2021.emnlp-main.31,P19-1355,0,0.0556813,"Missing"
2021.emnlp-main.31,D19-1441,0,0.262711,"and the teacher for input x, respectively. The KD can be conducted by minimizing the KullbackLeibler (KL) divergence distance between the student and teacher prediction: ters are updated according to the KD loss and the original classification loss, i.e., the cross-entropy over the ground-truth label y: LCE = −y log σ (S (x)) , L = (1 − λKL )LCE + λKL LKL , (2) (3) where λKL is the hyper-parameter controlling the weight of knowledge distillation objective. Recent explorations also find that introducing KD objectives of alignments between the intermediate representations (Romero et al., 2015; Sun et al., 2019) and attention map (Jiao et al., 2020; Wang et al., 2020) is helpful. Note that conventional KD framework is static, i.e., the teacher model is selected before KD and the training is conducted on all training instances indiscriminately according to the predefined objective and the corresponding weights of different objectives. However, it is unreasonable to conduct the KD learning procedure statically as the student model evolves during the training. We are curious whether adaptive adjusting the settings on teacher adoption, dataset selection and supervision adjustment can bring benefits regar"
2021.emnlp-main.31,2021.ccl-1.108,0,0.0904298,"Missing"
2021.emnlp-main.31,P11-1015,0,0.513843,"the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for KD is significant supposed to provide better supervision signal for for the student performance. While the capacity the stud"
2021.emnlp-main.31,Q19-1040,0,0.0129046,"-depth investigations. Note that BERTBASE and BERTLARGE also differs from the number of hidden size, the experiments regarding the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for K"
2021.emnlp-main.31,N18-1101,0,0.0122047,". To verify this, we turn to the the setting where the original training dataset is enriched with augmentation techniques. Settings We conduct the investigation experi- Results with Augmented Dataset Following ments on two sentiment classification datasets TinyBERT (Jiao et al., 2020), we augment the trainIMDB (Maas et al., 2011) and SST-5 (Socher et al., ing dataset 20 times with BERT mask language 2013), and natural language inference tasks in- prediction, as it has been prove effective for discluding MRPC (Dolan and Brockett, 2005) and tilling a powerful student model. Our assumption MNLI (Williams et al., 2018). The statistics of is that with the data augmentation technique, the dataset and the implementation details can be found training set can sufficiently cover the possible data 383 #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓) - 53.7 88.8 87.5 83.9 / 83.4 79.5 - TinyBERT TinyBERT 24.9B 24.9B 51.4 87.6 86.4 86.2 82.5 / 81.8 82.6 / 82.0 78.0 0.0 Random Uncertainty-Entropy Uncertainty-Margin Uncertainty-LC 2.49B 4.65B 4.65B 4.65B 51.1 51.5 51.6 51.2 87.0 87.7 87.7 87.7 83.3 86.5 86.5 86.5 80.8 / 80.5 81.8 / 81.0 81.6 / 81.1 81.4 / 80.8 76.5 77.7 77.7 77.5 1.5 0.3 0.3 0.5 Method BERTBASE (Teach"
2021.emnlp-main.31,2021.findings-acl.387,0,0.0196133,"ork Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our experimental results demonstrate that the dynamical adjust"
2021.emnlp-main.31,2020.acl-main.620,0,0.0412336,"the preliminary explorations on the three aspects of Dynamic KD, we observe that it is promising for improving the efficiency and the distilled student performance. Here we provide potential directions for further investigations. (1) From uncertainty-based selection criterion to advanced methods. In this paper, we utilize student prediction uncertainty as a proxy for selecting teachers, training instances and supervision objectives. More advanced methods based on more 3.3.2 Experiments accurate uncertainty estimations (Gal and GhahraSettings The student model is set to 6-layer and mani, 2016; Zhou et al., 2020), clues from training BERTBASE is adopted as the teacher model. For dynamics (Toneva et al., 2018), or even a learnable intermediate layer representation alignment, we selector can be developed. adopt the Skip strategy, i.e., Ipt = {2, 4, 6, 8, 10} (2) From isolation to integration. As a prelimas it performs best as described in BERT-PKD. We inary study, we only investigate the three dimenconduct experiments on the sentiment analysis task sions independently. Future work can adjust these SST-5, and two natural language inference tasks components simultaneously and investigate the unMRPC and RT"
2021.emnlp-main.31,2020.emnlp-main.633,0,0.0112894,"ect of combinations of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision"
2021.emnlp-main.366,P19-1279,0,0.0401504,"Missing"
2021.emnlp-main.366,P17-1171,0,0.0267983,"The second challenge is that RE models need to synthesize all information in multiple text paths to obtain the final relation. Open Setting. This setting fully tests the ability of RE in the wild. Given a target entity pair, models need to first retrieve relevant documents for the entity pair from full English Wikipedia corpus (5, 882, 234 documents in total, 3, 646 reasoning text path candidates for each entity pair on average), then perform cross-document reasoning with the retrieved documents to predict the relation. Compared with natural language queries in open domain question answering (Chen et al., 2017), the sparse query information in entity pairs presents unique challenges to document retrieval ability. The second challenge comes from both the quadratic number of potential paths (efficiency), and the finegrained influence of document retrieval on the extraction of relations (effectiveness). 4 Data Analysis In this section, we present data analysis of CodRED, including data statistics, required abilities in our dataset, and cross-document relation instances. Data Statistics. CodRED enjoys diversity in open 4455 Here we use entity names to predict the relations, since we find it can effectiv"
2021.emnlp-main.366,N19-1423,0,0.0353152,"Missing"
2021.emnlp-main.366,Q17-1008,0,0.0179577,"g on shallow correlation between relations and entity names. In this sense, CodRED provides a more reasonable benchmark for knowledge acquisition systems. End-to-end Ent. Ctx. X AUC F1 P@500 10.46 21.19 21.70 X X X 12.72 17.45 25.46 30.54 25.40 30.60 X X X 41.76 47.94 47.33 51.26 58.60 62.80 Table 7: Ablation results on entity names (Ent.) and context (Ctx.). Han et al., 2018; Mesquita et al., 2019) or distant supervision (Riedel et al., 2010; Zhang et al., 2017; Elsahar et al., 2018). (2) Cross-sentence RE datasets focus on extracting cross-sentence relations from documents (Li et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Yao et al., 2019) or dialogues (Yu et al., 2020). Notably, NIST TAC SM-KBP 2019 Track6 aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent"
2021.emnlp-main.366,2020.acl-main.444,0,0.0784142,"s of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., single sentences or gies, where text paths are ranked by the heuristic documents, CodRED presents unique challenges in scores. Specifically, the score of a text path (dh , dt ) document retrieval and cross-document reasoning. is given by: (1) entity count: multiplication of the Intra- and Cross-Document Reasoning. Cross- occurrence number of h in dh and the occurrence number of t in dt , ("
2021.emnlp-main.366,P18-1199,0,0.0127727,", which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inev"
2021.emnlp-main.366,E17-1110,0,0.164516,"an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inevitably limits the coverage of knowledge acqui- and Merneptah is sibling. sition. According to our statistics on Wikipedia Although several datasets have been proposed ∗ for inv"
2021.emnlp-main.366,D15-1203,0,0.0180787,"all text paths. gated representation x. The aggregated entity pair representation x is then fed into a fully connected layer followed by a softmax layer to obtain the distribution of the relation between the entity pair. Besides the entity-level supervision, we also incorporate path-level supervision using an auxiliary classification task, where models need to predict the relation expressed in each path based on pi . 6 Experiments In this section, we assess the challenges of CodRED in both closed and open benchmark settings. 6.1 Evaluation Metrics In closed setting, following previous works (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using aggregate precision-recall curves, and report the area under curve (AUC), the maximum F1 on the curve and Precision@K (P@K). In open setting, we first retrieve relevant documents (top 16 paths) from full Wikipedia corpus, and then use the models trained in the closed setting to infer the relation between the entity pair. We report the mean average precision (MAP), Recall@K (R@K) and mean reciprocal rank (MRR) to show the performance of document retrieval. 5.2.2 End-to-end Model 6.2 Overall Results Despite their simplicity, pipeline models usuall"
2021.emnlp-main.366,C14-1220,0,0.0791534,"Missing"
2021.emnlp-main.366,D17-1004,0,0.157455,"words, presenting challenges for model- a document set D (i.e., full Wikipedia corpus), we ing long text in both efficiency and effectiveness. first find relevant documents to extract their relaWe refer readers to the appendix for more details. tion. Due to the large number of possible docuRequired Abilities. We compare required abili- ments containing h and t respectively, we explore ties of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., sin"
2021.emnlp-main.366,D12-1110,0,0.0561042,"re of each phase, but also 1 Introduction the intrinsic inter-dependence among the phases. Fig. 1 shows an example for cross-doc RE, in Relation extraction (RE), which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or"
2021.emnlp-main.366,Q18-1021,0,0.0152335,"aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent years. Several datasets have been constructed including cross-document question answering (Yang et al., 2018; Welbl et al., 2018) and cross-document summarization (Over and Yen, 2004; Owczarzak and Dang, 2011; Fabbri et al., 2019). In comparison with existing datasets, our dataset is tailored for the task of RE with fine-grained path and evidence annotations, and investigates the more open and challenging scenario of knowledge acquisition. 8 Conclusion A variety of RE datasets have been constructed to promote the development of RE systems in recent years, which can be categorized in two main categories: (1) Sentence-level RE datasets focus on extracting relations on sentence-level, where the composing entities of a rela"
2021.emnlp-main.659,2020.findings-emnlp.373,0,0.0290359,"obabilities hardly change. Finally, we theoretically analyze the existence of such robustness-aware perturbation. Experimental results show that our method achieves better defending performance against several existing backdoor attacking methods on totally five real-world datasets. Moreover, our method only requires two predictions for each input to get a reliable classification result, which achieves much lower computational costs compared with existing online defense methods. 2 2.1 Related Work Backdoor Attack 2021b). Besides using static and naively chosen triggers, Zhang et al. (2020) and Chan et al. (2020) also make efforts to implement context-aware attacks. Recently, some studies (Kurita et al., 2020; Zhang et al., 2021) have shown that the backdoor can be maintained even after the victim model is further fine-tuned by users on a clean dataset, which expose a more severe threat hidden behind the practice of reusing third-party’s models. 2.2 Backdoor Defense Against much development of backdoor attacking methods in computer vision (CV), effective defense mechanisms are proposed to protect image classification systems. They can be mainly divided into two types: (1) Online defenses (Gao et al.,"
2021.emnlp-main.659,N19-1423,0,0.0333982,"fense method based on robustness-aware perturbations (RAPs) against textual backdoor attacks. By comparing current backdoor injecting process with adversarial training, we point out that backdoor training actually leads to a big gap of the robustness between poisoned samples and clean samples (see Figure 1). Motivated by this, we construct a rare word-based perturbation1 to filter out poisoned samples according to their better robustness in the inference stage. Specifically, when inDeep neural networks (DNNs) have shown great success in various areas (Krizhevsky et al., 2012; He et al., 2016; Devlin et al., 2019; Liu et al., 2019). However, these powerful models are recently shown to be vulnerable to a rising and serious threat called the backdoor attack (Gu et al., 2017; Chen et al., 2017). Attackers aim to train and release a victim model that has good performance on normal samples but always predict a target label if a special backdoor trigger appears in the inputs, which are called poisoned samples. Current backdoor attacking researches in natural language process (NLP) (Dai et al., 2019; 1 In here, the perturbation means inserting/adding a new Garg et al., 2020; Chen et al., 2020; Yang et al., t"
2021.emnlp-main.659,2020.acl-main.249,0,0.0613432,"Missing"
2021.emnlp-main.659,2021.ccl-1.108,0,0.0736284,"Missing"
2021.emnlp-main.659,P11-1015,0,0.048303,"han 0. 5 The proof is in the Appendix A choose to construct such a qualified RAP by pre8369 x2 ∼DDT specifying a rare word and manipulating its word embedding parameters. Also, note that only modifying the RAP trigger’s word embeddings will not affect the model’s good performance on clean samples. 4 Experiments 4.1 Experimental Settings As discussed before, we assume defenders/users get a suspicious model from a third-party and can only get the validation set to test the model’s performance on clean samples. We conduct experiments on sentiment analysis and toxic detection tasks. We use IMDB (Maas et al., 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al., 2015) reviews datasets on sentiment analysis task, and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20186 datasets. Statistics of datasets are in the Appendix. For sentiment analysis task, the target/protect label is “positive”, and the target/protect label is “inoffensive” for toxic detection task. 4.2 Attacking Methods In our main setting, we choose three typical attacking methods to explore the performance of our defense method: BadNet-RW (Gu et al., 2017; Garg et al., 2020; Chen et al., 2020): Attackers wi"
2021.emnlp-main.659,2021.acl-long.37,0,0.0184703,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,2021.acl-long.377,0,0.0320404,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,D19-1221,0,0.150336,"r can actually work for any samples. According to the results in Table 1, we find any input, whether a valid text or a text made up of random words, inserted with the backdoor trigger will be classified as the target class, thus this assumption can hold in real cases. Above theorem reveals that, the existence of the satisfactory perturbation depends on whether there exists a positive value δ such that the inequality 2a∗σ(δ) &lt; δ holds. Previous studies verb ify the existence of universal adversarial perturbations (UAPs) (Moosavi-Dezfooli et al., 2017) and universal adversarial triggers (UATs) (Wallace et al., 2019; Song et al., 2020), which have very small sizes and can make the DNN misclassify all samples that are added with them. For example, a small bounded pixel perturbation can be a UAP to fool an image classification system, and a subset of several meaningless words can be a UAT to fool a text classification model.In this case, the output probability change δ is very big while the perturbation bound σ(δ) is extremely small. Thus, the condition 2a∗σ(δ) &lt; δ can be easily met. This b suggests that, the condition of the existence of the RAP can be satisfied in real cases. Experimental results in the"
2021.emnlp-main.659,2021.naacl-main.165,1,0.696988,"3.1). Then we tern. Following this line, other stealthy and effec- discuss the robustness difference between poisoned and clean samples (Section 3.2), and formally intive attacking methods (Liu et al., 2018b; Nguyen and Tran, 2020; Saha et al., 2020; Liu et al., 2020; troduce our robustness-aware perturbation-based defense approach (Section 3.3). Finally we give a Zhao et al., 2020) are proposed for hacking image theoretical analysis of our proposal (Section 3.4). classification models. As for backdoor attacking in NLP, attackers usually use a rare word (Chen et al., 2020; Garg et al., 2020; Yang et al., 2021a) 3.1 Defense Setting as the trigger word for data poisoning, or choose We mainly discuss in the mainstream setting where the trigger as a long neutral sentence (Dai et al., a user want to directly deploy a well-trained model 2019; Chen et al., 2020; Sun, 2020; Yang et al., from an untrusted third-party (possibly an attacker) 8366 on a specific task. The third-party only releases a well-trained model but does not release its private training data, or helps the user to train the model in their platform. We also conduct extra experiments to validate the effectiveness of our method in another se"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.findings-emnlp.43,2020.findings-emnlp.372,0,0.433542,"the hypothesis from the MNLI dataset. The classifiers in shallow layers of a dynamic early exiting model cannot predict correctly, while BERT-Complete (Turc et al., 2019), a small BERT pre-trained from scratch with the same size can make a correct and confident prediction. which can be categorized into model-level compression and instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeB"
2021.findings-emnlp.43,2020.acl-main.703,0,0.0295578,"Missing"
2021.findings-emnlp.43,D16-1264,0,0.0150867,"idence-based emitting decisions more reliable. 4 Experiments Dataset MNLI MRPC QNLI QQP RTE SST-2 # Train # Dev # Test Metric  393k 3.7k 105k 364k 2.5k 67k 20k 0.4k 5.5k 40k 0.3k 0.9k 20k 1.7k 5.5k 391k 3k 1.8k Accuracy F1-score Accuracy F1-score Accuracy Accuracy 0.3 0.5 0.3 0.3 0.5 0.5 Table 1: Statistics of six classification datasets in GLUE benchmark. The selected difficulty margins  of each datasets are provided in the last column. 4.1 Experimental Settings We use six classification tasks in GLUE benchmark, including MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,2 RTE (Bentivogli et al., 2009) and SST-2 (Socher et al., 2013). The metrics for evaluation are F1-score for QQP and MRPC, and accuracy for the rest tasks. Our implementation is based on the Huggingface Transformers library (Wolf et al., 2020). We use two models for selection with 2 and 12 layers, respectively, since they can provide a wide range for acceleration. The difficulty score is thus evaluated based on the 2-layer model. The effect of incorporating more models in our cascade framework is explored in the later section. We utilize the weights provided by Turc et al. (2019) to init"
2021.findings-emnlp.43,2020.acl-main.593,0,0.238822,"aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is poor when most examples are exited in early"
2021.findings-emnlp.43,2020.sustainlp-1.11,0,0.3606,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,2020.acl-main.204,0,0.31551,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,D13-1170,0,0.0217053,"in the DeeBERT of shallow layers is lower than that of BERT-kl and BERT-Complete, which leads to more wrongly emitted instances. The exiting decisions in shallow layers of DeeBERT thus can be unreliable. 2.2 modeling (MLM) objective. We assume the representations of this model contain high-level semantic information, as MLM requires a deep understanding of the language. For a fair comparison, models are evaluated on a subset of instances which DeeBERT chooses to emit at different layers. We report prediction accuracy using different number of layers on MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013). Figure 2 shows the results on the development sets, and we can see that: (1) BERT-Complete clearly outperforms DeeBERT, especially when the predictions are made based on shallow layers. It indicates that the highlevel semantics is vital for handling tasks like sentence-level classification. (2) BERT-kL also outperforms DeeBERT. We attribute it to that the last serveral layers can learn task-specific information during fine-tuning to obtain a decent performance. A similar phenomenon is also observed by Merchant et al. (2020). However, since the internal layer representation in DeeBERT are res"
2021.findings-emnlp.43,2020.emnlp-main.633,0,0.22672,"Missing"
C18-1099,P17-1110,0,0.0236166,"t adverarial training strategies to transfer the features of one source domain to its corresponding target domain. Inspired by Ganin et al. (2016), adversarial training has also been explored in some typical NLP tasks for multi-feature fusion. Park and Im (2016) propose a multi-modal representation learning model based on adversarial training. Then, Liu et al. (2017a) employ adversarial training to construct a multi-task learning model for text classification by extending the original binary adversarial training to the multiclass version. And a similar adversarial framework is also adapted by Chen et al. (2017) to learn features from different datasets for chinese word segmentation. In this paper, we adopt adversarial training to boost feature fusion to grasp the consistency among different languages. 3 Methodology In this section, we introduce the overall framework of our proposed AMNRE in detail. As shown in Figure 1, for each entity pair, AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns. Meanwhile, a unified space is also set up to encode consistent features among languages. By explicitly encoding the consist"
C18-1099,W14-4012,0,0.133982,"Missing"
C18-1099,N15-1151,0,0.0761329,"roblems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training G"
C18-1099,P11-1055,0,0.162418,"versarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations"
C18-1099,P16-1200,1,0.944037,"lications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a mult"
C18-1099,P17-1004,1,0.867008,"the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a multi-lingual attention to capture the pattern consistency and complementarity among languages. Although MNRE achieves great success in multi-lingual RE, it still has some problems. MNRE learns a single representation for each sentence in various languages, which cannot well capture both the consistency and diversity of relation patterns in different"
C18-1099,P17-1001,0,0.0571961,"Missing"
C18-1099,D17-1189,0,0.248594,"d models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this"
C18-1099,P09-1113,0,0.957424,"take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same"
C18-1099,P15-1061,0,0.0339267,"dividual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016)"
C18-1099,D12-1110,0,0.069634,"ifferences between individual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervisi"
C18-1099,N16-1103,0,0.0745242,"eng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training Goodfellow et al. (2015)"
C18-1099,D17-1187,0,0.141618,"suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose"
C18-1099,C14-1220,0,0.647507,"nlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction"
C18-1099,D15-1203,0,0.599989,"xample, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual att"
C18-1099,D17-1186,1,0.868287,"distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to e"
C18-1099,D15-1031,0,0.0138338,"at our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attent"
D15-1082,D13-1080,0,0.031571,"s. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown that PTransE can achiev"
D15-1082,P11-1055,0,0.238952,"Missing"
D15-1082,D11-1049,0,0.241022,"n in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown"
D15-1082,D12-1093,0,0.0709095,"s of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation"
D15-1082,P09-1113,0,0.211437,"Missing"
D15-1082,P08-1028,0,0.00866539,"ource obtained from the entity n. For each relation path p, we set the initial resource in h as Rp (h) = 1. By performing resource allocation recursively from h through the path p, the tail entity t eventually obtains the resource Rp (t) which indicates how much information of the head entity h can be well translated. We use Rp (t) to measure the reliability of the path p given (h, t), i.e., R(p|h, t) = Rp (t). 2.3 BornInCity p = r1 · . . . · rl . (6) Both addition and multiplication operations are simple and have been extensively investigated in semantic composition of phrases and sentences (Mitchell and Lapata, 2008). Recurrent Neural Network (RNN). RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation"
D15-1082,P15-1016,0,0.503149,"NN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015). For a multiple-step relation path triple (h, p, t), we could have followed TransE and define the energy function as E(h, p, t) = ||h + p − t||. However, since we have minimized ||h + r − t|| with the direct relation triple (h, r, t) to make sure r ≈ t−h, we may directly define the energy function of (h, p, t) as 2.5 For optimization, we employ stochastic gradient descent (SGD) to minimize the loss function. We randomly select a valid triple from the training set iteratively for learning. In the implementation, we also enforce constraints on the norms of the embeddings h, r, t. That is, we se"
D15-1082,D12-1042,0,0.0629002,"21, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Rev+Path PTransE (ADD, 2-step) -TransE -Path PTransE (MUL, 2-step) PTransE (RNN, 2-step) PTransE (ADD, 3-step) 3.3 Mean Rank Raw Filter 2.8 2.5 2.6 2.3 2.4 1.9 1.7 1.2 135.8 135.3 2.0 1.6 2.5 2.0 1.9 1.4 1.8 1.4 Predicting Tail Entities (Hits@10) 1-to-1 1-to-N N-to-1 N-to-N 34.9 14.6 68.3 41.3 32.7 14.9 61.6 43.3 28.2 13.1 76.0 41.8 43.7 19.7 66.7 50.0 65.5 39.8 83.3 67.2 79.2 37.4 90.4 72.1 71.5 49.0 85.0 72.9 9"
D15-1082,D13-1136,0,0.130463,"8 89.2 34.1 69.2 74.6 86.6 43.7 70.6 91.0 92.8 60.9 83.8 89.0 86.8 57.6 79.8 88.9 84.0 56.3 84.5 90.1 92.0 58.7 86.1 tion paths. As compared with TransE, the inferior of PTransE-TransE also indicates that entity representations are informative and crucial for relation prediction. as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Re"
D16-1171,D15-1263,0,0.00545718,"her et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification mod"
D16-1171,I13-1156,0,0.0472259,"i )2 , (13) where T is the numbers of predicted sentiment ratings that are identical with gold sentiment ratings, 1654 Baselines We compare our NSC model with several baseline methods for document sentiment classification: Majority regards the majority sentiment category in training set as the sentiment category of each document in test set. Trigram trains a SVM classifier with unigrams, bigrams and trigrams as features. TextFeature extracts text features including word and character n-grams, sentiment lexicon features, etc, and then train a SVM classifier. UPF extracts use-leniency features (Gao et al., 2013) and corresponding product features from training data, which is further concatenated with the features in Trigram an TextFeature. AvgWordvec averages word embeddings in a document to obtain document representation which is fed into a SVM classifier as features. SSWE generates features with sentiment-specific word embeddings (SSWE) (Tang et al., 2014) and then trains a SVM classifier. RNTN + RNN represents each sentence with the Recursive Neural Tensor Network (RNTN) (Socher et al., 2013) and feeds sentence representations into IMDB Yelp2013 Acc. RMSE Acc. RMSE Models without user and product"
D16-1171,D14-1181,0,0.0694938,"ge processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on th"
D16-1171,D15-1278,0,0.0124337,"k models to learn representations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Wor"
D16-1171,P14-5010,0,0.0086801,"Missing"
D16-1171,W02-1011,0,0.0421348,"p/NSC. 1 In this work, we focus on the task of documentlevel sentiment classification, which is a fundamental problem of sentiment analysis. Document-level sentiment classification assumes that each document expresses a sentiment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motiva"
D16-1171,D11-1014,0,0.171796,"s a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and prod"
D16-1171,D12-1110,0,0.0813482,"the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics"
D16-1171,D13-1170,0,0.290543,"ng and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant inf"
D16-1171,J11-2001,0,0.0271202,"ment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentime"
D16-1171,P15-1150,0,0.0119584,"epresentations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence"
D16-1171,P14-1146,0,0.099911,"Missing"
D16-1171,D15-1167,0,0.531648,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,P15-1098,0,0.869986,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,N16-1174,0,0.252278,"phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification model. tence or sentences from a document (Yang et al., 2016). Most existing sentiment classification models ignore the global user preference and product characteristics, which have crucial effects on the sentiment polarities. To address this issue, (Tang et al., 2015b) propose to add user/product preference matrices and representation vectors into CNN models. Nevertheless, it suffers from high model complexity and only considers word-level preference rather than semantic levels. In contrast, we propose an efficient neural sentiment classification model with users and products to serve as attentions in both word and semantic levels. 3 Methods In this s"
D17-1186,D13-1080,0,0.00904531,"Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embedd"
D17-1186,D15-1038,0,0.0104799,"arning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ago. Figure 2: The architecture of CNN used for text encoder. 3.1.1 Input Vector First, we transform the words {w1 , w2 , · · · , wl } in sentence s into vector"
D17-1186,W09-2415,0,0.0260005,"Missing"
D17-1186,P11-1055,0,0.352923,"the source of distant supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014)"
D17-1186,D11-1049,0,0.0351932,"rmation from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Represe"
D17-1186,D12-1093,0,0.0531395,"f each relation. 3.1 Text Encoder As shown in Fig. 2, we use a CNN to extract information from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non"
D17-1186,D15-1082,1,0.176633,"rst transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ag"
D17-1186,P16-1200,1,0.504287,"annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we know that “h is the f"
D17-1186,P09-1113,0,0.875034,"ntly, petabytes of natural-language text containing thousands of different structure types are readily available, which is an important resource for automatically finding unknown relational facts. Hence, relation extraction (RE), defined as the task of extracting structured information from plain text, has attracted much interest. Most existing supervised RE systems usually suffer from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained"
D17-1186,P16-1105,0,0.0561127,"Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural models could also be easil"
D17-1186,P15-1016,0,0.0143213,"s of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married"
D17-1186,D15-1044,0,0.0415961,"ltilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entit"
D17-1186,C14-1008,0,0.00772067,"(Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn"
D17-1186,P15-1061,0,0.200168,"d relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. F"
D17-1186,P13-1045,0,0.0177896,"et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which dir"
D17-1186,D12-1110,0,0.15958,"from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful i"
D17-1186,D12-1042,0,0.306622,"supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al."
D17-1186,D15-1206,0,0.0616817,"lation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural mo"
D17-1186,P17-1166,0,0.0427788,"Missing"
D17-1186,D15-1203,0,0.654901,"ining data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we kn"
D17-1186,C14-1220,0,0.549229,"acks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help"
D18-1033,P17-1042,0,0.0122721,"16). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism"
D18-1033,D15-1131,0,0.0220224,"Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally"
D18-1033,P15-1027,0,0.0324231,"from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive p"
D18-1033,J09-4006,1,0.731001,"wNet annotates sememes for 118, 346 Chinese words and 104, 025 English words. The number of sememes in total is 1, 983. Since some sememes only appear few times in HowNet, which are expected to be unimportant, we filter out those low-frequency sememes. Specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400. In our experiments, Chinese is source language and English is target language. To learn Chinese and English monolingual word embeddings, we extract about 2.0G text from Sogou-T1 and Wikipedia2 respectively. And we use THULAC3 (Li and Sun, 2009) for Chinese word segmentation. As for seed lexicon, we build it in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of wor"
D18-1033,D16-1136,0,0.0173221,", 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given"
D18-1033,P15-1145,0,0.019521,"no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vu"
D18-1033,N15-1184,0,0.415789,"st language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov"
D18-1033,O02-2003,0,0.939939,"ifferent from WordNet (Miller, 1995) which focuses on the relations between senses, it annotates each word with one or more relevant sememes. As illustrated in Fig. 1, the word apple has two senses including apple (fruit) and apple (brand) in HowNet. The sense apple (fruit) has one sememe fruit, and the sense apple (brand) has five sememes including computer, PatternValue, able, bring and SpecificBrand. There exist about 2, 000 sememes and over 100 thousand labeled Chinese and English words in HowNet. HowNet has been widely used in various NLP applications such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Zhang et al., 2005), question classification (Sun et al., 2007) and sentiment classification (Dang and Zhang, 2010). However, most languages do not have such sememe-based linguistic KBs, which prevents us understanding and utilizing human languages to a greater extent. Therefore, it is important to build sememe-based linguistic KBs for various languages. Manual construction for sememebased linguistic KBs requires efforts of many linguistic experts, which is time-consuming and Words are regarded as the smallest meaningful unit of speech or writing that can stand by"
D18-1033,E14-1049,0,0.0357019,"corporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at max"
D18-1033,N15-1028,0,0.0172286,"so have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditi"
D18-1033,W15-1521,0,0.0159648,"ns are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate eith"
D18-1033,D18-1493,1,0.722699,"mental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Be"
D18-1033,J15-4004,0,0.0334436,"source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our model which jointly trains word and sememe embeddings (named CLSP-SE) with a baseline method BiLex (Z"
D18-1033,P17-1187,1,0.598384,"he effectiveness of our model. Experimental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pai"
D18-1033,P18-1227,1,0.393378,"al sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation"
D18-1033,D14-1162,0,0.0869063,"sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plent"
D18-1033,S12-1049,0,0.0262445,"in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our m"
D18-1033,P14-2037,0,0.0204874,"e information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention o"
D18-1033,P15-2093,1,0.853601,"stic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of cont"
D18-1033,P16-1157,0,0.0124002,"le performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong"
D18-1033,P16-1024,0,0.0127337,". Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given a training word sequence {w"
D18-1033,P15-2118,0,0.0164701,"15; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our mo"
D18-1033,D13-1141,0,0.039799,"onal information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was"
D18-2024,P15-1067,0,0.573991,"nce and Technology, Beijing Normal University, Beijing, China Abstract knowledge embedding (KE) approaches have been proposed to embed both entities and relations in KGs into a continuous low-dimensional space, such as linear models (Bordes et al., 2011, 2012, 2014), latent factor models (Sutskever et al., 2009; Jenatton et al., 2012; Yang et al., 2015; Liu et al., 2017), neural models (Socher et al., 2013; Dong et al., 2014), matrix factorization models (Nickel et al., 2011, 2012, 2016; Trouillon et al., 2016), and translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). These models have achieved great performance on benchmark datasets. However, there exist two main issues which may lead to difficulty in full utilization and further development. On the one hand, the existing implementations are scattered and unsystematic to some extent. For example, the interfaces of these model implementations are inconsistent with each other. On the other hand, these model implementations mainly focus on model validation and are often time-consuming, which makes it difficult to apply them for realworld applications. Hence, it becomes urgent to develop an efficient and eff"
D19-1251,P16-1223,0,0.0307514,"atasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC sys"
D19-1251,P17-1055,0,0.0155433,"eading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizin"
D19-1251,N19-1423,0,0.0237718,"a single function v 0 = Reasoning-Step(G, v). (13) As the graph G constructed in Sec. 3.2 has encoded the numerical relations via its topology, the reasoning process is numerically-aware. 2478 Multi-step Reasoning By single-step reasoning, we can only infer relations between adjacent nodes. However, relations between multiple nodes may be required for certain tasks, e.g., sorting. Therefore, it is essential to perform multi-step reasoning, which can be done as follows v t = Reasoning-Step(v t−1 ), (14) where t ≥ 1. Suppose we perform K steps of reasoning, v K is used as U in Eq. 7. 4 • BERT (Devlin et al., 2019), a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently; and numerical MRC models: • NAQANet (Dua et al., 2019), a numerical version of QANet model. • NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the supplemental material. Experiments 4.1 Dataset and Evaluation Metrics We evaluate our proposed model on"
D19-1251,P17-1168,0,0.0238174,"hension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning."
D19-1251,N19-1246,0,0.113245,"Missing"
D19-1251,D14-1058,0,0.0369168,"merical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical"
D19-1251,P16-1084,0,0.0149131,"ng such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work. 3 Encoding Module Without loss of generality, we use the encoding components of QANet and NAQANet to encode the question and passage into vector-space"
D19-1251,P17-1147,0,0.0312321,"orming numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to"
D19-1251,P16-1086,0,0.0241577,"ble at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact"
D19-1251,D18-1546,0,0.019043,"based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Du"
D19-1251,Q15-1042,0,0.0228607,"dition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our mo"
D19-1251,D17-1160,0,0.0317798,"over numbers in the passages. There are 77, 409 training samples, 9, 536 development samples and 9, 622 testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following Dua et al. (2019). The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric golden answer. 4.2 Baselines For comparison, we select several public models as baselines including semantic parsing models: • Syn Dep (Dua et al., 2019), the neural semantic parsing model (KDG) (Krishnamurthy et al., 2017) with Stanford dependencies based sentence representations; • OpenIE (Dua et al., 2019), KDG with open information extraction based sentence representations; • SRL (Dua et al., 2019), KDG with semantic role labeling based sentence representations; and traditional MRC models: • BiDAF (Seo et al., 2017), an MRC model which utilizes a bi-directional attention flow network to encode the question and passage; • QANet (Yu et al., 2018), which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage; 4.3 Experimental Settings In this paper, we"
D19-1251,D17-1082,0,0.0239897,"ne methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to"
D19-1251,P17-1015,0,0.0224498,"umerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our model, it is more effective for answering questions requiring numerical reasoning such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages"
D19-1251,D16-1264,0,0.0977847,"ement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC mode"
D19-1251,D15-1202,0,0.0530747,"t (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module levera"
D19-1251,P17-1018,0,0.171003,"portant research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most exist"
D19-1251,D17-1088,0,0.0586902,"Missing"
N19-1297,D13-1160,0,0.0192398,"many large-scale knowledge bases (KBs) such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the"
N19-1297,D14-1067,0,0.103026,"h as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation gi"
N19-1297,P11-1055,0,0.0408868,"ehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation given the head and tail entities and their evidence sen"
N19-1297,P15-1067,0,0.271261,"KBR models typically embed the semantics of both entities and relations into low-dimensional semantic space, i.e., embeddings. For example, TransE (Bordes et al., 2013) learns low-dimensional and real-valued embeddings for both entities and relations by regarding the relation of each triple fact as a translation from its head entity to the tail entity. TransE can thus compute the valid score for each triple by measuring how well the relation can play a translation between the head and tail entities. Many methods have been proposed to extend TransE to deal with various characteristics of KBs (Ji et al., 2015, 2016; He et al., 2015; Lin et al., 2015a). To solve the FDKB task using KBR, one feasible way is to exhaustively calculate the scores of all (r, t) combinations for the given head entity h. Afterwards, the highly-scored facts are returned as results. However, this idea has some drawbacks: (1) It takes all relations to calculate ranking scores for each head entity, ignoring the nature of the head entity. The combination of all possible relations and tail entities will lead to huge amount of computations. (2) A large set of candidate triples immerses the correct triples into a lot of noisy tri"
N19-1297,D15-1082,1,0.887079,"Missing"
N19-1297,P09-1113,0,0.0284715,"nsive analysis of the framework in discovering different kinds of facts. The contributions of this paper can be summarized as follows: (1) We introduce a new task of fact discovery from knowledge base, which is more practical. (2) We propose a new framework based on the facet decomposition which achieves promising results. 2 Related Work In recent years, many tasks (Wang et al., 2017) have been proposed to help represent and enrich KBs. Tasks such as knowledge base completion (KBC) (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015, 2016; Wang et al., 2017) and relation prediction (RP) (Mintz et al., 2009; Lin et al., 2015a; Xie et al., 2016) are widely studied and many models are proposed to improve the performance on these tasks. However, the intention of these tasks is to test the performance of models in representing KBs and thus they cannot be used directly to discover new facts of KBs. Moreover, our FDKB task is not a simple combination of the KBC and RP task since both of these two tasks require to know two of the triples while we assume we only know the head entity. A common approach to solving these tasks is to build a knowledge base representation (KBR) model with different kinds of"
P16-1200,P15-1061,0,0.768279,"o philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is t"
P16-1200,P05-1045,0,0.0389364,"of our model. And then we evaluate the effects of our selective attention and show its performance on the data with different set size. Finally, we compare the performance of our method to several state-of-the-art feature-based methods. 4.1 Dataset and Evaluation Metrics We evaluate our model on a widely used dataset1 which is developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012). This dataset was generated by aligning Freebase relations with the New York Times corpus (NYT). Entity mentions are found using the Stanford named entity tagger (Finkel et al., 2005), and are further matched to the names of Freebase entities. The Freebase relations are divided into two parts, one for training and one for testing. It aligns the the sentences from the corpus of the years 2005-2006 and regards them as training instances. And the testing instances are the aligned sentences from 2007. There are 53 possible relationships including a special relation NA which indicates there is no relation between head and tail entities. The training data contains 522,611 sentences, 281,270 entity pairs and 18,252 relational facts. The testing set contains 172,448 sentences, 96,"
P16-1200,P11-1055,0,0.982056,"or example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows:"
P16-1200,P09-1113,0,0.990881,"crosoft, founder, Bill Gates). Although existing KBs contain a ∗ Corresponding uzy@tsinghua.edu.cn). author: Zhiyuan Liu (limassive amount of facts, they are still far from complete compared to the infinite real-world facts. To enrich KBs, many efforts have been invested in automatically finding unknown relational facts. Therefore, relation extraction (RE), the process of generating relational data from plain text, is a crucial task in NLP. Most existing supervised RE systems require a large amount of labelled relation-specific training data, which is very time consuming and labor intensive. (Mintz et al., 2009) proposes distant supervision to automatically generate training data via aligning KBs and texts. They assume that if two entities have a relation in KBs, then all sentences that contain these two entities will express this relation. For example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill"
P16-1200,D12-1110,0,0.068894,"example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its c"
P16-1200,P13-1045,0,0.00975957,"h instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to incorporate the text informatio"
P16-1200,P07-1073,0,0.0221986,"nt supervision inevitably accompanies with the wrong labelling problem. To alleviate the wrong labelling problem, (Riedel et al., 2010) models distant supervision for relation extraction as a multiinstance single-label problem, and (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multiinstance multi-label learning in relation extraction. Multi-instance learning was originally proposed to address the issue of ambiguously-labelled training data when predicting the activity of drugs (Dietterich et al., 1997). Multi-instance learning considers the reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine tran"
P16-1200,D12-1042,0,0.957488,"founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compare"
P16-1200,C14-1008,0,0.0864291,"reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to"
P16-1200,C14-1220,0,0.886404,"e “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding senten"
P16-1200,D15-1203,0,0.676771,"ich cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is the weight given by sentence-level attention, and s indicates the representation of the sentence set. KBs due to the lack of human-annotated training data. Therefore, (Zeng et al., 2015) incorporates multi-instance learning with neural network model, which can build relation extractor based on distant supervision data. Although the method achieves significant improvement in relation extraction, it is still far from satisfactory. The method assumes that at least one sentence that mentions these two entities will express their relation, and only selects the most likely sentence for each entity pair in training and prediction. It’s apparent that the method will lose a large amount of rich information containing in neglected sentences. In this paper, we propose a sentence-level a"
P17-1004,C14-1192,1,0.776459,"Missing"
P17-1004,P15-1061,0,0.0328139,"Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015"
P17-1004,N15-1151,0,0.0855589,"ing English Sentence Representation Chinese 1 x1 n 2 x11 x1 English 1 x2 2 x2 n x22 Chinese Figure 1: Overall architecture of our multi-lingual attention which contains two languages including English and Chinese. The solid lines indicates mono-lingual attention and the dashed lines indicates cross-lingual attention. et al. (2016) further utilize sentence-level attention mechanism to consider all informative sentences jointly. Most existing RE systems are absorbed in extracting relations from mono-lingual data, ignoring massive information lying in texts from multiple languages. In this area, Faruqui and Kumar (2015) present a language independent open domain relation extraction system, and Verga et al. (2015) further employ Universal Schema to combine OpenIE and link-prediction perspective for multi-lingual relation extraction. Both the works focus on multi-lingual transfer learning and learn a predictive model on a new language for existing KBs, by leveraging uniﬁed representation learning for cross-lingual entities. Different from these works, our framework aims to jointly model the texts in multiple languages to enhance relation extraction with distant supervision. To the best of our knowledge, this i"
P17-1004,D15-1205,0,0.0230895,"Missing"
P17-1004,P11-1055,0,0.577715,"ence and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts fr"
P17-1004,P16-1200,1,0.43391,"ain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of human experiences and human cognitive systems. For instance, though New York and United States are expressed as 纽约 and 美国 respectively in Chinese, both Americans and Chinese share the fact th"
P17-1004,P09-1113,0,0.777584,"osong Sun1,2 1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems c"
P17-1004,D12-1110,0,0.0830015,"d on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015) attempt to connect neural networks with d"
P17-1004,D12-1042,0,0.239985,"ate Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In"
P17-1004,D15-1203,0,0.34937,"guage Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of"
P17-1004,C14-1220,0,0.352646,"in various lan2 Related Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. R"
P18-1161,D13-1160,0,0.532208,"Missing"
P18-1161,P17-1171,0,0.576465,"ow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open domain QA. In recent years, researchers attempt to answer opendomain questions with a large-scale unlabeled corpus. Chen et al. (2017) propose a distantly supervised open-domain question answering (DS-QA) system which uses information retrieval technique to obtain relevant text from Wikipedia, and then applies reading comprehension technique to extract the answer. Although DS-QA proposes an effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retriev"
P18-1161,E17-2114,0,0.0552077,"Missing"
P18-1161,P16-1046,0,0.00746877,"hes, which can aggregate the results extracted from each paragraph by existing DS-QA system to better determine the answer. However, the method relies on the pre-extracted answers of existing DS-QA models and still suffers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrie"
P18-1161,P17-1020,0,0.26701,"effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retrieved paragraph ‘Dublin is the capital of Ireland. Besides, Dublin is one of the famous tourist cities in Ireland and ...” is not the correct token of the answer. These noisy paragraphs and tokens are regarded as valid instances in DS-QA. To address this issue, Choi et al. (2017) separate the answer generation in DS-QA into two modules including selecting a target paragraph in document and extracting the correct answer from the target paragraph by reading comprehension. Further, Wang et al. (2018a) use reinforcement learning to train target paragraph selection and answer extraction jointly. These methods only extract the answer according to the most related paragraph, which will lose a large amount of rich information contained in 1736 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1736–1745 c Melbourne, Au"
P18-1161,P17-1055,0,0.171124,"noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot b"
P18-1161,P17-1168,0,0.544434,"tor to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension"
P18-1161,N16-1174,0,0.00903545,"erent from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting answer a through a multi-layer long short-term memory network. Overall, the probability Pr(a|q, P ) of extracting answer a g"
P18-1161,P17-1147,0,0.166145,"Missing"
P18-1161,P16-1200,1,0.0376917,"ers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting an"
P18-1161,D14-1162,0,0.103004,"512}, the number of LSTM layers for document and question encoder among {1, 2, 3, 4}, regularization weight α among {0.1, 0.5, 1.0, 2.0} and the batch size among {4, 8, 16, 32, 64, 128}. The optimal parameters are highlighted with bold faces. For other 1740 parameters, since they have little effect on the results, we simply follow the settings used in (Chen et al., 2017). For training, our Our+FULL model is first initialized by pre-training using Our+AVG model, and we set the iteration number over all the training data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe6 (Pennington et al., 2014) word embeddings learned from 840B Web crawl data. 4.4 Effect of Different Paragraph Selectors As our model incorporates different types of neural networks including MLP and RNN as our paragraph selector, we investigate the effect of different paragraph selector on the Quasar-T and SearchQA development set. As shown in Table 3, our RNN paragraph selector leads to statistically significant improvements on both Quasar-T and SearchQA. Note that Our+FULL which uses MLP paragraph selector even performs worse on Quasar-T dataset as compared to Our+AVG. It indicates that MLP paragraph selector is ins"
P18-1161,P17-1018,0,0.124425,"der to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open"
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1128,P18-2014,0,0.0429986,"en and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·)"
P19-1128,N19-1240,0,0.0430003,"Missing"
P19-1128,P18-1148,0,0.0240849,"15; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. T"
P19-1128,D17-1188,0,0.624311,"y an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·) indicates an embedding function, and θen denotes the parameters of the encoding module of n-th layer. 3.2 Pr"
P19-1128,P16-1200,1,0.891879,"and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat re"
P19-1128,D17-1159,0,0.0382641,"ning as compared to those models which extract relationships separately. Moreover, we also present three datasets, which could help future researchers compare their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the"
P19-1128,P16-1105,0,0.0569637,"that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · ,"
P19-1128,W15-1506,0,0.261678,"bjects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. ("
P19-1128,D15-1203,0,0.647955,"7) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochre"
P19-1128,C14-1220,0,0.816506,"e generic backpropagation and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le"
P19-1128,D17-1186,1,0.883827,"entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pai"
P19-1128,D18-1244,0,0.158096,"e their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the relationship during the learning process. In sharp contrast, this paper focuses on extracting relations from real-world relation datasets. 2.2 Rel"
P19-1128,D14-1162,0,0.0823489,"e n denotes the index of layer 1 , [·] means reshaping a vector as a matrix, BiLSTM encodes a sequence by concatenating tail hidden states of the forward LSTM and head hidden states of the backward LSTM together and MLP denotes a multilayer perceptron with non-linear activation σ. Word Representations We first map each token xt of sentence {x0 , x1 , . . . , xl−1 } to a kdimensional embedding vector xt using a word embedding matrix We ∈ R|V |×dw , where |V |is the size of the vocabulary. Throughout this paper, we stick to 50-dimensional GloVe embeddings pre-trained on a 6-billion-word corpus (Pennington et al., 2014). 1 Adding index to neural models means their parameters are different among layers. Position Embedding In this work, we consider a simple entity marking scheme2 : we mark each token in the sentence as either belonging to the first entity vi , the second entity vj or to neither of those. Each position marker is also mapped to a dp -dimensional vector by a position embedding matrix P ∈ R3×dp . We use notation pi,j t to represent the position embedding for xt corresponding to entity pair (vi , vj ). 4.2 Propagation Module Next, we use Eq. (2) to propagate information among nodes where the initia"
P19-1137,P17-1038,0,0.185671,"Missing"
P19-1137,D14-1164,0,0.0357125,"Missing"
P19-1137,P11-1055,0,0.181204,"a small number of human annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et a"
P19-1137,P18-1161,1,0.894749,"Missing"
P19-1137,P16-1200,1,0.908825,"e a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken th"
P19-1137,N16-1104,0,0.153144,"a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the k"
P19-1137,D17-1005,0,0.213057,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,D17-1189,0,0.680844,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,P17-1040,0,0.0220481,"et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant a"
P19-1137,P09-1113,0,0.925853,"s defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are t"
P19-1137,D12-1104,0,0.0947384,"improved the performance of the vanilla LSTM (Hochreiter and Schmidhuber, 1997) by utilizing RL to discover structured representations and Li et al. (2016) interpreted the sentiment prediction of neural models by employing RL to find the decision-changing phrases. However, NRE models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives"
P19-1137,C14-1220,0,0.100927,"atives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent ne"
P19-1137,D14-1162,0,0.0810357,"Missing"
P19-1137,P12-1087,0,0.0275027,"Missing"
P19-1137,P14-2119,1,0.82305,"itives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorp"
P19-1137,P18-1199,0,0.0429382,"se models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant and interpretable improvements, which is also the focus of this paper. As"
P19-1137,P05-1053,0,0.214768,": Two types of error labels, false negatives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it"
P19-1137,P16-2034,0,0.0665773,"Missing"
P19-1137,P15-1061,0,0.0475427,"ositives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction ("
P19-1137,D12-1042,0,0.111105,"n annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al.,"
P19-1137,P12-1076,0,0.0302716,"models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives at the same time. 3 Methodology Provided with DS-generated data and NRE models trained on them, DIAG-NRE can generate high-quality patterns for the WLF stage to produce denoised labels. As Figure 2 shows, DIAG-NRE contains two key stages in general: pattern extraction (Section"
P19-1137,D17-1187,0,0.107221,"le performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2"
P19-1137,D15-1203,0,0.38233,"vised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a resul"
P19-1227,P17-1171,0,0.241113,"guages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA. 1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks"
P19-1227,P17-1020,0,0.170533,"n questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual Ope"
P19-1227,P18-1078,0,0.399956,"aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build"
P19-1227,D18-1269,0,0.190208,"ly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 Proceedings of the 57th Annual Meeting of the Association for Com"
P19-1227,P17-1055,0,0.0333341,"h the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and"
P19-1227,N19-1423,0,0.63948,"nglish questionanswer pairs along with relevant documents. The development and test sets contain a total amount of 17, 358 and 16, 973 question-answer pairs respectively. All questions are naturally produced by native speakers, and potentially reflect cultural differences in different languages. Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in almost all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how diffic"
P19-1227,P17-1168,0,0.0200036,"ine systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Li"
P19-1227,W18-2413,0,0.0219766,"lated as “Fuyang Palace” in the question, while correctly translated in the retrieved document. In addition, as we can see from the underlined parts, highly similar expressions in the question and the retrieved document are translated into largely different ones. Compared to other words or phrases which occur more frequently in the training corpus, name entities are more flexible and various, and thus have worse translation results from prevailing Neural Machine Translation systems (Li et al., 2018). While some work has focused on solving this problem (Hassan et al., 2007; Jiang et al., 2007; Grundkiewicz and Heafield, 2018; Li et al., 2018), it remains largely underresearched. With a translation system that handles name entities better, we can potentially obtain better results from translation-based methods. 6.3 Zero-shot Cross-lingual Method Trained on pure English data without the involvement of machine translation systems, much effort has been saved using zero-shot cross-lingual methods. Moreover, a single model could be applied directly to various languages. Thus, compared to Origin Question: &lt;Query&gt;位于汉长安城外西南侧，与未央宫 之间曾有跨越城墙的复道相连？ Retrieved Text: ...在长安城外修建了建章宫...并且与未 央宫之间有跨越宫墙和城墙的复道相通... Answer: 建章宫 Transla"
P19-1227,C12-1089,0,0.0596235,"ith vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiven"
P19-1227,P18-1160,0,0.019114,"t al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and Wang et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, p"
P19-1227,N19-1380,0,0.035498,"tence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is still no existing benchmark for cross-lingual OpenQA. In addition, another line of research attempts to answer questions in one language using documents in other languages (Magnini et al., 2004; Vallin et al., 2005; Magnini et al., 2006). Different from their setting, we emphasize on building question answering systems for other languages using labeled data from a rich source language such as English, while the documents are in the same language as the questions. 3 Cross-lingual Open-domain Question Answering Existing OpenQA models usually first retrieve documents related t"
P19-1227,W17-2619,0,0.0173788,"l corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is sti"
P19-1227,L18-1560,0,0.0418847,"low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 P"
P19-1227,P18-1161,1,0.910792,"1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to"
P19-1227,W15-1521,0,0.0275638,"et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 201"
P19-1227,2020.amta-research.11,1,0.825841,"Missing"
