C04-1026,P01-1024,1,0.9126,"he underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples. While we make no a priori functionality assumptions about the relation of the linguistic dimensions, functional mappings can be obtained as a special case. We formalise our syntax-semantics interface using Extensible Dependency Grammar (XDG), a new grammar formalism which generalises earlier work on Topological Dependency Grammar (Duchier and Debusmann, 2001). The relational syntax-semantics interface is supported by a parser for XDG based on constraint programming. The crucial feature of this parser is that it supports the concurrent flow of possibly partial information between any two dimensions: once additional information becomes available on one dimension, it can be propagated to any other dimension. Grammaticality conditions and preferences (e. g. selectional restrictions) can be specified on their natural level of representation, and inferences on each dimension can help reduce ambiguity on the others. This generalises the idea of underspec"
C04-1026,E03-1054,1,0.834095,"ces on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as an underspecified semantic description. 4.1 Montague-style Interpretation In order to extr"
C04-1026,P98-1077,0,0.0225832,"ctic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which can be computed functionally from a syntactic analysis and encapsulates semantic ambiguity in a way that supports the enumeration of all semantic readings by need. In this paper, we introduce a completely relational syntax-semantics interface, building upon the underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples."
C04-1026,C88-1060,0,0.253565,"Missing"
C04-1026,C00-1067,1,0.832765,"a prep-child of student on ID. In the other direction, the solver will infer more dominances on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as"
C04-1026,P02-1003,1,0.835191,"e present, and that others are excluded. Partial analyses will play an important role in Section 3.3. Because propagation operates on all dimensions concurrently, the constraint solver can frequently infer information about one dimension from information on another, if there is a multi-dimensional principle linking the two dimensions. These inferences take place while the constraint problem is being solved, and they can often be drawn before the solver commits to any single solution. Because XDG allows us to write grammars with completely free word order, XDG solving is an NPcomplete problem (Koller and Striegnitz, 2002). This means that the worst-case complexity of the solver is exponential, but the average-case complexity for the hand-crafted grammars we experimented with is often better than this result suggests. We hope there are useful fragments of XDG that would guarantee polynomial worst-case complexity. 3 A Relational Syntax-Semantics Interface Now that we have the formal and processing frameworks in place, we can define a relational syntaxsemantics interface for XDG. We will first show how we encode semantics within the XDG framework. Then we will present an example grammar (including some principle"
C04-1026,J93-4001,0,0.0305064,"get bidirectional grammars for free. While the solver is reasonably efficient for many (hand-crafted) grammars, it is an important goal for the future to ensure that it can handle largescale grammars imported from e.g. XTAG (XTAG Research Group, 2001) or induced from treebanks. One way in which we hope to achieve this is to identify fragments of XDG with provably polynomial parsing algorithms, and which contain most useful grammars. Such grammars would probably have to specify word orders that are not completely free, and we would have to control the combinatorics of the different dimensions (Maxwell and Kaplan, 1993). One interesting question is also whether different dimensions can be compiled into a single dimension, which might improve efficiency in some cases, and also sidestep the monostratal vs. multistratal distinction. The crucial ingredient of XDG that make relational syntax-semantics processing possible are the declaratively specified principles. So far, we have only given some examples for principle specifications; while they could all be written as Horn clauses, we have not committed to any particular representation formalism. The development of such a representation formalism will of course b"
C04-1026,P99-1039,0,0.0162758,"nambiguous sentence can have multiple semantic readings. Conversely, a common situation in natural language generation is that one semantic representation can be verbalised in multiple ways. This means that the relation between syntax and semantics is not functional at all, but rather a true m-to-n relation. There is a variety of approaches in the literature on syntax-semantics interfaces for coping with this situation, but none of them is completely satisfactory. One way is to recast semantic ambiguity as syntactic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which"
C04-1026,C98-1074,0,\N,Missing
C12-2033,E09-1005,0,0.108167,"Missing"
C12-2033,D09-1046,0,0.406595,"s on the level of single instances. Erk et al. (2009) give the example of “paper” occurring in a sentence which clearly identifies a scientific context. All three annotators agree that the WordNet sense scholarly article fully applies and consistently assign a score of 5. However, the senses essay and medium for written communication are also assigned high scores by some of the annotators. This reflects these annotators’ intuitions that several senses apply simultaneously, and induces an ordering of the senses’ applicabilities. A first, supervised, computational model for GWSA is presented by Erk and McCarthy (2009). In this paper, we explore models that are unsupervised in the sense that they do not depend on annotated training material; in the WSD terminology, they belong to the class of knowledgebased WSD systems. More specifically, we address the task of ranking the WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three"
C12-2033,P09-1002,0,0.349553,"cular in connection with fine-grained sense inventories, like the one provided by WordNet (Fellbaum, 1998). The single-sense restriction typically leads to a somewhat arbitrary overspecification of word meaning, which may be detrimental to the use of WSD systems in practical applications. Moreover, both agreement between human annotators and accuracy of WSD systems tend to be rather low, which stands in contrast to the strong intuition that words in context generally have a well-understood meaning. Recently, the notion of graded word sense assignment (GWSA) has been brought into discussion by Erk et al. (2009, 2012), and two closely related GWSA data sets are now available. The underlying assumption of GWSA is that a word in context may in fact evoke more than one sense, and the different senses may participate in the meaning of the word to different degrees. To produce the aforementioned data sets, annotators were presented target instances, i.e., lemmas in the context of a sentence, and asked to assign a value, which indicates the applicability of the sense in the context, on a scale from 1 to 5 to each WordNet sense of the lemma independently. The annotation method allows more than one word sen"
C12-2033,D08-1094,0,0.0300232,"pus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and which we chose for the following reasons: (1) They are knowledge-lean, i.e., the only resource required is a semantic lexicon (such as WordNet), and they can be implemented quickly. (2) They exhibit state-of-the-art performance on the SemEval-2007 coarse-grained WSD task. 3.1 Vector Space-based WSD System We use the vector-space model (VSM) of Thater et al. (2011), which is closely related to the models of Thater et al. (2010) and Erk and Padó (2008). The general idea behind VSMs of word meaning is to represent words by vectors in a high-dimensional space. These vectors record co-occurrence statistics with context words in a large unlabeled text corpus, and their relative directions are taken to indicate semantic similarity. The particular model used in our experiments is the one of Thater et al. (2011), which provides context-specific (contextualized) vectors for words in their syntactic context. It can be applied to WSD and GWSA in a straightforward way: given a target word in a sentential context, we extract a set of sense paraphrases"
C12-2033,S12-1027,0,0.0205915,"n judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) describe an approach to unsupervised GWSA on the basis of a syntactically informed distributional similarity 330 model. The evaluation was carried out for three selected verb lemmas, and therefore has the character of a case study only. The study of Jurgens (2012), which explores the application of word sense induction techniques to GWSA, has a similar status: Since he needs a large part of the GWSA data set as a sense mapping corpus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and which we chose for the following reasons: (1) They are knowledge-lean, i.e., the only resource required is a semantic lexicon (such as WordNet), and they can be implemented quickly. (2) They exhibit state-of-the-art performance on the SemEval-2007 coarse-grained WS"
C12-2033,P10-1116,0,0.0308291,"e WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three different systems: two related models, which are based on the individual similarity scores between the contextualized vector representation of a target word in context and vector representations computed for the respective word senses (Thater et al., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) descr"
C12-2033,N04-3012,0,0.0667701,"valuation of GWSA. A total of 430 sentences for 11 different lemmas were extracted from SemCor and Senseval-3. Three untrained annotators provided judgments of the applicability of word senses of the lemmas in the context of the sentence on a scale from 1 to 5, where 1 means that the sense is not present at all in the sentence and 5 means that the sense totally matches the meaning of the word in the context. We refer to the task of ranking the senses of a word (lemma) in the context of a particular sentence as the lemma-sentence ranking task. 1 We are using the WordNet::Similarity toolkit of (Pedersen et al., 2004). We also experimented with other sense similarity measures, but the method suggested by Sinha and Mihalcea (2007) worked best with PageRank. 332 WSsim-2: In this round of data collection, eight annotators judged the applicability of the senses of 26 lemmas in 10 sentences each, resulting in a set of 260 sentences (Erk et al., 2012). Otherwise, the annotation procedure was identical to WSsim-1. 4.2 Correlation Analysis of Sense Ranking Erk and McCarthy (2009) propose Spearman’s rank correlation coefficient (ρ) as a measure of a system’s performance on the GWSA task. ρ compares two rankings whi"
C12-2033,P10-1154,0,0.0314661,"ce, the Topic Models approach might yield better performance using different parameter settings. We noticed that due to the sampling step inside the algorithm, the results varied by small, but non-negligible, amounts. We thus sum up the scores produced by the system across multiple (ten) runs in order to predict a more reliable ranking. This results in a slight increase of performance. 331 3.3 Graph-based WSD System To date, many graph-based WSD algorithms have been proposed, (among others by Sinha and Mihalcea, 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010; Tsatsaronis et al., 2010; Ponzetto and Navigli, 2010). We chose to reimplement the approach of Sinha and Mihalcea (2007) for several reasons. First, it is based on the PageRank algorithm, which is easy to understand and implement; second, a reference implementation was made available by the authors, which allowed for clarification in several issues; and third, its performance is robust. The algorithm consists of the following steps, which we illustrate using Figure 1. (1) Construction of the graph. When disambiguating a word (e.g. “order”), a graph is built using a context of N (2 in the example) content words on either side of the word. For eac"
C12-2033,P10-1097,1,0.848537,"l., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) describe an approach to unsupervised GWSA on the basis of a syntactically informed distributional similarity 330 model. The evaluation was carried out for three selected verb lemmas, and therefore has the character of a case study only. The study of Jurgens (2012), which explores the application of word sense induction techniques to GWSA, has a similar status: Since he needs a large part of the GWSA data set as a sense mapping corpus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and"
C12-2033,I11-1127,1,0.911785,"he task of ranking the WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three different systems: two related models, which are based on the individual similarity scores between the contextualized vector representation of a target word in context and vector representations computed for the respective word senses (Thater et al., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater e"
C12-2033,J13-3003,0,\N,Missing
D11-1072,P10-1097,1,0.193595,"Missing"
D11-1072,E06-1002,0,\N,Missing
D11-1072,W03-0419,0,\N,Missing
D11-1072,D07-1074,0,\N,Missing
D11-1072,P05-1045,0,\N,Missing
D16-1017,P14-1023,0,0.443736,"se sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was"
D16-1017,J10-4006,0,0.298302,"Greenberg (patient) Pado+McRae+Ferretti # ratings 414 1444 274 248 720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for"
D16-1017,P07-1028,0,0.177316,"y significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art computational models of thematic fit quantify the similarity between a role filler of a verb and the proto-typical filler for that role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 20"
D16-1017,W15-1106,1,0.918364,"720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art"
D16-1017,P98-1035,0,0.167769,"sad Sayeed and Dietrich Klakow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be"
D16-1017,P07-1071,0,0.278611,"inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for de"
D16-1017,C14-1134,0,0.0662196,"Missing"
D16-1017,N15-1003,1,0.727949,"diction, selectional preference models score the inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fo"
D16-1017,J15-1004,0,0.0645383,"however also model the interaction between different roles; see Figure 2 for an example of model predictions. We are only aware of one small dataset that can be used to systematically test the effectiveness of the compositionality for this task. The Bicknell et al. (2010) dataset contains triples like journalist check 178 Evaluation of event representations: sentence similarity To show that our model learns to represent input words and their roles in a useful way that reflects the meaning and interactions between inputs, we evaluate our non-incremental model on a sentence similarity task from Grefenstette and Sadrzadeh (2015). We assign similarity scores to sentence pairs by computing representations for each sentence by tak# ratings 199 NN RF 0.34 Kronecker 0.26 W2V 0.13 Humans 0.62 Table 5: Sentence similarity evaluation scores on GS2013 dataset (Grefenstette and Sadrzadeh, 2015), consisting of Spearman’s ρ correlations between human judgements and model output. Kronecker is the best performing model from Grefenstette and Sadrzadeh (2015). NN RF is the non-incremental model presented in this article, and W2V is the word2vec baseline. Human performance (inter-annotator agreement) shows the upper bound. Figure 2:"
D16-1017,D14-1140,0,0.0613349,"Missing"
D16-1017,P15-1115,0,0.0871915,". The latter could be useful for inferring missing information in entailment tasks or improving identification of thematic roles outside the sentence containing the predicate. Potential applications also include predicate prediction based on arguments and roles, which has been noted to be relevant for simultaneous machine translation for a verb-final to a verb-medial source language (Grissom II et al., 2014). Within cognitive modelling, our model could help to more accurately estimate semantic surprisal for broadcoverage texts, when used in combination with an incremental role labeller (e.g., Konstas and Keller, 2015), or to provide surprisal estimates for content words as a control variable for psycholinguistic experimental materials. In this work, we focus on the predictability of verbs and nouns, and we suggest that the predictability of these words depends to a large extent on the relationship of these words to other nouns and 171 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 171–182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics verbs, especially those connected via the same event. We choose a neural network (NN) mod"
D16-1017,W11-0607,0,0.788422,"t role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 2009; Greenberg et al., 2015a). The datasets include agent, patient, location and instrument roles. For example, in the Pad´o et al. (2009) dataset, the noun sound has a very low rating of 1.1 as the subject of hear a"
D16-1017,J12-3007,0,0.0185998,"akow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a"
D16-1017,D14-1004,0,0.242497,"Missing"
D16-1017,E09-1094,0,0.680448,"through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was not always achievable—we skipp"
E03-1024,P92-1005,0,0.022823,"@ps.uni sb.de stth@coli.uni sb.de Saarland University, Saarbriicken, Germany - Abstract We define a back-and-forth translation between Hole Semantics and dominance constraints, two formalisms used in underspecified semantics. There are fundamental differences between the two, but we show that they disappear on practically useful descriptions. Our encoding bridges a gap between two underspecification formalisms, and speeds up the processing of Hole Semantics. 1 Introduction In the past few years there has been considerable activity in the development of formalisms for underspecified semantics (Alshawi and Crouch, 1992; Reyle, 1993; Bos, 1996; Copestake et al., 1999; Egg et al., 2001). These approaches all aim at controlling the combinatorial explosion of readings of sentences with multiple ambiguities. The common idea is to delay the enumeration of all readings for as long as possible. Instead, they work with a compact underspecified representation for as long as possible, only enumerating readings from this representation by need. At first glance, many of these formalisms seem to be very similar to each other. Now the question arises how deep this similarity is — are all underspecification formalisms basi"
E03-1024,W98-0113,0,0.0322011,"scription like in Fig. 2. It has no plugging in Hole Semantics, as two different things would have to be plugged into one hole, but it is satisfiable as a dominance constraint. It is this fundamental difference that restricts our result in §5, and that we avoid by using chainconnected descriptions. f a b. 4 Figure 2: A description on which Hole Semantics and dominance constraints disagree. 3 Dominance Constraints Dominance constraints are a general framework for the partial description of trees. They have been used in various areas of computational linguistics (Rogers and Vijay-Shanker, 1994; Gardent and Webber, 1998). For underspecified semantics, we consider semantic representations like higherorder formulas as trees. Dominance constraints can be extended to CLLS (Egg et al., 2001), which adds parallelism constraints to model ellipsis and binding constraints to account for variable binding without using variable names. We do not use these extensions here, for simplicity, but all results remain true if we allow binding constraints. 3.1 Syntax and Semantics We assume a signature E of function symbols ranged over by f ,g, each of which is equipped with an arity ar(f) &gt; 0, and an infinite set Vars of variabl"
E14-1057,de-marneffe-etal-2006-generating,0,0.0133487,"Missing"
E14-1057,D10-1113,0,0.0382555,"control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a)"
E14-1057,N12-1076,1,0.755835,"any contextspecific substitutes outside the common core. 5 Ranking Paraphrases While there are several studies on modelling lexical substitutes, almost all reported results use McCarthy and Navigli’s S EM E VAL 2007 dataset. We now compare the results of three recent computational models on C O I N C O (our work) and on the S EM E VAL 2007 dataset to highlight similarities and differences between the two datasets. Models. We consider the paraphrase ranking models of Erk and Padó (2008, E P 08), Thater et al. (2010, T FP 10) and Thater et al. (2011, T FP 11). These models have been analysed by Dinu et al. (2012) as instances of the same general framework and have been shown to deliver state-of-the-art performance on the S EM E VAL 2007 dataset, with best results for Thater et al. (2011). The three models share the idea to represent the meaning of a target word in a specific context by 545 corpus syntactically structured syntactically filtered bag of words T FP 11 T FP 10 E P 08 T FP 11/E P 08 T FP 10 T FP 11/E P 08 T FP 10 random COINCO context baseline 47.8 46.2 46.0 44.6 47.4 46.2 47.4 45.8 41.9 38.8 46.2 44.7 40.8 37.5 33.0 S EM E VAL 2007 context baseline 52.5 43.7 48.6 42.7 49.4 43.7 50.1 44.4 4"
E14-1057,1993.eamt-1.1,0,0.520679,"Missing"
E14-1057,D09-1046,1,0.592235,"r substitutes? (b) Do parasets resemble word senses? (c) How similar are the parasets that correspond to the same word sense of a target? These questions have not been addressed before, and we would argue that they could not be addressed before, because previous corpora were either too small or were sampled in a way that was not conducive to this analysis. We use WordNet (Fellbaum, 1998), release 3.1, as a source for both lexical relations and word senses. WordNet is the de facto standard in N LP and is used for both W SD and broader investigations of word meaning (Navigli and Ponzetto, 2012; Erk and McCarthy, 2009). Multi-word substitutes are excluded from all analyses.4 4.1 Relating Targets and Substitutes We first look at the most canonical lexical relations between a target and its substitutes. Table 2 lists the percentage of substitutes that are synonyms (syn), direct/transitive (direct-/trans-) hypernyms (hyper) 3 Please see McCarthy and Navigli (2009) for a possible explanation of the generally low I AA numbers in this field. 543 4 All automatic lexical substitution approaches, including Section 5, omit multi-word expressions. Also, they can be expected to have WordNet coverage and normalisation i"
E14-1057,D08-1094,1,0.881083,".org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution candidates and choose contextual substitutions in each case. Erk and Padó (2008) treated the gold substitution candidates as given and focused on the context-specific ranking of those candidates. In this form, the task has been addressed through three types of (mostly unsupervised) approaches. The first group computes a single type representation and modifies it according C O I N C O – The M ASC All-Words Lexical Substitution Corpus1 Compared to, e. g., W SD, there still is little goldannotated data for lexical substitution. With the exception of the dataset created by Biemann (2013), all existing lexical substitution datasets are fairly small, covering at most several th"
E14-1057,P10-2017,1,0.861873,"study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can pro"
E14-1057,P13-2130,0,0.018836,"in two crucial respects: we annotate all instances of each target, and include all targets regardless of frequency or level of lexical ambiguity. We believe that our corpus is considerably more representative of running text. 541 1 Available as X ML-formatted corpus “Concepts in Context” (C O I N C O) from http://goo.gl/5C0jBH. Also scheduled for release as part of M ASC. 3.2 Crowdsourcing We used the Amazon Mechanical Turk (A MT) platform to obtain substitutes by crowdsourcing. Interannotator variability and quality issues due to nonexpert annotators are well-known difficulties (see, e. g., Fossati et al. (2013)). Our design choices were shaped by “best practices in A MT”, including Mason and Suri (2012) and Biemann (2013). Defining H ITs. An A MT task consists of Human Intelligence Tasks (H ITs), each of which is supposed to represent a minimal, self-contained task. In our case, potential H ITs were annotations of (all target words in) one sentence, or just one target word. The two main advantages of annotating a complete sentence at a time are (a) less overhead, because the sentence has only to be read once; (b) higher reliability, since all words within a sentence will be annotated by the same per"
E14-1057,ide-etal-2008-masc,0,0.033068,"Missing"
E14-1057,P10-2013,0,0.0463841,"d in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution can"
E14-1057,D11-1097,0,0.0405471,"Missing"
E14-1057,S01-1005,0,0.0600907,"(Mitchell and Lapata, 2010). There are, however, important shortcomings of the work in the Lexical Substitution paradigm. All existing datasets (McCarthy and Navigli, 2009; Sinha and Mihalcea, 2014; Biemann, 2013; McCarthy et al., 2013) are either comparatively small, are “lexical sample” datasets, or both. “Lexical sample” datasets consist of sample sentences for each target word drawn from large corpora, with just one target word substituted in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-w"
E14-1057,N10-1013,0,0.0305819,"ask bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose t"
E14-1057,N13-1133,0,0.373781,"es. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can profit from the preexisting annotations and (b) we can release our annotations as part of M ASC. Since we could not annotate the complete M ASC, we selected (complete) text docume"
E14-1057,P10-1097,1,0.962657,"to substitute all content words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotat"
E14-1057,I11-1127,1,0.859251,"tent words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several laye"
E14-1057,D11-1094,0,0.0496376,"Missing"
E14-1057,D13-1198,0,\N,Missing
I11-1127,J93-3001,0,0.113073,"Missing"
I11-1127,J90-1003,0,0.261605,"ector in the vector space V spanned by the set of basis vectors {e(r,w0 ) |r ∈ R, w0 ∈ W }. Such a vector records the association strength between w and any context word w0 occurring in relation r. Specifically, we associate a word w ∈ W with a vector v(w) ∈ V by setting ∑ v(w) := r∈R,w0 ∈W f (w, r, w0 ) · e(r,w0 ) where f is a function that assigns a weight to the dependency triple (w, r, w0 ). In the simplest case, this could be the frequency of w occurring together with w0 in relation r in a corpus of dependency trees. In the experiments reported below, we use pointwise mutual information (Church and Hanks, 1990) instead, as it proved superior to raw frequency counts: PMI(w, r, w0 ) = log p(w, w0 |r) p(w, · |r)p(·, w0 |r) Here the dots stand for marginalization over the relevant variables. Given an occurrence of a word w in the context of another word wc , related by the syntactic relation rc , we now define a contextualized version of v(w) by reweighting the vector components. We set vrc ,wc (w) := ∑ r∈R,w0 ∈W αrc ,wc ,r,w0 · f (w, r, w0 ) · e(r,w0 ) Here, the weights αrc ,wc ,r,w0 quantify the degree to which a vector dimension (r, w0 ) is compatible with the observed context (rc , wc ). We consider"
I11-1127,de-marneffe-etal-2006-generating,0,0.00454815,"Missing"
I11-1127,D10-1113,0,0.323566,"ing syntactic relations r1 , . . . , rn , we obtain a contextualized vector of w by superimposing the vectors vri ,wi (1 ≤ i ≤ n) through vector addition: n vr1 ,w1 ,...,rn ,wn (w) := ∑ vri ,wi (w) i=1 The resulting vector vr1 ,w1 ,...,rn ,wn (w) is our completely contextualized representation for the word w that contains information about all context words. 4 Ranking Paraphrases In this section, we evaluate to what extent our model supports the choice of contextually appropriate paraphrases for different uses of a target word. We follow previous work (Thater et al., 2010; Erk and Padó, 2010; Dinu and Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task dataset (McCarthy and Na"
I11-1127,D08-1094,0,0.471674,"be properly reflected in their respective meaning vectors, since the former, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In this paper, we will d"
I11-1127,P04-1036,0,0.0400925,"nal information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure), which does not rely on a fixed set of dicti"
I11-1127,P08-1028,0,0.580079,"arge/impose a fee will not be properly reflected in their respective meaning vectors, since the former, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In"
I11-1127,S07-1006,0,0.0342037,"Missing"
I11-1127,P10-2017,0,0.270154,", wn and corresponding syntactic relations r1 , . . . , rn , we obtain a contextualized vector of w by superimposing the vectors vri ,wi (1 ≤ i ≤ n) through vector addition: n vr1 ,w1 ,...,rn ,wn (w) := ∑ vri ,wi (w) i=1 The resulting vector vr1 ,w1 ,...,rn ,wn (w) is our completely contextualized representation for the word w that contains information about all context words. 4 Ranking Paraphrases In this section, we evaluate to what extent our model supports the choice of contextually appropriate paraphrases for different uses of a target word. We follow previous work (Thater et al., 2010; Erk and Padó, 2010; Dinu and Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task"
I11-1127,J07-2002,0,0.120831,"Missing"
I11-1127,P07-1028,0,0.0456217,"ase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure), which does not rely on a fixed set of dictionary senses with their well-known problems (Kilgarr"
I11-1127,P10-1154,0,0.0132838,"ext, without relying on any manually annotated training data. Our system is knowledge-based, according to the classification of WSD approaches proposed in McCarthy 1139 (2009) and Navigli (2009). It is a knowledge-lean system, in contrast to many other systems that exploit external resources, since it uses only a small subset of the structural information provided by WordNet – just as much as is required to adapt our contextualization model to the WSD task. The state of the art in knowledge-based WSD systems not trained on annotated data is defined by the models of Navigli and Velardi (2005), Ponzetto and Navigli (2010) and Li et al. (2010). The former two rely on a rich inventory of additional knowledge resources. Li et al. (2010) restricts itself to WordNet information in a similar way as our approach, and therefore is our natural benchmark. 5.1 Method We frame the task of choosing the right WordNet sense as a paraphrase ranking task like the one considered in Section 4, with all possible synonyms of the target word constituting the set of (lexical) paraphrase candidates. The basic idea for predicting a sense of the target word is to choose the synset that contains the most similar paraphrase. As the WordN"
I11-1127,P10-1116,0,0.155754,"syntactic arguments. Contextualization is modeled as above in terms of vector composition. Among the aforementioned approaches, their proposal performs best, but at the cost of a rather complex and unintuitive concept of second-order co-occurrence vectors. Other approaches achieve good results without using vector composition. Dinu and Lapata (2010) represent word meaning in context by using a latent variable model, where context-dependence is modeled by conditioning the latent variable on the context in which a word occurs. Similar proposals have been made by Reisinger and Mooney (2010a) and Li et al. (2010). A different approach has been taken by Erk and Padó (2010) and Reisinger and Mooney (2010b). Instead of “refining” vector representations ranging over all words in a corpus by means of vector composition, they start out from “token” vectors for individual instances of words in context, and then group these token vectors into different sensespecific clusters. 2 3 Related work Inspired by earlier work of Kintsch (2001), who proposes a network algorithm to extract contextspecific vector representations for words in context, Mitchell and Lapata (2008) investigate the systematic combination of di"
I11-1127,S07-1009,0,0.0594949,"d Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). The original dataset contains 10 instances for each of 201 target words (nouns, verbs, adjectives and adverbs) in different sentential contexts. For each instance, five subjects were asked to name appropriate paraphrases. Table 1 shows an example of three instances of charge together with their gold standard paraphrases. Each paraphrase comes with a weight, which corresponds to the number of times it was chosen by the different subjects. The original task addresses two subtasks: identifying paraphrase candidates and ranking them according to the context. Here, we restrict ourselves to the se"
I11-1127,D10-1114,0,0.0830357,"Missing"
I11-1127,N10-1013,0,0.334544,"on the task of word sense disambiguation in Section 5. Section 6 concludes. tionality, but it can also be taken to be a method to contextualize a target word through its dependents. Erk and Padó (2008) propose structured vector representations, where each word is characterized by a standard co-occurrence vector, plus separate vector representations for the (inverse) selectional preferences for subject, object, and other syntactic relations. Contextualization is modeled by combining, e.g., the basic vector of the target verb with the selectional preferences of subject and object. Thater et al. (2010) propose a similar approach, where word meaning is modeled as a second-order vector obtained by summing over first-order vectors representing the inverse selectional preferences of a word’s syntactic arguments. Contextualization is modeled as above in terms of vector composition. Among the aforementioned approaches, their proposal performs best, but at the cost of a rather complex and unintuitive concept of second-order co-occurrence vectors. Other approaches achieve good results without using vector composition. Dinu and Lapata (2010) represent word meaning in context by using a latent variab"
I11-1127,J98-1004,0,0.826718,"ts components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure),"
I11-1127,P10-1097,1,0.789295,"ormer, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In this paper, we will demonstrate that one can keep the model simple and at the same time outp"
I17-2007,P16-1004,0,0.0136662,"r1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015)"
I17-2007,D15-1166,0,0.0288107,"Missing"
I17-2007,K16-1028,0,0.0186085,"tional Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use S"
I17-2007,P02-1040,0,0.100146,"the timestep i. We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016) where they only use a single layer. So we in fact experiment with Bidirectional-LSTM multi-layer RNN (BiLSTM) and Bidirectional-GRU multilayer RNN (BiGRU). • Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.e., the standard evaluation metric used in machine translation), which measures surface similarity between predicted and actual sentences. We complement this evaluation by measuring prediction accuracy on the semantic level. To this purpose, we use the gold paraphrase sets of event descriptions in the D E S CRIPT corpus, e.g., “Remove cake”, “Remove from oven” and “Take the cake out of oven” belong to the same gold paraphrase set of taking out oven. The gold paraphrase sets allow us to access the correctness of the prediction which could not be attained by using the BLEU mea"
I17-2007,D14-1179,0,0.010377,"Missing"
I17-2007,W15-4915,0,0.0483245,"Missing"
I17-2007,2015.eamt-1.16,0,\N,Missing
I17-2007,P16-1027,0,\N,Missing
L16-1030,A00-1031,0,0.0235379,"eral aim is to minimize the number of words that are OOV to the tagger, as tagging accuracy for these words is generally much lower than for IV words. We leverage two methods to achieve this goal: One is to correct misspelled words before tagging into their most likely correct form, while the second is to extend the tagger lexicon with OOV words that occur in the reference material of a learner tex; we call them lexical gaps. We first describe the decision process, for which method will be applied, and then discuss both variants of dealing with OOV words in more detail. We use the TnT tagger (Brants, 2000) in all of our experiments, and the TIGER corpus (Brants et al., 2004) is the newspaper corpus, which we use to train a standard tagger model. 201 IV LA V/A/N + not in Ref Misspelling: generate normalization candidates ranking replace re-run tagger not in Ref OOV in Ref Lexical gap: Lexical extension Figure 1: System overview with the handling of misspellings (upper branch) and lexical gaps (lower branch) 4.1. Decision Between Lexical Gaps and Misspellings The lexicon of a POS tagger depends on the training corpus, i.e. all words not seen during training are OOV. Words can be OOV for two diffe"
L16-1030,D12-1039,0,0.0653324,"butions. First, we describe a method to improve the POS tagging performance on learner language by automatically inducing POS information for OOV tokens. Second, we provide POS and normalization annotation on top of the CREG corpus (Meurers et al., 2011). We use the annotation to evaluate our method, but it is also a potentially useful resource in its own right. As no substantially large learner language corpora are readily available and learner language lacks the systematicity of other non-standard texts (such as microposts from Twitter), most domain adaptation methods from related work like Han et al. (2012), Rehbein (2013) or Prange et al. (2015) are not applicable. Instead our approach is to address the problem by exploiting the structure of reading comprehension questions, a typical language learning task: Students’ answers are linked to a reading text in standard language; when students answer reading questions, they rely on the given textual material and tend to repeat words or even copy whole phrases from the question or the reading text (referred to as reference texts below), a phenomenon known as lifting. Therefore learner answers tend to have a high lexical overlap with the reference tex"
L16-1030,D12-1000,0,0.598531,"rst, we describe a method to improve the POS tagging performance on learner language by automatically inducing POS information for OOV tokens. Second, we provide POS and normalization annotation on top of the CREG corpus (Meurers et al., 2011). We use the annotation to evaluate our method, but it is also a potentially useful resource in its own right. As no substantially large learner language corpora are readily available and learner language lacks the systematicity of other non-standard texts (such as microposts from Twitter), most domain adaptation methods from related work like Han et al. (2012), Rehbein (2013) or Prange et al. (2015) are not applicable. Instead our approach is to address the problem by exploiting the structure of reading comprehension questions, a typical language learning task: Students’ answers are linked to a reading text in standard language; when students answer reading questions, they rely on the given textual material and tend to repeat words or even copy whole phrases from the question or the reading text (referred to as reference texts below), a phenomenon known as lifting. Therefore learner answers tend to have a high lexical overlap with the reference tex"
L16-1030,W14-3505,1,0.860252,"see one main difference between our learner language corpus and other resources of non-standard language: For the methods described above to work, OOV words have to be frequent enough in some untagged corpus that they can be covered in e.g. distributional models. For many CMC domains such as Twitter, large untagged corpora are available and thus many OOV tokens indeed occur with a 199 3.1. Data We use CREG, the Corpus of Reading Comprehension Exercises (Meurers et al., 2011), as the basis of our annotations. It is the main German corpus for the task of short-answer scoring (Ziai et al., 2012; Koleva et al., 2014) and consists of 1032 learner answers (LAs) given to 177 questions about a total of 32 reading text, as well as teacher-specified target answers. The data had been collected in German language courses at two universities in the United States, all learners were American native speakers. For our annotations, we focus on the learner language material, i.e. the learner answers. We use only those answers given primarily in German. Answers had been transcribed twice in the corpus; we always use the first transcript and tokenized it using the Stanford CoreNLP tokenizer (Manning et al., 2014). The tok"
L16-1030,P14-5010,0,0.00367997,"l., 2012; Koleva et al., 2014) and consists of 1032 learner answers (LAs) given to 177 questions about a total of 32 reading text, as well as teacher-specified target answers. The data had been collected in German language courses at two universities in the United States, all learners were American native speakers. For our annotations, we focus on the learner language material, i.e. the learner answers. We use only those answers given primarily in German. Answers had been transcribed twice in the corpus; we always use the first transcript and tokenized it using the Stanford CoreNLP tokenizer (Manning et al., 2014). The tokens we thereby get for our annotation study sum up to a total of 12175. In our approach to automatically improve POS tagging on learner answers, we also make use of the additional material, which consists of standard language data that is lexically related to the learner language data. 3.2. Normalization In cases where a learner answer deviates from standard language we asked our annotators to form a target hypothesis, i.e., to formulate what the language learner presumably intended to say. We distinguish two normalization levels: on the first level (N1), we normalize misspellings; on"
L16-1030,W11-2401,0,0.0791719,"agger chooses erroneously to tag the word as an adjective instead of a noun. (1) Learner: Viel Erflog und geld haben Normalized: Viel Erfolg und Geld haben Tagger: ADV VVFIN KON ADJA VAINF Gold: ADV NN KON NN VAINF (2) Learner: Der Herd und die Dusche Normalized: Der Herd und die Dusche Tagger: ART NN KON ART ADJA Gold: ART NN KON ART NN This paper makes two contributions. First, we describe a method to improve the POS tagging performance on learner language by automatically inducing POS information for OOV tokens. Second, we provide POS and normalization annotation on top of the CREG corpus (Meurers et al., 2011). We use the annotation to evaluate our method, but it is also a potentially useful resource in its own right. As no substantially large learner language corpora are readily available and learner language lacks the systematicity of other non-standard texts (such as microposts from Twitter), most domain adaptation methods from related work like Han et al. (2012), Rehbein (2013) or Prange et al. (2015) are not applicable. Instead our approach is to address the problem by exploiting the structure of reading comprehension questions, a typical language learning task: Students’ answers are linked to"
L16-1030,W12-2022,0,0.0212303,"ing similarity. We see one main difference between our learner language corpus and other resources of non-standard language: For the methods described above to work, OOV words have to be frequent enough in some untagged corpus that they can be covered in e.g. distributional models. For many CMC domains such as Twitter, large untagged corpora are available and thus many OOV tokens indeed occur with a 199 3.1. Data We use CREG, the Corpus of Reading Comprehension Exercises (Meurers et al., 2011), as the basis of our annotations. It is the main German corpus for the task of short-answer scoring (Ziai et al., 2012; Koleva et al., 2014) and consists of 1032 learner answers (LAs) given to 177 questions about a total of 32 reading text, as well as teacher-specified target answers. The data had been collected in German language courses at two universities in the United States, all learners were American native speakers. For our annotations, we focus on the learner language material, i.e. the learner answers. We use only those answers given primarily in German. Answers had been transcribed twice in the corpus; we always use the first transcript and tokenized it using the Stanford CoreNLP tokenizer (Manning"
L16-1135,E06-1042,0,0.0348704,"l¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by standard approaches to supervised word sense disambiguation, we use the (lemmatized) words which occur within the same"
L16-1135,W07-1106,0,0.129658,"nderspecification: the Dem Hochzeitsunterhalter Robbie kommt der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was"
L16-1135,J09-1005,0,0.0733409,"the Dem Hochzeitsunterhalter Robbie kommt der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by"
L16-1135,W02-1006,0,0.06052,"skip n-grams, 1 to 6-grams in a window that spans 3 positions to the left and the right of the compound to be classified, skipping the compound itself. Additionally, we collect as features POS information of context words in the same window (pos) and the POS tag of the compound itself (pos0 ) or the second part if it is written as two individual tokens. To cover syntactic information of an item, we use the subject and accusative object of the compound as assigned by the Zurich parser (Sennrich et al., 2009), as well as their part-of-speech tags (syn). All these features have been proposed by (Lee and Ng, 2002) for a word sense disambiguation task. We also use selectional preference information (sel), counting how frequently the subject and accusative object head noun in a specific occurrence of the verb group occur as the subject or object, respectively, of the base verb (first component). For example, Bild (picture) occurs often as a subject of h¨angen, which we take as an indicator that an instance of h¨angen bleiben with Bild as a subject is used literally. Finally, we add topical information (topic) for the news article in which an item occurs using manually annotated topic categories in the Wa"
L16-1135,E09-1086,0,0.0798742,"der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by standard approaches to supervised word sense"
L16-1270,W11-2203,0,0.0844646,"ction and annotation in Section 3, and present the general idea of our system in Section 4. We present and discuss our results in Section 5, before evaluating our system on the original SemEval-2010 task in Section 6 and concluding in Section 7. 2. Related Work Most of the proposed systems for cross-lingual lexical substitution were presented for the SemEval-2010 CrossLingual Lexical Substitution Task (Mihalcea et al., 2010). Out of the 15 system variants that took part in the task, 11 use parallel corpora in of way or the other. A parallel corpus is also used in a more recent contribution by Apidianaki (2011). Systems for SemEval-2010 not using parallel corpora do lexical substitution on either the source language side (TYO, SWAT-E) or in the target language (SWATS, UBA-T) assuming an unambiguously translated context sentence (Mihalcea et al., 2010; Wicentowski et al., 2010; Basile and Semeraro, 2010). Also, two more recent contributions (Sinha, 2013; Zahran et al., 2015) use a machine translation system to translate the context sentence before applying their method of contextual disambiguation between substitution candidates. Systems using parallel corpora are not suitable for most low-resource l"
L16-1270,S10-1054,0,0.0226393,"s for cross-lingual lexical substitution were presented for the SemEval-2010 CrossLingual Lexical Substitution Task (Mihalcea et al., 2010). Out of the 15 system variants that took part in the task, 11 use parallel corpora in of way or the other. A parallel corpus is also used in a more recent contribution by Apidianaki (2011). Systems for SemEval-2010 not using parallel corpora do lexical substitution on either the source language side (TYO, SWAT-E) or in the target language (SWATS, UBA-T) assuming an unambiguously translated context sentence (Mihalcea et al., 2010; Wicentowski et al., 2010; Basile and Semeraro, 2010). Also, two more recent contributions (Sinha, 2013; Zahran et al., 2015) use a machine translation system to translate the context sentence before applying their method of contextual disambiguation between substitution candidates. Systems using parallel corpora are not suitable for most low-resource languages, as the availability of such resources cannot be assumed. For our language pair Nynorsk–English we do not have access to a suitable parallel corpus. Systems using off-the-shelf machine translation systems could be considered when the language pair in question is available. However, not al"
L16-1270,C12-2033,1,0.856168,"Missing"
L16-1270,2005.mtsummit-papers.11,0,0.0219217,"e SemEval-2010 Cross-Lingual Lexical Substitution Task use some sort of parallel corpora or a machine translation system (McCarthy et al., 2013). For low-resource languages, aligned parallel corpora or off-the-shelf machine translation programs are often not of comparable size and quality to the language pair Spanish–English used in that task, if such resources are available at all. A common choice for the parallel corpus used in the CLLS systems is the sentence-aligned Europarl corpus for the language pair English–Spanish. Releases of comparable size are available for 9 additional languages (Koehn, 2005). Incidentally, most of these languages are already commonly offered in language schools and online learning platforms. The releases for the languages of the newer EU members are naturally much smaller in size. For machine translation, only three (English, French, Spanish) out of 30 European languages are reported to have better than fragmentary support (Ananiadou et al., 2012). Of course, many more languages in the world are of potential interest to learners. For many of those languages, the availability and quality of language resources relevant to this task are likely to be worse. We propos"
L16-1270,W09-2412,0,0.0358943,"Missing"
L16-1270,H05-1052,0,0.501665,"well with Nynorsk; many words are left untranslated. Consequently, none of the described systems is directly applicable to our task. Cross-Lingual Lexical Substitution could also be viewed as word sense disambiguation with translations as the sense descriptors. FCC-LS (Vilarino et al., 2010), for example, makes use of cross-lingual WSD as a first step for lexical substitution, Apidianaki (2011) does word sense induction Lemmas Contexts Raters/context Translations/lemma Noun Verb Adj All 8 40 2.60 6.25 7 35 2.40 8.71 5 25 2.3 12.2 20 100 2.47 8.60 Table 1: Corpus overview on a parallel corpus. Mihalcea (2005) proposes an unsupervised graph-based algorithm for WSD, which we hypothesize would also work with simple lemmas instead of a predefined sense inventory, and would therefore be suitable for both monolingual and cross-lingual lexical substitution. 3. Data and Annotation Since we want to test the performance of our system specifically for low-resource languages, no suitable evaluation data could be obtained from any standard tasks. Therefore, we constructed a small evaluation corpus containing 20 ambiguous lemmas selected from a set of high- and mediumfrequency words in Norwegian Nynorsk, in the"
L16-1270,W12-3018,0,0.0205872,"Missing"
L16-1270,reese-etal-2010-wikicorpus,0,0.0250641,"Missing"
L16-1270,2009.freeopmt-1.7,0,0.0747118,"Missing"
L16-1270,S10-1023,0,0.0648342,"Missing"
L16-1270,S10-1025,0,0.0156793,"ost of the proposed systems for cross-lingual lexical substitution were presented for the SemEval-2010 CrossLingual Lexical Substitution Task (Mihalcea et al., 2010). Out of the 15 system variants that took part in the task, 11 use parallel corpora in of way or the other. A parallel corpus is also used in a more recent contribution by Apidianaki (2011). Systems for SemEval-2010 not using parallel corpora do lexical substitution on either the source language side (TYO, SWAT-E) or in the target language (SWATS, UBA-T) assuming an unambiguously translated context sentence (Mihalcea et al., 2010; Wicentowski et al., 2010; Basile and Semeraro, 2010). Also, two more recent contributions (Sinha, 2013; Zahran et al., 2015) use a machine translation system to translate the context sentence before applying their method of contextual disambiguation between substitution candidates. Systems using parallel corpora are not suitable for most low-resource languages, as the availability of such resources cannot be assumed. For our language pair Nynorsk–English we do not have access to a suitable parallel corpus. Systems using off-the-shelf machine translation systems could be considered when the language pair in question i"
L16-1270,zesch-etal-2008-extracting,0,0.0154146,"Missing"
L16-1556,P09-1068,0,0.282407,"nt role for the computational modeling of cognitive abilities (in particular for natural language processing), but making this kind of knowledge available for use in modeling is not easy. On the one hand, the manual creation of widecoverage knowledge bases is infeasible, due to the size and complexity of relevant script knowledge. On the other hand, texts typically refer only to certain steps in a script and leave a large part of this knowledge implicit, relying on the reader’s ability to infer the full script in detail. Thus, extraction of script knowledge from large text corpora (as done by Chambers and Jurafsky (2009)) is difficult and the outcome can be noisy. In this work, we present a large-scale crowdsourced collection and annotation of explicit linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experience"
L16-1556,E14-1006,1,0.867864,"ts, the clustering algorithm would need information on script-specific semantic similarity that goes beyond pure semantic similarity. For instance, in the FLYING IN AN AIRPLANE scenario, it is not trivial for any semantical similarity measure to predict that walk up the ramp and board plane are functionally similar with respect to the given scenario. To address this issue, we collect partial alignment information that we will use as seed data in future work on semi-supervised clustering. The alignment annotations are also suitable for a semi-supervised extension of the event-ordering model of Frermann et al. (2014). In this work, we have taken measures to provide a sound empirical basis for better-quality script models, by extending existing corpora in two different ways. First, we crowdsourced a corpus of 40 scenarios with 100 ESDs each, thus going beyond the size of previous script collections. Second, we enriched the corpus with partial alignments of ESDs, done by human annotators. The result is a corpus of partially-aligned generic activity descriptions, the DeScript corpus (Describing Script Structure). More generally, DeScript is a valuable resource for any task involving alignment and paraphrase"
L16-1556,L16-1555,1,0.846009,"e noisy. In this work, we present a large-scale crowdsourced collection and annotation of explicit linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experiences and instantiate the same scripts (Modi et al., 2016). Predecessors of our work are the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010), containing multiple eventsequence descriptions (ESDs) for specific activity types or scenarios. Figure 1 shows some example ESDs for the BAKING A CAKE scenario. As can be seen from the examples, the linguistic descriptions of the same event are different, but 1. Take out box of cake mix from shelf 2. Gather together cake ingredients 3. Get mixing bowl 4. Get mixing tool or spoon or fork 5. Add ingredients to bowl 6. Stir together and mix 7. Use fork to breakup clumps 8. Preheat oven 9. Spray"
L16-1556,P10-1100,1,0.839263,"linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experiences and instantiate the same scripts (Modi et al., 2016). Predecessors of our work are the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010), containing multiple eventsequence descriptions (ESDs) for specific activity types or scenarios. Figure 1 shows some example ESDs for the BAKING A CAKE scenario. As can be seen from the examples, the linguistic descriptions of the same event are different, but 1. Take out box of cake mix from shelf 2. Gather together cake ingredients 3. Get mixing bowl 4. Get mixing tool or spoon or fork 5. Add ingredients to bowl 6. Stir together and mix 7. Use fork to breakup clumps 8. Preheat oven 9. Spray pan with non stick or grease 10. Pour cake mix into pan 11. Put pan into oven 12. Set timer on oven 1"
L18-1512,N16-1067,0,0.0237228,"Missing"
L18-1512,P08-1090,0,0.23743,"t is substantially above the random baseline. A possible reason is that the compositional computation of the clause-level entailment type amounts to a generalization to the worst case. Thus a number of pairs end up in the Non-Entailment class although there is only a minor local incompatibility. 7. Background and Related Work Our work is based on script representations in which events are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, ev"
L18-1512,P09-1068,0,0.56057,"at the cake was put into the oven, because it is obvious that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase set"
L18-1512,W17-0905,0,0.0271532,"ed in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representations used in our study are learned from crowdsourced sequences of event descriptions, which are more focused and more detailed compared to narrative chains: They also contain events which are often not mentioned in text, since they are assumed to be background knowledge (Chambers, 2017). These two differences imply that the results of our annotation cannot easily be transferred to script representations along the lines of Chambers and Jurafsky (2008). Text-to-script mapping is similar to the task of recognizing textual entailment (RTE, Dagan et al. (2006)), in which systems have to decide whether a text entails a hypothesis. The text entails the hypothesis if a human reader would infer from the text that the hypothesis is most likely true. In our case, event mentions and event descriptions correspond to texts and hypotheses, respectively. The lexical entailment annotation in"
L18-1512,H92-1045,0,0.120866,"shows the fully labeled instance with participant and event annotations. To simplify the annotation, we make the assumption that each noun has only one sense per scenario: In the PLANTING A TREE scenario, the polysemous word stem e.g. always describes a part of a tree. In order to reduce the annotation effort, we presented all different noun types per participant type only once, rather than every single mentioned token in its sentential context. This on sense per scenario assumption is similar to the one sense per discourse hypothesis, which is often used in word sense disambiguation models (Gale et al., 1992). 4. Clause-Level Entailment: Composition In the previous section, we described the lexical entailment annotation on verbs and nouns, i.e. on a sub-event level. In this section, we now explain a method for an automatic, quasi-compositional computation of clausal-level entailment types. We compose the types from the lexical-level entailment labels of the verb and all its annotated noun dependents. Inspired by the textual inference method used in MacCartney and Manning (2007) and MacCartney and Manning (2009), we compute the type of clause-level entailment between InScript event mentions and DeS"
L18-1512,W07-1431,0,0.0933549,"r scenario assumption is similar to the one sense per discourse hypothesis, which is often used in word sense disambiguation models (Gale et al., 1992). 4. Clause-Level Entailment: Composition In the previous section, we described the lexical entailment annotation on verbs and nouns, i.e. on a sub-event level. In this section, we now explain a method for an automatic, quasi-compositional computation of clausal-level entailment types. We compose the types from the lexical-level entailment labels of the verb and all its annotated noun dependents. Inspired by the textual inference method used in MacCartney and Manning (2007) and MacCartney and Manning (2009), we compute the type of clause-level entailment between InScript event mentions and DeScript patterns from the manually annotated word-level entailment labels. Following MacCartney, we group these labels according to their truthconditional effects, and associate each group with one of six entailment types, shown in Figure 6. We adopt four entailment types from the schema of MacCartney and Manning (2009) and add two new types: We extend the schema with Identity, which is logically speaking a sub-case of Equality. Also, we use Partial Entailment to cover all ca"
L18-1512,W09-3714,0,0.198242,"in the DIG event into patterns (upper part) and the actual verb annotation (lower part). all script-relevant verb instances and all relevant participant instances in every story. 3. Lexical Entailment: Annotation Study To identify the type of semantic relation that needs to be modeled in order to align an event-denoting clause in the text with a paraphrase set representing the same event, the most straightforward way would be to conduct a clauselevel entailment annotation between the clause and EDs in the paraphrase set, e.g. with a set of clausal entailment types similar to the ones used in (MacCartney and Manning, 2009). We found, however, that the assessment of entailment types is time-consuming and unreliable, when based on a direct comparison of complex text clauses and paraphrase sets. We therefore simplify the task, breaking it down into two steps: First, annotators were asked to assign semantic relations between the event descriptions in the paraphrase sets and their instantiations in narrative texts on the lexical level only, labeling the event-denoting verbs and the participant-denoting noun phrases, as is described in this section. Second, we automatically derive an approximate clause-level entailme"
L18-1512,W14-1606,0,0.0151203,"rast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdso"
L18-1512,L16-1555,1,0.957132,"that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of"
L18-1512,N16-1098,0,0.0927158,"Missing"
L18-1512,S17-1016,1,0.903696,"s acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link event mentions in texts to the corresponding event types of a script, as indicated by the dotted lines in Figure 2. To our knowledge, Ostermann et al. (2017) is the only existing work on this text-to-script mapping task. Their approach is based on RKP-style script representations. Using this representation, the identification of the correct event type of an event mention is in many cases reduced to a simple identity check 3240 GET_ TREE GET_ TOOLS - buy a tree - get a tree from the store -… DIG - get a shovel - take the shovel -… After finding a shovel, PLANT - dig hole - use your shovel - use shovel for a hole -… I - plant the tree - put the tree into the hole -… made a very deep ditch and put the tree inside HOLE GET_TOOLS SHOVEL GARDENER DIG De"
L18-1512,E14-1024,0,0.128676,"oven, because it is obvious that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1."
L18-1512,P16-1027,0,0.0132769,"nts are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representations used in our study are learned from crowdsourced sequences of event descriptions, which are more focuse"
L18-1512,P10-1100,1,0.904319,"ariety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link ev"
L18-1512,D15-1195,0,0.231744,"compatibility. 7. Background and Related Work Our work is based on script representations in which events are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representat"
L18-1512,L16-1556,1,0.91254,"events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link event mentions in texts to the corresponding event types of a script, as indicated by the dotted lines in Figure 2. To our knowledge, Ostermann et al. (2017) is the only existing work on this text-to-script mapping task. Their approach is based on RKP-style script representations. Using this representation, the identification of the correct event type of an event mention is in many cases reduced to a simple identity check 3240 GET_ TREE GET_ TOOLS - buy a tree - get a tree from the store -…"
L18-1512,W17-0901,1,0.787181,"t does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an"
L18-1564,P09-1068,0,0.0612194,"spects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each ques"
L18-1564,P16-1223,0,0.0634014,"computed in the same way. We use different weight matrices for a, t and q, respectively. A combined representation p for the text–question pair is then constructed using a bilinear transformation matrix W: p = t> Wq (1) We compute a score for each answer by using the dot product and pass the scores for both answers through a softmax layer for prediction. The probability p for an answer a to be correct is thus defined as: Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text fit. Bidirectional GRUs are used to encode questions, texts and answers into hidden representations. For a question q and an answer a, the last state of the GRUs, q and a, are used as representations, while the text is encoded as a sequence of hidden states t1 ...tn . We then compute an attention score sj for each hidden state tj using the question representation q, a weight matrix Wa , and an attention bias b. Last, a text representation t"
L18-1564,P17-1147,0,0.0871477,"Missing"
L18-1564,D17-1082,0,0.139289,"Missing"
L18-1564,W14-1606,1,0.892066,"to tell who ate the food: Rachel or the waitress. In contrast, if we utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney,"
L18-1564,L16-1555,1,0.938737,"scribe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section 2.1.). In Section 2.2., we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk1 (henceforth MTurk). Section 2.3. gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section 2.4. gives statistics about the final dataset. 2.1. Pilot Study As a starting point for our pilots, we made use of texts from the InScript corpus (Modi et al., 2016), which provides stories centered around everyday situations (see Section 2.2.2.). We conducted three different pilot studies to determine the best way of collecting questions that require inference over commonsense knowledge: The most intuitive way of collecting reading comprehension questions is to show texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require comm"
L18-1564,K16-1008,1,0.87344,"i and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,00"
L18-1564,D14-1162,0,0.0796453,"Missing"
L18-1564,E14-1024,0,0.0215786,"ly, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx."
L18-1564,P16-1027,0,0.0137069,"(Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of"
L18-1564,D16-1264,0,0.0964981,"Missing"
L18-1564,P10-1100,1,0.940718,"e utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connectio"
L18-1564,D13-1020,0,0.338565,"Missing"
L18-1564,W17-2623,0,0.0482171,"rdson et al., 2013), BAbI (Weston et al., 2015), the Children’s Book Test (CBT, Hill et al. (2015)), CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)), and RACE (Lai et al., 2017). These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understand3572 ing, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge re"
L18-1564,L16-1556,1,0.888375,"rio and that can be answered from different texts (about that scenario), but for which a text does not need to provide the answer explicitly. The next section will describe the mode of collection chosen for the final dataset, based on the third pilot, in more detail. 2.2. Data Collection 2.2.1. Scenario Selection As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use 1 www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. 2.2.2. Texts For the collection of texts, we followed Modi et al. (2016), where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are syntactically simple, facilitating ma"
L18-1564,W17-0901,1,0.858709,"knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks."
L18-1564,P08-1090,0,\N,Missing
N12-1076,D10-1113,1,0.881896,"ó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, bo"
N12-1076,D08-1094,0,0.520018,"t for vector space models which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a Sören Laue Friedrich-Schiller Universität Jena, Germany soeren.laue@uni-jena.de word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and"
N12-1076,W09-0208,0,0.0160847,"ubstitution candidate is a synonym of the target in the given context. We omit the precise description of the evaluation setting here, as we follow the methodology described in Thater et al. (2011). Results are shown in Table 1, where the first column gives the GAP (Generalized Average Precision) score of the model and the second column gives the difference to the result reported in the literature. TFP10 and EP08 perform much better than the original proposals, as we obtain very significant gains of 4 and 14 GAP points. ted in the brief presentation in Section 2), which are difficult to tune (Erk and Padó (2009)), disappear this way. We can observe that the differences between the three methods, when simplified and tested in an unified setting, largely disappear. This is to be expected as all three methods implement very similar, all motivated intuitions: TFP11 reweights the vector of the target acquire with the second order vector of the context knowledge, i.e. with the vector of similarities of knowledge to all other words in the vocabulary. TFP10 takes a complementary approach: it reweights the vector of knowledge with the second order vector of acquire. In both these methods, anything outside the"
N12-1076,P10-2017,0,0.190342,"ue@uni-jena.de word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approa"
N12-1076,P08-1028,0,0.520997,"his is particularly difficult for vector space models which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a Sören Laue Friedrich-Schiller Universität Jena, Germany soeren.laue@uni-jena.de word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has b"
N12-1076,D11-1097,0,0.0356536,"(2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, both their formal presentations an"
N12-1076,I11-1079,0,0.0112926,"s between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidde"
N12-1076,N10-1013,0,0.0761248,"usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they at"
N12-1076,P10-1097,1,0.940094,"s which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a Sören Laue Friedrich-Schiller Universität Jena, Germany soeren.laue@uni-jena.de word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghd"
N12-1076,I11-1127,1,0.90213,"the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, both their formal presentations and the results obtained vary to a great extent. The answer given in this paper is surprising: the three models are essentially equivalent if syntactic information is ignored; in a syntactic space the three methods implement only slightly different 611 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611–615, c Montr´eal, Canada, June 3-8, 2012. 2012 Association"
N12-1076,D11-1094,0,0.0621642,"Missing"
P01-1028,E89-1001,0,0.0914324,"Missing"
P01-1028,C92-2092,0,0.262711,"hrases) built when gener ating a phrase with  intersective modifiers is ÿ in the case where the grammar imposes a single linear ordering of these modifiers. For instance, when generating “The fierce little black cat”, a naive constructive approach will also build the subphrases (1) only to find that these cannot be part of the output as they do not exhaust the input semantics. (1) The fierce black cat, The fierce little cat, The little black cat, The black cat, The fierce cat, The little cat, The cat. To remedy this shortcoming, various heuristics and parsing strategies have been proposed. (Brew, 1992) combines a constraint-propagation mechanism with a shift-reduce generator, propagating constraints after every reduction step. (Carroll et al., 1999) advocate a two-step generation algorithm in which first, the basic structure of the sentence is generated and second, intersective modifiers are adjoined in. And (Poznanski et al., 1995) make use of a tree reconstruction method which incrementally improves the syntactic tree until it is accepted by the grammar. In effect, the constraint-based encoding of the axiomatic view of generation proposed here takes advantage of Brew’s observation that co"
P01-1028,P96-1027,0,0.211938,"asic linguistic units are trees rather than categories and (iii) it assumes a flat semantics. In what follows we show that this combination of features results in a generator which integrates the positive aspects of both top-down and bottom-up generators. In this sense, it is not unlike (Shieber et al., 1990)’s semantic-head-driven generation. As will become clear in the following section however, it differs from it in that it integrates stronger lexicalist (i.e. bottom-up) information. 5.1 Bottom-Up Generation Bottom-up or “lexically-driven” generators (e.g., (Shieber, 1988; Whitelock, 1992; Kay, 1996; Carroll et al., 1999)) start from a bag of lexical items with instantiated semantics and generates a syntactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input. There are two known disadvantages to bottomup generators. On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics. On the other hand, they are often overly nondeterministic (though see (Carroll et al., 1999) for an exception). We now show how these problems are de"
P01-1028,C00-2087,0,0.0211279,"d bottom-up generators, Section 6 reports on a proof-of-concept implementation and Section 7 concludes with pointers for further research. 2 Description Grammars There is a range of grammar formalisms which depart from Tree Adjoining Grammar (TAG) by taking as basic building blocks tree descriptions rather than trees. D-Tree Grammar (DTG) is proposed in (Rambow et al., 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi, 1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (Vijay-Shanker, 1992)). DG differs from DTG and TDG however in that it adopts an axiomatic rather than a generative view of grammar: whereas in DTG and TDG, derived trees are constructed through a sequence of rewriting steps, in DG deriv"
P01-1028,P95-1035,0,0.0189947,"f the output as they do not exhaust the input semantics. (1) The fierce black cat, The fierce little cat, The little black cat, The black cat, The fierce cat, The little cat, The cat. To remedy this shortcoming, various heuristics and parsing strategies have been proposed. (Brew, 1992) combines a constraint-propagation mechanism with a shift-reduce generator, propagating constraints after every reduction step. (Carroll et al., 1999) advocate a two-step generation algorithm in which first, the basic structure of the sentence is generated and second, intersective modifiers are adjoined in. And (Poznanski et al., 1995) make use of a tree reconstruction method which incrementally improves the syntactic tree until it is accepted by the grammar. In effect, the constraint-based encoding of the axiomatic view of generation proposed here takes advantage of Brew’s observation that constraint propagation can be very effective in pruning the search space involved in the generation process. In constraint programming, the solutions to a constraint satisfaction problem (CSP) are found by alternating propagation with distribution steps. Propagation is a process of deterministic inference which fills out the consequences"
P01-1028,P95-1021,0,0.162754,"for generation, a model generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms. In specific, we argue that the change of perspective offered by the constraint-based, axiomatic approach to processing presents some interesting differences with"
P01-1028,J90-1004,0,0.259668,"Missing"
P01-1028,C88-2128,0,0.658731,")) nor with a grammar allowing for trace anchored lexical entries. The mirror restriction for generation is that each lexical entry must be associated with exactly one semantic proposition. The resulting shortcomings are that the generator can deal neither with a lexical entry having an empty semantics nor with a lexical entry having a multipropositional semantics. We first show that these restrictions are too strong. We then show how to adapt the generator so as to lift them. Empty Semantics. Arguably there are words such as “that” or infinitival “to” whose semantic contribution is void. As (Shieber, 1988) showed, the problem with such words is that they cannot be selected on the basis of the input semantics. To circumvent this problem, we take advantage of the TAG extended domain of locality to avoid having such entries in the grammar. For instance, complementizer “that” does not anchor a tree description by itself but occurs in all lexical tree descriptions providing an appropriate syntactic context for it, e.g. in the tree description for “say”. Multiple Propositions. Lexical entries with a multi-propositional semantics are also very common. For instance, a neo-Davidsonian semantics would as"
P01-1028,P97-1026,0,0.225213,"P:  z{ — John saw Mary S: &lt; S op G( ""#34X. ( UV$5$'& &lt;'(Y?W(G . 0 S X , ', fragments (fully specified subtrees) are always positive; except for the anchor, all leaves of fragments are negative, and internal node variables are neutral. This guarantees that in a saturated model, tree fragments that belong to the denotation of distinct tree descriptions do not overlap. Second, we require that every lexical tree description has a single minimal free model, which essentially means that the lexical descriptions must be tree shaped. Semantic representation. Following (Stone and Doran, 1997), we represent meaning using a flat semantic representation, i.e. as multisets, or conjunctions, of non-recursive propositions. This treatment offers a simple syntax-semantics interface in that the meaning of a tree is just the conjunction of meanings of the lexical tree descriptions used to derive it once the free variables occurring in the propositions are instantiated. A free variable is instantiated as follows: each free variable labels a syntactic node variable  and is unified with the label of any node variable identified with  . For the purpose of this paper, a simple semantic represe"
P01-1028,J92-4004,0,0.0460004,"to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi, 1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (Vijay-Shanker, 1992)). DG differs from DTG and TDG however in that it adopts an axiomatic rather than a generative view of grammar: whereas in DTG and TDG, derived trees are constructed through a sequence of rewriting steps, in DG derived trees are models satisfying a conjunction of elementary tree descriptions. Moreover, DG differs from Interaction Grammars in that it uses a flat rather than a Montague style recursive semantics thereby permitting a simple syntax/semantics interface (see below). A Description Grammar is a set of lexical entries of the form   where  is a tree description and  is the sema"
P01-1028,C92-2117,0,0.15025,"ar in which the basic linguistic units are trees rather than categories and (iii) it assumes a flat semantics. In what follows we show that this combination of features results in a generator which integrates the positive aspects of both top-down and bottom-up generators. In this sense, it is not unlike (Shieber et al., 1990)’s semantic-head-driven generation. As will become clear in the following section however, it differs from it in that it integrates stronger lexicalist (i.e. bottom-up) information. 5.1 Bottom-Up Generation Bottom-up or “lexically-driven” generators (e.g., (Shieber, 1988; Whitelock, 1992; Kay, 1996; Carroll et al., 1999)) start from a bag of lexical items with instantiated semantics and generates a syntactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input. There are two known disadvantages to bottomup generators. On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics. On the other hand, they are often overly nondeterministic (though see (Carroll et al., 1999) for an exception). We now show how these prob"
P03-1047,P92-1005,0,0.434104,"n Underspecification Formalisms: Minimal Recursion Semantics as Dominance Constraints Joachim Niehren Programming Systems Lab Universit¨at des Saarlandes niehren@ps.uni-sb.de Abstract Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspecified semantics. We present the first provably efficient algorithm to enumerate the readings of MRS structures, by translating them into normal dominance constraints. 1 Introduction In the past few years there has been considerable activity in the development of formalisms for underspecified semantics (Alshawi and Crouch, 1992; Reyle, 1993; Bos, 1996; Copestake et al., 1999; Egg et al., 2001). The common idea is to delay the enumeration of all readings for as long as possible. Instead, they work with a compact underspecified representation; readings are enumerated from this representation by need. Minimal Recursion Semantics (MRS) (Copestake et al., 1999) is the standard formalism for semantic underspecification used in large-scale HPSG grammars (Pollard and Sag, 1994; Copestake and Flickinger, ). Despite this clear relevance, the most obvious questions about MRS are still open: 1. Is it possible to enumerate the r"
P03-1047,P01-1019,0,0.0981128,"ith the same handle. These EP-conjunctions can be replaced in a preprocessing step introducing additional EPs that make conjunctions explicit. Second, our outscope constraints are slightly less restrictive than the original “qeq-constraints.” A handle h is qeq to a handle h0 in an MRS M, h =q h0 , if either h = h0 or a quantifier h : Qx (h1 , h2 ) occurs in M and h2 is qeq to h0 in M. Thus, h =q h0 implies h ≤ h0 , but not the other way round. We believe that the additional strength of qeq-constraints is not needed in practice for modeling scope. Recent work in semantic construction for HPSG (Copestake et al., 2001) supports our conjecture: the examples discussed there are compatible with our simplification. Third, we depart in some minor details: we use sets instead of multi-sets and omit top-handles which are useful only during semantics construction. 3 Dominance Constraints Dominance constraints are a general framework for describing trees, and thus syntax trees of logical formulas. Dominance constraints are the core language underlying CLLS (Egg et al., 2001) which adds parallelism and binding constraints. 3.1 Syntax and Semantics We assume a possibly infinite signature Σ of function symbols with fix"
P03-1047,P03-1047,1,0.0522477,"b.de We distinguish the sublanguages of MRS nets and normal dominance nets, and show that they can be intertranslated. This translation answers the first question: existing constraint solvers for normal dominance constraints can be used to enumerate the readings of MRS nets in low polynomial time. The translation also answers the second question restricted to pure scope underspecification. It shows the equivalence of a large fragment of MRSs and a corresponding fragment of normal dominance constraints, which in turn is equivalent to a large fragment of Hole Semantics (Bos, 1996) as proven in (Koller et al., 2003). Additional underspecified treatments of ellipsis or reinterpretation, however, are available for extensions of dominance constraint only (CLLS, the constraint language for lambda structures (Egg et al., 2001)). Our results are subject to a new proof technique which reduces reasoning about MRS structures to reasoning about weakly normal dominance constraints (Bodirsky et al., 2003). The previous proof techniques for normal dominance constraints (Koller et al., 2003) do not apply. 2 Minimal Recursion Semantics We define a simplified version of Minimal Recursion Semantics and discuss difference"
P03-1047,copestake-flickinger-2000-open,0,\N,Missing
P04-1032,P92-1005,0,0.344327,"uchss,koller,stth}@coli.uni-sb.de Abstract We show that a practical translation of MRS descriptions into normal dominance constraints is feasible. We start from a recent theoretical translation and verify its assumptions on the outputs of the English Resource Grammar (ERG) on the Redwoods corpus. The main assumption of the translation— that all relevant underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language"
P04-1032,copestake-flickinger-2000-open,0,0.0533312,"nd concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal dominance constraints. This translation clarified the precise relationship between these two related formalisms, and made the powerful meta-theory of dominance constraints accessible to MRS. Their goal was to also make the large grammars for MRS ∗ Supported by the CHORUS project of the SFB 378 of the DFG. and the efficient constraint solvers for dominance constraints"
P04-1032,1995.tmi-1.2,0,0.0357009,"nt underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal do"
P04-1032,P01-1019,0,0.0646999,"the Redwoods corpus. The main assumption of the translation— that all relevant underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater"
P04-1032,P03-1047,1,0.529018,"stake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal dominance constraints. This translation clarified the precise relationship between these two related formalisms, and made the powerful meta-theory of dominance constraints accessible to MRS. Their goal was to also make the large grammars for MRS ∗ Supported by the CHORUS project of the SFB 378 of the DFG. and the efficient constraint solvers for dominance constraints available to the other formalism. However, Niehren and Thater made three technical assumptions: 1. that EP-conjunction can be resolved in a preprocessing step; 2"
P04-1032,C02-2025,0,0.0619071,"s an interesting side effect, we also compare the run-times of the constraint-solvers we used, and we find that the dominance constraint solver typically outperforms the MRS solver, often by significant margins. Grammar and Resources. We use the English Resource Grammar (ERG), a large-scale HPSG grammar, in connection with the LKB system, a grammar development environment for typed feature grammars (Copestake and Flickinger, 2000). We use the system to parse sentences and output MRS constraints which we then translate into dominance constraints. As a test corpus, we use the Redwoods Treebank (Oepen et al., 2002) which contains 6612 sentences. We exclude the sentences that cannot be parsed due to memory capacities or words and grammatical structures that are not included in the ERG, or which produce ill-formed MRS expressions (typically violating M1) and thus base our evaluation on a corpus containing 6242 sentences. In case of syntactic ambiguity, we only use the first reading output by the LKB system. To enumerate the solutions of MRS constraints and their translations, we use the MRS solver built into the LKB system and a solver for weakly normal dominance constraints (Bodirsky et al., 2004), prop"
P05-3003,W02-1503,0,0.0773241,"ost efficient solver for scope underspecification; it also converts between different underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55, but the treebank also contains extreme cases su"
P05-3003,copestake-flickinger-2000-open,0,0.422574,"vel terminus at Flåm. (Rondane 650) We present the currently most efficient solver for scope underspecification; it also converts between different underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55"
P05-3003,W04-3321,0,0.0173775,"erent underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55, but the treebank also contains extreme cases such as (1) below, which according to the ERG has about 2.4 trillion (1012 ) readings: In o"
P05-3003,C00-1067,1,0.748174,"Missing"
P05-3003,P03-1047,1,0.928535,"vers) are built for different underspecification formalisms. To help alleviate this problem, utool can read and write underspecified descriptions and write out solutions in a variety of different formats: 60 50 40 30 20 10 • dominance graphs; 0 0 5 10 15 20 25 30 35 40 • descriptions of Minimal Recursion Semantics; • descriptions of Hole Semantics. The input and output functionality is provided by codecs, which translate between descriptions in one of these formalisms and the internal dominance graph format. The codecs for MRS and Hole Semantics are based on the (non-trivial) translations in (Koller et al., 2003; Niehren and Thater, 2003) and are only defined on nets, i.e. constraints whose graphs satisfy certain structural restrictions. This is not a very limiting restriction in practice (Flickinger et al., 2005). utool also allows the user to test efficiently whether a description is a net. In practice, utool can be used to convert descriptions between the three underspecification formalisms. Because the codecs work with concrete syntaxes that are used in existing systems, utool can be used as a drop-in replacement e.g. in the LKB grammar development system (Copestake and Flickinger, 2000). 2.3 Run"
P06-1052,P04-1032,1,0.849762,"demand that the holes are “plugged” by roots while realising the dominance edges as dominance, as in the two configurations (of five) shown to the right. These configurations are trees that encode semantic representations of the sentence. We will freely read configurations as ground terms over the signature Σ. 2.1 Hypernormally connected graphs Throughout this paper, we will only consider hypernormally connected (hnc) dominance graphs. Hnc graphs are equivalent to chain-connected dominance constraints (Koller et al., 2003), and are closely related to dominance nets (Niehren and Thater, 2003). Fuchss et al. (2004) have presented a corpus study that strongly suggests that all dominance graphs that are generated by current largescale grammars are (or should be) hnc. Technically, a graph G is hypernormally connected iff each pair of nodes is connected by a simple hypernormal path in G. A hypernormal path (Althaus et al., 2003) in G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. Hnc graphs have a number of very useful structural properties on which this paper rests. One which is particularly relevant here is that we can predict in which"
P06-1052,P05-3003,1,0.639277,"he first time in the open. (Rondane 892) Introduction Underspecification is nowadays the standard approach to dealing with scope ambiguities in computational semantics (van Deemter and Peters, 1996; Copestake et al., 2004; Egg et al., 2001; Blackburn and Bos, 2005). The basic idea behind it is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of the individual semantic representations from an USR (Koller and Thater, 2005b). A major promise of underspecification is that it makes it possible, in principle, to rule out entire subsets of readings that we are not interested in wholesale, without even enumerating them. For instance, real-world sentences with scope ambiguities often have many readings that are semantically equivalent. Subsequent modules (e.g. for doing inference) will typically only be interested in one reading from each equivalence class, and all others could be deleted. This situation is illustrated by the following two (out of many) sentences from the Rondane treebank, which is distributed with F"
P06-1052,W05-1105,1,0.58061,"he first time in the open. (Rondane 892) Introduction Underspecification is nowadays the standard approach to dealing with scope ambiguities in computational semantics (van Deemter and Peters, 1996; Copestake et al., 2004; Egg et al., 2001; Blackburn and Bos, 2005). The basic idea behind it is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of the individual semantic representations from an USR (Koller and Thater, 2005b). A major promise of underspecification is that it makes it possible, in principle, to rule out entire subsets of readings that we are not interested in wholesale, without even enumerating them. For instance, real-world sentences with scope ambiguities often have many readings that are semantically equivalent. Subsequent modules (e.g. for doing inference) will typically only be interested in one reading from each equivalence class, and all others could be deleted. This situation is illustrated by the following two (out of many) sentences from the Rondane treebank, which is distributed with F"
P06-1052,W06-3904,1,0.837563,"n terms of rewrite rules that permute quantifiers without changing the semantics of the readings. The particular USRs we work with are underspecified chart representations, which can be computed from dominance graphs (or USRs in some other underspecification formalisms) efficiently (Koller and Thater, 2005b). We evaluate the performance of the algorithm on the Rondane treebank and show that it reduces the median number of readings from 56 to 4, by up to a factor of 666.240 for individual USRs, while running in negligible time. To our knowledge, our algorithm and its less powerful predecessor (Koller and Thater, 2006) are the first redundancy elimination algorithms in the literature that operate on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class (Vestre, 1991; Chaves, 2003), but these approaches don’t maintain underspecification: After running their algorithms, they are left with a set of readings rather than an underspecified representation, i.e. we could no longer run other algorithms on an USR. The paper is structured as follows. We will first define dominance graphs and review the necessary background theory in Section 2. We will th"
P06-1052,P03-1047,1,0.843619,"aph can serve as an USR for the sentence “a representative of a company saw a sample” if we demand that the holes are “plugged” by roots while realising the dominance edges as dominance, as in the two configurations (of five) shown to the right. These configurations are trees that encode semantic representations of the sentence. We will freely read configurations as ground terms over the signature Σ. 2.1 Hypernormally connected graphs Throughout this paper, we will only consider hypernormally connected (hnc) dominance graphs. Hnc graphs are equivalent to chain-connected dominance constraints (Koller et al., 2003), and are closely related to dominance nets (Niehren and Thater, 2003). Fuchss et al. (2004) have presented a corpus study that strongly suggests that all dominance graphs that are generated by current largescale grammars are (or should be) hnc. Technically, a graph G is hypernormally connected iff each pair of nodes is connected by a simple hypernormal path in G. A hypernormal path (Althaus et al., 2003) in G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. Hnc graphs have a number of very useful structural properties on whi"
P06-1052,C02-2025,0,0.0999359,"r earlier algorithm (Koller and Thater, 2006), which computed a chart with four configurations for the graph in which 1 and 2 are existential and 3 is universal, as opposed to the three equivalence classes of this graph’s configurations. 5 Evaluation In this final section, we evaluate the the effectiveness and efficiency of the elimination algorithm: We run it on USRs from a treebank and measure how many readings are redundant, to what extent the algorithm eliminates this redundancy, and how much time it takes to do this. Resources. The experiments are based on the Rondane corpus, a Redwoods (Oepen et al., 2002) style corpus which is distributed with the English Resource Grammar (Flickinger, 2002). The corpus contains analyses for 1076 sentences from the tourism domain, which are associated with USRs based upon Minimal Recursion Semantics (MRS). The MRS representations are translated into dominance graphs using the open-source utool tool (Koller and Thater, 2005a), which is restricted to MRS representations whose translations are hnc. By restricting ourselves to such MRSs, we end up with a data set of 999 dominance graphs. The average number of scope bearing operators in the data set is 6.5, and the"
P06-1052,E91-1044,0,0.7474,"n some other underspecification formalisms) efficiently (Koller and Thater, 2005b). We evaluate the performance of the algorithm on the Rondane treebank and show that it reduces the median number of readings from 56 to 4, by up to a factor of 666.240 for individual USRs, while running in negligible time. To our knowledge, our algorithm and its less powerful predecessor (Koller and Thater, 2006) are the first redundancy elimination algorithms in the literature that operate on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class (Vestre, 1991; Chaves, 2003), but these approaches don’t maintain underspecification: After running their algorithms, they are left with a set of readings rather than an underspecified representation, i.e. we could no longer run other algorithms on an USR. The paper is structured as follows. We will first define dominance graphs and review the necessary background theory in Section 2. We will then introduce our notion of equivalence in Section 3, and present the redundancy elimination algorithm in Section 4. In Section 5, we describe the evaluation of the algorithm on the Rondane corpus. Finally, Section 6"
P06-1052,W06-3905,0,\N,Missing
P08-1026,copestake-flickinger-2000-open,0,0.144585,"Missing"
P08-1026,N04-1014,0,0.0319917,"he soft disjointness edges as as angled double-headed arrows. Each soft edge is annotated with its weight. The hard backbone of this dominance graph is our example graph from Fig. 1, so it has the same five configurations. The weighted graph assigns a weight of 8 to configuration (a), a weight of 1 to (d), and a weight of 9 to (e); this is also the configuration of maximum weight. 5.2 Weighted tree grammars In order to compute the maximal-weight configuration of a weighted dominance graph, we will first translate it into a weighted regular tree grammar. A weighted regular tree grammar (wRTG) (Graehl and Knight, 2004) is a 5-tuple G = (S, N, Σ, R, c) such 224 that G0 = (S, N, Σ, R) is a regular tree grammar and c : R → R is a function that assigns each production rule a weight. G accepts the same language of trees as G0 . It assigns each derivation a cost equal to the product of the costs of the production rules used in this derivation, and it assigns each tree in the language a cost equal to the sum of the costs of its derivations. Thus wRTGs define weights in a way that is extremely similar to PCFGs, except that we don’t require any weights to sum to one. Given a weighted, hypernormally connected dominan"
P08-1026,J03-1004,0,0.44626,"mination amounts to intersection of regular tree up, plus dominance or outscoping relations between languages. Furthermore, we show how to define a these building blocks. This has been a very suc- PCFG-style cost model on RTGs and compute best cessful approach, but recent algorithms for elimi- readings of deterministic RTGs efficiently, and illusnating subsets of readings have pushed the expres- trate this model on a machine learning based model 218 Proceedings of ACL-08: HLT, pages 218–226, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics of scope preferences (Higgins and Sadock, 2003). To our knowledge, this is the first efficient algorithm for computing best readings of a scope ambiguity in the literature. The paper is structured as follows. In Section 2, we will first sketch the existing standard approach to underspecification. We will then define regular tree grammars and show how to see them as an underspecification formalism in Section 3. We will present the new redundancy elimination algorithm, based on language intersection, in Section 4, and show how to equip RTGs with weights and compute best readings in Section 5. We conclude in Section 6. 2 Underspecification pe"
P08-1026,C00-1067,1,0.919121,"Missing"
P08-1026,P05-3003,1,0.943422,"rtant here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simplifies semantics construction, and current algorithms (Koller and Thater, 2005a) support the efficient enumeration of readings from an USR when it is necessary. Furthermore, it is possible to perform certain semantic processing tasks such as eliminating redundant readings (see Section 4) directly on the level of underspecified representations without explicitly enumerating individual readings. Under the “standard model” of scope underspecification, readings are considered as formulas or trees. USRs specify the “semantic material” common to all readings, plus dominance or outscopes relations between these building blocks. In this paper, we consider dominance graphs (Egg"
P08-1026,W05-1105,1,0.947348,"rtant here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simplifies semantics construction, and current algorithms (Koller and Thater, 2005a) support the efficient enumeration of readings from an USR when it is necessary. Furthermore, it is possible to perform certain semantic processing tasks such as eliminating redundant readings (see Section 4) directly on the level of underspecified representations without explicitly enumerating individual readings. Under the “standard model” of scope underspecification, readings are considered as formulas or trees. USRs specify the “semantic material” common to all readings, plus dominance or outscopes relations between these building blocks. In this paper, we consider dominance graphs (Egg"
P08-1026,W06-3904,1,0.875959,"arser avoids comput- approach for specifying sets of trees in theoretical ing all scope readings. Instead, it computes a single computer science, and are closely related to regucompact underspecified description for each parse. lar tree transducers as used e.g. in recent work on One can then strengthen the underspecified descrip- statistical MT (Knight and Graehl, 2005) and gramtion to efficiently eliminate subsets of readings that mar formalisms (Shieber, 2006). We show that the were not intended in the given context (Koller and “dominance charts” proposed by Koller and Thater Niehren, 2000; Koller and Thater, 2006); so when (2005b) can be naturally seen as regular tree gramthe individual readings are eventually computed, the mars; using their algorithm, classical underspecified number of remaining readings is much smaller and descriptions (dominance graphs) can be translated much closer to the actual perceived ambiguity of the into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete sentence. In the past few years, a “standard model” of scope because every finite tree language is also regular. We underspecification has emerged: A range of for- exploit this inc"
P08-1026,N06-1045,0,0.0228615,"of computing the best tree is NPcomplete (Sima’an, 1996). However, if the weighted regular tree automaton corresponding to the wRTG is deterministic, every tree has only one derivation, and thus computing best trees becomes easy again. The tree automata for dominance charts are always deterministic, and the automata for RTGs as in Section 3.2 (whose terminals correspond to the graph’s node labels) are also typically deterministic if the variable names are part of the quantifier node labels. Furthermore, there are algorithms for determinizing weighted tree automata (Borchardt and Vogler, 2003; May and Knight, 2006), which could be applied as preprocessing steps for wRTGs. 6 Conclusion In this paper, we have shown how regular tree grammars can be used as a formalism for scope underspecification, and have exploited the power of this view in a novel, simpler, and more complete algorithm for redundancy elimination and the first efficient algorithm for computing the best reading of a scope ambiguity. In both cases, we have adapted standard algorithms for RTGs, which illustrates the usefulness of using such a well-understood formalism. In the worst case, the RTG for a scope ambiguity is exponential in the num"
P08-1026,P03-1047,1,0.908669,"for computing best readings of a scope ambiguity in the literature. The paper is structured as follows. In Section 2, we will first sketch the existing standard approach to underspecification. We will then define regular tree grammars and show how to see them as an underspecification formalism in Section 3. We will present the new redundancy elimination algorithm, based on language intersection, in Section 4, and show how to equip RTGs with weights and compute best readings in Section 5. We conclude in Section 6. 2 Underspecification pernormally connected dominance graphs, or dominance nets (Niehren and Thater, 2003). The precise definition of dominance nets is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simpl"
P08-1026,C02-2025,0,0.0706165,"Missing"
P08-1026,P08-2062,1,0.795162,"Missing"
P08-1026,E06-1048,0,0.165256,"malism. Regular stake and Flickinger (2000)). The key idea behind tree grammars (Comon et al., 2007) are a standard underspecification is that the parser avoids comput- approach for specifying sets of trees in theoretical ing all scope readings. Instead, it computes a single computer science, and are closely related to regucompact underspecified description for each parse. lar tree transducers as used e.g. in recent work on One can then strengthen the underspecified descrip- statistical MT (Knight and Graehl, 2005) and gramtion to efficiently eliminate subsets of readings that mar formalisms (Shieber, 2006). We show that the were not intended in the given context (Koller and “dominance charts” proposed by Koller and Thater Niehren, 2000; Koller and Thater, 2006); so when (2005b) can be naturally seen as regular tree gramthe individual readings are eventually computed, the mars; using their algorithm, classical underspecified number of remaining readings is much smaller and descriptions (dominance graphs) can be translated much closer to the actual perceived ambiguity of the into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete sentence. In the past"
P08-1026,C96-2215,0,0.0646807,"Missing"
P08-1026,E91-1044,0,0.572228,"mproved redundancy elimination algorithm. 4.1 ings of U, but every reading in U is semantically equivalent to some reading in U 0 . For instance, the following sentence from the Rondane treebank is analyzed as having six quantifiers and 480 readings by the ERG grammar; these readings fall into just two semantic equivalence classes, characterized by the relative scope of “the lee of” and “a small hillside”. A redundancy elimination would therefore ideally reduce the underspecified description to one that has only two readings (one for each class). Redundancy elimination Redundancy elimination (Vestre, 1991; Chaves, 2003; Koller and Thater, 2006) is the problem of deriving from an USR U another USR U 0 , such that the readings of U 0 are a proper subset of the read221 Based on this definition, Koller and Thater (2006) present an algorithm (henceforth, KT06) that deletes rules from a dominance chart and thus removes subsets of readings from the USR. The KT06 algorithm is fast and quite effective in practice. However, it essentially predicts for each production rule of a dominance chart whether each configuration that can be built with this rule is equivalent to a configuration that can be built w"
P08-1026,P06-1052,1,\N,Missing
P10-1004,C00-1067,1,0.745991,"weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the 3.1 Dominance graphs A (labelled) dominance graph D (Althaus et al., 2003) is a directed graph that consists of a"
P10-1004,P05-3003,1,0.884123,"system, and then construct a tree grammar for the relative normal forms of the combined system. This algorithm performs redundancy elimination and computes weakest readings at the same time, and in our example retains only a single configuration, namely Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool release. We use Utool to automatically translate the 999 MRS descriptions for which this is possible into RTGs. To simplify the specification of the rewrite systems, we restrict ourselves to the subcorpus in which all scope-taking operators (labels"
P10-1004,bos-2008-lets,0,0.0298507,"these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all other read30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computatio"
P10-1004,W02-1503,0,0.0605201,"Missing"
P10-1004,P03-1047,1,0.915748,"regular tree grammars are called regular tree languages (RTLs), and regular tree grammars are equivalent to finite tree automata, which are defined essentially like the well-known finite string automata, except that they assign states to the nodes in a tree rather than the positions in a string. Regular tree languages enjoy many of the closure properties of regular string languages. In particular, we will later exploit that RTLs are closed under intersection and complement. 3.3 Dominance graphs as RTGs An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are 32 always hnc subgraphs of D. In the worst case, GD can be exponentially bigger t"
P10-1004,copestake-flickinger-2000-open,0,0.0639585,"Missing"
P10-1004,P08-1026,1,0.874212,"with respect to this order – only express “safe” information that is common to all other read30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 2008). We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules. This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by Koller et al. (2008) can be seen as a special case of our construction. Thus our system can serve as a general framework for removing unintended readings from an underspecified representation. The paper is structured as follows. Section 2 starts by reviewing related work. We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to futur"
P10-1004,C08-1066,0,0.038867,"Missing"
P10-1004,C02-2025,0,0.169586,"s makes it possible to work with semantic representations derived by deep large-scale grammars. 1 Introduction Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the obse"
P10-1004,J08-3004,0,0.0200498,"ly once. 4.2 The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et al. (2008). We will now define context tree transducers. Let Σ be a ranked signature, and let Xm be a set of m variables. We write Con(m) (Σ) for the contexts with m holes, i.e. those trees in T (Σ ∪ Xm ) in which each element of Xm occurs exactly once, and always as a leaf. If C ∈ Con(m) (Σ), then C[t1 , . . . ,tm ] = C[t1 /x1 , . . . ,tm /xm ], where x1 , . . . , xm are the variables from left to right. A (top-down) context tree transducer from Σ to ∆ is a 5-tuple M = (Q, Σ, ∆, q0 , δ ). Σ and ∆ are ranked signatures, Q is a finite set of states, and q0 ∈ Q is the start state. δ is a finite set of tra"
P10-1004,J03-1004,0,0.311779,"(ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all o"
P10-1004,E95-1001,0,0.0473012,"nce, the signature of the trees in Fig. 1 is {∀x |2, ∃y |2, compz |0, . . .}. Finite constructor trees can be seen as ground terms over Σ that respect the arities. We write T (Σ) for the finite constructor trees over Σ. Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about an"
P10-1004,P83-1009,0,0.666373,"f , g, a, . . .}, each of which is equipped with an arity ar( f ) ≥ 0. We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol. For instance, the signature of the trees in Fig. 1 is {∀x |2, ∃y |2, compz |0, . . .}. Finite constructor trees can be seen as ground terms over Σ that respect the arities. We write T (Σ) for the finite constructor trees over Σ. Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically li"
P10-1004,E91-1044,0,0.234609,"em R, in time linear in the size of GD and linear in the size of R. This is a dramatic improvement over the best previous algorithm, which was quadratic in |conf(D)|. 5 Redundancy elimination, revisited The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR , and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0 ) ⊆ conf(D) and An example Consider an annotated rewrite system that contains rule (1) plus the following rewrite rule: [−] ∃z (P, ∀x (Q, R)) → ∀x (∃z (P, Q), R) {6}{q} ¯ → repr-ofx,z Figure 4: RTG for the weakest readings of Fig. 1. −1 L(G0 ) = L(GD ) ∩ τM (L(GD )). R 4.4 {5}{q} ¯ → compz (3) This rewrite system translates in"
P10-1004,P08-1000,0,\N,Missing
P10-1097,P93-1016,0,0.0283255,"Missing"
P10-1097,P98-2127,0,0.530918,"distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapat"
P10-1097,J90-1003,0,0.0611287,"se dimensions correspond to pairs of a relation and a word. Recall that any vector of V1 can be represented as a finite sum of the form ∑ ai~er,w0 with appropriate scalar factors ai . In this vector space we define the first-order vector [w] of a word w as follows: [w] = ∑ ω(w, r, w0 ) ·~er,w0 r∈R w0 ∈W where ω is a function that assigns the dependency triple (w, r, w0 ) a corresponding weight. In the simplest case, ω would denote the frequency in a corpus of dependency trees of w occurring together with w0 in relation r. In the experiments reported below, we use pointwise mutual information (Church and Hanks, 1990) instead as it proved superior to raw frequency counts: pmi(w, r, w0 ) = log p(w, w0 |r) p(w |r)p(w0 |r) We further consider a similarly defined vector space V2 , spanned by an orthonormal basis {~er,r0 ,w0 |r, r0 ∈ R, w0 ∈ W }. Its dimensions therefore correspond to triples of two relations and a word. Evidently this is a higher dimensional space than V1 , which therefore can be embedded into V2 by the “lifting maps” Lr : V1 ,→ V2 defined by Lr (~er0 ,w0 ) := ~er,r0 ,w0 (and by linear extension therefore on all vectors of V1 ). Using these lifting maps we define the second-order vector [[w]]"
P10-1097,de-marneffe-etal-2006-generating,0,0.00414648,"Missing"
P10-1097,P08-2008,0,0.0103235,"of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms oth"
P10-1097,D09-1046,0,0.130457,"le. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms other unsupervised methods that have been proposed in the literature to rank words with respect to their semantic similarity in a given linguistic context. In a second experiment, we apply our model to the “word sense similarity task” recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a varia"
P10-1097,D08-1094,0,0.824911,"the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Assoc"
P10-1097,W09-0208,0,0.292215,"Missing"
P10-1097,J03-4004,0,0.0589275,"expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide invariant representations averaging over all di"
P10-1097,P08-1028,0,0.910717,"ng over all different usages of the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Ann"
P10-1097,J07-2002,0,0.0413469,"ds (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting,"
P10-1097,D08-1048,0,0.015881,"Missing"
P10-1097,J98-1004,0,0.922668,"rocessing is highly inefficient and expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide"
P10-1097,W09-2506,1,0.904909,"l Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct tw"
P10-1097,S07-1009,0,\N,Missing
P10-1097,C98-2122,0,\N,Missing
Q13-1003,W11-2503,0,0.0687814,"bach  , Dominikus Wetzel ∗ , Stefan Thater ∗ , Bernt Schiele  and Manfred Pinkal ∗ ∗ Department of Computational Linguistics, Saarland University, Saarbr¨ucken, Germany (regneri|dwetzel|stth|pinkal)@coli.uni-saarland.de  Max Planck Institute for Informatics, Saarbr¨ucken, Germany (rohrbach|schiele)@mpi-inf.mpg.de Abstract grounding meaning in visual information, in particular by extending the distributional approach to jointly cover texts and images (Feng and Lapata, 2010; Bruni et al., 2011). As a clear result, visual information improves the quality of distributional models. Bruni et al. (2011) show that visual information drawn from images is particularly relevant for concrete common nouns and adjectives. A natural next step is to integrate visual information from videos into a semantic model of event and action verbs. Psychological studies have shown the connection between action semantics and videos (Glenberg, 2002; Howell et al., 2005), but to our knowledge, we are the first to provide a suitable data source and to implement such a model. The contribution of this paper is three-fold: Recent work has shown that the integration of visual information into text-based models can subs"
Q13-1003,P11-1020,0,0.0111386,"n the landscape of related work (Sec. 2), then we introduce our corpus (Sec. 3). Sec. 4 reports our action similarity annotation experiment and Sec. 5 introduces the similarity measures we apply to the annotated data. We outline the results of our evaluation in Sec. 6, and conclude the paper with a summary and directions for future work (Sec. 7). 2 Related Work A large multimodal resource combining language and visual information resulted from the ESP game (von Ahn and Dabbish, 2004). The dataset contains many images tagged with several one-word labels. The Microsoft Video Description Corpus (Chen and Dolan, 2011, MSVD) is a resource providing textual descriptions of videos. It consists of multiple crowd-sourced textual descriptions of short video snippets. The MSVD corpus is much larger than our corpus, but most of the videos are of relatively low quality and therefore too challenging for state-ofthe-art video processing to extract relevant information. The videos are typically short and summarized with a single sentence. Our corpus contains coherent textual descriptions of longer video sequences, where each sentence is associated with a timeframe. Gupta et al. (2009) present another useful resource:"
Q13-1003,N12-1094,0,0.00938379,"th visual information, by incorporating features from article illustrations. They achieve better results when incorporating the visual information, providing an enriched model that pairs a single text with a picture. Bruni et al. (2011) used the ESP game data to create a visually grounded semantic model. Their results outperform purely text-based models using visual information from pictures for the task of modeling noun similarities. They model single words, and mostly visual features lead only to moderate improvements, which might be due to the mixed quality and random choice of the images. Dodge et al. (2012) recently investigated which words can actually be grounded in images at all, producing an automatic classifier for visual words. An interesting in-depth study by Mathe et al. (2008) automatically learnt the semantics of motion verbs as abstract features from videos. The study captures 4 actions with 8-10 videos for each of the actions, and would need a perfect object recognition from a visual classifier to scale up. Steyvers (2010) and later Silberer and Lapata (2012) present an alternative approach to incorporating visual information directly: they use so-called feature norms, which consist"
Q13-1003,D09-1046,0,0.0381859,"hich vegetable was prepared). We asked the annotators explicitly to ignore the actor of the action (e.g. whether it is a man or a woman) and score the similarities of the underlying actions rather than their verbalizations. Each subject rated all 900 pairs, which were shown to them in completely random order, with a different order for each subject. We compute inter-annotator agreement (and the forthcoming evaluation scores) using Spearman’s rank correlation coefficient (ρ), a non-parametric test which is widely used for similar evaluation tasks (Mitchell and Lapata, 2008; Bruni et al., 2011; Erk and McCarthy, 2009). Spearman’s ρ evaluates how the samples are ranked relative to each other rather than the numerical distance between the rankings. Fig. 5 shows the average similarity ratings in the different settings and the inter-annotator agreement. The average inter-rater agreement was ρ = 0.73 (averaged over pairwise rater agreements), with pairwise results of ρ = 0.77, 0.72, and 0.69, respectively, which are all highly significant at p < 0.001. As expected, pairs with the same activity and object are rated very similar (4.19) on average, while the similarity of different activities on the same object is"
Q13-1003,N10-1011,0,0.00751241,"tion in videos. However, the corpus contains no natural language texts. The connection between natural language sentences and videos has so far been mostly explored 26 by the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others). Our resource is intended to be used for action recognition as well, but in this paper, we focus on the inverse effect of visual data on language processing. Feng and Lapata (2010) were the first to enrich topic models for newspaper articles with visual information, by incorporating features from article illustrations. They achieve better results when incorporating the visual information, providing an enriched model that pairs a single text with a picture. Bruni et al. (2011) used the ESP game data to create a visually grounded semantic model. Their results outperform purely text-based models using visual information from pictures for the task of modeling noun similarities. They model single words, and mostly visual features lead only to moderate improvements, which mig"
Q13-1003,P08-1028,0,0.00670692,"but we noted the relevant kitchen task (i.e. which vegetable was prepared). We asked the annotators explicitly to ignore the actor of the action (e.g. whether it is a man or a woman) and score the similarities of the underlying actions rather than their verbalizations. Each subject rated all 900 pairs, which were shown to them in completely random order, with a different order for each subject. We compute inter-annotator agreement (and the forthcoming evaluation scores) using Spearman’s rank correlation coefficient (ρ), a non-parametric test which is widely used for similar evaluation tasks (Mitchell and Lapata, 2008; Bruni et al., 2011; Erk and McCarthy, 2009). Spearman’s ρ evaluates how the samples are ranked relative to each other rather than the numerical distance between the rankings. Fig. 5 shows the average similarity ratings in the different settings and the inter-annotator agreement. The average inter-rater agreement was ρ = 0.73 (averaged over pairwise rater agreements), with pairwise results of ρ = 0.77, 0.72, and 0.69, respectively, which are all highly significant at p < 0.001. As expected, pairs with the same activity and object are rated very similar (4.19) on average, while the similarity"
Q13-1003,W11-0126,0,0.00903921,"ey use so-called feature norms, which consist of human associations for many given words, as a proxy for general perceptual information. Because this model is trained and evaluated on those feature norms, it is not directly comparable to our approach. The Restaurant Game by Orkin and Roy (2009) grounds written chat dialogues in actions carried out in a computer game. While this work is outstanding from the social learning perspective, the actions that ground the dialogues are clicks on a screen rather than real-world actions. The dataset has successfully been used to model determiner meaning (Reckman et al., 2011) in the context of the Restaurant Game, but it is unclear how this approach could scale up to content words and other domains. 3 The TACOS Corpus We build our corpus on top of the “MPII Cooking Composite Activities” video corpus (Rohrbach et al., 2012b, MPII Composites), which contains videos of different activities in the cooking domain, e.g., preparing carrots or separating eggs. We extend the existing corpus with multiple textual descriptions collected by crowd-sourcing via Amazon Mechanical Turk1 (MTurk). To facilitate the alignment of sentences describing activities with their proper vide"
Q13-1003,D12-1130,0,0.0415458,"d mostly visual features lead only to moderate improvements, which might be due to the mixed quality and random choice of the images. Dodge et al. (2012) recently investigated which words can actually be grounded in images at all, producing an automatic classifier for visual words. An interesting in-depth study by Mathe et al. (2008) automatically learnt the semantics of motion verbs as abstract features from videos. The study captures 4 actions with 8-10 videos for each of the actions, and would need a perfect object recognition from a visual classifier to scale up. Steyvers (2010) and later Silberer and Lapata (2012) present an alternative approach to incorporating visual information directly: they use so-called feature norms, which consist of human associations for many given words, as a proxy for general perceptual information. Because this model is trained and evaluated on those feature norms, it is not directly comparable to our approach. The Restaurant Game by Orkin and Roy (2009) grounds written chat dialogues in actions carried out in a computer game. While this work is outstanding from the social learning perspective, the actions that ground the dialogues are clicks on a screen rather than real-wo"
Q13-1003,I11-1127,1,0.381911,"Missing"
Q13-1003,J13-3003,0,\N,Missing
Q13-1003,P09-1002,0,\N,Missing
S12-1089,D10-1115,0,0.0610919,"rgiana.dinu@unitn.it Stefan Thater Dept. of Computational Linguistics Universit¨at des Saarlandes stth@coli.uni-saarland.de Abstract In recent work Mitchell and Lapata (2008) has drawn the attention to the question of building vectorial meaning representations for sentences by combining individual word vectors. They propose a family of simple “compositional” models that compute a vector for a phrase or a sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication. More refined models have been proposed recently by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). This paper describes our system for the Semeval 2012 Sentence Textual Similarity task. The system is based on a combination of few simple vector space-based methods for word meaning similarity. Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when i"
S12-1089,N12-1076,1,0.780307,"the literature (Mitchell and Lapata, 2008) 604 where C(w) is the context in which w occurs, i.e. all words related to w by a dependency relation such as subject or object, including inverse relations. Remark. The contextualization model presented above is a slightly simplified version of the original model of Thater et al. (2011): it uses standard bag-ofwords vectors instead of syntax-based vectors. This simplified version performs better on the training dataset. Furthermore, the simplified model has been shown to be equivalent to the models of Erk and Pad´o (2008) and Thater et al. (2010) by Dinu and Thater (2012), so the results reported below carry over directly to these other models as well. 2.2 Vector Composition and Alignment The two vector space models sketched above represent the meaning of words, and thus cannot be applied directly to model similarity of phrases or sentences. One obvious and straightforward way to extend these models to the sentence level is to follow Mitchell and Lapata (2008) and represent sentences by vectors obtained by summing over the individual vectors of the constituent words. These “compositional” models can then be used to compute similarity scores between sentence pa"
S12-1089,D08-1094,0,0.128434,"Missing"
S12-1089,D11-1129,0,0.0651106,"Missing"
S12-1089,P08-1028,0,0.431225,"utional information about the words in the target’s context. While this approach is not “compositional” in the sense described above, it still captures some meaning of the complete phrase in which a target word occurs. In this paper, we report on the system we used in the Semeval 2012 Sentence Textual Similarity shared task and describe an approach that uses a combination of few simple vector-based components. We extend the model of Thater et al. (2011), which has been shown to perform well on a closely related paraphrase ranking task, with an additive composition operation along the lines of Mitchell and Lapata (2008), and compare it with a simple alignment-based approach which in turn uses vector-based similarity scores. Results show that in particular the alignmentbased approach can achieve good performance on the Microsoft Research Video Description dataset. On the other datasets, all vector-based components are outperformed by a surprisingly competitive word 603 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603–607, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics overlap baseline, but they still bring improvements over this baseline when"
S12-1089,P10-1097,1,0.880709,"Missing"
S12-1089,I11-1127,1,0.8789,"Missing"
S17-1015,D15-1177,0,0.0439185,"Missing"
S17-1015,P16-1191,0,0.00990687,"Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that they assign the same weight to ever"
S17-1015,P16-1101,0,0.00456847,"ord. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word correspond"
S17-1015,J15-4004,0,0.0198179,"Missing"
S17-1015,P12-1092,0,0.0244217,"ptures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most suitable topic of a word"
S17-1015,P15-1010,0,0.0522639,"Nguyen1 , Dat Quoc Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,K16-1008,1,0.408695,"enses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense o"
S17-1015,W14-1606,1,0.616494,"is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense"
S17-1015,N15-1070,0,0.0357356,"Missing"
S17-1015,Q17-1003,1,0.820726,"model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinge"
S17-1015,D14-1113,0,0.0546105,"Missing"
S17-1015,Q15-1016,0,0.0359882,"he word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,t . 1 We use an unigram distribution raised"
S17-1015,Q15-1022,1,0.436169,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D15-1200,0,0.0397679,"t size. In addition, v tor representation of the word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,"
S17-1015,K17-3014,1,0.253101,"we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific"
S17-1015,U15-1014,1,0.855226,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D14-1162,0,0.126941,"ntal results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Co"
S17-1015,W13-3512,0,0.17894,"Missing"
S17-1015,C14-1016,0,0.133176,"Missing"
S17-1015,D16-1174,0,0.425139,"Missing"
S17-1015,C14-1015,0,0.0135853,"Missing"
S17-1015,N15-1058,0,0.0460639,"Missing"
S17-1015,N10-1013,0,0.0136891,"dding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most sui"
S17-1015,P15-1173,0,0.0914243,"Missing"
S17-1015,D15-1036,0,0.0294844,"Missing"
S17-1015,K15-1026,0,0.0326275,"c Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,D13-1170,0,0.00363313,"r words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, appr"
S17-1015,D14-1110,0,\N,Missing
S17-1015,P15-2003,0,\N,Missing
S17-1015,P14-1023,0,\N,Missing
S17-1015,L16-1046,0,\N,Missing
S17-1016,P10-1143,0,0.026874,"narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities"
S17-1016,P09-1068,0,0.160534,"the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et"
S17-1016,L16-1555,1,0.896046,"veryday activities, so called event sequence descriptions (ESDs). ESDs consist of short telegram-style descriptions of single events (event descriptions, ED). The textual order of EDs corresponds to the temporal order of respective events, i.e. temporal information is explicitly encoded. DeScript contains 50 ESDs for each of 40 different scenarios. Alongside the ESDs, it also provides gold event paraphrase sets, i.e. clusters of all event descriptions denoting the same event type, labeled with the respective type. While DeScript is a source of structured script knowledge, the InScript corpus (Modi et al., 2016) provides us with the appropriate kind of narrative texts. InScript is a collection of 910 stories centered around some specific scenario, for 10 of the 40 scenarios in DeScript, e.g. BAKING A CAKE, RIDING A BUS , TAKING A SHOWER . All verbs occurring in the texts are annotated with an event type if they are relevant to the script instantiated by the story; as non-script event otherwise. In the upper part of Fig. 1, you see the initial fragment of a story about baking a cake; together with a script excerpt in the lower part, depicted by labeled event paraphrase sets. I looked up the recipe and"
S17-1016,W14-1606,0,0.175518,"75); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any h"
S17-1016,E14-1006,1,0.837053,"fsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regne"
S17-1016,Q17-1003,1,0.848171,"n that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect expli"
S17-1016,J02-3001,0,0.0278061,"derstanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long"
S17-1016,E12-1034,0,0.415153,"Missing"
S17-1016,D12-1045,0,0.0269025,"ipt event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to"
S17-1016,P10-1100,1,0.84377,"tructure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address"
S17-1016,P16-1113,1,0.812832,"h the current script scenario, we employ two features: a binary feature indicating whether the verb is used in the ESDs for the given scenario; and a scenariospecific tf–idf score that is computed by treating all ESDs from a scenario as one document, summed over the verb and its dependents. In Section 4.2, we evaluate models with and without script features, to test the impact of scenario-specific information. Frame Feature. We further employ framesemantic information because we expect script events to typically evoke certain frames.We use a state-of-the-art semantic role labeler (Roth, 2016; Roth and Lapata, 2016) based on FrameNet (Rup2 For EDs, we use all mentioned head nouns. To emphasize the importance of the verb, we double its weight when averaging. 4 Because our CRF model only supports nominal features, we discretize embeddings from code.google.com/ archive/p/word2vec/ by binning the component values into three intervals [−∞, −], [−, ], [, ∞]. The hyperparameter  is determined on a held-out development set. 3 P Lemma Our model Our model (scen. indep.) R F1 0.365 0.949 0.526 0.628 0.817 0.709 0.513 0.877 0.645 Table 1: Identification of script-relevant verbs within a scenario and independent"
S17-1016,S15-1024,1,0.873865,"Missing"
S17-1016,L16-1556,1,0.905957,"ssed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first at"
S17-1016,W17-0901,1,0.781511,"task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first attempt, we assume that three steps are necessary to solve it, although in the long run, an integrated approach will be preferable: First, the script which is addressed by the event mention must be identified. Second, it has to be decided whether a verb denotes a script event at all. Fina"
S17-1016,S15-2132,0,\N,Missing
S17-1016,J82-1004,0,\N,Missing
S18-1119,D15-1075,0,0.0806933,"Missing"
S18-1119,P08-1090,0,0.586286,"corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine compreh"
S18-1119,P09-1068,0,0.104067,"Missing"
S18-1119,P16-1223,0,0.0969874,"Missing"
S18-1119,P17-1168,0,0.024165,"are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) employs a BiLSTM with attention to find similarities between texts, questions, and answers. Each word is represented based on Numberbatch embeddings, which encode information from ConceptNet. 752 YNU AI1799 (Liu et al., 2018) submitted an ensemble of neural network models based on LSTMs, RNNs, and BiLSTM/CNN combinations, with attention mechanisms. In addition to word2vec embeddings, positional embeddings are used that are generated based on word embeddings. Rank Team name y/n what why who where when 1 2 3 4 5 6 7 8 9 10 11 Yuanfudao MITRE J"
S18-1119,S18-1174,0,0.0156272,"2: The accuracy of participating systems and the two baselines in total, on commonsense-based questions (CS), text-based questions (TXT) and on out-of-domain questions (from the 5 held-out testing scenarios). The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line. The last line shows the human upper bound (Ostermann et al., 2018) as comparison. (based on ConceptNet). The model is pretrained on another large machine comprehension dataset, namely the RACE corpus. YNU Deep (Ding and Zhou, 2018) test different LSTMs and BiLSTMs variants to encode questions, answers and texts. A simple attention mechanism is applied between question–answer and text–answer pairs. The final submission is an ensemble of five model instances. MITRE (Merkhofer et al., 2018) use a combination of 3 systems - two LSTMs with attention mechanisms, and one logistic regression model using patterns based on the vocabulary of the training set. The two neural models use different word embeddings - one trained on GoogleNews, another one trained on Twitter, which were enriched with word overlap features. Interestingly"
S18-1119,S18-1176,0,0.0287908,"Missing"
S18-1119,S18-1172,0,0.0613083,"Missing"
S18-1119,P17-1147,0,0.0637147,"Missing"
S18-1119,S18-1180,0,0.0405537,"other one trained on Twitter, which were enriched with word overlap features. Interestingly, the simple logistic regression model achieves competitive performance and would have ranked 4th as an individual system. Jiangnan (Xia, 2018) applies a BiLSTM over GloVe and CoVe embeddings (McCann et al., 2017) with an additional attention mechanism. The attention mechanism computes soft word alignment between words in the question and the text or answer. Manual features, including part-of-speech tags, named entitity types, and term frequencies, are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) e"
S18-1119,S18-1120,0,0.229149,"Missing"
S18-1119,S18-1173,0,0.0325421,"Missing"
S18-1119,S18-1181,0,0.0662618,"Missing"
S18-1119,K16-1008,1,0.821428,"ate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets. In Section 3, we describe"
S18-1119,L16-1555,1,0.854976,"riety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proce"
S18-1119,W14-1606,1,0.907601,"the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the durat"
S18-1119,Q17-1003,1,0.839109,"she thus refers to Rachel. This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems. In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc. (Schank and Abelson, 1975). The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979; Schank, 1982; Modi et al., 2017). From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative"
S18-1119,N16-1098,0,0.0711233,"mind think about nothing but peaceful, happy thoughts. I stayed in there for only about ten minutes because it was so hot and steamy. When I got out, I turned the sauna off to save energy and took a cool shower. I got out of the shower and dried off. After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home. 2 Q1 Where did they sit inside the sauna? a. on the floor b. on a bench Q2 How long did they stay in the sauna? a. about ten min- b. over thirty utes minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions. (Mostafazadeh et al., 2016)). These tasks test a system’s ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks. Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension. The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text. In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions r"
S18-1119,L18-1564,1,0.88915,"ms with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text. 4.2 Baselines We provide results of two baseline systems as lower bounds for comparison: a rule-based baseline (Sliding Window) and a neural end-to-end system (Attentive Reader). Both baselines are described in 2 IUCM cluster MCScript texts and try to find answers also in other texts, that are topically similar. In that sense, MCScript itself is used to represent commonsense knowledge. more detail below. For details about the tuning of hyperparameters, we refer to Ostermann et al. (2018). Sliding Window The sliding window baseline is a simple rule-based method that answers a question on a text by predicting the answer option with the highest similarity to the text. The intuition underlying this method is that answers similar to a text should be more plausible than answer options that are different from the text (independent of the question). In our baseline implementation, we compute similarity using a sliding window that compares each answer option to any possible “window” of w tokens of the text. For comparison, each window and each answer is represented by an average vecto"
S18-1119,D14-1162,0,0.0806018,"Missing"
S18-1119,D16-1264,0,0.12393,"atasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of 100,000 questions on Wikipedia articles collected via crowdsourcing. In that dataset, the answer to a question corresponds to a segment/span from the reading passage. Since Wikipedia articles mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reaso"
S18-1119,P10-1100,1,0.886473,"plicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally i"
S18-1119,S18-1179,0,0.0225614,"ion of the softmax function over both answer options for the question: p(a|t, q) = sof tmax(t> Ws a) 5 embeddings (Speer et al., 2017). One participating system made use of script knowledge in the form of event sequence descriptions. Resources commonly used by participants include pretrained word embeddings such as GloVe (Pennington et al., 2014) or word2vec (Mikolov et al., 2013), and preprocessing pipelines such as NLTK4 . In the following, we provide short summaries of the participants’ systems and we give an overview of models and resources used by them (Table 1). Non-neural methods IUCM (Reznikova and Derczynski, 2018) applied an unsupervised approach that assigns the correct answer to a question based on text overlap. Text overlap is computed based on the given passage and text sources of the same topic. Different clustering and topic modeling techniques are used to identify such text sources in MCScript and DeScript. (2) Participants We ran our shared task through the CodaLab platform3 . 24 teams submitted results during the evaluation period, out of which 11 teams provided system descriptions: 8 teams from China, and one team each from Spain, Russia and the US. The full leader board containing all 24 sub"
S18-1119,D13-1020,0,0.150799,"ned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts. For example, consider the short narrative in (1). For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an Related Work Recently, a number of datasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of"
S18-1119,S15-1024,1,0.874914,"Missing"
S18-1119,D15-1195,0,0.0608887,"Missing"
S18-1119,S18-1175,0,0.0341503,"Missing"
S18-1119,W17-2623,0,0.0302808,"s mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reasoning to solve different tasks. In contrast to our dataset, the artificial texts in BAbI are not reflective of a typically occurring narrative text. Two recently published datasets that also have a larger focus on commonsense reasoning are NewsQA and TriviaQA. NewsQA (Trischler et al., 2017) contains newswire texts from CNN with crowdsourced questions and answers. During the question collection, workers were only presented with the title of the text, and a short summary. This 748 method ensures that literal repetitions of the text are avoided and the generation of non-trivial questions requiring background knowledge is supported. The NewsQA text collection differs from MCScript in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Tr"
S18-1119,W17-0901,1,0.841966,"cript knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper i"
S18-1119,C00-2137,0,0.237567,"Missing"
S18-1119,S18-1177,0,0.0482581,"Missing"
W04-3320,W00-2027,0,0.0286812,"in the constraint framework. We have started work into this direction by implementing a prototypical constraint parser for LTAG, and investigating its properties. The implementation can be done in a straightforward way by transforming the axiomatization of well-ordered derivation trees that was given in Section 3 into a constraint satisfaction problem along the lines of Duchier (2003). The resulting parser is available as a module for the XDG system (Debusmann, 2003). Preliminary evaluation of the parser using the XTAG grammar shows that it is not competitive with state-ofthe-art TAG parsers (Sarkar, 2000) in terms of run-time; however, this measure is not the most significant one for an evaluation of the constraint-based approach anyway. More importantly, a closer look on the search spaces ex6 The parsing problem of LTAG can be decided in time O(n6 ). plored by the parser indicates that the inferences drawn from the axiomatic principles are not strong enough to rule out branches of the search that lead to only inconsistent assignments of the problem variables. Future work needs to closely investigate this issue; ideally, we would arrive at an implementation that enumerates all well-ordered der"
W04-3320,P01-1024,1,0.830681,"0/ (c2 , ε) 0/ (d1 , ε) 0/ (d2 , ε) 0/ (b1 , ε) 0/ (b2 , ε) 0/ (b2 , 2) 0/ inserted pasted 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ {b1 , c2 } {a1 } 0/ {c2 } 0/ {d2 } 0/ {a2 } 0/ {c1 } 0/ {d1 } 0/ {a2 , b2 , c1 , c2 , d1 } 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ below 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ {a1 } {a2 } {c1 } {c2 } {d1 } {d2 } {a1 , a2 , b1 , b2 , c1 , c2 , d1 , d2 } {a2 , b2 , c1 , c2 , d1 } {b1 , b2 , c1 , c2 } Figure 2: Yields in the analysis of string aabbccdd employ an existing constraint-based parser for Topological Dependency Grammar (Duchier and Debusmann, 2001). In light of the fact that surface realization is an NP-complete problem, the efficiency of this parser is quite remarkable. One of the major questions for a description-based approach to LTAG parsing is, whether the benign computational properties of existing, derivationbased parsers for LTAG6 can be exploited even in the constraint framework. We have started work into this direction by implementing a prototypical constraint parser for LTAG, and investigating its properties. The implementation can be done in a straightforward way by transforming the axiomatization of well-ordered derivation"
W05-1105,copestake-flickinger-2000-open,0,0.817703,"e a sequence of four solvers, ranging from a purely logic-based saturation algorithm (Koller et al., 1998) over a solver based on constraint programming (Duchier and Niehren, 2000) to efficient solvers based on graph algorithms (Bodirsky et al., 2004). The first three solvers have been described in the literature before, but we also present a new variant of the graph solver that uses caching to obtain a considerable speedup. Finally we present a new evaluation that compares all four solvers with each other and with a different underspecification solver from the LKB grammar development system (Copestake and Flickinger, 2000). The paper is structured as follows. We will first sketch the problem that our algorithms solve (Section 2). Then we present the solvers (Section 3) and conclude with the evaluation (Section 4). 2 The Problem The problem we use to illustrate the progress towards efficient solvers is that of enumerating all readings of an underspecified description. Underspecification is a technique for dealing with the combinatorial problems associated with quantifier scope ambiguities, certain semantic ambiguities that occur in sentences such as the following: ∀x → ∃y ∧ stud ∃y ∧ x read book y y x ∀x book →"
W05-1105,P03-1047,1,0.710933,"). The treebank solver even goes down. This demonstrates an imcontains syntactic annotations for sentences from proved management of the combinatorial explothe tourism domain such as (4) above, together sion. It is also interesting that the line of the setwith corresponding semantic representations. constraint solver is almost parallel to that of the The semantics is represented using MRS de- graph solver, which means that the solver really scriptions, which we convert into normal domi- does exploit a polynomial fragment on real-world nance constraints using the translation specified by data. Niehren and Thater (2003). The translation is reThe LKB solver performs very well for smaller stricted to MRS constraints having certain struc- constraints (which make up about half of the data tural properties (called nets). The treebank con- set): Except for the chart algorithm introduced in tains 961 MRS constrains, 879 of which are nets. this paper, it outperforms all other solvers. For For the runtime evaluation, we restricted the larger constraints, however, the LKB solver gets test set to the 852 nets with less than one mil- very slow. What isn’t visible in this graph is that lion solved forms. The distribution"
W06-3904,copestake-flickinger-2000-open,0,0.0682435,"rrent algorithms support the efficient enumeration of readings from an USR [10]. In addition, underspecification has the potential for eliminating incorrect or redundant readings by inferences based on context or world knowledge, without even enumerating them. For instance, sentences with scope ambiguities often have readings which are semantically equivalent. In this case, we typically need to retain only one reading from each equivalence class. This situation is illustrated by the following two sentences from the Rondane treebank, which is distributed with the English Resource Grammar (ERG; [5]), a broad-coverage HPSG grammar. (1) For travellers going to Finnmark there is a bus service from Oslo to Alta through Sweden. (Rondane 1262) (2) We quickly put up the tents in the lee of a small hillside and cook for the first time in the open. (Rondane 892) For the two example sentences, the ERG (Version 01-2006) derives USRs with seven and six quantifiers, respectively, that correspond to various types of noun phrases (including proper names and pronouns). The USR for (1) describes 3960 readings, which are all semantically equivalent to each other. On the other hand, the USR for (2) has 48"
W06-3904,P04-1032,1,0.886026,"G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. We say that G is hypernormally connected (hnc) iff each pair of nodes is connected by a simple hypernormal path in G. Hnc graphs are equivalent to chain-connected dominance constraints [9], and are closely related to dominance nets [11]. The results in this paper are restricted to hnc graphs, but this does not limit the applicability of our results: an empirical study suggests that all dominance graphs that are generated by current large-scale grammars are (or should be) hnc [8]. The key property of hnc dominance graphs is that their solved forms correspond to configurations, and we will freely switch between solved forms and their corresponding configurations. Another important property of hnc graphs which we will use extensively in the proofs below is that it is possible to predict which holes of fragments can dominate other fragments in a solved form. Lemma 2.2 Let G be a hnc graph with free fragment F. Then all weakly connected components of G − F are hnc. Proposition 2.3 Let F1 , F2 be fragments in a hnc dominance graph G. If there is a solved form S of G in whi"
W06-3904,P03-1047,1,0.89333,"still exponentially smaller than the entire set of readings because common subgraphs (such as {2, 5, 7} in the example) are represented only once. Thus the chart can still serve as an underspecified representation. 2.2 Hypernormally connected dominance graphs A hypernormal path [1] in a graph G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. We say that G is hypernormally connected (hnc) iff each pair of nodes is connected by a simple hypernormal path in G. Hnc graphs are equivalent to chain-connected dominance constraints [9], and are closely related to dominance nets [11]. The results in this paper are restricted to hnc graphs, but this does not limit the applicability of our results: an empirical study suggests that all dominance graphs that are generated by current large-scale grammars are (or should be) hnc [8]. The key property of hnc dominance graphs is that their solved forms correspond to configurations, and we will freely switch between solved forms and their corresponding configurations. Another important property of hnc graphs which we will use extensively in the proofs below is that it is possible to p"
W06-3904,W05-1105,1,0.917191,"largescale grammars. To our knowledge, it is the first redundancy elimination algorithm which maintains underspecification, rather than just enumerating non-redundant readings. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguities in computational semantics [12,6,7,2]. The basic idea is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of readings from an USR [10]. In addition, underspecification has the potential for eliminating incorrect or redundant readings by inferences based on context or world knowledge, without even enumerating them. For instance, sentences with scope ambiguities often have readings which are semantically equivalent. In this case, we typically need to retain only one reading from each equivalence class. This situation is illustrated by the following two sentences from the Rondane treebank, which is distributed with the English Resource Grammar (ERG; [5]), a broad-coverage HPSG grammar. (1) For travellers going to Finnmark there"
W06-3904,E91-1044,0,0.657686,"changing the semantics of the readings. The particular USRs we work with are underspecified chart representations, which can be computed from dominance graphs (or USRs in some other underspecification formalisms) efficiently [10]. The algorithm can deal with many interesting cases, but is incomplete in the sense that the resulting USR may still describe multiple equivalent readings. To our knowledge, this is the first algorithm in the literature for redundancy elimination on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class [13,4], but these approaches don’t maintain underspecification: After running their algorithms, we have a set of readings rather than an underspecified representation. Plan of the paper. We will first define dominance graphs and review the necessary background theory in Section 2. We will then give a formal definition of equivalence and derive some first results in Section 3. Section 4 presents the redundancy elimination algorithm. Finally, Section 5 concludes and points to further work. 2 Dominance Graphs The basic underspecification formalism we assume here are labelled dominance graphs [1]. Domin"
W07-1402,W05-1206,0,\N,Missing
W07-1402,P98-1013,0,\N,Missing
W07-1402,C98-1013,0,\N,Missing
W07-1402,P02-1035,0,\N,Missing
W09-2506,D08-1094,0,0.742797,"king Paraphrases in Context Stefan Thater Universität des Saarlandes stth@coli.uni-sb.de Georgiana Dinu Universität des Saarlandes dinu@coli.uni-sb.de Abstract argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into the compositional picture. In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado’s paper (“E&P” in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task data shows that our model significantly outperforms E&P in terms of average precision. We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An eval"
W09-2506,W09-0208,0,0.361254,"them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restricted vector of the input verb using cosine similarity. In order to compare our model with state of the art, we reimplement E&P’s structured vector space model. We filter stop words, and compute lexical vectors in a “syntactic” space using the most frequent 2000 words from the BNC as basis. We also consider a variant in which the basis corresponds to words indexed by their grammatical roles. We choose parameters that Erk and Padó (2009) report to perform best, and use the method described in Erk and Padó (2009) to compute vectors in context. Generalized average precision (GAP) is a more precise measure than Poot : Applied to a ranking task with about 20 candidates, Poot just gives the percentage of good candidates found in the upper half of the proposed ranking. Average precision is sensitive to the relative position of correct and incorrect candidates in the ranking, GAP moreover rewards the correct order of positive cases w.r.t. their gold standard weight. We define average precision first: AP = Σni=1 xi pi R pi = Σik=1 xk"
W09-2506,P05-1014,0,0.0179912,"finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔ X throw Y ” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose Plan of the paper. Section 2 presents our model and briefly relates it to previous work. Section 3 describes the evaluation of our model on the lexical substitution task data. Section 4 concludes. 2 A model for meaning in context We propose a dependency-based model whose dimensions reflect dependency relations, and distinguish two kinds or layers of lexical meaning: argument meaning and predicate meaning. The argument meaning of a word w is a vector representing frequencies of all pairs (w0 , r0 ) o"
W09-2506,P93-1016,0,0.0133603,"tances from the dataset for which the target verb or one of its arguments is not in the BNC. We obtain a set of 162 instances for 34 different verbs. We also remove paraphrases that are not in the BNC. On average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts. Poot GAP Random baseline E&P (target only) E&P (add, object only) E&P (min, both) TDP TDP (target only) 54.25 64.61 (63.31) 66.20 (62.90) 64.86 (59.62) 63.32 62.60 26.03 29.95 (32.02) 29.93 (31.54) 32.22 (31.28) 36.54 33.04 Table 2: Results Experimental setup. We parse the BNC using MiniPar (Lin, 1993) and extract co-occurrence frequencies, considering only dependency relations for the most frequent 2000 verbs. We don’t use raw frequency counts directly but reweight the vectors by pointwise mutual information. To rank paraphrases in context, we compute contextually constrained vectors for the verb in the input sentence and all its paraphrase candidates by taking the corresponding predicate vectors and restricting them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restri"
W09-2506,P08-1028,0,0.40197,"Missing"
W09-2506,N07-1071,0,0.0556872,"Missing"
W09-2506,W04-3206,0,0.0299526,"on of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective. 1 Manfred Pinkal Universität des Saarlandes pinkal@coli.uni-sb.de Introduction Knowledge about paraphrases is of central importance to textual inference modeling. Systems which support automatic extraction of large repositories of paraphrase or inference rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔"
W09-2506,P07-1058,0,0.0150323,"nce rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔ X throw Y ” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of conte"
W09-2506,C00-2137,0,0.109924,"Missing"
W09-2506,S07-1009,0,\N,Missing
W09-2506,W07-1401,0,\N,Missing
W16-2608,P07-2053,0,0.167448,"Missing"
W16-2608,D12-1039,0,0.0451375,"Missing"
W16-2608,R11-1006,0,0.0505771,"Missing"
W17-0901,N15-1122,0,0.0123315,"t learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums for “special” events such as WEDDING or BARBECUE, in contrast to everyday, trivial activites such as MAKING COFFEE or in recall. However, they had the option to label stories where they felt a scenari"
W17-0901,P16-1167,0,0.0133192,"omputing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums for “special” events such as WEDDING or BARBECUE, in contrast to everyday, trivial activites such as MAKING COFFEE or in recall. However, they had the option to label stories where they felt a scenario was only partially addressed in a different way, thus setting these cases apart from those where the scenario was centrally addressed. 5 While we take the judgment about the “C” class to be quite re"
W17-0901,P08-1090,0,0.743518,"n Thater Manfred Pinkal Universit¨at des Saarlandes Saarland, 66123, Germany {wanzare,zarcone,stth,pinkal}coli.uni-saarland.de Abstract tive abilities and has the potential to support NLP tasks such as anaphora resolution (Rahman and Ng, 2011), discourse relation detection, semantic role labeling, temporal order analysis, and applications such as text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989), question answering (Hajishirzi and Mueller, 2012). Several methods for the automatic acquisition of script knowledge have been proposed. Seminal work by Chambers and Jurafsky (2008; 2009) provided methods for the unsupervised widecoverage extraction of script knowledge from large text corpora. However, texts typically only mention small parts of a script, banking on the reader’s ability to infer missing script-related events. The task is therefore challenging, and the results are quite noisy. The work presented in this paper follows the approach proposed in Regneri et al. (2010) (henceforth “RKP”) who crowdsourced scenario descriptions by asking people how they typically carry out a particular activity. The collected event sequence descriptions provide generic descripti"
W17-0901,P09-1068,0,0.758462,"Missing"
W17-0901,E14-1006,1,0.933762,"efer to the same event, as the broader discourse context would suggest. To address this issue, we propose a semi-supervised approach, capitalizing on previous work by Klein et Datasets and gold standards. Three large crowdsourced collections of activity descriptions in terms of ESDs are available: the OMICS corpus (Gupta and Kochenderfer, 2004), the SMILE corpus (Regneri et al., 2010) and DeScript corpus (Wanzare et al., 2016). Sections 3-4 of this paper focus on a subset of ESDs for 14 scenarios from SMILE and OMICS, with on average 29.9 ESDs per scenario. In RKP, in the follow-up studies by Frermann et al. (2014) and Modi and Titov (2014) as well as in the present study, 4 of these scenarios were used as development set and 10 as test set. RKP provided two gold standard datasets for this subset: the RKP paraphrase dataset contains judgments for 60 event description pairs per scenario, the RKP temporal order dataset contains 60 event description pairs that are separately annotated in both directions, for a total of 120 datapoints per scenario. In order to directly evaluate our models for clustering quality, we also created a clustering gold standard for the RKP test set, adopting the experimental setup"
W17-0901,W16-2505,0,0.0137758,"n substantially contribute to the task of text understanding. P Figure 5: Example ROC-story with scenario annotation. ficient coverage for the analysis of script knowledge in natural-language texts? Answering this question is not trivial, as scenarios vary considerably in granularity and it is not trivial that the type of script knowledge we model can capture all kinds of event structures, even in narrative texts. In order to provide a rough estimate of coverage for the currently existing script material, we carried out a simple annotation study on the recently published ROC-stories database (Mostafazadeh et al., 2016a). The database consists of 50,000 short narrative texts, collected via Mechanical Turk. Workers were asked to write a 5-sentence length story about an everyday commonsense event, and they were encouraged to write about “anything they have in mind” to guarantee wide distribution across topics. For our annotation study, we merged the available datasets containing crowdsourced ESD collections (i.e. OMICS, SMILE, and DeScript), excluding two extremely general scenarios (GO OUTSIDE , CHILDHOOD ), which gives us a total of 226 different scenarios. We randomly selected 500 of the ROC-stories and as"
W17-0901,E14-1024,0,0.178579,"to consider the full scenario list4 . The three annotations were merged us6 Related Work Following the seminal work of Chambers and Jurafsky (2008) and (2009) on the induction of script-like narrative schemas from large, unlabeled corpora of news articles, a series of models have been presented for improving the induction method or explore alternative data sources for script learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event stru"
W17-0901,E12-1034,0,0.378855,"Missing"
W17-0901,W16-3644,0,0.0119913,"uction of script-like narrative schemas from large, unlabeled corpora of news articles, a series of models have been presented for improving the induction method or explore alternative data sources for script learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums"
W17-0901,P10-1100,1,0.925342,"tion extraction (Rau et al., 1989), question answering (Hajishirzi and Mueller, 2012). Several methods for the automatic acquisition of script knowledge have been proposed. Seminal work by Chambers and Jurafsky (2008; 2009) provided methods for the unsupervised widecoverage extraction of script knowledge from large text corpora. However, texts typically only mention small parts of a script, banking on the reader’s ability to infer missing script-related events. The task is therefore challenging, and the results are quite noisy. The work presented in this paper follows the approach proposed in Regneri et al. (2010) (henceforth “RKP”) who crowdsourced scenario descriptions by asking people how they typically carry out a particular activity. The collected event sequence descriptions provide generic descriptions of a given scenario (e.g. BAKING A CAKE ) in concise telegram style (Fig. 1a). Based on these crowdsourced event sequence descriptions or ESDs, RKP extracted high-quality script knowledge for a variety of different scenarios, in the form of temporal script graphs (Fig. 1b). Temporal script graphs are partially ordered structures whose nodes are sets of alternative descriptions denoting the same eve"
W17-0901,W14-1606,0,0.656732,"the broader discourse context would suggest. To address this issue, we propose a semi-supervised approach, capitalizing on previous work by Klein et Datasets and gold standards. Three large crowdsourced collections of activity descriptions in terms of ESDs are available: the OMICS corpus (Gupta and Kochenderfer, 2004), the SMILE corpus (Regneri et al., 2010) and DeScript corpus (Wanzare et al., 2016). Sections 3-4 of this paper focus on a subset of ESDs for 14 scenarios from SMILE and OMICS, with on average 29.9 ESDs per scenario. In RKP, in the follow-up studies by Frermann et al. (2014) and Modi and Titov (2014) as well as in the present study, 4 of these scenarios were used as development set and 10 as test set. RKP provided two gold standard datasets for this subset: the RKP paraphrase dataset contains judgments for 60 event description pairs per scenario, the RKP temporal order dataset contains 60 event description pairs that are separately annotated in both directions, for a total of 120 datapoints per scenario. In order to directly evaluate our models for clustering quality, we also created a clustering gold standard for the RKP test set, adopting the experimental setup in Wanzare et al. (2016):"
W17-0901,N16-1098,0,0.0751556,"n substantially contribute to the task of text understanding. P Figure 5: Example ROC-story with scenario annotation. ficient coverage for the analysis of script knowledge in natural-language texts? Answering this question is not trivial, as scenarios vary considerably in granularity and it is not trivial that the type of script knowledge we model can capture all kinds of event structures, even in narrative texts. In order to provide a rough estimate of coverage for the currently existing script material, we carried out a simple annotation study on the recently published ROC-stories database (Mostafazadeh et al., 2016a). The database consists of 50,000 short narrative texts, collected via Mechanical Turk. Workers were asked to write a 5-sentence length story about an everyday commonsense event, and they were encouraged to write about “anything they have in mind” to guarantee wide distribution across topics. For our annotation study, we merged the available datasets containing crowdsourced ESD collections (i.e. OMICS, SMILE, and DeScript), excluding two extremely general scenarios (GO OUTSIDE , CHILDHOOD ), which gives us a total of 226 different scenarios. We randomly selected 500 of the ROC-stories and as"
W17-0901,L16-1556,1,0.899884,"ake (a) – take out box of ingredients from shelf – gather all cake ingredients – get cake mix get ingred. choose recipe – look up recipe – find cake recipe – get your recipe – stir to combine – mix well – mix ingredients together in bowls – stir cake ingredients – pour cake mix in bowl – add ingredients to bowl – add cake ingredients add ingred. buy ingred. prepare ingred. – purchase ingredients – buy cake mix – buy proper ingredients put cake into oven – place cake into oven – put cake in oven (b) Figure 1: Example ESDs (a) and induced script structure (b) for the BAKING A CAKE scenario from Wanzare et al. (2016) 2 Data We will now introduce the resources used in our study, namely the datasets of ESDs, the gold standards and the crowdsourced alignments between event descriptions. scriptions occurring in similar positions in ESDs tend to denote the same event type. However, MSA makes far too strong an assumption about the temporal ordering information in the ESDs. It does not allow for crossing edges and thus must assume a fixed and invariable order, while the ordering of events in a script is to some degree flexible (e.g., one can preheat the oven before or after mixing ingredients). We propose cluste"
W17-0901,D16-1017,1,\N,Missing
