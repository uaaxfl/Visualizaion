2020.acl-main.508,N19-1423,0,0.0346863,". To ensure the clear boundary between positive and negative examples in WinoWhy, only positive and negative reasons are selected to evaluate models. In total, WinoWhy contains 1,270 positive and 1,595 negative examples. 4 WSC Experiments In this section, we present the performance of current models on WSC. By doing so, we can better understand their strengths and limitations. 5740 4.1 Evaluated Methods and Implementation Recently, pre-trained language representation models have achieved significant improvement on the WSC task. In this section, we evaluate the following three models: 1. BERT (Devlin et al., 2019): As a powerful contextualized word representation model, it has been proven helpful in many downstream NLP tasks. As shown in (Kocijan et al., 2019), we can first convert the original WSC task into a token prediction task and then leverage BERT to solve the problem. We denote the base and large model of BERT as BERT (base) and BERT (large) respectively. 2. GPT-2 (Radford et al., 2019): GPT-2 is one of the best pre-trained language models for generation tasks. As reported in the original paper, we can first replace the pronouns with different candidates and leverage the probability of the full"
2020.acl-main.508,P19-1388,0,0.0932551,"Missing"
2020.acl-main.508,D18-1220,0,0.0998029,"Missing"
2020.acl-main.508,P19-1478,0,0.100466,"Missing"
2020.acl-main.508,D19-1282,0,0.0274214,"nse reasoning models. In this paper, we use the WSC task as the breaking point to conduct a deep diagnosis of essential commonsense knowledge types, which sheds some light on how to achieve a better commonsense reasoning system in the future. Related Work As an important knowledge resource for many artificial intelligence systems, commonsense knowledge covers various knowledge categories like causality (Sap et al., 2019), reasoning (Schubert, 2015), property (Liu and Singh, 2004), and quantity (Elazar et al., 2019), and has been proven crucial in many downstream tasks like question answering (Lin et al., 2019), dialogue system (Zhou et al., 2018), reading comprehension (Wang et al., 2018), and pronoun coreference resolution (Levesque et al., 2012). Among all these tasks, Winograd Schema Challenge (WSC) (Levesque et al., 2012) is viewed as one of the most challenging ones because solving WSC questions typically requires inference over various kinds of commonsense knowledge. Conventionally, people tried to solve WSC questions in an unsupervised way by leveraging either search 7 Conclusion In this paper, we presented the first deep diagnosis of essential commonsense knowledge for answering Winograd Sc"
2020.acl-main.508,2021.ccl-1.108,0,0.13329,"Missing"
2020.acl-main.508,S18-1119,0,0.0238743,"els have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. WinoWhy and all codes are available at: https://github.com/ HKUST-KnowComp/WinoWhy. 1 Figure 1: A pair of questions in WSC. Introduction Commonsense reasoning, as an important problem of natural language understanding, has attracted much more attention in the NLP community recently (Levesque et al., 2012; Zhou et al., 2018; Ostermann et al., 2018; Talmor et al., ∗ Equal contribution. 2019). Among all developed commonsense reasoning tasks, the Winograd Schema Challenge (WSC) (Levesque et al., 2012), which is a hard pronoun coreference resolution task, is one of the most influential ones. All questions in WSC are grouped into pairs such that paired questions have minor differences (mostly one-word difference), but reversed answers. For each question, we denote the other question in the same pair as its reverse question. One pair of the WSC task is shown in Figure 1. Based on the design guideline of WSC, all commonly used features (e.g.,"
2020.acl-main.508,D12-1071,0,0.0703071,"nd denote those settings as GPT-2 (small, full), GPT-2 (small, partial), GPT-2 (large, full), and GPT-2 (large, partial) respectively. 3. RoBERTa (Liu et al., 2019): RoBERTa is a recent improved version of BERT with larger amount of training instances and techniques such as dynamic masking, which performs consistently better than BERT over many benchmark datasets. We denote the base and large models of RoBERTa as RoBERTa (base) and RoBERTa (large) respectively. Besides unsupervised models, as indicated by (Kocijan et al., 2019), fine-tuning BERT with a similar pronoun resolution dataset WSCR (Rahman and Ng, 2012) can help boost the performance. A later work (Sakaguchi et al., 2019) has further enhanced the performance by fine-tuning RoBERTa with a larger and more balanced dataset WinoGrande. Statistics of these datasets are presented in Table 3. In our experiments, we evaluate the combination of different pre-trained models and fine-tuning datasets, and denote them as BERT (base/large) + WSCR/Grande and RoBERTa (base/large) + WSCR/Grande respectively. Dataset #Problems Average Length #Vocab WSC WSCR WinoGrande 273 1,886 43,972 19.1 15.9 20.6 919 4,127 16,469 Table 3: Statistics of WSC and related data"
2020.acl-main.508,N19-1421,0,0.111976,"Missing"
2020.acl-main.508,S18-1120,0,0.0147061,"to conduct a deep diagnosis of essential commonsense knowledge types, which sheds some light on how to achieve a better commonsense reasoning system in the future. Related Work As an important knowledge resource for many artificial intelligence systems, commonsense knowledge covers various knowledge categories like causality (Sap et al., 2019), reasoning (Schubert, 2015), property (Liu and Singh, 2004), and quantity (Elazar et al., 2019), and has been proven crucial in many downstream tasks like question answering (Lin et al., 2019), dialogue system (Zhou et al., 2018), reading comprehension (Wang et al., 2018), and pronoun coreference resolution (Levesque et al., 2012). Among all these tasks, Winograd Schema Challenge (WSC) (Levesque et al., 2012) is viewed as one of the most challenging ones because solving WSC questions typically requires inference over various kinds of commonsense knowledge. Conventionally, people tried to solve WSC questions in an unsupervised way by leveraging either search 7 Conclusion In this paper, we presented the first deep diagnosis of essential commonsense knowledge for answering Winograd Schema Challenge questions. By doing so, we better understand the strengths and li"
2020.acl-main.508,P19-1071,1,0.849276,"e machine-learning based models over WinoWhy can be sensitive to the dataset distribution, it is reasonable to suspect that the improvement achieved by fine-tuning over a similar or same dataset might come from better dataset fitting rather than better commonsense reasoning. As the original purpose of proposing both WSC and WinoWhy is to evaluate how good current AI systems can understand commonsense knowledge rather than solve these questions by fitting the dataset, the unsupervised setting might be the more reasonable evaluation setting. 6 engines (Emami et al., 2018), linguistic knowledge (Zhang et al., 2019, 2020), or language representation models (Kocijan et al., 2019). Experimental results showed that these models still cannot fully solve the problem but we are not clear about how to further improve them. One important reason behind this is that the conventional definition of commonsense knowledge is too vague and thus we are not clear about what kinds of knowledge are still challenging for current commonsense reasoning models. In this paper, we use the WSC task as the breaking point to conduct a deep diagnosis of essential commonsense knowledge types, which sheds some light on how to achieve"
2020.acl-main.508,D14-1162,0,0.0891615,"Missing"
2020.emnlp-main.119,P98-1013,0,0.330579,"12 We randomly select another sub-event sequence that describes the same process from WikiHow, which could be different from the currently tested sequence. As a result, adding such sequence cannot help predict all missing events. 1548 4 Related Works Throughout history, considering the importance of events in understanding human language (e.g., commonsense knowledge (Zhang et al., 2020a)), many efforts have been devoted to define, represent, and understand events. For example, VerbNet (Schuler, 2005) created a verb lexicon to represent the semantic relations among verbs. After that, FrameNet (Baker et al., 1998) proposed to represent the event semantics with schemas, which has one predicate and several arguments. Apart from the structure of events, understanding events by predicting relations among them also becomes a popular research topic (e.g., TimeBank (Pustejovsky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Su"
2020.emnlp-main.119,D14-1159,0,0.0626597,"for unseen processes and can help predict missing events. 1 Figure 1: An illustration of leveraging known processes to predict the sub-event sequence of a new process. Introduction Understanding events has long been a challenging task in NLP, to which many efforts have been devoted by the community. However, most existing works are focusing on procedural (or horizontal) event prediction tasks. Examples include predicting the next event given an observed event sequence (Radinsky et al., 2012) and identifying the effect of a biological process (i.e., a sequence of events) on involved entities (Berant et al., 2014). These tasks mostly focus on predicting related events in a procedure based on their statistical correlations in previously observed text. As a result, understanding the meaning of an event might ∗ This work was done when the first author was visiting the University of Pennsylvania. 1 Code is available at: http://cogcomp.org/ page/publication_view/910. not be crucial for these horizontal tasks. For example, simply selecting the most frequently cooccurring event can offer acceptable performance on the event prediction task (Granroth-Wilding and Clark, 2016). Computational and cognitive studies"
2020.emnlp-main.119,J06-1003,0,0.00623629,"e C) algorithm, which first expands all possible subsets (i.e., it includes all subsets of EC for all C) and then leverages the sort and filter technique to select the final subsets, we reduce the time complexity from O(|C |· |E|2 ) to O(n · |C |· |E|), where n is the number of conceptualized events and is typically much smaller than |E|. 2.2.2 Conceptualization Scoring As mentioned above, for each pair of a sub-event E and its potential conceptualization C, we propose a scoring function F (E, C) to measure how much “semantic information&quot; is preserved after the conceptualization. Motivated by Budanitsky and Hirst (2006) and based on the assumption that the more abstract the conceptualized event is, the more semantic details are lost, we define F (E, C) to be: F (E, C) = m Y Conceptualized Event Ordering After conceptualizing and merging all sub-events, we need to determine their loosely temporal order (e.g., whether they typically appear at the beginning or the end of these sub-event sequences). Let the set of selected conceptualized events be C ∗ . For each C ∈ C ∗ , we define its order score T (C), indicating how likely C is to appear first, as: T (C) = X C 0 ∈C∗ E ,w C ) i wD(wi , (2) i=1 where D(wiE , wi"
2020.emnlp-main.119,P08-1090,0,0.147589,"ening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explicitly. 5 Conclusion In this paper, we try to understand events vertically by viewing them as processes and predicting their sub-event sequences. Our APSI framework is motivated by the notion of analogous processes, and attempts to transfer knowledge from (a very small number of) familiar processes to a"
2020.emnlp-main.119,N19-1423,0,0.0157797,"Missing"
2020.emnlp-main.119,glavas-etal-2014-hieve,0,0.122438,"or temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explicitly. 5 Conclusion In t"
2020.emnlp-main.119,W13-1203,0,0.0619251,"sky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and them is that we are trying to understand events by directly generating the sub-event sequences rather than extracting such information from text. Another line of related works is the narrative schema prediction (Chambers and Jurafsky, 2008), which also holds the assumption that event schemata can help understand events. But their research focus is using the overall process implicitly to help predict future events while this work tries to understand events by knowing the relation between processes and their sub-event sequences explici"
2020.emnlp-main.119,W04-1013,0,0.0116262,"ar process (Jaccard), (GloVe), and (RoBERTa), respectively. For each process, we also present a randomly generated sequence and a human-generated sequence9 as the lower-bound and upper-bound for sub-event sequence prediction models. 3.3 Intrinsic Evaluation We first present the intrinsic evaluation to show the quality of the predicted sub-event sequences of unseen processes. For each test process, we provide the process name and the sub-event sequence length10 to evaluated systems and ask them to generate a fixed-length sub-event sequence. 3.3.1 Evaluation Metric Motivated by the ROUGE score (Lin, 2004), we propose an event-based ROUGE (E-ROUGE) to evaluate the quality of the predicted sub-event sequence. Specifically, similar to ROUGE, which evaluates the generation quality based on N-gram token occurrence, we evaluate how much percentage of the sub-event and time-ordered sub-event pairs in the induced sequence is covered by the human-provided references. We denote the evaluation over single event and event pairs as EROUGE1 and E-ROUGE2, respectively. We also provide two covering standards to better understand the prediction quality: (1) “String Match”: all words in the predicted event/pair"
2020.emnlp-main.119,2021.ccl-1.108,0,0.0499337,"Missing"
2020.emnlp-main.119,P19-1472,0,0.0462531,"Missing"
2020.emnlp-main.119,D14-1162,0,0.0834515,"e (Seq2seq): One intuitive solution to the sub-event sequence prediction task would be modeling it as a sequence to sequence problem, where the process is treated as the input and the sub-event sequence the output. Here we 7 https://www.wikihow.com. We do not need a development set because the proposed solution APSI is not a learning-based method. 8 adopt the standard GRU-based encoder-decoder framework (Sutskever et al., 2014) as the base framework and change the generation unit from words to events. For each process or sub-event, we leverage pre-trained word embeddings (i.e., GloVe-6b-300d (Pennington et al., 2014)) or language models (i.e., RoBERTa-base (Liu et al., 2019)) as the representation, which are denoted as Seq2seq (GloVe) and Seq2seq (RoBERTa). Top One Similar Process: Another baseline is the “top one similar process”. For each new process, we can always find the most similar observed process. Then we can use the sub-event sequence of the observed process as the prediction. We employ different methods (i.e., token-level Jaccard coefficient or cosine similarity of GloVe/RoBERTa process representations) to measure the process similarity. We denote them as Top one similar process (Jaccard), (Glo"
2020.emnlp-main.119,P18-1043,0,0.0245094,"uage (e.g., commonsense knowledge (Zhang et al., 2020a)), many efforts have been devoted to define, represent, and understand events. For example, VerbNet (Schuler, 2005) created a verb lexicon to represent the semantic relations among verbs. After that, FrameNet (Baker et al., 1998) proposed to represent the event semantics with schemas, which has one predicate and several arguments. Apart from the structure of events, understanding events by predicting relations among them also becomes a popular research topic (e.g., TimeBank (Pustejovsky et al., 2003) for temporal relations and Event2Mind (Rashkin et al., 2018) for causal relations). Different from these horizontal relations between events, in this paper, we propose to understand event vertically by treating each event as a process and trying to understand what is happening (i.e., sub-event) inside the target event. Such knowledge is also referred to as event schemata (Zacks and Tversky, 2001) and shown crucial for how humans understand events (Abbott et al., 1985). One line of related works in the NLP community is extracting super-sub event relations from textual corpus (Hovy et al., 2013; Glavas et al., 2014). The difference between this work and"
2020.emnlp-main.199,S19-2007,0,0.0589168,"Missing"
2020.emnlp-main.199,2020.acl-main.485,0,0.106454,"Missing"
2020.emnlp-main.199,W19-3504,0,0.324298,"ntroversial hashtags may result in a set of social media posts generated by a limited number of users (Arango et al., 2019). This would lead to an inherent bias in hate speech datasets similar to other tasks involving social data (Olteanu et al., 2019) as opposed to a selection bias (Heckman, 1977) particular to hate speech data. Mitigation methods usually point out the classification performance and investigate how to debias the detection given false positives caused by gender group identity words such as “women” (Park et al., 2018), racial terms reclaimed by communities in certain contexts (Davidson et al., 2019), or names of groups that belong to the intersection of gender and racial terms such as “black men” (Kim et al., 2020). The various aspects of the dataset construction are less studied though it has recently been shown, by looking at historical documents, that we may somehow neglect the data collection process (Jo and Gebru, 2020). Thus, in the present work, we are interested in improving hate speech data collection with evaluation before focusing on classification performance. We conduct a comparative study on English, French, German, Arabic, Italian, Portuguese, and Indonesian datasets using"
2020.emnlp-main.199,W19-3510,0,0.286997,"ation performance. We conduct a comparative study on English, French, German, Arabic, Italian, Portuguese, and Indonesian datasets using topic models, specifically Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between topics and the whole set of keywords, and the second one evaluates how often"
2020.emnlp-main.199,E14-1056,0,0.161315,"(Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag recommendation (Godin et al., 2013), authorship attribution (Seroussi et al., 2014), and text categorization (Zhou et al., 2009). In order to evaluate the consistency of the generated topics, Newman et al. (2010) used crowdsourcing and semantic similarity metrics, essentially based on Pointwise Mutual Information (PMI), to assess the coherence; Mimno et al. (2011) estimated coherence scores using conditional log-probability instead of PMI; Lau et al. (2014) enhanced this formulation based on normalized PMI (NPMI); and Lau and Baldwin (2016) investigated the effect of cardinality on topic generation. Similarly, we use topics and semantic similarity metrics to determine the quality of hate speech datasets, and test on corpora that vary in language, size, and general collection purposes for the sake of examining bias up to different facets. 3 Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on keywords and/or hashtags. However, the lack of an unequivocal definition of hate speech, the use"
2020.emnlp-main.199,P19-1339,0,0.0219771,"volve the way abusive language spreads online (Mathew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al., 2019), bias in hate speech classification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main"
2020.emnlp-main.199,P19-1267,0,0.053149,"al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why we choose to investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture latent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag"
2020.emnlp-main.199,R15-1086,0,0.0288565,"Missing"
2020.emnlp-main.199,2020.acl-main.487,0,0.0454389,"and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why we choose to investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture latent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to h"
2020.emnlp-main.199,W19-3506,0,0.156436,"evaluation before focusing on classification performance. We conduct a comparative study on English, French, German, Arabic, Italian, Portuguese, and Indonesian datasets using topic models, specifically Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between topics and the whole set of keywords,"
2020.emnlp-main.199,D11-1024,0,0.408771,"ent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag recommendation (Godin et al., 2013), authorship attribution (Seroussi et al., 2014), and text categorization (Zhou et al., 2009). In order to evaluate the consistency of the generated topics, Newman et al. (2010) used crowdsourcing and semantic similarity metrics, essentially based on Pointwise Mutual Information (PMI), to assess the coherence; Mimno et al. (2011) estimated coherence scores using conditional log-probability instead of PMI; Lau et al. (2014) enhanced this formulation based on normalized PMI (NPMI); and Lau and Baldwin (2016) investigated the effect of cardinality on topic generation. Similarly, we use topics and semantic similarity metrics to determine the quality of hate speech datasets, and test on corpora that vary in language, size, and general collection purposes for the sake of examining bias up to different facets. 3 Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on k"
2020.emnlp-main.199,W19-3512,0,0.102036,"t Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between topics and the whole set of keywords, and the second one evaluates how often keywords appear in topics. We analyze our methods through different use cases which explain how we can benefit from the assessment. Our main contributions consist of (1) desig"
2020.emnlp-main.199,N10-1012,0,0.379947,"investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture latent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag recommendation (Godin et al., 2013), authorship attribution (Seroussi et al., 2014), and text categorization (Zhou et al., 2009). In order to evaluate the consistency of the generated topics, Newman et al. (2010) used crowdsourcing and semantic similarity metrics, essentially based on Pointwise Mutual Information (PMI), to assess the coherence; Mimno et al. (2011) estimated coherence scores using conditional log-probability instead of PMI; Lau et al. (2014) enhanced this formulation based on normalized PMI (NPMI); and Lau and Baldwin (2016) investigated the effect of cardinality on topic generation. Similarly, we use topics and semantic similarity metrics to determine the quality of hate speech datasets, and test on corpora that vary in language, size, and general collection purposes for the sake of e"
2020.emnlp-main.199,D19-1474,1,0.0686038,"datasets using topic models, specifically Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between topics and the whole set of keywords, and the second one evaluates how often keywords appear in topics. We analyze our methods through different use cases which explain how we can benefit from the ass"
2020.emnlp-main.199,2020.acl-main.483,0,0.0105306,"Italian (Sanguinetti et al., 2018), German (Ross et al., 2016), Indonesian (Ibrohim and Budi, 2019), French (Ousidhoum et al., 2019), Dutch (Hee et al., 2015), and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019). Challenging questions being tackled in this area involve the way abusive language spreads online (Mathew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al., 2019), bias in hate speech classification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups suc"
2020.emnlp-main.199,D18-1302,0,0.487341,"share such content. Hence, a search based on generic hate speech keywords or controversial hashtags may result in a set of social media posts generated by a limited number of users (Arango et al., 2019). This would lead to an inherent bias in hate speech datasets similar to other tasks involving social data (Olteanu et al., 2019) as opposed to a selection bias (Heckman, 1977) particular to hate speech data. Mitigation methods usually point out the classification performance and investigate how to debias the detection given false positives caused by gender group identity words such as “women” (Park et al., 2018), racial terms reclaimed by communities in certain contexts (Davidson et al., 2019), or names of groups that belong to the intersection of gender and racial terms such as “black men” (Kim et al., 2020). The various aspects of the dataset construction are less studied though it has recently been shown, by looking at historical documents, that we may somehow neglect the data collection process (Jo and Gebru, 2020). Thus, in the present work, we are interested in improving hate speech data collection with evaluation before focusing on classification performance. We conduct a comparative study on"
2020.emnlp-main.199,N16-1057,0,0.0168087,"ions such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag recommendation (Godin et al., 2013), authorship attribution (Seroussi et al., 2014), and text categorization (Zhou et al., 2009). In order to evaluate the consistency of the generated topics, Newman et al. (2010) used crowdsourcing and semantic similarity metrics, essentially based on Pointwise Mutual Information (PMI), to assess the coherence; Mimno et al. (2011) estimated coherence scores using conditional log-probability instead of PMI; Lau et al. (2014) enhanced this formulation based on normalized PMI (NPMI); and Lau and Baldwin (2016) investigated the effect of cardinality on topic generation. Similarly, we use topics and semantic similarity metrics to determine the quality of hate speech datasets, and test on corpora that vary in language, size, and general collection purposes for the sake of examining bias up to different facets. 3 Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on keywords and/or hashtags. However, the lack of an unequivocal definition of hate speech, the use of slurs in friendly conversations as opposed to sarcasm and metaphors in elusive ha"
2020.emnlp-main.199,D18-1391,0,0.0209194,"6–20, 2020. 2020 Association for Computational Linguistics 2 Related Work Hate speech labeling schemes depend on the general purpose of the dataset. The annotations may include hateful vs. non hateful (Basile et al., 2019), racist, sexist, and none (Waseem and Hovy, 2016); as well as discriminating target attributes (ElSherief et al., 2018), the degree of intensity (Sanguinetti et al., 2018), and the annotator’s sentiment towards the tweets (Ousidhoum et al., 2019). Besides English (Basile et al., 2019; Waseem and Hovy, 2016; Davidson et al., 2017; ElSherief et al., 2018; Founta et al., 2018; Qian et al., 2018), we notice a growing interest in the study of hate speech in other languages, such as Portuguese (Fortuna et al., 2019), Italian (Sanguinetti et al., 2018), German (Ross et al., 2016), Indonesian (Ibrohim and Budi, 2019), French (Ousidhoum et al., 2019), Dutch (Hee et al., 2015), and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019). Challenging questions being tackled in this area involve the way abusive language spreads online (Mathew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al.,"
2020.emnlp-main.199,N16-2013,0,0.0865494,"Arabic, Italian, Portuguese, and Indonesian datasets using topic models, specifically Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between topics and the whole set of keywords, and the second one evaluates how often keywords appear in topics. We analyze our methods through different use cases"
2020.emnlp-main.199,D17-1323,0,0.0514608,"hew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al., 2019), bias in hate speech classification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why w"
2020.emnlp-main.199,L18-1443,0,0.390183,"roving hate speech data collection with evaluation before focusing on classification performance. We conduct a comparative study on English, French, German, Arabic, Italian, Portuguese, and Indonesian datasets using topic models, specifically Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We use multilingual word embeddings or word associations to compute the semantic similarity scores between topic words and predefined keywords and define two metrics that calculate bias in hate speech based on these measures. We use the same list of keywords reported by Ross et al. (2016) for German, Sanguinetti et al. (2018) for Italian, Ibrohim and Budi (2019) for Indonesian, Fortuna et al. (2019) for Portuguese; allow more flexibility in both English (Waseem and Hovy, 2016; Founta et al., 2018; Ousidhoum et al., 2019) and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019) in order to compare different datasets based on shared concepts that have been reported in their respective paper descriptions; and for French, we make use of a subset of keywords that covers most of the targets reported by Ousidhoum et al. (2019). Our first bias evaluation metric measures the average similarity between t"
2020.emnlp-main.199,P19-1163,0,0.21933,"tions being tackled in this area involve the way abusive language spreads online (Mathew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al., 2019), bias in hate speech classification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial rea"
2020.emnlp-main.199,2020.acl-main.486,0,0.0672604,"assification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why we choose to investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture late"
2020.emnlp-main.199,J14-2003,0,0.0287083,"rocess to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why we choose to investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture latent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data exploration (Rodriguez and Storer, 2020), Twitter hashtag recommendation (Godin et al., 2013), authorship attribution (Seroussi et al., 2014), and text categorization (Zhou et al., 2009). In order to evaluate the consistency of the generated topics, Newman et al. (2010) used crowdsourcing and semantic similarity metrics, essentially based on Pointwise Mutual Information (PMI), to assess the coherence; Mimno et al. (2011) estimated coherence scores using conditional log-probability instead of PMI; Lau et al. (2014) enhanced this formulation based on normalized PMI (NPMI); and Lau and Baldwin (2016) investigated the effect of cardinality on topic generation. Similarly, we use topics and semantic similarity metrics to determine the qu"
2020.emnlp-main.199,2020.acl-main.468,0,0.0747297,"et al., 2015), and Arabic (Albadi et al., 2018; Mulki et al., 2019; Ousidhoum et al., 2019). Challenging questions being tackled in this area involve the way abusive language spreads online (Mathew et al., 2019), fast changing topics during data collection (Liu et al., 2019), user bias in publicly available datasets (Arango et al., 2019), bias in hate speech classification and different methods to reduce it (Park et al., 2018; Davidson et al., 2019; Kennedy et al., 2020). Bias in social data is broad and addresses a wide range of issues (Olteanu et al., 2019; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and"
2020.emnlp-main.199,P19-1159,0,0.0115966,"; Papakyriakopoulos et al., 2020). Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019), selection bias (Garimella et al., 2019), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Existing work deals with bias through the construction of large datasets and the definition of social frames (Sap et al., 2020), the investigation of how current NLP models might be non-inclusive of marginalized groups such as people with disabilities (Hutchinson et al., 2020), mitigation (Dixon et al., 2018; Sun et al., 2019), or better data splits (Gorman and Bedrick, 2019). However, Blodgett et al. (2020) report a missing normative process to inspect the initial reasons behind bias in NLP without the main focus being on the performance which is why we choose to investigate the data collection process in the first place. In order to operationalize the evaluation of selection bias, we use topic models to capture latent semantics. Regularly used topic modeling techniques such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have proven their efficiency to handle several NLP applications such as data explora"
2020.emnlp-main.199,W16-5618,0,0.025367,"hen tested on posts that contain controversial or search-related identity words (Park et al., 2018; Sap et al., 2019; Davidson et al., 2019; Kim et al., 2020). To claim whether a dataset is rather robust to keyword-based selection or not, we present two label-agnostic metrics to evaluate bias using topic models. First, we generate topics using Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Then, we compare topics to predefined sets of keywords using a semantic similarity measure. We test our methods on different numbers of topics and topic words. 3.1 Predefined Keywords In contrast to Waseem (2016), who legitimately questions the labeling process by comparing ama2533 DATASET K EYWORDS DATASET T OPIC WORDS Ousidhoum et al. (2019) Waseem and Hovy (2016) Founta et al. (2018) Ousidhoum et al. (2019) ni**er, invasion, attack Founta et al. (2018) Ousidhoum et al. (2019) Waseem and Hovy (2016) Ousidhoum et al. (2019) f***ing, like, know ret***ed, sh*t**le, c*** sexist, andre, like FR m*ng*l, gauchiste, sale EN mon*y, leftist, filthy Albadi et al. (2018) AR Albadi et al. (2018) Ousidhoum et al. (2019) Mulki et al. (2019) Ibrohim and Budi (2019) Sanguinetti et al. (2018) Fortuna et al. (2019) Ro"
2020.emnlp-main.502,D16-1039,0,0.019144,"fer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have no ability to cope with the Type-II sparsity, which is the main focus of our work. Another line of retrofitting methods (Vuli´c et al., 2018; Vuli´"
2020.emnlp-main.502,E12-1004,0,0.0224723,"esults comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is reported. The statistics of datase"
2020.emnlp-main.502,W11-2501,0,0.0409711,"oller et al. (2018); Le et al. (2019). To make experimental results comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels"
2020.emnlp-main.502,P14-1113,0,0.0219829,"form matrix factorization or embedding learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every"
2020.emnlp-main.502,P05-1014,0,0.368832,"Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hyponym’s contexts should be roughly contained by the hypernym’s. Although applicable to any word in a corpus, they are suggested to be inferior to pattern-based ones fed with sufficient extracted pairs (Roller et al., 2018; Le et al., 2019). Since pattern-based methods have unresolved sparsity issues, while distributional ones are more broadly applicable but globally inferior, neither of them can dominate the other in every aspect. In this light, we are interested in two questions: • Is the Type-II sparsity severe in practice? • If so, how to complement pattern-based ap"
2020.emnlp-main.502,C18-1042,0,0.0132967,"efer. On several benchmark datasets, our framework achieves competitive improvements and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, res"
2020.emnlp-main.502,C92-2082,0,0.588063,"taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement p"
2020.emnlp-main.502,W19-4310,0,0.0260493,"Missing"
2020.emnlp-main.502,P15-2020,0,0.0348367,"Missing"
2020.emnlp-main.502,P19-1313,0,0.120901,"extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hyponym’s contexts should be roughly contained by the hypernym’s. Although applicable to any w"
2020.emnlp-main.502,I13-1095,0,0.017723,"nterpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extrac"
2020.emnlp-main.502,K16-1006,0,0.012767,": X 1 xh = MLPh (c), |C(x)| c∈C(x) n 1X c= cj . n j=1 To obtain yH , a similar network is applied, though the two MLPs do not share parameters to reflect the asymmetry of hypernymy. We fix the embeddings of context word vectors during training because satisfactory performance is observed. Due to its simplicity, NB OW is efficient to train. However, it ignores the order of context words and may not well reserve semantics. CONTEXT 2 VEC with M EAN -Pooling. To study the impacts of positional information within the context, we also attempt to substitute the NB OW with the CONTEXT 2 VEC encoder (Melamud et al., 2016). In CONTEXT 2 VEC, two LSTMs are used −c and ← − to encode the left and right contexts → c of an occurrence of x, respectively. The two output vectors are concatenated as the final context representation c for the same transformation and averaging as for NB OW. Formally,  −−−−→ − ←−−−− ←  c = LSTM(→ c ); LSTM( − c) . Note that the encoder for y still has separate parameters from those of x. Hierarchical Attention Networks. NB OW and CONTEXT 2 VEC with M EAN -Pooling both aggregate every context word’s information into xh and yH . Given several long contexts and the fixed output dimension, i"
2020.emnlp-main.502,D15-1075,0,0.0258947,"ompetitive improvements and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distribut"
2020.emnlp-main.502,N18-1045,0,0.014936,", 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C, a set of extracted pairs P ⊆ V × V , i.e., is-"
2020.emnlp-main.502,W09-0215,0,0.0891622,"kel and Kiela, 2017, 2018; Ganea et al., 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C,"
2020.emnlp-main.502,D14-1162,0,0.0837142,"the subscripts to reflect the asymmetry. In practice, we adopt networks with separate parameters for C(x) and C(y), which is detailed in the next section. 6211 4.2 Encoding Queried Words To implement the distributional model, we encode C(x) and C(y) into hypernymy-specific representations xh and yH , respectively. There are various off-the-shelf models to encode sentential contexts. We take the following four approaches. Transformed Word Vector. Instead of working directly on the original contexts C(x) and C(y), this approach takes as input the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014) x and y of x and y, and apply two MultiLayer Perceptrons (MLPs), respectively: xh = MLPh (x), yH = MLPH (y). The intuition is that word vectors roughly depend on the contexts and encode the distributional semantics. To make the MLPs generalize to V rather than VP , the word vectors are fixed during training. Inspired by the post specialization in Vuli´c et al. (2018), it also takes a similar approach to generalize task-specific word vector transformations to unseen words, though their evaluation task is not hypernymy detection. NB OW with M EAN-Pooling. Given words {cj }nj=1 in a context c ∈"
2020.emnlp-main.502,P18-2101,0,0.025295,"Missing"
2020.emnlp-main.502,P18-2057,0,0.316437,"aches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hypony"
2020.emnlp-main.502,E14-4008,0,0.109007,"2017, 2018; Ganea et al., 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C, a set of extracted pai"
2020.emnlp-main.502,W15-4208,0,0.013496,"2019). To make experimental results comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is repor"
2020.emnlp-main.502,L16-1056,0,0.0254213,"opose a framework of complementing patternbased approaches with distributional models where the former is invalid. 3) We systematically conduct comparisons on several common datasets, validating the superiority of our framework. 2 Related Work Pattern-Based Approaches. Taxonomies from experts (e.g., WordNet (Miller, 1995)) have proved effective in various reasoning applications (Song et al., 2011; Zhang et al., 2020). Meanwhile, Hearst patterns (Hearst, 1992) make large corpora a good resource of explicit is-a pairs, resulting in automatically built hypernymy knowledge bases (Wu et al., 2012; Seitner et al., 2016) of large scales. The coverage of both words and hypernymy pairs in those resources are far from complete. To infer unknown hypernymies between known words, e.g., implicit is-a pairs in transitive closures, pattern-based models are proposed. Roller et al. (2018) and Le et al. (2019) show that, on a broad range of benchmarks, simple matrix decomposition or embeddings on pattern-based word cooccurrence statistics provide robust performance. On Probase (Wu et al., 2012) - a Hearst-patternbased taxonomy, Yu et al. (2015) use embeddings to address the same sparsity problem. Some methods (Vendrov et"
2020.emnlp-main.502,P16-1226,0,0.037554,"Missing"
2020.emnlp-main.502,E17-1007,0,0.0140098,"on extracted pairs involving xq or yq . The separate contexts of xq and yq in corpus C turn out to serve as the basis and input of the distributional model, respectively. Given the superior performance of pattern-based models on IP pairs (Roller et al., 2018), the distributional model g is only responsible to answer OOP pairs. Various choices exist to implement the distributional model. We may apply unsupervised metrics (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014) on manual features extracted from contexts of xq and yq , which are robust to the distribution change of training data (Shwartz et al., 2017). However, the scores of those metrics are not necessarily in the same scale with those output by the patternbased model f for IP pairs. Such inconsistency will harm downstream systems which involve the scores for ranking or calculation. Given sufficient supervision signals from f and the inherent noise of natural language, we implement the distributional model g by a supervised neural-network-based approach. Specifically, the network encodes the contexts of x and y in C, i.e., C(x) and C(y), to be xh and yH , respectively, and makes predictions by a dot product, i.e., g(x, y) = hxh , yH i. No"
2020.emnlp-main.502,N18-1056,0,0.0230058,"Missing"
2020.emnlp-main.502,E17-2087,0,0.0124122,"rization or embedding learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nev"
2020.emnlp-main.502,J17-4004,0,0.0306628,"Missing"
2020.emnlp-main.502,N18-1048,0,0.0326359,"Missing"
2020.emnlp-main.502,N18-1103,0,0.184824,"Missing"
2020.emnlp-main.502,2020.acl-main.334,0,0.0278634,"learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have"
2020.emnlp-main.502,N18-1102,0,0.0118353,"ust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have no ability to cope with the Type-II sparsity, which is the main focus of our work. Another line of retrofitting methods (Vuli´c et al., 2018; Vuli´c and Mrkˇsi´c, 2018), i.e., adjusting distributional vectors to satisfy external linguistic constraints, has been applied to hypernymy detection. However, they strictly require more additional resources e.g., synonym and antonym to achieve better performance (Kamath et al., 2019). To the best of our knowledge, we are the first to propos"
2020.emnlp-main.502,C14-1212,0,0.0164794,"Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is reported. The statistics of datasets are shown in Table 1. The three tasks require algorithms to ou"
2020.emnlp-main.502,C04-1146,0,0.268422,"Missing"
2020.emnlp-main.502,2020.emnlp-main.608,0,0.0183376,"Missing"
2020.emnlp-main.502,N16-1174,0,0.0136159,"ht contexts → c of an occurrence of x, respectively. The two output vectors are concatenated as the final context representation c for the same transformation and averaging as for NB OW. Formally,  −−−−→ − ←−−−− ←  c = LSTM(→ c ); LSTM( − c) . Note that the encoder for y still has separate parameters from those of x. Hierarchical Attention Networks. NB OW and CONTEXT 2 VEC with M EAN -Pooling both aggregate every context word’s information into xh and yH . Given several long contexts and the fixed output dimension, it is vital for encoders to capture the most useful information. Inspired by Yang et al. (2016), we incorporate attention on different words and contexts. We use a feed-forward network to estimate the importance, and combine the information, of each context word to obtain c: n   X αj = softmax wa> tanh(Wa cj ) , c = αj cj . j=1 Then, another similar network is applied to all c(i) ∈ C(x) to obtain the representation of xh : |C(x)|   X > (i) βi = softmax wb tanh(Wb c ) , xh = βi c(i) . i=1 For word y, the encoder is similar but still has separate parameters from those of x. 4.3 Training the Distributional Model We train the distributional model g’s parameters Φ with supervision signal"
2020.emnlp-main.502,2020.acl-main.336,1,0.692425,"ts and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-ba"
2020.emnlp-main.502,2020.emnlp-main.119,1,0.65306,"fferent mixtures of Type-I and -II sparsity. Our contributions are summarized as : 1) We confirm that a specific type of sparsity issue of current pattern-based approaches is non-negligible. 2) We propose a framework of complementing patternbased approaches with distributional models where the former is invalid. 3) We systematically conduct comparisons on several common datasets, validating the superiority of our framework. 2 Related Work Pattern-Based Approaches. Taxonomies from experts (e.g., WordNet (Miller, 1995)) have proved effective in various reasoning applications (Song et al., 2011; Zhang et al., 2020). Meanwhile, Hearst patterns (Hearst, 1992) make large corpora a good resource of explicit is-a pairs, resulting in automatically built hypernymy knowledge bases (Wu et al., 2012; Seitner et al., 2016) of large scales. The coverage of both words and hypernymy pairs in those resources are far from complete. To infer unknown hypernymies between known words, e.g., implicit is-a pairs in transitive closures, pattern-based models are proposed. Roller et al. (2018) and Le et al. (2019) show that, on a broad range of benchmarks, simple matrix decomposition or embeddings on pattern-based word cooccurr"
2020.lrec-1.548,M98-1001,0,0.512021,"se structures are typical in fine-grained entity typing and show how well they perform on our dataset. We also show the possibility of improving Chinese fine-grained entity typing through cross-lingual transfer learning. Keywords: Fine-grained entity typing, Entity typing 1. Introduction The task of fine-grained entity typing (Ling and Weld, 2012; Gillick et al., 2014) assigns fine-grained types such as /person/politician, /organization/company to entity mentions in texts. It provides additional details to entity mentions compared with the typing in traditional named entity recognition tasks (Chinchor, 1998; Finkel et al., 2005), which typically categorize entity mentions into very general types such as person, location, or organization. Ultra-fine Entity Typing (Choi et al., 2018) introduces a new fine-grained entity typing task that requires to predict an open set of types for entity mentions. The dataset constructed for this task uses a very large tag set that contains around 10k free-form type phrases, while previous fine-grained entity typing datasets usually use tag sets with no greater than 200 types. This task presents a much closer view for each entity mention. Consider the sentence: “T"
2020.lrec-1.548,P18-1009,0,0.189968,"yping through cross-lingual transfer learning. Keywords: Fine-grained entity typing, Entity typing 1. Introduction The task of fine-grained entity typing (Ling and Weld, 2012; Gillick et al., 2014) assigns fine-grained types such as /person/politician, /organization/company to entity mentions in texts. It provides additional details to entity mentions compared with the typing in traditional named entity recognition tasks (Chinchor, 1998; Finkel et al., 2005), which typically categorize entity mentions into very general types such as person, location, or organization. Ultra-fine Entity Typing (Choi et al., 2018) introduces a new fine-grained entity typing task that requires to predict an open set of types for entity mentions. The dataset constructed for this task uses a very large tag set that contains around 10k free-form type phrases, while previous fine-grained entity typing datasets usually use tag sets with no greater than 200 types. This task presents a much closer view for each entity mention. Consider the sentence: “Tim Cook announced the new iPhone this morning.” With the dataset constructed by (Gillick et al., 2014), the mention “Tim Cook” can only be identified as /person/business. But wit"
2020.lrec-1.548,D19-1643,1,0.832022,"ed and distant Dataset Method MRR BiLSTM BiLSTM + General Types BERT BERT + General Types 0.199 0.200 0.281 0.310 Our dataset P R 30.5 46.6 42.2 64.1 14.6 17.5 30.9 38.2 F1 19.8 25.5 35.7 47.9 Ultra-fine dataset MRR P R F1 0.160 0.221 - 27.0 47.9 - 16.2 20.6 - 20.3 28.8 - Table 4: Fine-grained entity typing performance on the test set. We report mean reciprocal rank (MRR), macro-averaged precision, recall and F1 score. “+ General Types” indicates adding the general type mapping. 3. Experiments Experiments are conducted with neural entity typing models that follow the design of previous works (Dai et al., 2019; Shimaoka et al., 2016). We experimented with structures such as bi-LSTM and BERT (Devlin et al., 2019). We also trained both models on the Ultra fine-grained dataset (Choi et al., 2018) for comparison. with the greatest global score. Also similar to previous works (Dai et al., 2019; Abhishek et al., 2017), we use a customized hinge loss that better reflects the training objective of our data. When training with the general types on our dataset, or training on the Ultra-fine dataset which contain different level of granularity of types, we use a multitask objective function: 3.1. Experimental"
2020.lrec-1.548,N19-1423,0,0.0386291,"0 0.281 0.310 Our dataset P R 30.5 46.6 42.2 64.1 14.6 17.5 30.9 38.2 F1 19.8 25.5 35.7 47.9 Ultra-fine dataset MRR P R F1 0.160 0.221 - 27.0 47.9 - 16.2 20.6 - 20.3 28.8 - Table 4: Fine-grained entity typing performance on the test set. We report mean reciprocal rank (MRR), macro-averaged precision, recall and F1 score. “+ General Types” indicates adding the general type mapping. 3. Experiments Experiments are conducted with neural entity typing models that follow the design of previous works (Dai et al., 2019; Shimaoka et al., 2016). We experimented with structures such as bi-LSTM and BERT (Devlin et al., 2019). We also trained both models on the Ultra fine-grained dataset (Choi et al., 2018) for comparison. with the greatest global score. Also similar to previous works (Dai et al., 2019; Abhishek et al., 2017), we use a customized hinge loss that better reflects the training objective of our data. When training with the general types on our dataset, or training on the Ultra-fine dataset which contain different level of granularity of types, we use a multitask objective function: 3.1. Experimental Settings Similar to the typical neural entity typing models, the architecture of the models we experime"
2020.lrec-1.548,P05-1045,0,0.131754,"e typical in fine-grained entity typing and show how well they perform on our dataset. We also show the possibility of improving Chinese fine-grained entity typing through cross-lingual transfer learning. Keywords: Fine-grained entity typing, Entity typing 1. Introduction The task of fine-grained entity typing (Ling and Weld, 2012; Gillick et al., 2014) assigns fine-grained types such as /person/politician, /organization/company to entity mentions in texts. It provides additional details to entity mentions compared with the typing in traditional named entity recognition tasks (Chinchor, 1998; Finkel et al., 2005), which typically categorize entity mentions into very general types such as person, location, or organization. Ultra-fine Entity Typing (Choi et al., 2018) introduces a new fine-grained entity typing task that requires to predict an open set of types for entity mentions. The dataset constructed for this task uses a very large tag set that contains around 10k free-form type phrases, while previous fine-grained entity typing datasets usually use tag sets with no greater than 200 types. This task presents a much closer view for each entity mention. Consider the sentence: “Tim Cook announced the"
2020.lrec-1.548,L18-1008,0,0.0186988,"d hinge loss that better reflects the training objective of our data. When training with the general types on our dataset, or training on the Ultra-fine dataset which contain different level of granularity of types, we use a multitask objective function: 3.1. Experimental Settings Similar to the typical neural entity typing models, the architecture of the models we experimented consist of three parts: context sequence representation, mention representation, and the final inference layer. We adapted certain model architectures to better match our Fine-grained typing objective. We use fastText (Mikolov et al., 2018) for Chinese word embedding and Glove (Pennington et al., 2014) for English word Embedding. Both BERT implementation from HuggingFace5 and bidirectional LSTM are experimented to construct the context representation. Given a sentence x1 , ..., xn , we aim to construct a representation of the mention xm with the information provided by the context in the sentence. We substitute the mention xm with a [MASK] token and feed the whole sentence into the models. For the BiLSTMs models, we use two layers of BiLSTMs, producing output vectors h1 , h2 . We then extract the vectors at position m from each"
2020.lrec-1.548,P09-1113,0,0.326733,"Missing"
2020.lrec-1.548,D14-1162,0,0.0931128,"our data. When training with the general types on our dataset, or training on the Ultra-fine dataset which contain different level of granularity of types, we use a multitask objective function: 3.1. Experimental Settings Similar to the typical neural entity typing models, the architecture of the models we experimented consist of three parts: context sequence representation, mention representation, and the final inference layer. We adapted certain model architectures to better match our Fine-grained typing objective. We use fastText (Mikolov et al., 2018) for Chinese word embedding and Glove (Pennington et al., 2014) for English word Embedding. Both BERT implementation from HuggingFace5 and bidirectional LSTM are experimented to construct the context representation. Given a sentence x1 , ..., xn , we aim to construct a representation of the mention xm with the information provided by the context in the sentence. We substitute the mention xm with a [MASK] token and feed the whole sentence into the models. For the BiLSTMs models, we use two layers of BiLSTMs, producing output vectors h1 , h2 . We then extract the vectors at position m from each hidden layer, and take the addition fc = h1m + h2m as the conte"
2020.lrec-1.548,W16-1313,0,0.07919,"aset Method MRR BiLSTM BiLSTM + General Types BERT BERT + General Types 0.199 0.200 0.281 0.310 Our dataset P R 30.5 46.6 42.2 64.1 14.6 17.5 30.9 38.2 F1 19.8 25.5 35.7 47.9 Ultra-fine dataset MRR P R F1 0.160 0.221 - 27.0 47.9 - 16.2 20.6 - 20.3 28.8 - Table 4: Fine-grained entity typing performance on the test set. We report mean reciprocal rank (MRR), macro-averaged precision, recall and F1 score. “+ General Types” indicates adding the general type mapping. 3. Experiments Experiments are conducted with neural entity typing models that follow the design of previous works (Dai et al., 2019; Shimaoka et al., 2016). We experimented with structures such as bi-LSTM and BERT (Devlin et al., 2019). We also trained both models on the Ultra fine-grained dataset (Choi et al., 2018) for comparison. with the greatest global score. Also similar to previous works (Dai et al., 2019; Abhishek et al., 2017), we use a customized hinge loss that better reflects the training objective of our data. When training with the general types on our dataset, or training on the Ultra-fine dataset which contain different level of granularity of types, we use a multitask objective function: 3.1. Experimental Settings Similar to the"
2020.lrec-1.548,P15-2048,0,0.0284946,"h2 . We then extract the vectors at position m from each hidden layer, and take the addition fc = h1m + h2m as the context representation of the mention xm . Similarly, when using BERT for the context representation, we take the vector at position m in final output layer as the context sequence representation. To construct the mention representation, we simply ∑l take average fs = ( i=1 wi )/l of the word embedding for the words in the entity mention string. We then use the concatenation [fc ; fs ] as our input to a dense layer and obtain the output. Following previous work (Dai et al., 2019; Yogatama et al., 2015), we assign each type a vector and compute its dot product with the output of dense layer as the score for each type. A type is predicted if its score is greater than 0. If none of the types is, we pick the type 5 https://github.com/huggingface/transformers J= ∑ Ji · 1i (t). (1) i Here i indicates the level of granularity. For the Ultrafine dataset, i can be general, fine, and ultra-fine. In our dataset, i can be general or fine-grained. The input t indicates the ground truth type of a mention m. We only update loss for the ith level when the ground truth contain at least one label of such lev"
2020.lrec-1.548,E17-1075,0,\N,Missing
2021.acl-long.141,2020.acl-main.749,0,0.597774,"Missing"
2021.acl-long.141,P18-1009,0,0.26152,"ce of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping. 1 Introduction Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020). There are two main approaches to obtaining"
2021.acl-long.141,D19-1643,1,0.756561,"typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task. 3 Methodology Our methodology consists of two main steps. First, we obtain weak ultra-fine entity t"
2021.acl-long.141,N19-1423,0,0.0491819,"ecall. Second, neither of the above approaches can create a large number of training samples for pronoun mentions. Third, it is difficult to obtain types that are highly dependent on the context. For example, in “I met the movie star Leonardo DiCaprio on the plane to L.A.,” the type passenger is correct for “Leonardo DiCaprio.” However, this type cannot be obtained by linking to knowledge bases. In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al., 2019), to generate weak labels for ultra-fine entity typing. Given a sentence that contains a mention, our approach adds a short piece of text that contains a “[MASK]” token into it 1790 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1790–1799 August 1–6, 2021. ©2021 Association for Computational Linguistics Input In late 2015, [MASK] such as Leonardo DiCaprio starred in The Revenant. At some clinics, they and some other [MASK] are told the doctors don’t know how to deal with A"
2021.acl-long.141,C92-2082,0,0.276077,"abels. As a result, models trained from the automatically obtained labels have a low recall. Second, neither of the above approaches can create a large number of training samples for pronoun mentions. Third, it is difficult to obtain types that are highly dependent on the context. For example, in “I met the movie star Leonardo DiCaprio on the plane to L.A.,” the type passenger is correct for “Leonardo DiCaprio.” However, this type cannot be obtained by linking to knowledge bases. In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al., 2019), to generate weak labels for ultra-fine entity typing. Given a sentence that contains a mention, our approach adds a short piece of text that contains a “[MASK]” token into it 1790 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1790–1799 August 1–6, 2021. ©2021 Association for Computational Linguistics Input In late 2015, [MASK] such as Leonardo DiCaprio starred in The Revenant."
2021.acl-long.141,2020.tacl-1.28,0,0.0298171,"used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task. 3 Methodology Our methodology consists of two main steps. First, we obtain weak ultra-fine entity typing labels from a BERT masked language model. Second, we use the generated labels in model training to learn"
2021.acl-long.141,D19-1502,0,0.0217119,"ultra-fine entity typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task. 3 Methodology Our methodology consists of two main steps. First, we obtain weak u"
2021.acl-long.141,D14-1203,0,0.0250103,"s. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping. 1 Introduction Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a result, most existing works use weak labels that are automatically ge"
2021.acl-long.141,2020.lrec-1.548,1,0.839181,", etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020). There are two main approaches to obtaining weakly labeled training examples. One approach is to find the Wikipedia pages that correspond to entity mentions, which can be done by using hyperlinks to Wikipedia or applying entity linking. Then the entity types can be obtained from knowledge bases. The other approach is to directly use the head words of nominal mentions as ultra-fine type labels. For example, if a nominal mention is “a famous actor,” then the head word “actor” can be used as its type label. Several problems exist when using these weak labels for the ultra-fine typing task. First"
2021.acl-long.141,D19-1641,0,0.2047,"e traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely 1791 related to ultra-fine entity typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at"
2021.acl-long.141,Q15-1023,0,0.0291264,"pendent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping. 1 Introduction Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a res"
2021.acl-long.141,P18-1010,0,0.189792,"y disambiguation. The traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely 1791 related to ultra-fine entity typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. Howe"
2021.acl-long.141,2021.acl-long.160,0,0.480825,"hoi et al. (2018) uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task (Ling and Weld, 2012) that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by (Onoe and Durrett, 2019) filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by (Xiong et al., 2019) to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. Onoe et al. (2021) use box embeddings to capture latent type hierarchies. There is also some work on the applications of ultra-fine entity typing: Onoe and Durrett (2020) apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation. The traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely 1791 related to ultra-fine entity typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size"
2021.acl-long.141,N19-1250,0,0.317261,"lts. • We conduct experiments on both an ultra-fine entity typing dataset and a traditional finegrained entity typing dataset to verify the effectiveness of our method. Our code is available at https://github.com/ HKUST-KnowComp/MLMET. 2 Related Work The ultra-fine entity typing task proposed by Choi et al. (2018) uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task (Ling and Weld, 2012) that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by (Onoe and Durrett, 2019) filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by (Xiong et al., 2019) to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. Onoe et al. (2021) use box embeddings to capture latent type hierarchies. There is also some work on the applications of ultra-fine entity typing: Onoe and Durrett (2020) apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation. The t"
2021.acl-long.141,2020.findings-emnlp.54,0,0.0705415,"f the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping. 1 Introduction Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a result, most existing works"
2021.acl-long.141,D19-1250,0,0.0632333,"Missing"
2021.acl-long.141,L16-1056,0,0.32379,"sult, models trained from the automatically obtained labels have a low recall. Second, neither of the above approaches can create a large number of training samples for pronoun mentions. Third, it is difficult to obtain types that are highly dependent on the context. For example, in “I met the movie star Leonardo DiCaprio on the plane to L.A.,” the type passenger is correct for “Leonardo DiCaprio.” However, this type cannot be obtained by linking to knowledge bases. In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al., 2019), to generate weak labels for ultra-fine entity typing. Given a sentence that contains a mention, our approach adds a short piece of text that contains a “[MASK]” token into it 1790 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1790–1799 August 1–6, 2021. ©2021 Association for Computational Linguistics Input In late 2015, [MASK] such as Leonardo DiCaprio starred in The Revenant. At some clinics, they a"
2021.acl-long.141,N19-1084,0,0.130305,"code is available at https://github.com/ HKUST-KnowComp/MLMET. 2 Related Work The ultra-fine entity typing task proposed by Choi et al. (2018) uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task (Ling and Weld, 2012) that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by (Onoe and Durrett, 2019) filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by (Xiong et al., 2019) to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. Onoe et al. (2021) use box embeddings to capture latent type hierarchies. There is also some work on the applications of ultra-fine entity typing: Onoe and Durrett (2020) apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation. The traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely 1791 related to ultra-fine entity typing. Automatic annotation (Ling and"
2021.acl-long.141,C12-2133,0,0.0808807,"Missing"
2021.acl-long.141,2020.acl-main.725,0,0.018925,"of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc. Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task. 3 Methodology Our methodology consists of two main steps. First, we obtain weak ultra-fine entity typing labels from a BERT masked language model. Second, we use the generated labels in model training to learn better ultra-fine en"
2021.acl-long.306,2020.acl-main.287,0,0.0737156,"Missing"
2021.acl-long.306,D15-1263,0,0.0282516,"fully explored. Zeng et al. (2020) examined how the contexts and the dynamic progress of argumentative conversations influence the comparative persuasiveness of an argumentation process. Durmus et al. (2019) created a new dataset based on argument claims and impact votes from a debate platform kialo.com, and experiments showed that incorporating contexts is useful to classify the argument impact. Understanding discourse relations is one of the fundamental tasks of natural language understanding, and it is beneficial for various downstream tasks such as sentiment analysis (Nejat et al., 2017; Bhatia et al., 2015), machine translation (Li et al., 2014) and text generation (Bosselut et al., 2018). Discourse information is also considered indicative for various tasks of computational argumentation. Eckle-Kohler et al. (2015) analyzed the role of discourse markers for discriminating claims and premises in argumentative discourse and found that particular semantic group of discourse markers are highly predictive features. Hidey and McKeown (2018) concatenated sentence vectors with discourse relation embeddings as sentence features for persuasiveness prediction and showed that discourse embeddings helped im"
2021.acl-long.306,N19-1423,0,0.00622164,"ates to decide whether to remember or forget during encoding, but it cannot handle long-range information with limited memory. Recently, transformer-based encoders have shown remarkable performance in various complicated tasks. These models regard sequences as fully connected graphs to learn the correlations and representations for each token. People assume that transformers can learn whether two tokens are relevant and how strong the correlation is by back-propagation. Table 3 illustrates different possible ways to aggregation context information. Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) adopt full-range attentions while TransformerXL (Dai et al., 2019) and XLNet (Yang et al., 2019) regard historical encoded representations as memories to reuse hidden states. SparseTransformer (Child et al., 2019), in the opposite direction, stacks hundreds of layers by narrow the attention scope by sparse factorization. Information can still spread after propagations in several layers. Inspired by these observations, we design D IS COC (Discourse Context Oriented Classifier) to capture contextualized features by localized attentions and imitate recurrent models to reduce noises from long dis"
2021.acl-long.306,N18-1094,0,0.129289,"es or out of a motivation for personal revenge. Impactful Impactful Figure 1: Example of an argument tree from Kialo. Stances, impact labels, and discourse relations are annotated in orange, red, and violet respectively. Introduction It is an interesting natural language understanding problem to identify the impact and the persuasiveness of an argument in a conversation. Previous works have shown that many factors can affect the persuasiveness prediction, ranging from textual and argumentation features (Wei et al., 2016), style factors (Baff et al., 2020), to the traits of source or audience (Durmus and Cardie, 2018, 2019; ShmueliScheuer et al., 2019). Discourse relations, such as Restatement and Instantiation, among arguments reveal logical structures of a debate conversation. It is natural to consider using the discourse structure to study the argument impact. Durmus et al. (2019) initiated a new study of the influence of discourse contexts on determining argument quality by constructing a new dataset Kialo. ∗ This work was done when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree"
2021.acl-long.306,D19-1568,0,0.407278,"ence of discourse relations influence a claim’s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose D IS COC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance. 1 Oppose Support Result Result S1: Torture can help force prisoners to reveal information that could prevent attacks and save lives. Support Result S2: The knowledge that torture is acceptable and may be applied is in and of itself a strong incentive for prisoners to cooperate with their captors. Not Impactful O2: If torture is allowed, then it could easily be misused or performed in excess. Oppose Support Contrast Restatement, Instantiation O1: Torture is ineffective at getting p"
2021.acl-long.306,D15-1267,0,0.0266016,") created a new dataset based on argument claims and impact votes from a debate platform kialo.com, and experiments showed that incorporating contexts is useful to classify the argument impact. Understanding discourse relations is one of the fundamental tasks of natural language understanding, and it is beneficial for various downstream tasks such as sentiment analysis (Nejat et al., 2017; Bhatia et al., 2015), machine translation (Li et al., 2014) and text generation (Bosselut et al., 2018). Discourse information is also considered indicative for various tasks of computational argumentation. Eckle-Kohler et al. (2015) analyzed the role of discourse markers for discriminating claims and premises in argumentative discourse and found that particular semantic group of discourse markers are highly predictive features. Hidey and McKeown (2018) concatenated sentence vectors with discourse relation embeddings as sentence features for persuasiveness prediction and showed that discourse embeddings helped improve performance. 6 Related Work There is an increasing interest in computational argumentation to evaluate the qualitative impact of arguments based on corpus extracted from Web Argumentation sources such as CMV"
2021.acl-long.306,D16-1129,0,0.0431429,"Missing"
2021.acl-long.306,D17-1134,0,0.0207708,"e when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree for a controversial topic. They argue contexts reflect the discourse of arguments and conduct experiments to utilize historical arguments. They find BERT with flat context concatenation is the best, but discourse structures are not easily captured by this method because it is difficult to reflect implicit discourse relations by the surface form of two arguments (Prasad et al., 2008; Lin et al., 2009; Xue et al., 2015; Lan et al., 2017; Varia et al., 2019). Therefore, there is still a gap to study how discourse relations and their sequential structures or patterns affect the argument impact and persuasiveness prediction. In this paper, we acquire discourse relations for argument pairs with the state-of-the-art classifier for implicit discourse relations. Then we train a BiLSTM whose input is the sequence of discourse relations between two adjacent arguments to predict the last argument’s impact, and the performance is comparable to that of a BiLSTM on raw text. This indicates that a sequence of discourse re3958 Proceedings"
2021.acl-long.306,2020.emnlp-main.716,0,0.082788,"Missing"
2021.acl-long.306,P14-2047,0,0.0202375,"d how the contexts and the dynamic progress of argumentative conversations influence the comparative persuasiveness of an argumentation process. Durmus et al. (2019) created a new dataset based on argument claims and impact votes from a debate platform kialo.com, and experiments showed that incorporating contexts is useful to classify the argument impact. Understanding discourse relations is one of the fundamental tasks of natural language understanding, and it is beneficial for various downstream tasks such as sentiment analysis (Nejat et al., 2017; Bhatia et al., 2015), machine translation (Li et al., 2014) and text generation (Bosselut et al., 2018). Discourse information is also considered indicative for various tasks of computational argumentation. Eckle-Kohler et al. (2015) analyzed the role of discourse markers for discriminating claims and premises in argumentative discourse and found that particular semantic group of discourse markers are highly predictive features. Hidey and McKeown (2018) concatenated sentence vectors with discourse relation embeddings as sentence features for persuasiveness prediction and showed that discourse embeddings helped improve performance. 6 Related Work There"
2021.acl-long.306,D09-1036,0,0.055762,"w dataset Kialo. ∗ This work was done when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree for a controversial topic. They argue contexts reflect the discourse of arguments and conduct experiments to utilize historical arguments. They find BERT with flat context concatenation is the best, but discourse structures are not easily captured by this method because it is difficult to reflect implicit discourse relations by the surface form of two arguments (Prasad et al., 2008; Lin et al., 2009; Xue et al., 2015; Lan et al., 2017; Varia et al., 2019). Therefore, there is still a gap to study how discourse relations and their sequential structures or patterns affect the argument impact and persuasiveness prediction. In this paper, we acquire discourse relations for argument pairs with the state-of-the-art classifier for implicit discourse relations. Then we train a BiLSTM whose input is the sequence of discourse relations between two adjacent arguments to predict the last argument’s impact, and the performance is comparable to that of a BiLSTM on raw text. This indicates that a seque"
2021.acl-long.306,D19-1387,0,0.0197635,"ords by self-attention. We use these encoders to encode the flat context concatenation like [CTX] C 0 [SEP] [CTX] · · · [CTX] C l−1 [SEP] as Segment A and [CLS] C l [SEP] as Segment B. After getting [CTX] and [CLS] representations, a gated transformer layer and a MLP predict impacts. As for XLNet, we follow its default setting so that [CTX] and [CLS] are located at the end of claims. Interval-MLMs. Flat-MLMs regard the context path as a whole segment and ignore the real discourse structures except the adjacency, e.g., distances between two claims are missing. We borrow the idea from BERT-SUM (Liu and Lapata, 2019): segment embeddings of C i are assigned depending on whether the distance to C l is odd or even. Context-MLMs. We also compare pretrained encoders with context masks. A context mask is to localize the attention scope from the previous to the next. That is, C i can attends words in C i−1 and C i+1 except for itself if 1 ≤ i < l; C 0 can only attend C 0 , C 1 , and C l can only attend C l−1 , C l . Memory-MLMs. XLNet utilizes memory to extend the capability of self-attention to learn super long historical text information. We also extend Flat-MLMs under this setting. 4.2 Model Configuration and"
2021.acl-long.306,2021.ccl-1.108,0,0.0398358,"Missing"
2021.acl-long.306,W17-5535,0,0.0239126,"e crucial by not yet fully explored. Zeng et al. (2020) examined how the contexts and the dynamic progress of argumentative conversations influence the comparative persuasiveness of an argumentation process. Durmus et al. (2019) created a new dataset based on argument claims and impact votes from a debate platform kialo.com, and experiments showed that incorporating contexts is useful to classify the argument impact. Understanding discourse relations is one of the fundamental tasks of natural language understanding, and it is beneficial for various downstream tasks such as sentiment analysis (Nejat et al., 2017; Bhatia et al., 2015), machine translation (Li et al., 2014) and text generation (Bosselut et al., 2018). Discourse information is also considered indicative for various tasks of computational argumentation. Eckle-Kohler et al. (2015) analyzed the role of discourse markers for discriminating claims and premises in argumentative discourse and found that particular semantic group of discourse markers are highly predictive features. Hidey and McKeown (2018) concatenated sentence vectors with discourse relation embeddings as sentence features for persuasiveness prediction and showed that discours"
2021.acl-long.306,D14-1162,0,0.0838613,"Missing"
2021.acl-long.306,prasad-etal-2008-penn,0,0.464219,"by constructing a new dataset Kialo. ∗ This work was done when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree for a controversial topic. They argue contexts reflect the discourse of arguments and conduct experiments to utilize historical arguments. They find BERT with flat context concatenation is the best, but discourse structures are not easily captured by this method because it is difficult to reflect implicit discourse relations by the surface form of two arguments (Prasad et al., 2008; Lin et al., 2009; Xue et al., 2015; Lan et al., 2017; Varia et al., 2019). Therefore, there is still a gap to study how discourse relations and their sequential structures or patterns affect the argument impact and persuasiveness prediction. In this paper, we acquire discourse relations for argument pairs with the state-of-the-art classifier for implicit discourse relations. Then we train a BiLSTM whose input is the sequence of discourse relations between two adjacent arguments to predict the last argument’s impact, and the performance is comparable to that of a BiLSTM on raw text. This indi"
2021.acl-long.306,W19-5951,0,0.023054,"an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree for a controversial topic. They argue contexts reflect the discourse of arguments and conduct experiments to utilize historical arguments. They find BERT with flat context concatenation is the best, but discourse structures are not easily captured by this method because it is difficult to reflect implicit discourse relations by the surface form of two arguments (Prasad et al., 2008; Lin et al., 2009; Xue et al., 2015; Lan et al., 2017; Varia et al., 2019). Therefore, there is still a gap to study how discourse relations and their sequential structures or patterns affect the argument impact and persuasiveness prediction. In this paper, we acquire discourse relations for argument pairs with the state-of-the-art classifier for implicit discourse relations. Then we train a BiLSTM whose input is the sequence of discourse relations between two adjacent arguments to predict the last argument’s impact, and the performance is comparable to that of a BiLSTM on raw text. This indicates that a sequence of discourse re3958 Proceedings of the 59th Annual Me"
2021.acl-long.306,P16-2032,0,0.186186,"gators and prison guards could torture prisoners solely to fulfill their own sadistic desires or out of a motivation for personal revenge. Impactful Impactful Figure 1: Example of an argument tree from Kialo. Stances, impact labels, and discourse relations are annotated in orange, red, and violet respectively. Introduction It is an interesting natural language understanding problem to identify the impact and the persuasiveness of an argument in a conversation. Previous works have shown that many factors can affect the persuasiveness prediction, ranging from textual and argumentation features (Wei et al., 2016), style factors (Baff et al., 2020), to the traits of source or audience (Durmus and Cardie, 2018, 2019; ShmueliScheuer et al., 2019). Discourse relations, such as Restatement and Instantiation, among arguments reveal logical structures of a debate conversation. It is natural to consider using the discourse structure to study the argument impact. Durmus et al. (2019) initiated a new study of the influence of discourse contexts on determining argument quality by constructing a new dataset Kialo. ∗ This work was done when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it c"
2021.acl-long.306,K15-2001,0,0.17632,"This work was done when Xin Liu was an intern at Huawei Noah’s Ark Lab. As shown in Figure 1, it consists of arguments, impact labels, stances where every argument is located in an argument tree for a controversial topic. They argue contexts reflect the discourse of arguments and conduct experiments to utilize historical arguments. They find BERT with flat context concatenation is the best, but discourse structures are not easily captured by this method because it is difficult to reflect implicit discourse relations by the surface form of two arguments (Prasad et al., 2008; Lin et al., 2009; Xue et al., 2015; Lan et al., 2017; Varia et al., 2019). Therefore, there is still a gap to study how discourse relations and their sequential structures or patterns affect the argument impact and persuasiveness prediction. In this paper, we acquire discourse relations for argument pairs with the state-of-the-art classifier for implicit discourse relations. Then we train a BiLSTM whose input is the sequence of discourse relations between two adjacent arguments to predict the last argument’s impact, and the performance is comparable to that of a BiLSTM on raw text. This indicates that a sequence of discourse r"
2021.acl-long.306,N16-1174,0,0.15293,"Missing"
2021.acl-long.329,2020.osact-1.2,0,0.0617615,"Missing"
2021.acl-long.329,2020.findings-emnlp.301,0,0.0325858,"of our methodology to assess and mitigate the toxicity transmitted by PTLMs. 1 Introduction The recent gain in size of pre-trained language models (PTLMs) has had a large impact on state-of-theart NLP models. Although their efficiency and usefulness in different NLP tasks is incontestable, their shortcomings such as their learning and reproduction of harmful biases cannot be overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to m"
2021.acl-long.329,P19-1267,0,0.0630627,"Missing"
2021.acl-long.329,W19-3504,0,0.0422535,"Missing"
2021.acl-long.329,N19-1419,0,0.0268809,"efficiency and usefulness in different NLP tasks is incontestable, their shortcomings such as their learning and reproduction of harmful biases cannot be overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements wh"
2021.acl-long.329,2020.acl-main.487,0,0.0354153,"Missing"
2021.acl-long.329,N19-1423,0,0.023654,"otice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap et al., 2019b). Then, we use a PTLM to predict possible reasons for these actions. We look into how BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-2 (Radford et al., 2019) associate unrelated and detrimental causes to basic everyday actions and examine how frequently the predicted words relate to specific social groups. Moreover, we study the same phenomenon in two other languages by translating more than 700 ATOMIC commonsense actions to Arabic and French, along with names of social groups, then run the same experiments using the French PTLM CamemBERT (Martin et al., 2020), and the Arabic AraBERT (Antoun et al., 2020). We find that, overall, the predicted content can also be irrelevant and offensive"
2021.acl-long.329,2020.tacl-1.3,0,0.0498448,"Missing"
2021.acl-long.329,2020.emnlp-main.48,0,0.0749292,"Missing"
2021.acl-long.329,S18-2005,0,0.0422088,"Missing"
2021.acl-long.329,W19-3823,0,0.0355704,"Missing"
2021.acl-long.329,2020.emnlp-main.557,0,0.0172396,"overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap"
2021.acl-long.329,2021.ccl-1.108,0,0.0679216,"Missing"
2021.acl-long.329,D18-1151,0,0.029137,"in different NLP tasks is incontestable, their shortcomings such as their learning and reproduction of harmful biases cannot be overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explic"
2021.acl-long.329,D19-1250,0,0.0377658,"Missing"
2021.acl-long.329,2020.emnlp-main.259,0,0.0460537,"Missing"
2021.acl-long.329,2020.acl-main.702,0,0.0611794,"Missing"
2021.acl-long.329,W19-3512,0,0.0635854,"Missing"
2021.acl-long.329,2020.emnlp-main.470,0,0.0848344,"Missing"
2021.acl-long.329,2021.naacl-main.191,0,0.0164437,"towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap et al., 2019b). Then, we use a PTLM to predict possible reasons for these actions. We look into how"
2021.acl-long.329,D19-1474,1,0.889597,"Missing"
2021.acl-long.329,2020.emnlp-main.199,1,0.805059,"Missing"
2021.acl-long.329,N16-3020,0,0.103992,"Missing"
2021.acl-long.329,2020.acl-main.442,0,0.0371956,"Missing"
2021.acl-long.329,2020.tacl-1.54,0,0.0219081,"Missing"
2021.acl-long.329,P19-1163,0,0.159046,"20). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups. In this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap et al., 2019b). Then, we use a PTLM to predict possible reasons for these actions. We look into how BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-2 (Radford et al., 2019) associate unrelated and detrimental causes to basic everyday actions and examine how frequently the predicted words relate to specific social groups. Moreover, we study the same phenomenon in two other languages by translating more than 700 ATOMIC commonsense actions to Arabic and French, along with names of social groups, then run the same experiments using the French PTLM CamemBERT (Martin et al., 2020), and the Arabi"
2021.acl-long.329,2020.acl-main.468,0,0.0425824,"Missing"
2021.acl-long.329,D19-1339,0,0.0415629,"Missing"
2021.acl-long.329,D18-1302,0,0.0579553,"Missing"
2021.acl-long.329,2020.acl-main.384,0,0.0935087,"Missing"
2021.acl-long.329,2020.acl-main.477,0,0.0200051,"to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs. 1 Introduction The recent gain in size of pre-trained language models (PTLMs) has had a large impact on state-of-theart NLP models. Although their efficiency and usefulness in different NLP tasks is incontestable, their shortcomings such as their learning and reproduction of harmful biases cannot be overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or syste"
2021.acl-long.329,S19-2010,0,0.0650118,"Missing"
2021.acl-long.329,D17-1323,0,0.065999,"Missing"
2021.crac-1.1,D13-1057,0,0.0211248,"ons. For example, “they” can refer to “the company” (Hardmeier et al., 2018). Another exception is to prevent generic masculine, where “they” can refer to singular entity in genderneutral language. 4 Some datasets (e.g., CoNLL-2012 shared task) are originally designed for the general coreference resolution task. Nonetheless, we can easily convert them into a PCR task. 2 used evaluation benchmark for the coreference resolution task. framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models (Chang et al., 2013). 4. WikiCoref: Recently, a new coreference dataset WikiCoref (Ghaddar and Langlais, 2016) was proposed as a supplementary of CoNLL shared tasks. Different from CoNLL, where most of the corpus is from the newswire, WikiCoref directly annotates Wikipedia pages, which provides a new way to evaluate models’ performances in the out-of-domain setting. 2.2.2 End-to-end Model Leveraging human-designed rules or features can help accurately resolve some pronouns, but it is hard to manually design rules to cover all cases. To solve this problem, an end-to-end deep model (Lee et al., 2017) was proposed."
2021.crac-1.1,W16-3612,0,0.0118654,"ably need a more robust model. 4 ing coreference relations in electronic medical records. As reported in (Zhang et al., 2019c), the training set of I2b2 contains 2,024 third personal pronouns, 685 possessive pronouns, and 270 demonstrative pronouns. Its test set contains 1,244 third personal pronouns, 367 possessive pronouns, and 166 demonstrative pronouns. As a dataset in a relatively narrow domain, the usage of domain knowledge becomes important. As shown in (Zhang et al., 2019c), i2b2 can be used as an additional dataset to evaluate models’ cross-domain abilities. 3. PCR for Chatbots: CIC (Chen and Choi, 2016) is a dataset focusing on identifying coreference relations in multi-party conversations. Compared with the ordinary PCR tasks, which are mostly annotated on formal textual data (e.g., newswire), identifying coreference relation in conversation is more challenging. 4. PCR for Studying Gender Bias: Nowadays, gender bias has been a hot research topic in the NLP community (Rudinger et al., 2018; Zhao et al., 2018). WinoGender (Rudinger et al., 2018) is among the most popular works. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one"
2021.crac-1.1,P15-1136,0,0.0541759,"Missing"
2021.crac-1.1,D16-1245,0,0.124589,"onoun coreference resolution tasks are often defined over formal textual corpus (e.g., news3 One exception is the entities that are related to organizations. For example, “they” can refer to “the company” (Hardmeier et al., 2018). Another exception is to prevent generic masculine, where “they” can refer to singular entity in genderneutral language. 4 Some datasets (e.g., CoNLL-2012 shared task) are originally designed for the general coreference resolution task. Nonetheless, we can easily convert them into a PCR task. 2 used evaluation benchmark for the coreference resolution task. framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models (Chang et al., 2013). 4. WikiCoref: Recently, a new coreference dataset WikiCoref (Ghaddar and Langlais, 2016) was proposed as a supplementary of CoNLL shared tasks. Different from CoNLL, where most of the corpus is from the newswire, WikiCoref directly annotates Wikipedia pages, which provides a new way to evaluate models’ performances in the out-of-domain setting. 2.2.2 End-to-end Model Leveraging human-designed rules or features can help accurately resolve some pro"
2021.crac-1.1,N19-1423,0,0.0218517,"cy of all examples (i.e., Correct, Wrong, and NA examples) vised BERT on WSC because models based on BERT, which relies on predicting the probability of candidate words, cannot get rid of such noise. 2. Different from them, language model based methods represent knowledge contained in human language with an implicit approach, and thus do not have the matching issue and achieve better overall performance. 3.2.3 Fine-tuning Representation Models Last but not least, we introduce current bestperforming models on the WSC task, which finetunes pre-trained language representation models (e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019)) with a similar dataset (e.g., DPR (Rahman and Ng, 2012) or WinoGrande (Sakaguchi et al., 2020)). CorefBERT (Ye et al., 2020), in addition, introduced a new pre-training task that requires models to predict mention references. This idea of fine-tuning was originally proposed by (Kocijan et al., 2019), which first converts the original WSC task into a token prediction task and then selects the candidate with higher probability as the final prediction. In general, the stronger the language models and the larger the fine-tuning datasets are, the better the model can"
2021.crac-1.1,D18-1220,0,0.0501326,"Missing"
2021.crac-1.1,P19-1386,0,0.0147319,"inding about these language model based approaches is that they proposed two settings to predict the probability: (1) Full: use the probability of the whole sentence as the final prediction; (2) Partial: only consider the probability of the partial sentence after the target pronoun. Experiments show that the partial model always outperforms the full model. One explanation is that the influence of the imbalanced distribution of candidate words is relieved by only considering the sentence probability after them. Such observation also explains why GPT-2 can outperform unsuper4. KnowRef: KnowRef (Emami et al., 2019), similar to WinoGrande, also aimed at creating a larger scale WSC dataset but with a different approach. Instead of using crowd-sourcing + adversarial filtering framework, KnowRef tried to extract WSC-like questions from raw sentences. As a result, KnowRef collected eight thousand WSC-like questions. 3.2 Methods In this subsection, we introduce existing approaches for the hard PCR task. As the majority of the methods are evaluated based on WSC, all the discussion and analysis are based on their performance on WSC. 3.2.1 Reasoning with Structured Knowledge At first, people tried to leverage di"
2021.crac-1.1,L16-1021,0,0.0152102,"her exception is to prevent generic masculine, where “they” can refer to singular entity in genderneutral language. 4 Some datasets (e.g., CoNLL-2012 shared task) are originally designed for the general coreference resolution task. Nonetheless, we can easily convert them into a PCR task. 2 used evaluation benchmark for the coreference resolution task. framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models (Chang et al., 2013). 4. WikiCoref: Recently, a new coreference dataset WikiCoref (Ghaddar and Langlais, 2016) was proposed as a supplementary of CoNLL shared tasks. Different from CoNLL, where most of the corpus is from the newswire, WikiCoref directly annotates Wikipedia pages, which provides a new way to evaluate models’ performances in the out-of-domain setting. 2.2.2 End-to-end Model Leveraging human-designed rules or features can help accurately resolve some pronouns, but it is hard to manually design rules to cover all cases. To solve this problem, an end-to-end deep model (Lee et al., 2017) was proposed. Different from other machine learning-based methods, it does not use any human-defined rul"
2021.crac-1.1,C96-1079,0,0.720024,"ntion set. Similarly, each candidate span is denoted as s ∈ S, where S is the set of all candidate spans. Note that in the case where no golden mentions are provided, all possible spans in D are used to form S. The task is thus to identify C out of S. In the rest of this section, we introduce the widely used datasets as well as the progress and limitation of current approaches. 2.1 Datasets Throughout the years, researchers in the NLP community have devoted great efforts to developing high-quality coreference resolution datasets4 and we introduce representative ones as follows: 1. MUC: MUC-6 (Grishman and Sundheim, 1996) and MUC-7 (Chinchor, 1998), which were developed for the 6th and 7th message understanding conferences respectively, are the earliest coreference resolution datasets. They are focusing on English news articles and are relatively small compared with modern datasets. 2. ACE: The ACE dataset (Doddington et al., 2004) was proposed as part of the Automatic Content Extraction program. Compared with MUC datasets, ACE extends the corpus domain from news to other domains like telephonic speeches and broadcast conversations. 3. CoNLL shared tasks: CoNLL-2011 (Pradhan et al., 2011) and CoNLL-2012 (Pradh"
2021.crac-1.1,guillou-etal-2014-parcor,0,0.0608429,"Missing"
2021.crac-1.1,D08-1031,0,0.0663734,"han et al., 2010), knowledge (Ponzetto and Strube, 2006; Versley et al., 2016), and features (Ng, 2005; Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, ‘he’ typically refers to males and ‘she’ typically refers to females; ‘it’ typically refers to singular objects and ‘them’ typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth, 2008), a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, (Clark and Manning, 2015) proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning 1. Higher-order Information: One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-o"
2021.crac-1.1,2020.tacl-1.5,0,0.0970559,"(6,843) P R F1 Demonstrative (546) P R F1 P Overall (25,536) R F1 Deterministic (Raghunathan et al., 2010) Statistical (Clark and Manning, 2015) Deep-RL (Clark and Manning, 2016) 25.5 25.8 78.6 58.9 62.1 63.9 35.6 36.5 70.5 22.9 28.9 73.3 64.3 64.9 68.9 33.8 40.0 71.0 3.4 9.8 3.7 5.7 6.3 2.9 4.2 7.6 5.5 23.4 25.4 76.4 57.0 59.3 61.2 33.4 36.5 68.0 End-to-end (Lee et al., 2017) 70.7 77.8 74.1 75.6 74.0 74.8 37.8 71.7 49.5 68.3 76.4 72.1 80.0 82.4 75.6 80.5 77.7 81.5 81.7 83.9 72.2 81.0 76.7 82.4 50.8 52.0 64.6 61.5 56.9 56.4 77.9 82.2 74.0 80.2 75.9 81.2 + KG (Zhang et al., 2019c) + SpanBERT (Joshi et al., 2020) Table 2: Performances of different models on the CoNLL-2012 shared task. Precision (P), recall (R), and the F1 score are reported. Numbers of different types of pronouns in the test set are shown in the brackets. Best models are indicated with the bold font. Model Training data 2019c; Joshi et al., 2020) on the CoNLL-2012 dataset (Pradhan et al., 2012). The experiment setting (both detection the mentions and resolving the coreference relations) and evaluation metric are the same as these previous works on CoNLL2012. From the results in Table 2, we can observe that with the help of the end-to-"
2021.crac-1.1,1995.tmi-1.6,0,0.285554,"Missing"
2021.crac-1.1,P19-1066,0,0.0112975,"2 + SpanBERT CoNLL i2b2 79.6 28.5 40.8 80.5 Table 3: Models’ performance (in F1 score) in crossdomain setting on different training/test data. ios. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, (Zhang et al., 2019b) requires converting external knowledge into features while (Zhang et al., 2019c) directly uses external knowledge in the format of triplets. 3. Stronger Language Representation Models: Recently, along with the fast development of language representation models, a few works (Kantor and Globerson, 2019; Joshi et al., 2020) have been trying to replace the encoding layer of the original end-to-end model with more powerful language representation models. SpanBERT (Joshi et al., 2020) replaces ELMo with SpanBERT and boosts the performance by 6.6 F1 over the general coreference resolution task. 2.3 2.3.1 Cross-domain Performance To investigate whether current PCR models are good enough to be used in real applications, which can be out of the training domain, we conduct experiments on the cross-domain setting. In detail, we select two different PCR datasets from different domains (i.e., CoNLL (Pr"
2021.crac-1.1,N18-1202,0,0.346191,"ides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning 1. Higher-order Information: One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed in (Lee et al., 2018) to provide the higher-order coreference resolution inference ability (i.e., leveraging the coreference cluster to better predict the coreference relations). Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective. 2. Structured Knowledge: Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. However, in real applications, it is labor-intensive and almost impossible to annotate a large-scale dataset to contain all scenar5 These models once achieved better performance either on the general coreference resolution task or the PCR task. 3 Third Personal (18,147) P R F1 Model Possessive (6,843) P R F1 Demonstrative ("
2021.crac-1.1,P19-1478,0,0.0493219,"Missing"
2021.crac-1.1,N19-1176,0,0.0290008,"Missing"
2021.crac-1.1,J18-3007,0,0.0176726,"s), and demonstrative pronoun (e.g., this, that, these, those). The first and second personal pronouns are typically not considered as they often refer to the current speakers, which are normally out of the conversation or document. Besides that, conventional PCR works (Ng, 2005; Zhang et al., 2019b,c) mostly focusing on identifying coreference relations between pronouns and noun phrases rather than coreference relation between pronouns. 1 Some pronouns may refer to non-nominal antecedents. For example, the pronoun “it” in “It is too cold in the Winter here” does not refer to any real object (Kolhatkar et al., 2018). But in this survey, we only focus on pronouns that refer to nominal antecedents. 2 PCR is also known as anaphora resolution (Versley et al., 2016). Previous studies (Ng, 2005; Zhang et al., 2019c) mainly 1 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 1–11 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics mentions are provided, models only need to predict the coreference relations (i.e., the task of distinguishing between referential and non-referential instances is ignore"
2021.crac-1.1,N06-1025,0,0.135153,"Missing"
2021.crac-1.1,W12-4501,0,0.559867,"such as machine translation (Mitkov et al., 1995; Lapshinova-Koltunski et al., 2018), summarization (Steinberger et al., 2007), and dialog systems (Strube and Müller, 2003). To investigate the difference between PCR and the general coreference resolution task, which tries to identify not only the coreference relations between noun phrases (NP) and pronouns (P) but also potential coreference relations between noun phrases or coreference relations between pronouns, we conduct experiments with one recent breakthrough model (i.e., End-to-end model (Lee et al., 2017)) on the CoNLL-2012 shard task (Pradhan et al., 2012) under two settings: one without the gold mention and one with the gold mention. In the ‘without gold mention’ setting, models are required to first identify spans from the documents as the mentions and then predict the coreference relations among these mentions. As a comparison, if gold Introduction The question of how human beings resolve pronouns1 has long been of interest to both linguistic and natural language processing (NLP) communities, for the reason that a pronoun itself only having weak semantic meaning brings challenges to natural language understanding. To explore solutions for th"
2021.crac-1.1,L18-1065,0,0.0176746,"nfrequent objects). All experiment codes will be available upon acceptance. NP-NP NP-P P-P 25,828 43,883 41,741 0.690 0.667 0.754 0.768 0.707 0.763 Overall 111,452 0.705 0.742 Table 1: The performance of the End-to-end model on the CoNLL-2012 shared task coreference resolution dataset. The model’s performances of different coreference types are reported separately. task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be useful for a series of downstream tasks, such as machine translation (Mitkov et al., 1995; Lapshinova-Koltunski et al., 2018), summarization (Steinberger et al., 2007), and dialog systems (Strube and Müller, 2003). To investigate the difference between PCR and the general coreference resolution task, which tries to identify not only the coreference relations between noun phrases (NP) and pronouns (P) but also potential coreference relations between noun phrases or coreference relations between pronouns, we conduct experiments with one recent breakthrough model (i.e., End-to-end model (Lee et al., 2017)) on the CoNLL-2012 shard task (Pradhan et al., 2012) under two settings: one without the gold mention and one with"
2021.crac-1.1,W11-1901,0,0.123614,"Missing"
2021.crac-1.1,J13-4004,0,0.0200726,"troduce a few recent improvements over the end-to-end model. 2.2.3 Further Improvements Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model5 : 2.2.1 Rule and Feature Based Methods Before the deep learning era, human-designed rules (Hobbs, 1978; Raghunathan et al., 2010), knowledge (Ponzetto and Strube, 2006; Versley et al., 2016), and features (Ng, 2005; Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, ‘he’ typically refers to males and ‘she’ typically refers to females; ‘it’ typically refers to singular objects and ‘them’ typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth, 2008), a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, (Clark and Manning, 2015) proposes an entity-centric coreference system that can lea"
2021.crac-1.1,D17-1018,0,0.254869,"n to be useful for a series of downstream tasks, such as machine translation (Mitkov et al., 1995; Lapshinova-Koltunski et al., 2018), summarization (Steinberger et al., 2007), and dialog systems (Strube and Müller, 2003). To investigate the difference between PCR and the general coreference resolution task, which tries to identify not only the coreference relations between noun phrases (NP) and pronouns (P) but also potential coreference relations between noun phrases or coreference relations between pronouns, we conduct experiments with one recent breakthrough model (i.e., End-to-end model (Lee et al., 2017)) on the CoNLL-2012 shard task (Pradhan et al., 2012) under two settings: one without the gold mention and one with the gold mention. In the ‘without gold mention’ setting, models are required to first identify spans from the documents as the mentions and then predict the coreference relations among these mentions. As a comparison, if gold Introduction The question of how human beings resolve pronouns1 has long been of interest to both linguistic and natural language processing (NLP) communities, for the reason that a pronoun itself only having weak semantic meaning brings challenges to natura"
2021.crac-1.1,D10-1048,0,0.501939,"for the ordinary PCR task. We first briefly introduce conventional approaches that rely on human-designed rules or features and then introduce the end-to-end model, which is a groundbreaking model for solving coreference resolution tasks. After that, we briefly introduce a few recent improvements over the end-to-end model. 2.2.3 Further Improvements Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model5 : 2.2.1 Rule and Feature Based Methods Before the deep learning era, human-designed rules (Hobbs, 1978; Raghunathan et al., 2010), knowledge (Ponzetto and Strube, 2006; Versley et al., 2016), and features (Ng, 2005; Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, ‘he’ typically refers to males and ‘she’ typically refers to females; ‘it’ typically refers to singular objects and ‘them’ typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth"
2021.crac-1.1,N18-2108,0,0.0282393,"Missing"
2021.crac-1.1,D12-1071,0,0.115448,"t be used to train a good supervised model and can only be used as the evaluation set. Hard PCR Another 2. Definite Pronoun Resolution: hard pronoun coreference resolution dataset As aforementioned, the correct resolution of pronouns requires the inference over both linguistic knowledge and commonsense knowledge. To clearly reflect how models can resolve pronouns 8 The latest version of WSC has 284 questions, but as all the following works are evaluated based on the 273-question version, we still use the 273-question version in this survey. 5 is the definite pronoun resolution dataset (DPR)9 (Rahman and Ng, 2012). Different from WSC, DPR leveraged undergraduates rather than experts to create the dataset. In total, DPR collected 1,886 questions, which is a slightly larger scale than the official WSC. However, as DPR can not guarantee that all DPR questions follow the strict design guideline of WSC, questions in DPR are relatively simpler. knowledge. It first searched WSC questions in search engines and then used the returned searching results to solve WSC questions. SP-10K (Zhang et al., 2019a) conducted experiments to show that selectional preference (SP) knowledge such as human beings are more likely"
2021.crac-1.1,N18-2002,0,0.0279034,"Missing"
2021.crac-1.1,N18-2003,0,0.0160707,"domain knowledge becomes important. As shown in (Zhang et al., 2019c), i2b2 can be used as an additional dataset to evaluate models’ cross-domain abilities. 3. PCR for Chatbots: CIC (Chen and Choi, 2016) is a dataset focusing on identifying coreference relations in multi-party conversations. Compared with the ordinary PCR tasks, which are mostly annotated on formal textual data (e.g., newswire), identifying coreference relation in conversation is more challenging. 4. PCR for Studying Gender Bias: Nowadays, gender bias has been a hot research topic in the NLP community (Rudinger et al., 2018; Zhao et al., 2018). WinoGender (Rudinger et al., 2018) is among the most popular works. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one target pronoun and two candidate noun phrases and the models are required to select the correct antecedent from the two candidates. But the purpose is different. WSC aims at evaluating models’ abilities to understand commonsense knowledge, while WinoGender aims at evaluating how well models can predict without the influence of gender bias. The experiments show that some gender bias (e.g., ‘he’ is more likely t"
2021.crac-1.1,P03-1022,0,0.0540625,"43,883 41,741 0.690 0.667 0.754 0.768 0.707 0.763 Overall 111,452 0.705 0.742 Table 1: The performance of the End-to-end model on the CoNLL-2012 shared task coreference resolution dataset. The model’s performances of different coreference types are reported separately. task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be useful for a series of downstream tasks, such as machine translation (Mitkov et al., 1995; Lapshinova-Koltunski et al., 2018), summarization (Steinberger et al., 2007), and dialog systems (Strube and Müller, 2003). To investigate the difference between PCR and the general coreference resolution task, which tries to identify not only the coreference relations between noun phrases (NP) and pronouns (P) but also potential coreference relations between noun phrases or coreference relations between pronouns, we conduct experiments with one recent breakthrough model (i.e., End-to-end model (Lee et al., 2017)) on the CoNLL-2012 shard task (Pradhan et al., 2012) under two settings: one without the gold mention and one with the gold mention. In the ‘without gold mention’ setting, models are required to first id"
2021.crac-1.1,N16-1114,0,0.0528289,"Missing"
2021.crac-1.1,2020.emnlp-main.582,0,0.014635,"te words, cannot get rid of such noise. 2. Different from them, language model based methods represent knowledge contained in human language with an implicit approach, and thus do not have the matching issue and achieve better overall performance. 3.2.3 Fine-tuning Representation Models Last but not least, we introduce current bestperforming models on the WSC task, which finetunes pre-trained language representation models (e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019)) with a similar dataset (e.g., DPR (Rahman and Ng, 2012) or WinoGrande (Sakaguchi et al., 2020)). CorefBERT (Ye et al., 2020), in addition, introduced a new pre-training task that requires models to predict mention references. This idea of fine-tuning was originally proposed by (Kocijan et al., 2019), which first converts the original WSC task into a token prediction task and then selects the candidate with higher probability as the final prediction. In general, the stronger the language models and the larger the fine-tuning datasets are, the better the model can perform on the WSC task. 3.3 3. In general, fine-tuning pre-trained language representation models (e.g., BERT and RoBERTa) with similar datasets (e.g., DP"
2021.crac-1.1,D19-1516,1,0.868138,"Missing"
2021.crac-1.1,P19-1071,1,0.656374,"To explore solutions for that question, pronoun coreference resolution (PCR) (Hobbs, 1978) was proposed.2 As a challenging yet vital natural language understanding focus on three kinds of pronouns: third personal pronoun (e.g., she, her, he, him, them, they, it), possessive pronoun (e.g., his, hers, its, their, theirs), and demonstrative pronoun (e.g., this, that, these, those). The first and second personal pronouns are typically not considered as they often refer to the current speakers, which are normally out of the conversation or document. Besides that, conventional PCR works (Ng, 2005; Zhang et al., 2019b,c) mostly focusing on identifying coreference relations between pronouns and noun phrases rather than coreference relation between pronouns. 1 Some pronouns may refer to non-nominal antecedents. For example, the pronoun “it” in “It is too cold in the Winter here” does not refer to any real object (Kolhatkar et al., 2018). But in this survey, we only focus on pronouns that refer to nominal antecedents. 2 PCR is also known as anaphora resolution (Versley et al., 2016). Previous studies (Ng, 2005; Zhang et al., 2019c) mainly 1 Proceedings of the 4th Workshop on Computational Models of Reference"
2021.crac-1.1,N19-1093,1,0.339691,"To explore solutions for that question, pronoun coreference resolution (PCR) (Hobbs, 1978) was proposed.2 As a challenging yet vital natural language understanding focus on three kinds of pronouns: third personal pronoun (e.g., she, her, he, him, them, they, it), possessive pronoun (e.g., his, hers, its, their, theirs), and demonstrative pronoun (e.g., this, that, these, those). The first and second personal pronouns are typically not considered as they often refer to the current speakers, which are normally out of the conversation or document. Besides that, conventional PCR works (Ng, 2005; Zhang et al., 2019b,c) mostly focusing on identifying coreference relations between pronouns and noun phrases rather than coreference relation between pronouns. 1 Some pronouns may refer to non-nominal antecedents. For example, the pronoun “it” in “It is too cold in the Winter here” does not refer to any real object (Kolhatkar et al., 2018). But in this survey, we only focus on pronouns that refer to nominal antecedents. 2 PCR is also known as anaphora resolution (Versley et al., 2016). Previous studies (Ng, 2005; Zhang et al., 2019c) mainly 1 Proceedings of the 4th Workshop on Computational Models of Reference"
2021.crac-1.1,P19-1083,1,0.423924,"To explore solutions for that question, pronoun coreference resolution (PCR) (Hobbs, 1978) was proposed.2 As a challenging yet vital natural language understanding focus on three kinds of pronouns: third personal pronoun (e.g., she, her, he, him, them, they, it), possessive pronoun (e.g., his, hers, its, their, theirs), and demonstrative pronoun (e.g., this, that, these, those). The first and second personal pronouns are typically not considered as they often refer to the current speakers, which are normally out of the conversation or document. Besides that, conventional PCR works (Ng, 2005; Zhang et al., 2019b,c) mostly focusing on identifying coreference relations between pronouns and noun phrases rather than coreference relation between pronouns. 1 Some pronouns may refer to non-nominal antecedents. For example, the pronoun “it” in “It is too cold in the Winter here” does not refer to any real object (Kolhatkar et al., 2018). But in this survey, we only focus on pronouns that refer to nominal antecedents. 2 PCR is also known as anaphora resolution (Versley et al., 2016). Previous studies (Ng, 2005; Zhang et al., 2019c) mainly 1 Proceedings of the 4th Workshop on Computational Models of Reference"
2021.eacl-main.285,D14-1082,0,0.0119257,"e use a CNN model as the sentiment polarity classifier. And we pretrain it by generating pseudo labels for documents which contain userprovided keywords in the training set. The CNN architecture is the same as the one in Keyword Pretrain. VWS-PR is VWS method with proposed posterior regularization. 3.3 Keywords and Opinion Word Extraction We manually select three keywords for each class. The details of keywords of three datasets are shown in table 2. For opinion word extraction, we adopt four rules proposed by VWS (Zeng et al., 2019) in the implementation. All rules rely on dependency parser (Chen and Manning, 2014). When a target word and an opinion word satisfy a dependency relation, we will extract the opinion word. The details of dependency relation and examples are provided in Table 3. When a pair of words satisfy one rule, there are still some restrictions on head and tails words to be satisfied. There is no restriction for Rule 1. For Rule 2, the head word should be an adjective and the tail word should a noun. For Rule 3, the head word should be one of the following four words: “like,” “dislike,” “love,” and “hate.” For Rule 4, the head word should be one of the following word: “seem,” “look,” “f"
2021.eacl-main.285,kamps-etal-2004-using,0,0.276978,"Missing"
2021.eacl-main.285,P09-1028,0,0.0613557,"Missing"
2021.eacl-main.285,P02-1053,0,0.0787953,"iction variance. 1 Introduction Sentiment analysis is a task of identifying the sentiment polarity expressed in textual data (Liu, 2012). Most state-of-the-art sentiment analysis methods in the literature are supervised methods which require many labeled training data. However, human annotations in the real world are scarce. While we assume there is abundant annotated data to train more and more complex models, there is still a need to consider weakly supervised methods that require less human annotation. One way to perform weakly supervised sentiment analysis is using a predefinited lexicon (Turney, 2002; Taboada et al., 2011). A lexicon consists of many opinion words. For each opinion word, its polarity (positive or negative) and strength (the degree to which the opinion word is positive or negative) are annotated by domain experts. lexiconbased weakly supervised methods perform a dictionary lookup and assign a polarity according to all opinion words extracted from a document. A good lexicon requires high precision and high coverage, which needs a lot of human effort. Another way to do weakly supervised sentiment analysis is using limited keywords (Meng et al., 2018; Zeng et al., 2019). Comp"
2021.eacl-main.285,H05-1044,0,0.267066,"set. (3) Amazon Review: We use the Amazon reviews polarity dataset from (Zhang et al., 2015) and randomly extracted 20, 000 reviews from its original test set as the corpus for evaluation. For hyper-parameter tuning, we also extract 2, 000 documents from the original training set of (Zhang et al., 2015) to serve as a development set. Table 1 provides the details of these datasets. 3.2 Compared Methods Lexicon uses an opinion lexicon to assign sentiment polarity to a document (Read and Carroll, 2009; Pablos et al., 2015). We combine two popular opinion lexicons used by (Hu and Liu, 2004) and (Wilson et al., 2005) to get a larger lexicon. If an extracted opinion is in the positive (negative) lexicon, it votes for positive (negative). When the opinion word is with a negation word such as “no” and “not”, its polarity will be the opposite. Then, the polarity of a document is determined by using majority voting among all extracted opinion words. When the number of positive and negative words is equal, the document will be randomly assigned a polarity. WeSTClass (Meng et al., 2018) first generates pseudo labels for documents which contain userprovided keywords. Keywords are expanded to generate more pseudo"
2021.eacl-main.285,P11-1015,0,0.100781,"classes and perfectly balanced. For all methods, we use a development set for hyper-parameter tuning. For all methods, we use the training set as the test set since all methods do not use the ground truth in the training set. (1) Yelp Review: We use the Yelp reviews polarity dataset from (Zhang et al., 2015) and take its test set containing 38,000 documents as the corpus for evaluation. For hyper-parameter tuning, we also extract 3,800 documents from the original training set of (Zhang et al., 2015) to serve as a development set. (2) IMDB Review: We use the IMDB reviews polarity dataset from (Maas et al., 2011) and randomly extract 20, 000 reviews from its original test set as the corpus for evaluation. For hyperparameter tuning, we also extract 2, 000 documents from the original training set of (Maas et al., 2011) to serve as a development set. (3) Amazon Review: We use the Amazon reviews polarity dataset from (Zhang et al., 2015) and randomly extracted 20, 000 reviews from its original test set as the corpus for evaluation. For hyper-parameter tuning, we also extract 2, 000 documents from the original training set of (Zhang et al., 2015) to serve as a development set. Table 1 provides the details"
2021.eacl-main.285,Q16-1017,0,0.0239059,"nction in Eq. (3) requires the summation over all opinion words in the vocabulary. Since the size of the opinion word vocabulary is large, VWS uses the negative sampling technique (Mikolov et al., 2013) to approximate Eq. (3). Specifically, VWS approximates p(wo |c) in the objective (3) with the following objective function: X   log σ ϕ(wo , c) + log 1 − σ ϕ(wo0 , c) , wo0 ∈N (6) where is a negative sample in opinion words vocabulary, N is the set of negative samples and σ(·) is the sigmoid function. In order to ensure that the approximation part and the entropy term are on the same scale (Marcheggiani and Titov, 2016), a hyper-parameter α is added to the entropy term. The objective function becomes: X X   L2 = Eq(C|x) log σ ϕ(wo , c) wo0 x∈X wo ∈Px x∈X wo ∈Px = X X x∈X wo ∈Px log X + p(wo , c) + X + x∈X wo ∈Px H(q(C|x)), X X αH(q(C|x)). (7) x∈X wo ∈Px 2.2 x∈X wo ∈Px X X   log 1 − σ ϕ(wo0 , c) + log p(c) wo0 ∈N c h p(w , c) i o = log q(c|x) q(c|x) c x∈X wo ∈Px h X X X p(wo , c) i ≥ q(c|x) log q(c|x) x∈X wo ∈Px c X X   = Eq(C|x) log p(wo |c)p(c) X X X (5) Posterior Regularization As pointed out by (Ganchev et al., 2010), controlling the posterior distribution is crucial for models that estimate poster"
2021.eacl-main.285,2020.emnlp-main.724,0,0.0389591,"Missing"
2021.eacl-main.285,N19-1036,1,0.915892,"ed lexicon (Turney, 2002; Taboada et al., 2011). A lexicon consists of many opinion words. For each opinion word, its polarity (positive or negative) and strength (the degree to which the opinion word is positive or negative) are annotated by domain experts. lexiconbased weakly supervised methods perform a dictionary lookup and assign a polarity according to all opinion words extracted from a document. A good lexicon requires high precision and high coverage, which needs a lot of human effort. Another way to do weakly supervised sentiment analysis is using limited keywords (Meng et al., 2018; Zeng et al., 2019). Compared with lexiconbased methods, user-provided keywords require less human effort. Among keyword-based methods, there are two directions. First, (Meng et al., 2018) leveraged limited keywords to expand more keywords and generate pseudo-labeled data, and then performed self-training on real unlabeled data for model refinement. Possible improvements of this direction include investigating more advanced keywords expansion techniques to generate better pseudo-labeled samples (Miller et al., 2012) and developing more advanced self-training algorithms(Coden et al., 2014). Second, the Variationa"
2021.eacl-main.285,C14-1126,0,0.0387913,"Missing"
2021.eacl-main.285,C12-1109,0,0.0179928,"nother way to do weakly supervised sentiment analysis is using limited keywords (Meng et al., 2018; Zeng et al., 2019). Compared with lexiconbased methods, user-provided keywords require less human effort. Among keyword-based methods, there are two directions. First, (Meng et al., 2018) leveraged limited keywords to expand more keywords and generate pseudo-labeled data, and then performed self-training on real unlabeled data for model refinement. Possible improvements of this direction include investigating more advanced keywords expansion techniques to generate better pseudo-labeled samples (Miller et al., 2012) and developing more advanced self-training algorithms(Coden et al., 2014). Second, the Variational Weakly Supervised (VWS) sentiment analysis (Zeng et al., 2019) used target-opinion word pairs as supervision signal. Its objective function is to predict an opinion word given a target word. For example, in a sentence “the room is big,” “room” is a target word and “big” is an opinion word. By introducing a latent variable (the sentiment polarity), they can learn a wellapproximated posterior distribution via optimizing the evidence lower bound. The posterior probability here is the probability of"
2021.eacl-main.285,S15-2121,0,0.0193163,"000 documents from the original training set of (Maas et al., 2011) to serve as a development set. (3) Amazon Review: We use the Amazon reviews polarity dataset from (Zhang et al., 2015) and randomly extracted 20, 000 reviews from its original test set as the corpus for evaluation. For hyper-parameter tuning, we also extract 2, 000 documents from the original training set of (Zhang et al., 2015) to serve as a development set. Table 1 provides the details of these datasets. 3.2 Compared Methods Lexicon uses an opinion lexicon to assign sentiment polarity to a document (Read and Carroll, 2009; Pablos et al., 2015). We combine two popular opinion lexicons used by (Hu and Liu, 2004) and (Wilson et al., 2005) to get a larger lexicon. If an extracted opinion is in the positive (negative) lexicon, it votes for positive (negative). When the opinion word is with a negation word such as “no” and “not”, its polarity will be the opposite. Then, the polarity of a document is determined by using majority voting among all extracted opinion words. When the number of positive and negative words is equal, the document will be randomly assigned a polarity. WeSTClass (Meng et al., 2018) first generates pseudo labels for"
2021.eacl-main.285,J11-2001,0,0.122144,"e. 1 Introduction Sentiment analysis is a task of identifying the sentiment polarity expressed in textual data (Liu, 2012). Most state-of-the-art sentiment analysis methods in the literature are supervised methods which require many labeled training data. However, human annotations in the real world are scarce. While we assume there is abundant annotated data to train more and more complex models, there is still a need to consider weakly supervised methods that require less human annotation. One way to perform weakly supervised sentiment analysis is using a predefinited lexicon (Turney, 2002; Taboada et al., 2011). A lexicon consists of many opinion words. For each opinion word, its polarity (positive or negative) and strength (the degree to which the opinion word is positive or negative) are annotated by domain experts. lexiconbased weakly supervised methods perform a dictionary lookup and assign a polarity according to all opinion words extracted from a document. A good lexicon requires high precision and high coverage, which needs a lot of human effort. Another way to do weakly supervised sentiment analysis is using limited keywords (Meng et al., 2018; Zeng et al., 2019). Compared with lexiconbased"
2021.eacl-main.43,K17-1023,0,0.031182,"Missing"
2021.eacl-main.43,W16-3612,0,0.039122,"Missing"
2021.eacl-main.43,D13-1029,0,0.0263181,"rity of the mentions are pronouns. In the experiments, we have demonstrated that when the local context is not enough, the richer context information provided by the coreference clusters could be very helpful for linking mentions to the correct characters. In the NLP community, people have long been thinking that the coreference resolution task and entity linking should be able to help each other. For example, Ratinov and Roth (2012) show how to use knowledge from named-entity linking to improve the coreference resolution, but do not consider doing it in a joint learning approach. After that, Hajishirzi et al. (2013) demonstrate that the coreference resolution and entity linking are complementary in terms of reducing the errors in both tasks. Motivated by these observations, a joint model for coreference, typing, and linking is proposed (Durrett and Klein, 2014) to improve the performance on three tasks at the same time. Compared with previous works, the main contributions of this paper are two-fold: (1) we tackle the challenging character linking problem; (2) we design a novel mention representation encoding method, which has been shown effective on both the coreference resolution and character linking t"
2021.eacl-main.43,D19-1588,0,0.165227,"e introduce the proposed C2 framework, which is illustrated in Figure 3. With the conversation and all mentions as input, we first encode them with a shared mention representation encoder module, which includes a pre-trained transformer text encoder and a mention-level selfattention (MLSA) module. After that, we make predictions for both tasks via two separate modules. In the end, a joint loss function is devised so that the model can be effectively trained on both tasks simultaneously. Details are as follows. 3.1 Mention Representation We use pre-trained language models (Devlin et al., 2018; Joshi et al., 2019a) to obtain the contextualized representations for mentions. As speaker information is critical for the conversation understanding, we also include that information by appending speaker embeddings to each mention. As 540 1 The performance on the coreference resolution is evaluated based on the average F1 score of B3, CEAFφ 4, and BLANC. The performance on the character linking task is evaluated by the average F1 score of the micro and macro F1. Linking Coreference ? where k indicates the number of mentions in a document, and the g (i) means the mention representation from the i-th layer of ML"
2021.eacl-main.43,P19-1066,0,0.0378603,"Missing"
2021.eacl-main.43,N19-1093,1,0.849173,"in utterance 6. In this case, our model performs better mainly because it can use the information gathered from the nearby co-referents to adjust its linking prediction, as its nearby co-referents are correctly linked to corresponding entities. Error Analysis We use the example in Figure 6 to emphasize the error analysis that compares the performance of our model and the baseline models. The details are as 6 Related Works Coreference resolution is the task of grouping mentions to clusters such that all the mentions in the same cluster refer to the same real-world entity (Pradhan et al., 2012; Zhang et al., 2019a,b; Yu et al., 2019). With the help of higher-order coreference resolution mechanism (Lee et al., 2018) and strong pre-trained language models (e.g., SpanBERT (Joshi et al., 2019b)), the end-to-end based coreference resolution systems have been achieving impressive performance on the standard evaluation dataset (Pradhan et al., 2012). Recently, motivated by the success of the transfer learning, Wu et al. (2020) propose to model the coreference resolution task as a question answering problem. Through the careful fine-tuning on a high-quality QA dataset (i.e., SQUAD-2.0 (Rajpurkar et al., 2018)"
2021.eacl-main.43,N18-2108,0,0.143165,"ingle piece of information about its referent, e.g., “person is a male” or “the person works with Monica.” Given the coreference cluster, the mentions refer to the same person, and the pieces of information are put together to jointly determining the referent. As a result, it is easier for a model to do character linking with resolved coreference. Similar observations are also made in (Chen et al., 2017). At the same time, we also noticed that coreference resolution, especially those involving pronouns, is also not trivial. As shown by the recent literature on the coreference resolution task (Lee et al., 2018; Kantor and Globerson, 2019), the task is still challenging for current models and the key challenge is how to utilize the global information about entities. And that is exactly what the character linking model can provide. For example, in Figure 2, it is difficult for a coreference model to correctly resolve the last mention he in the utterance given by Ross based on its local context, because another major male character (Joey) joins the conversation, which can distract and mislead the coreference model. However, if the model knows the mention he links to the character Paul and Paul works w"
2021.eacl-main.43,P19-1083,1,0.84302,"in utterance 6. In this case, our model performs better mainly because it can use the information gathered from the nearby co-referents to adjust its linking prediction, as its nearby co-referents are correctly linked to corresponding entities. Error Analysis We use the example in Figure 6 to emphasize the error analysis that compares the performance of our model and the baseline models. The details are as 6 Related Works Coreference resolution is the task of grouping mentions to clusters such that all the mentions in the same cluster refer to the same real-world entity (Pradhan et al., 2012; Zhang et al., 2019a,b; Yu et al., 2019). With the help of higher-order coreference resolution mechanism (Lee et al., 2018) and strong pre-trained language models (e.g., SpanBERT (Joshi et al., 2019b)), the end-to-end based coreference resolution systems have been achieving impressive performance on the standard evaluation dataset (Pradhan et al., 2012). Recently, motivated by the success of the transfer learning, Wu et al. (2020) propose to model the coreference resolution task as a question answering problem. Through the careful fine-tuning on a high-quality QA dataset (i.e., SQUAD-2.0 (Rajpurkar et al., 2018)"
2021.eacl-main.43,C18-1003,0,0.210153,"ich has n layers of transformer encoder structure (Vaswani et al., 2017) and is denoted as T . Formally, this iterative mention refinement process can be described by (i+1) g1 (i+1) , ..., gk (i) (i) = T (g1 , ..., gk ), (2) (4) (5) (n) sa (i, j) = F F N Na ([gi , gj ]), a result, the initial representation of mention i is: gi (3) (6) where g (n) stands for the last layer mention representation resulted from the MLSA and F F N N indicates the feed-forward neural network. 3.3 Character Linking The character linking is formulated as a multi-class classification problem, following previous work (Zhou and Choi, 2018). Given the mention representations g (n) , the linking can be done with a simple feed-forward network, denoted as F F N N (·). Specifically, the probability of character entity zi is linked with a given mention i can be calculated by: (n) Q(zi ) = Sof tmax(F F N Nl (gi ))zi , (7) where the notation (.)z represents the z-th composition of a given vector. 3.4 Joint Learning To jointly optimize both coreference resolution and entity linking, we design a joint loss of both tasks. For coreference resolution, given the gold clusters, we minimize the negative log-likelihood of 541 DATASET E PISODES"
2021.eacl-main.43,W12-4501,0,0.574783,"uses the mention and mention-cluster embeddings as input to do character linking (Zhou and Choi, 2018). • BERT/SpanBERT: A text-span classification model consists of a transformer text encoder followed by a feed-forward network. 4.3 5 Results and Analysis In this section, we discuss the experimental results and present a detailed analysis. Evaluation Metrics We follow the previous work (Zhou and Choi, 2018) for the evaluation metrics. Specifically, for coreference resolution, three evaluation metrics, B3 , CEAFφ4 , and BLANC, are used. The metrics are all proposed by the CoNNL’12 shared task (Pradhan et al., 2012) to evaluate the output coreference cluster against the gold clusters. We follow Zhou and Choi (2018) to use BLANC (Recasens and Hovy, 2011) to replace MUC (Vilain et al., 1995) because BLANC takes singletons into consideration but MUC does not. As for the character linking task, we use the Micro and Macro F1 scores to evaluate the multi-class classification performance. 4.4 and we use n = 2 layers of the mention-level self-attention (MLSA). The feed-forward networks are implemented by two fully connected layers with ReLU activations. Following the previous work, (Zhou and Choi, 2018), the sce"
2021.eacl-main.43,P18-2124,0,0.0333548,"ERT: A text-span classification model consists of a transformer text encoder followed by a feed-forward network. 4.3 5 Results and Analysis In this section, we discuss the experimental results and present a detailed analysis. Evaluation Metrics We follow the previous work (Zhou and Choi, 2018) for the evaluation metrics. Specifically, for coreference resolution, three evaluation metrics, B3 , CEAFφ4 , and BLANC, are used. The metrics are all proposed by the CoNNL’12 shared task (Pradhan et al., 2012) to evaluate the output coreference cluster against the gold clusters. We follow Zhou and Choi (2018) to use BLANC (Recasens and Hovy, 2011) to replace MUC (Vilain et al., 1995) because BLANC takes singletons into consideration but MUC does not. As for the character linking task, we use the Micro and Macro F1 scores to evaluate the multi-class classification performance. 4.4 and we use n = 2 layers of the mention-level self-attention (MLSA). The feed-forward networks are implemented by two fully connected layers with ReLU activations. Following the previous work, (Zhou and Choi, 2018), the scene-level setting is used, where, each scene is regarded as a document for coreference resolution and"
2021.eacl-main.43,D12-1113,0,0.0226107,"the mentions are named entities and the main challenge is the disambiguation. However, as a special case of the entity linking, the character linking task has its challenge that the majority of the mentions are pronouns. In the experiments, we have demonstrated that when the local context is not enough, the richer context information provided by the coreference clusters could be very helpful for linking mentions to the correct characters. In the NLP community, people have long been thinking that the coreference resolution task and entity linking should be able to help each other. For example, Ratinov and Roth (2012) show how to use knowledge from named-entity linking to improve the coreference resolution, but do not consider doing it in a joint learning approach. After that, Hajishirzi et al. (2013) demonstrate that the coreference resolution and entity linking are complementary in terms of reducing the errors in both tasks. Motivated by these observations, a joint model for coreference, typing, and linking is proposed (Durrett and Klein, 2014) to improve the performance on three tasks at the same time. Compared with previous works, the main contributions of this paper are two-fold: (1) we tackle the cha"
2021.eacl-main.43,M95-1005,0,0.236607,"r text encoder followed by a feed-forward network. 4.3 5 Results and Analysis In this section, we discuss the experimental results and present a detailed analysis. Evaluation Metrics We follow the previous work (Zhou and Choi, 2018) for the evaluation metrics. Specifically, for coreference resolution, three evaluation metrics, B3 , CEAFφ4 , and BLANC, are used. The metrics are all proposed by the CoNNL’12 shared task (Pradhan et al., 2012) to evaluate the output coreference cluster against the gold clusters. We follow Zhou and Choi (2018) to use BLANC (Recasens and Hovy, 2011) to replace MUC (Vilain et al., 1995) because BLANC takes singletons into consideration but MUC does not. As for the character linking task, we use the Micro and Macro F1 scores to evaluate the multi-class classification performance. 4.4 and we use n = 2 layers of the mention-level self-attention (MLSA). The feed-forward networks are implemented by two fully connected layers with ReLU activations. Following the previous work, (Zhou and Choi, 2018), the scene-level setting is used, where, each scene is regarded as a document for coreference resolution and linking. During the training, each mini-batch consists of segments obtained"
2021.eacl-main.43,2020.acl-main.622,0,0.115838,"e previous work (Zhou and Choi, 2018). For the coreference resolution task, we compare with the following methods. https://github.com/emorynlp/character-identification 542 • ACNN: A CNN-based model (Zhou and Choi, 2018) coreference resolution model that can also produce the mention and mention-cluster embeddings at the same time. • C2F: The end-to-end coarse-to-fine coreference model (Joshi et al., 2019b) with BERT (Devlin et al., 2018) or SpanBERT (Joshi et al., 2019a) as the encoder. • CorefQA: An approach that reformulates the coreference resolution problem as a question answering problem (Wu et al., 2020) and being able to be benefited from fine-tuned question-answer text encoders. P REC . B3 R EC . F1 P REC . CEAFφ4 R EC . F1 P REC . BLANC R EC . F1 ACNN C OREF QA (S PAN BERT-L ARGE ) 84.30 73.72 71.90 75.55 77.60 74.62 54.50 65.82 71.80 72.38 62.00 68.94 84.30 86.82 80.40 84.69 82.10 85.75 73.96 (0.97) 76.44 (0.20) C2F (BERT-BASE ) C2F (BERT-L ARGE ) C2F (S PAN BERT-BASE ) C2F (S PAN BERT-L ARGE ) 69.62 71.72 72.49 81.93 76.11 80.25 77.88 84.38 72.72 75.75 75.08 82.57 66.44 69.97 66.00 78.04 60.92 62.61 64.23 71.99 63.56 66.08 65.10 74.89 79.38 81.65 81.60 88.15 86.05 88.23 87.43 91.09 82.38"
2021.eacl-main.43,D19-1516,1,0.848194,"case, our model performs better mainly because it can use the information gathered from the nearby co-referents to adjust its linking prediction, as its nearby co-referents are correctly linked to corresponding entities. Error Analysis We use the example in Figure 6 to emphasize the error analysis that compares the performance of our model and the baseline models. The details are as 6 Related Works Coreference resolution is the task of grouping mentions to clusters such that all the mentions in the same cluster refer to the same real-world entity (Pradhan et al., 2012; Zhang et al., 2019a,b; Yu et al., 2019). With the help of higher-order coreference resolution mechanism (Lee et al., 2018) and strong pre-trained language models (e.g., SpanBERT (Joshi et al., 2019b)), the end-to-end based coreference resolution systems have been achieving impressive performance on the standard evaluation dataset (Pradhan et al., 2012). Recently, motivated by the success of the transfer learning, Wu et al. (2020) propose to model the coreference resolution task as a question answering problem. Through the careful fine-tuning on a high-quality QA dataset (i.e., SQUAD-2.0 (Rajpurkar et al., 2018)), it achieves the st"
2021.emnlp-main.311,W18-0701,0,0.0583601,"Missing"
2021.emnlp-main.705,2020.emnlp-main.370,0,0.60299,"Instead of directly treating the random samples as negative, solid human annotations are needed to provide hard labels for commonsense triples. Second, the human evaluation in the original paper of CSKB population (Fang et al., 2021) cannot be generally used for benchmarking. They first populate the CSKB and then asked human annotators to annotate a small subset to check whether the populated results are accurate or not. A better benchmark should be based on random samples from all candidates and the scale should be large enough to cover diverse events and states. et al., 2020), and GLUCOSE (Mostafazadeh et al., 2020), are used to constitute the commonsense relations. We align the CSKBs together into the same format and ground them to a large-scale eventuality (including activity, state, and event) knowledge graph, ASER (Zhang et al., 2020, 2021). Then, instead of annotating every possible node pair in the graph, which takes an infeasible O(|V |2 ) amount of annotation, we sample a large subset of candidate edges grounded in ASER to annotate. In total, 31.7K high-quality triples are annotated as the development set and test set. To evaluate the commonsense reasoning ability of machine learning models based"
2021.emnlp-main.705,D19-1250,0,0.0660429,"Missing"
2021.emnlp-main.705,N13-1008,0,0.0420204,"n proposed (Roth and Yih, 2002; eventuality (including activity, state, and event) knowledge graph extracted from texts, denoted as Chan and Roth, 2010; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., G = (V, E), where V is the set of all vertices and E is the set of edges. G c is the graph acquired by 2012; Lin et al., 2016; Zeng et al., 2017). Universal aligning C and G into the same format. The goal schema and matrix factorization can also be used of CSKB population is to learn a scoring function to learn latent features of databases and perform population (Riedel et al., 2013; Verga et al., 2016; given a candidate triple (h, r, t), where plausible commonsense triples should be scored higher. The Toutanova et al., 2015; McCallum et al., 2017). training of CSKB population can inherit the setting Besides completion tasks on conventional entityof triple classification, where ground truth examcentric KBs like Freebase (Bollacker et al., 2008), ples are from the CSKB C and negative triples are completion tasks on CSKBs are also studied on randomly sampled. In the evaluation phase, the ConceptNet and ATOMIC. Bi-linear models are model is required to score the triples fro"
2021.emnlp-main.705,N16-1103,0,0.0234887,"Yih, 2002; eventuality (including activity, state, and event) knowledge graph extracted from texts, denoted as Chan and Roth, 2010; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., G = (V, E), where V is the set of all vertices and E is the set of edges. G c is the graph acquired by 2012; Lin et al., 2016; Zeng et al., 2017). Universal aligning C and G into the same format. The goal schema and matrix factorization can also be used of CSKB population is to learn a scoring function to learn latent features of databases and perform population (Riedel et al., 2013; Verga et al., 2016; given a candidate triple (h, r, t), where plausible commonsense triples should be scored higher. The Toutanova et al., 2015; McCallum et al., 2017). training of CSKB population can inherit the setting Besides completion tasks on conventional entityof triple classification, where ground truth examcentric KBs like Freebase (Bollacker et al., 2008), ples are from the CSKB C and negative triples are completion tasks on CSKBs are also studied on randomly sampled. In the evaluation phase, the ConceptNet and ATOMIC. Bi-linear models are model is required to score the triples from G that used to con"
2021.emnlp-main.705,D17-1186,0,0.0245112,"t ∈ T }, where H, R, and and slot filling (Surdeanu and Ji, 2014) for con- T are the set of the commonsense heads, relations, ventional KBs, and many relation extraction ap- and tails. Suppose we have another much larger proaches have been proposed (Roth and Yih, 2002; eventuality (including activity, state, and event) knowledge graph extracted from texts, denoted as Chan and Roth, 2010; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., G = (V, E), where V is the set of all vertices and E is the set of edges. G c is the graph acquired by 2012; Lin et al., 2016; Zeng et al., 2017). Universal aligning C and G into the same format. The goal schema and matrix factorization can also be used of CSKB population is to learn a scoring function to learn latent features of databases and perform population (Riedel et al., 2013; Verga et al., 2016; given a candidate triple (h, r, t), where plausible commonsense triples should be scored higher. The Toutanova et al., 2015; McCallum et al., 2017). training of CSKB population can inherit the setting Besides completion tasks on conventional entityof triple classification, where ground truth examcentric KBs like Freebase (Bollacker et a"
D16-1038,W99-0201,0,0.402049,"present events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-re"
D16-1038,P10-1143,0,0.037571,"rison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009) is an early work based on supervised learning. We also report HDP-Coref results as an unsupervised baseline (Bejan and Harabagiu, 2010), which utilizes nonparametric Bayesian models. Moreover, we create another unsupervised event co-reference baseline (Type+SharedMen): we treat events of the same type which share at least one co-referent entity (inside event arguments) as coreferred. On TAC-KBP corpus, we report results from the top ranking system of the TAC-2015 Event Nugget Evaluation Task as TAC-TOP. We name our event mention detection module in MSEP similarity-based event mention detection MSEP-EMD system. For event co-reference, the proposed similarity based co-reference detection MSEP-Coref method has a number of variat"
D16-1038,D08-1031,1,0.422316,"Thus, when we encounter missing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We als"
D16-1038,P15-2061,0,0.0576145,"vector representation is illustrated in Fig. 2. If there are missing event arguments, we set the corresponding vector to be “NIL” (we set each position as “NaN”). We also augment the event vector representation by concatenating more text fragments to enhance the interactions between the action and other arguments, as shown in Fig. 3. Essentially, we flatten the event structure to preserve the alignment of event arguments so that the structured information can be reflected in our vector space. 2.2 Event Mention Detection Motivated by the seed-based event trigger labeling technique employed in Bronstein et al. (2015), we turn to ACE annotation guidelines for event examples described under each event type label. For instance, the ACE-2005 guidelines list the example “Mary Smith joined Foo Corp. in June 1998.” for label “START-POSITION”. Altogether, we collect 172 event examples from 33 event types (5 each on average).4 We can then get vector representations for these example events following the procedures in Sec. 2.1. We define the event type representation as the numerical average of all vector representations corresponding to example events under that type. We use the similarity between an event candida"
D16-1038,J92-4003,0,0.527401,"verted index for each word to search the Wikipedia corpus. The text fragment representation is thus a weighted combination of the concept vectors corresponding to its words. We use the same setting as in Chang et al. (2008) to filter out pages with fewer than 100 words and those containing fewer than 5 hyperlinks. To balance between the effectiveness of ESA representations and its cost, we use the 200 concepts with the highest weights. Thus, we convert each text fragment to a very sparse vector of millions of dimensions (but we just store 200 non-zero values). Brown Cluster BC was proposed by Brown et al. (1992) as a way to support abstraction in NLP tasks, measuring words’ distributional similarities. This method generates a hierarchical tree of word clusters by evaluating the word co-occurrence based on a n-gram model. Then, paths traced from root to 396 leaves can be used as word representations. We use the implementation by Song and Roth (2014), generated over the latest Wikipedia dump. We set the maximum tree depth to 20, and use a combination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use"
D16-1038,W09-3208,0,0.0573058,"rge body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how w"
D16-1038,W09-4303,0,0.0956789,"CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009) is an early work based on supervised learning. We also report HDP-Coref results as an unsupervised baseline (Bejan and Harabagiu, 2010), which utilizes nonparametric Bayesian models. Moreover, we create another unsupervised event co-reference baseline (Type+SharedMen): we treat events of the same type which share at least one co-referent entity (inside event arguments) as coreferred. On TAC-KBP corpus, we report results from the top ranking system of the TAC-2015 Event Nugget Evaluation Task as TAC-TOP. We name our event mention detection module in MSEP similarity-based event mention detectio"
D16-1038,P15-1017,0,0.241504,"the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from the text and then average, element-wise, all remaining word vectors to produce the resulting vector representation of the text fragment. 4 Experiments 4.1 Datasets ACE The ACE-2005 English corpus (NIST, 2005) contains fine-grained event annotations, including event trigger, argument, entity, and time-stamp annotations. We select 40 documents from newswire articles for event detection evaluation and the rest for training (same as Chen et al. (2015)). We do 10fold cross-validation for event co-reference. TAC-KBP The TAC-KBP-2015 corpus is annotated with event nuggets that fall into 38 types and coreference relations between events. 7 We use the train/test data split provided by the official TAC6 https://levyomer.wordpress.com/2014/04/25/dependencybased-word-embeddings 7 The event ontology of TAC-KBP (based on ERE annotation) is almost the same to that of ACE. To adapt our system to the TAC-KBP corpus, we use all ACE event seeds of “Contact.Phone-Write” for “Contact.Correspondence” and separate ACE event seeds of “Movement.Transport” into"
D16-1038,D13-1184,1,0.762027,"e use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We also augment the argument type set with NER labels: PER (person) and ORG (organization). We add either of the NER labels if we detect such a named entity. For each pair of events, we check event arguments agentsub and agentobj respectively. If none of the types for the aligned event ar"
D16-1038,D12-1062,1,0.747638,"in research on events, including the dif400 ficulty to annotate events and their relations. At the heart of our approach is the design of structured vector representations for events which, as we show, supports a good level of generalization within and across domains. The resulting approach outperforms state-of-art supervised methods on some of the key metrics, and adapts significantly better to a new domain. One of the key research directions is to extend this unsupervised approach to a range of other relations among events, including temporal and causality relations, as is (Do et al., 2011; Do et al., 2012). Acknowledgments The authors would like to thank Eric Horn for comments that helped to improve this work. This material is based on research sponsored by the US Defense Advanced Research Projects Agency (DARPA) under agreements FA8750-13-2-000 and HR0011-15-2-0025. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or imp"
D16-1038,P13-2083,0,0.0206993,"using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed"
D16-1038,P11-1113,0,0.0283793,"ANC — 74.0 75.1 68.7 72.9 71.7 72.3 72.8 72.5 73.1 73.5 AVG 75.7 74.4 75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity co"
D16-1038,W13-1203,0,0.197967,"er Science and Engineering, Hong Kong University of Science and Technology 1 {hpeng7,danr}@illinois.edu, 2 yqsong@cse.ust.hk Abstract a minimum, determining whether two snippets of text represent the same event or not – the event coreference problem. Events have been studied for years, but they still remain a key challenge. One reason is that the frame-based structure of events necessitates addressing multiple coupled problems that are not easy to study in isolation. Perhaps an even more fundamental difficulty is that it is not clear whether our current set of events’ definitions is adequate (Hovy et al., 2013). Thus, given the complexity and fundamental difficulties, the current evaluation methodology in this area focuses on a limited domain of events, e.g. 33 types in ACE 2005 (NIST, 2005) and 38 types in TAC KBP (Mitamura et al., 2015). Consequently, this allows researchers to train supervised systems that are tailored to these sets of events and that overfit the small domain covered in the annotated data, rather than address the realistic problem of understanding events in text. An important aspect of natural language understanding involves recognizing and categorizing events and the relations a"
D16-1038,E12-1029,0,0.0303396,"7 72.9 71.7 72.3 72.8 72.5 73.1 73.5 AVG 75.7 74.4 75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work foll"
D16-1038,P16-1025,0,0.0528716,"y model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how well their system performed in each of these two cases. Huang et al. (2016) also considered ACE (Cross-Validation) SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC 47.1 42.1 40.2 B3 59.9 60.3 58.6 CEAFe 58.7 59.0 57.4 BLANC 44.4 44.1 43.8 AVG 52.5 51.4 50.0 TAC-KBP (Test Data) TAC-TOP SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC — 34.9 33.1 30.2 B3 — 44.2 44.6 43.9 CEAFe — 39.6 39.7 38.7 BLANC — 37.1 36.8 35.7 AVG 39.1 39.0 38.5 37.1 Table 6: Event Co-reference End-To-End Results. Train Event Detection In Domain NW Out of Domain DF In Domain DF Out of Domain NW Event Co-referenc"
D16-1038,W97-1311,0,0.0998694,"mantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy et al. (2013). All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference. Early works (Humphreys et al., 1997; Bagga and Baldwin, 1999) performed event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and"
D16-1038,P08-1030,0,0.187651,"equal number of documents. When trained on NW and tested on DF, supervised methods encounter out-of-domain situations. However, the MSEP system can adapt well.12 Table 7 shows that MSEP outperforms supervised methods in out-of-domain situations for both tasks. The differences are statistically significant with p < 0.05. 5 Related Work Event detection has been studied mainly in the newswire domain as the task of detecting event triggers and determining event types and arguments. Most earlier work has taken a pipeline approach where local classifiers identify triggers first, and then arguments (Ji and Grishman, 2008; Liao and 12 Note that the supervised method needs to be re-trained and its parameters re-tuned while MSEP does not need training and its cut-off threshold is fixed ahead of time using event examples. ACE (Cross-Validation) Graph Joint Supervised SupervisedBase SupervisedExtend Type+SharedMen Unsupervised HDP-Coref MSEP-CorefESA MSEP-CorefBC MSEP-CorefW2V MSEP MSEP-CorefDEP MSEP-CorefESA+AUG MSEP-CorefESA+AUG+KNOW MSEP-CorefESA+AUG+KNOW (GA) MUC — 74.8 73.6 74.9 59.1 — 65.9 65.0 65.1 65.9 67.4 68.0 68.8 B3 — 92.2 91.6 92.8 83.2 83.8 91.5 89.8 90.1 92.3 92.6 92.9 92.5 CEAFe 84.5 87.0 85.9 87.1"
D16-1038,D12-1045,0,0.153158,"d event co-reference on scenario spe399 cific events. Both Naughton (2009) and Elkhlifi and Faiz (2009) worked on sentence-level co-reference, which is closer to the definition of Danlos and Gaiffe (2003). Pradhan et al. (2007) dealt with both entity and event coreference by taking a three-layer approach. Chen and Ji (2009) proposed a clustering algorithm using a maximum entropy model with a range of features. Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference. Lee et al. (2012) formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference. More recently, Hovy et al. (2013) presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how well their system performed in each of these two cases. Huang et al. (2016) also considered ACE (Cross-Validation) SSED + SupervisedExtend SSED + MSEP-CorefESA+AUG+KNOW MSEP-EMD + MSEP-CorefESA+AUG+KNOW MUC 47.1 42.1 40.2 B3 59.9 60.3 58.6 CEAFe 58.7 59.0 57.4 BLANC 44.4 44.1 43.8 AVG 52.5 51"
D16-1038,P14-2050,0,0.0137596,"mbination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use the skip-gram tool by Mikolov et al. (2013) over the latest Wikipedia dump, resulting in word vectors of dimensionality 200. Dependency-Based Embedding DEP is the generalization of the skip-gram model with negative sampling to include arbitrary contexts. In particular, it deals with dependency-based contexts, and produces markedly different embeddings. DEP exhibits more functional similarity than the original skip-gram embeddings (Levy and Goldberg, 2014). We directly use the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from the text and then average, element-wise, all remaining word vectors to produce the resulting vector representation of the text fragment. 4 Experiments 4.1 Datasets ACE The ACE-2005 English corpus (NIST, 2005) contains fine-grained event annotations, including event trigger, argument, entity, and time-stamp annotations. We select 40 documents from newswire articles for event detection evaluation and the rest for"
D16-1038,P13-1008,0,0.0905247,"75.5 68.1 72.9 71.7 71.9 72.8 73.3 73.8 73.9 Table 5: Event Co-reference Results on Gold Event Triggers. “MSEP-CorefESA,BC,W2V,DEP ” are variations of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Dependency Embedding representations respectively. “MSEP-CorefESA+AUG ” uses augmented ESA event vector representation and “MSEP-CorefESA+AUG+KNOW ” applies knowledge to detect conflicting events. (GA) means that we use gold event arguments instead of approximated ones from SRL. Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012a; Huang and Riloff, 2012b). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (Goyal et al., 2013). A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words. In our work, we preserve event structure via structured vector representations constructed from event components. Event co-reference is much less studied in comparison to the large body of work on entity coreference. Our work follows the event co-reference definition in Hovy"
D16-1038,P10-1081,0,0.0389618,"Missing"
D16-1038,W15-0807,0,0.0469874,"respectively. We use SSED and SupervisedBase as the supervised modules for comparison. For event detection, we compare F1 scores of span plus type match while we report the average F1 scores for event co-reference. the problem of event clustering. They represented event structures based on AMR (Abstract Meaning Representation) and distributional semantics, and further generated event schemas composing event triggers and argument roles. Recently, TAC has organized Event Nugget Detection and Co-reference Evaluations, resulting in interesting works, some of which contributed to our comparisons (Liu et al., 2015; Mitamura et al., 2015; Hsi et al., 2015; Sammons et al., 2015). 6 Conclusion This paper proposes a novel event detection and co-reference approach with minimal supervision, addressing some of the key issues slowing down progress in research on events, including the dif400 ficulty to annotate events and their relations. At the heart of our approach is the design of structured vector representations for events which, as we show, supports a good level of generalization within and across domains. The resulting approach outperforms state-of-art supervised methods on some of the key metrics, and a"
D16-1038,H05-1004,0,0.0920777,"ng. 4.2 use augmented ESA vector representation (AUG)8 , and whether we use knowledge during co-reference inference (KNOW). We also develop a supervised event co-reference system following the work of Sammons et al. (2015), namely SupervisedBase . We also add additional event vector representations9 as features to this supervised system and get SupervisedExtend . 4.3 For event detection, we use standard precision, recall and F1 metrics. For event co-reference, we compare all systems using standard F1 metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determine the event type. For event co-reference, Joint (Chen et al., 2009)"
D16-1038,N13-1090,0,0.0185368,"straction in NLP tasks, measuring words’ distributional similarities. This method generates a hierarchical tree of word clusters by evaluating the word co-occurrence based on a n-gram model. Then, paths traced from root to 396 leaves can be used as word representations. We use the implementation by Song and Roth (2014), generated over the latest Wikipedia dump. We set the maximum tree depth to 20, and use a combination of path prefixes of length 4,6 and 10 as our BC representation. Thus, we convert each word to a vector of 24 + 26 + 210 = 1104 dimensions. Word2Vec We use the skip-gram tool by Mikolov et al. (2013) over the latest Wikipedia dump, resulting in word vectors of dimensionality 200. Dependency-Based Embedding DEP is the generalization of the skip-gram model with negative sampling to include arbitrary contexts. In particular, it deals with dependency-based contexts, and produces markedly different embeddings. DEP exhibits more functional similarity than the original skip-gram embeddings (Levy and Goldberg, 2014). We directly use the released 300-dimension word embeddings6 . Note that it is straightforward text-vector conversion for ESA. But for BC, W2V and DEP, we first remove stop words from"
D16-1038,W15-0809,0,0.0436271,"t coreference problem. Events have been studied for years, but they still remain a key challenge. One reason is that the frame-based structure of events necessitates addressing multiple coupled problems that are not easy to study in isolation. Perhaps an even more fundamental difficulty is that it is not clear whether our current set of events’ definitions is adequate (Hovy et al., 2013). Thus, given the complexity and fundamental difficulties, the current evaluation methodology in this area focuses on a limited domain of events, e.g. 33 types in ACE 2005 (NIST, 2005) and 38 types in TAC KBP (Mitamura et al., 2015). Consequently, this allows researchers to train supervised systems that are tailored to these sets of events and that overfit the small domain covered in the annotated data, rather than address the realistic problem of understanding events in text. An important aspect of natural language understanding involves recognizing and categorizing events and the relations among them. However, these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task, resulting in supervised systems that attempt to learn complex models from small amounts of dat"
D16-1038,P02-1014,0,0.0350973,"ec(a)k #|a 6= NIL| . Thus, when we encounter missing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set"
D16-1038,C04-1197,1,0.184492,"ion, time and sentence/clause. Figure 3: Augmented event vector representation. Event vector is the concatenation of vectors corresponding to basic event vector representation, agentsub + action, agentobj + action, location + action and time + action. Here, “+” means that we first put text fragments together and then convert the combined text fragment into an ESA vector. event types. Event arguments are largely entity mentions or temporal/spatial arguments. They serve as specific roles in events, similarly to SRL arguments that are assigned role labels for predicates. We use the Illinois SRL (Punyakanok et al., 2004) tool to pre-process the text. We evaluate the SRL coverage on both event triggers and event arguments, shown in Table 2.3 For event triggers, we only focus on recall since we expect the event mention detection module to filter out most non-trigger predicates. Results show a good coverage of SRL predicates and arguments on event triggers and arguments. Even though we only get approximate event arguments, it is easier and more reliable to categorize them into five abstract roles, than to determine the exact role label with respect to event triggers. We identify the five most important and abstr"
D16-1038,P10-2029,0,0.0229096,"issing event arguments, we use Spair (a = NIL) to replace the corresponding term in the numerator in S(e1 , e2 ) while using Ssingle (a = NIL) in the denominator. These average contributed scores are corpus independent, and can be pre-computed ahead of time. We use a cut-off threshold to determine that an event does not belong to any event types, and can thus be eliminated. This threshold is set by tuning only on the set of event examples, which is corpus independent.5 2.3 Event Co-reference Similar to the mention-pair model in entity coreference (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010), we use cosine similarities computed from pairs of event mentions: S(e1 , e2 ) (as in Eq. (1)). Before applying the co-reference model, we first use external knowledge bases to identify conflict events. We use the Illinois Wikification (Cheng and Roth, 2013) tool to link event arguments to Wikipedia pages. Using the Wikipedia IDs, we map event arguments to Freebase entries. We view the top-level Freebase type as the event argument type. An event argument can contain multiple wikified entities, leading to multiple Wikipedia pages and thus a set of Freebase types. We also augment the argument t"
D16-1038,M95-1005,0,0.701236,"supervised methods. For MSEP, we only need to run on each corpus once for testing. 4.2 use augmented ESA vector representation (AUG)8 , and whether we use knowledge during co-reference inference (KNOW). We also develop a supervised event co-reference system following the work of Sammons et al. (2015), namely SupervisedBase . We also add additional event vector representations9 as features to this supervised system and get SupervisedExtend . 4.3 For event detection, we use standard precision, recall and F1 metrics. For event co-reference, we compare all systems using standard F1 metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe ) (Luo, 2005) and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10 4.4 Compared Systems For event detection, we compare with DMCNN (Chen et al., 2015), the state-of-art supervised event detection system. We also implement another supervised model, named supervised structured event detection SSED system following the work of Sammons et al. (2015). The system utilizes rich semantic features and applies a trigger identification classifier on every SRL predicate to determ"
D16-1038,N12-3008,1,0.853533,".9 85.7 73.5 82.3 F1 — — — 88.0 81.9 86.4 TAC KBP Predicates over Triggers SRL Args over Event Args Verb-SRL Nom-SRL All Verb-SRL Nom-SRL All Precision — — — 89.8 88.2 89.5 Recall 90.6 85.5 88.1 83.6 69.9 81.0 F1 — — — 86.6 78.0 85.0 Table 2: Semantic role labeling coverage. We evaluate both “Predicates over Triggers” and “SRL Arguments over Event Arguments”. “All” stands for the combination of Verb-SRL and Nom-SRL. The evaluation is done on all data. ists. 5) We set the SRL temporal argument as event time. If there is no such SRL label, we then use the Illinois Temporal Expression Extractor (Zhao et al., 2012) to find the temporal argument within an event’s sentence/clause. 6) We allow one or more missing event arguments among agentsub , agentobj , location or time, but require actions to always exist. Given the above structured information, we convert each event component to its corresponding vector representation, discussed in detail in Section 3. We then concatenate the vectors of all components together in a specific order: action, agentsub , agentobj , location, time and sentence/clause. We treat the whole sentence/clause, to which the “action” belongs, as context, and we append its correspond"
D17-1027,P14-1146,0,0.0205954,"posing Chinese characters are not enough. Evaluation on both word similarity and word analogy tasks demonstrates the superior performance of our model. 1 Introduction Distributed word representation represents a word as a vector in a continuous vector space and can better uncover both the semantic and syntactic information over traditional one-hot representations. It has been successfully applied to many downstream natural language processing (NLP) tasks as input features, such as named entity recognition (Collobert et al., 2011), text classification (Joulin et al., 2016), sentiment analysis (Tang et al., 2014), and question answering (Zhou et al., 2015). Among many embedding methods (Bengio et al., 2003; Mnih and Hinton, 2009), CBOW and Skip-Gram models are very popular due to their simplicity and efficiency, making it feasible to learn good embeddings of words from large scale training corpora (Mikolov et al., 2013b,a). Despite the success and popularity of word embeddings, most of the existing methods treat each 1 https://en.wikipedia.org/wiki/Written_ Chinese 286 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 286–291 c Copenhagen, Denmark, September"
D17-1027,D16-1157,0,0.0246245,"Missing"
D17-1027,N16-1119,0,0.355888,"Processing, pages 286–291 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and data are available at https://github.com/ HKUST-KnowComp/JWE. mation for Chinese word embeddings. Sun et al. (2014) and Li et al. (2015) proposed methods to enhance Chinese character embeddings with radicals based on C&W model (Collobert and Weston, 2008) and word2vec models (Mikolov et al., 2013a,b) respectively. Chen et al. (2015) used Chinese characters to improve Chinese word embeddings and proposed the CWE model to jointly learn Chinese word and character embeddings. Xu et al. (2016) extended the CWE model by exploiting the internal semantic similarity between a word and its characters in a cross-lingual manner. To combine both the radical-character and character-word compositions, Yin et al. (2016) proposed a multi-granularity embedding (MGE) model based on the CWE model, which represents the context as a combination of surrounding words, surrounding characters, and the radicals of the target word. Particularly, they developed a dictionary of 20,847 characters and 296 radicals. 2 Joint Learning Word Embedding In this section, we introduce our joint learning word embeddin"
D17-1027,D15-1098,0,0.104073,"ity and efficiency, making it feasible to learn good embeddings of words from large scale training corpora (Mikolov et al., 2013b,a). Despite the success and popularity of word embeddings, most of the existing methods treat each 1 https://en.wikipedia.org/wiki/Written_ Chinese 286 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 286–291 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and data are available at https://github.com/ HKUST-KnowComp/JWE. mation for Chinese word embeddings. Sun et al. (2014) and Li et al. (2015) proposed methods to enhance Chinese character embeddings with radicals based on C&W model (Collobert and Weston, 2008) and word2vec models (Mikolov et al., 2013a,b) respectively. Chen et al. (2015) used Chinese characters to improve Chinese word embeddings and proposed the CWE model to jointly learn Chinese word and character embeddings. Xu et al. (2016) extended the CWE model by exploiting the internal semantic similarity between a word and its characters in a cross-lingual manner. To combine both the radical-character and character-word compositions, Yin et al. (2016) proposed a multi-granu"
D17-1027,D16-1100,0,0.691173,". Sun et al. (2014) and Li et al. (2015) proposed methods to enhance Chinese character embeddings with radicals based on C&W model (Collobert and Weston, 2008) and word2vec models (Mikolov et al., 2013a,b) respectively. Chen et al. (2015) used Chinese characters to improve Chinese word embeddings and proposed the CWE model to jointly learn Chinese word and character embeddings. Xu et al. (2016) extended the CWE model by exploiting the internal semantic similarity between a word and its characters in a cross-lingual manner. To combine both the radical-character and character-word compositions, Yin et al. (2016) proposed a multi-granularity embedding (MGE) model based on the CWE model, which represents the context as a combination of surrounding words, surrounding characters, and the radicals of the target word. Particularly, they developed a dictionary of 20,847 characters and 296 radicals. 2 Joint Learning Word Embedding In this section, we introduce our joint learning word embedding model (JWE), which combines words, characters, and subcharacter components information. Our model is based on CBOW model (Mikolov et al., 2013a). JWE uses the average of context word vectors, the average of context cha"
D17-1027,W13-3512,0,0.15226,"Missing"
D17-1027,P15-1025,0,0.0106413,"aluation on both word similarity and word analogy tasks demonstrates the superior performance of our model. 1 Introduction Distributed word representation represents a word as a vector in a continuous vector space and can better uncover both the semantic and syntactic information over traditional one-hot representations. It has been successfully applied to many downstream natural language processing (NLP) tasks as input features, such as named entity recognition (Collobert et al., 2011), text classification (Joulin et al., 2016), sentiment analysis (Tang et al., 2014), and question answering (Zhou et al., 2015). Among many embedding methods (Bengio et al., 2003; Mnih and Hinton, 2009), CBOW and Skip-Gram models are very popular due to their simplicity and efficiency, making it feasible to learn good embeddings of words from large scale training corpora (Mikolov et al., 2013b,a). Despite the success and popularity of word embeddings, most of the existing methods treat each 1 https://en.wikipedia.org/wiki/Written_ Chinese 286 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 286–291 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computatio"
D17-1027,C14-1015,0,0.0396437,"Missing"
D17-1027,P15-2098,0,0.167857,"als are character components used to index Chinese characters in dictionaries. Although many of the radicals are also semantic components, a character has only one radical, which cannot fully uncover the semantics and structure of the character. Besides over 200 radicals, there are more than 10,000 components which are also semantically meaningful or phonetically useful. For example, Chinese character 照 (illuminate, reflect, mirror, picture) has one radical 灬 (the corresponding traditional Chinese radical is 火, meaning fire) and three other components, i.e., 日 (sun), 刀 (knife), and 口 (mouth). Shi et al. (2015) proposed using WUBI input method to decompose the Chinese characters into components. However, WUBI input method uses rules to group Chinese characters into meaningless clusters which can fit the alphabet based keyboard. The semantics of the components are not straightforwardly meaningful. ci+1 si 1 si+1 si Figure 1: Illustration of JWE. wi is the target word. wi−1 and wi+1 are the left word and right word of wi respectively. ci−1 and ci+1 represent the characters in the context. si−1 and si+1 represent the subcharacters in the context, si represents the subcharacters of the target word wi ."
D17-1217,D13-1172,0,0.0200246,"Missing"
D17-1217,D16-1171,0,0.00467266,"s have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair. However, these tasks are sentencelevel. Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task. This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure. However, they did not work on multiple aspects. Machine Comprehension. Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016). Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words. Hermann et"
D17-1217,P14-1033,0,0.011534,"sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair. However, these tasks are sentencelevel. Another related research field is documentlevel sentiment classification because we can tr"
D17-1217,P15-1162,0,0.00579867,"Missing"
D17-1217,D14-1181,0,0.00229991,"lei and Mcauliffe, 2010) which is a statistical model of labeled documents. NBoW is a neural bag-of-words model averaging embeddings of all words in a document and feeds the resulted embeddings into SVM classifier. DAN is a deep averaging network model which consists of several fully connected layers with averaged word embeddings as input. One novel word dropout strategy is employed to boost model performances (Iyyer et al., 2015). CNN continuously performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer (Kim, 2014). 4 http://nlp.stanford.edu/software/corenlp.shtml LSTM is one variant of recurrent neural network and has been proved to be one of state-ofthe-art models for document-level sentiment classification (Tang et al., 2015a). We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information. HAN means the hierarchical attention network which is proposed in (Yang et al., 2016) for document classification. Note that, the original HAN depends GRU as the encoder. In our experiments, LSTM-based HAN obtains slightly better results. Thus, we report the results of HAN with LSTM as"
D17-1217,D16-1011,0,0.201936,"Missing"
D17-1217,D14-1052,0,0.0615664,"4 45.2 45 44.8 word-hop sentence-hop 1 2 3 4 5 6 7 0 5 10 15 20 25 30 35 Figure 5: Results of different hops and different sizes of question keywords. Left: different the hop numbers; Right: different sizes of keywords. crafted features to predict aspect ratings. To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective. There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network"
D17-1217,S14-2004,0,0.0415419,"ed a dependency term in final multi-class SVM objective. There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of asp"
D17-1217,W14-5905,0,0.0316138,"sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair. However, these tasks are sentencelevel. Another related research field is documentle"
D17-1217,D16-1264,0,0.0194042,"ch field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task. This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure. However, they did not work on multiple aspects. Machine Comprehension. Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016). Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words. Hermann et al. (2015) used BiLSTM to encode document and query, and proposed Attentive Reader and Impatient Reader. The first one attends document based on the query representation, and the second one attends document by the representation of each token in query with an incremental manner. Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015) attend and reason document representation in a multi"
D17-1217,D15-1167,0,0.770177,"nt-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010). There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc. Most of reviews are very comprehensive and thus long documents. Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management. Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016). However, predicting an overall score for each long document is not enough, because the document can mention different aspects of the corresponding product or service. For example, in Figure 1, there could be different aspects for a review of hotel. These aspects help customer service better understand what are the major pros and cons of the product or service. Compared to the overall rating, users are less motivated to give aspect ratings. Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different"
D17-1217,P15-1098,0,0.448049,"nt-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010). There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc. Most of reviews are very comprehensive and thus long documents. Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management. Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016). However, predicting an overall score for each long document is not enough, because the document can mention different aspects of the corresponding product or service. For example, in Figure 1, there could be different aspects for a review of hotel. These aspects help customer service better understand what are the major pros and cons of the product or service. Compared to the overall rating, users are less motivated to give aspect ratings. Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different"
D17-1217,D16-1021,0,0.0295841,"r example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair. However, these tasks are sentencelevel. Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task. This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a"
D17-1217,P08-1036,0,0.0184361,"0 46.4 46.2 46 45.8 45.6 45.4 45.2 45 46.4 46.2 46 45.8 45.6 45.4 45.2 45 44.8 word-hop sentence-hop 1 2 3 4 5 6 7 0 5 10 15 20 25 30 35 Figure 5: Results of different hops and different sizes of question keywords. Left: different the hop numbers; Right: different sizes of keywords. crafted features to predict aspect ratings. To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective. There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the dev"
D17-1217,N16-1174,0,0.203461,"ssification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010). There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc. Most of reviews are very comprehensive and thus long documents. Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management. Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016). However, predicting an overall score for each long document is not enough, because the document can mention different aspects of the corresponding product or service. For example, in Figure 1, there could be different aspects for a review of hotel. These aspects help customer service better understand what are the major pros and cons of the product or service. Compared to the overall rating, users are less motivated to give aspect ratings. Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspec"
D18-1227,D07-1074,0,0.0388389,"ve experiments and analysis on this dataset with a learning to rank model that takes different types of features as input, as well as a few state-of-theart entity linking approaches. Our experimental results show that two types of features that are not available in traditional entity linking: social features and location features, can be very helpful for this task. 1 Introduction Entity linking is the task of determining the identities of entities mentioned in texts. Most existing studies on entity linking have focused on linking entity mentions to their referred entities in a knowledge base (Cucerzan, 2007; Liu et al., 2013; Ling et al., 2015). However, on social media platforms such as Twitter, Instagram, Yelp, Facebook, etc., the texts produced on them may often mention entities that cannot be found in a knowledge base, but can be found on the platform itself. For example, consider Yelp, a platform where users can write reviews about businesses such as restaurants, hotels, etc., a restaurant review on Yelp may mention another restaurant to compare, which is also likely to be on Yelp but cannot be found in a knowledge base such as Wikipedia. As another example, when people post a photo on a so"
D18-1227,P05-1045,0,0.0866397,"Missing"
D18-1227,N16-1150,0,0.0201055,"tity mentions and the target entities are within a social media platform. Specifically, the entity mentions are from the texts (which we will refer to as mention texts) produced by the users on a social media platform; and these mentions are linked to the accounts on this platform. It is not straightforward to apply existing entity linking systems that link to a knowledge base to this problem, because they usually take advantage of the rich information knowledge bases provide for the entities. For example, they can use detailed text descriptions, varies kinds of attributes, etc., as features (Francis-Landau et al., 2016; Gupta et al., 2017; Tan et al., 2017), or even additional signals such as the anchor texts in Wikipedia articles (Guo and Barbosa, 2014; Globerson et al., 2016; Ganea et al., 2016). However, on social media platforms, most of these resources or information are either unavailable or of poor quality. On the other hand, social media platforms also have some unique resources that can be exploited. One that commonly exists on all of them is social 2023 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2023–2032 c Brussels, Belgium, October 31 - November"
D18-1227,P16-1059,0,0.100255,"s) produced by the users on a social media platform; and these mentions are linked to the accounts on this platform. It is not straightforward to apply existing entity linking systems that link to a knowledge base to this problem, because they usually take advantage of the rich information knowledge bases provide for the entities. For example, they can use detailed text descriptions, varies kinds of attributes, etc., as features (Francis-Landau et al., 2016; Gupta et al., 2017; Tan et al., 2017), or even additional signals such as the anchor texts in Wikipedia articles (Guo and Barbosa, 2014; Globerson et al., 2016; Ganea et al., 2016). However, on social media platforms, most of these resources or information are either unavailable or of poor quality. On the other hand, social media platforms also have some unique resources that can be exploited. One that commonly exists on all of them is social 2023 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2023–2032 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ... Users BizName: The Shop Addr.: 1505 S Pavilion Center Dr, Las Vegas Review #1: I normally buy a copy"
D18-1227,D17-1284,0,0.059056,"entities are within a social media platform. Specifically, the entity mentions are from the texts (which we will refer to as mention texts) produced by the users on a social media platform; and these mentions are linked to the accounts on this platform. It is not straightforward to apply existing entity linking systems that link to a knowledge base to this problem, because they usually take advantage of the rich information knowledge bases provide for the entities. For example, they can use detailed text descriptions, varies kinds of attributes, etc., as features (Francis-Landau et al., 2016; Gupta et al., 2017; Tan et al., 2017), or even additional signals such as the anchor texts in Wikipedia articles (Guo and Barbosa, 2014; Globerson et al., 2016; Ganea et al., 2016). However, on social media platforms, most of these resources or information are either unavailable or of poor quality. On the other hand, social media platforms also have some unique resources that can be exploited. One that commonly exists on all of them is social 2023 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2023–2032 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Assoc"
D18-1227,P13-2006,0,0.0282522,"he results are shown in Table 5. We can successfully identify both the entity names and their locations on Yelp. We also selected the top three frequently compared pairs and compare with the stars provided by Yelp dataset. From Table 6 we can see that the text comparison is consistent with star ratings. 6 Related Work The traditional entity linking task of mapping mentions in articles to their corresponding entities in a knowledge base has been studied extensively (Shen et al., 2015; Ling et al., 2015). Various kinds of methods have been studied, e.g., neural network models (Sun et al., 2015; He et al., 2013), generative models (Li et al., 2013), etc. A large group of the existing entity linking approaches are called collective approaches, which are based on the observation that the entities mentioned in a same context are usually related with each other. Thus they usually form entity linking as an optimization problem that tries to maximizes both local mention-entity compatibility and global entity-entity coherence (Han et al., 2011; Nguyen et al., 2016). LinkYelp does not consider global 2030 entity-entity coherence as it is not the focus of this paper, but it can be applied to our problem too."
D18-1227,P14-1036,0,0.585901,"at can be designed. For example, we can also consider the locations of the other businesses that are 2027 reviewed by the user. We only use the above two since we find in our experiments that including them already provides high performance boost. 5 5.1 Experiments Method Accuracy (mean±std) DirectLink ELT SSRegu LinkYelp 0.6684±0.008 0.8451±0.012 0.7970±0.013 0.9034±0.014 Compared Methods We compare with a baseline method we name as DirectLink, as well as two existing entity linking methods including the method proposed by (Liu et al., 2013) (which we refer to as ELT) and SSRegu proposed by (Huang et al., 2014). DirectLink simply links each mention to the corresponding reviewed business. Many business mentions in Yelp reviews actually refer to the business that is being reviewed. This baseline method tells us how many of these mentions there are in Yelp-EL. ELT collectively links a set of mentions with an objective to maximize local compatibility and global consistence. It achieves this by integrating three types of similarities: mention-entity similarity, entity-entity similarity, and mention-mention similarity. To apply ELT to Yelp-EL, we use the conventional features introduced in Section 4.1 for"
D18-1227,D17-1007,0,0.0124472,"a social media platform. Specifically, the entity mentions are from the texts (which we will refer to as mention texts) produced by the users on a social media platform; and these mentions are linked to the accounts on this platform. It is not straightforward to apply existing entity linking systems that link to a knowledge base to this problem, because they usually take advantage of the rich information knowledge bases provide for the entities. For example, they can use detailed text descriptions, varies kinds of attributes, etc., as features (Francis-Landau et al., 2016; Gupta et al., 2017; Tan et al., 2017), or even additional signals such as the anchor texts in Wikipedia articles (Guo and Barbosa, 2014; Globerson et al., 2016; Ganea et al., 2016). However, on social media platforms, most of these resources or information are either unavailable or of poor quality. On the other hand, social media platforms also have some unique resources that can be exploited. One that commonly exists on all of them is social 2023 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2023–2032 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computat"
D18-1227,P17-2085,0,0.0181419,"it to get an indication of whether two mentions are related. Finally, although these studies focused on entity linking for tweets, they still use entities in knowledge bases as the target. There are a few entity linking studies that do not link mentions to knowledge bases. (Shen et al., 2017) proposed to link entity mentions to an HIN such as DBLP and IMDB. However, their articles are collected from the Internet through searching and thus are not related to the target entities. They also used an HIN based method, but their use is restricted to get the relatedness between different entities. (Lin et al., 2017) studied the entity linking problem where the entities are included in different lists and entities of the same type belong to the same list. They only used this information along with the name of each entity to perform entity linking. Thus their focus is very different from ours. 7 Conclusions In this paper, we propose a new entity linking problem where both entity mentions and target entities are in a same social media platform. To study this problem, we first create a dataset called Yelp-EL, and then conduct extensive experiments and analysis on it with a learning to rank model that takes t"
D18-1227,Q15-1023,0,0.631126,"s dataset with a learning to rank model that takes different types of features as input, as well as a few state-of-theart entity linking approaches. Our experimental results show that two types of features that are not available in traditional entity linking: social features and location features, can be very helpful for this task. 1 Introduction Entity linking is the task of determining the identities of entities mentioned in texts. Most existing studies on entity linking have focused on linking entity mentions to their referred entities in a knowledge base (Cucerzan, 2007; Liu et al., 2013; Ling et al., 2015). However, on social media platforms such as Twitter, Instagram, Yelp, Facebook, etc., the texts produced on them may often mention entities that cannot be found in a knowledge base, but can be found on the platform itself. For example, consider Yelp, a platform where users can write reviews about businesses such as restaurants, hotels, etc., a restaurant review on Yelp may mention another restaurant to compare, which is also likely to be on Yelp but cannot be found in a knowledge base such as Wikipedia. As another example, when people post a photo on a social media platform, their friends may"
D18-1227,P13-1128,0,0.403398,"nd analysis on this dataset with a learning to rank model that takes different types of features as input, as well as a few state-of-theart entity linking approaches. Our experimental results show that two types of features that are not available in traditional entity linking: social features and location features, can be very helpful for this task. 1 Introduction Entity linking is the task of determining the identities of entities mentioned in texts. Most existing studies on entity linking have focused on linking entity mentions to their referred entities in a knowledge base (Cucerzan, 2007; Liu et al., 2013; Ling et al., 2015). However, on social media platforms such as Twitter, Instagram, Yelp, Facebook, etc., the texts produced on them may often mention entities that cannot be found in a knowledge base, but can be found on the platform itself. For example, consider Yelp, a platform where users can write reviews about businesses such as restaurants, hotels, etc., a restaurant review on Yelp may mention another restaurant to compare, which is also likely to be on Yelp but cannot be found in a knowledge base such as Wikipedia. As another example, when people post a photo on a social media platfor"
D18-1227,N15-1119,0,0.0486922,"Missing"
D19-1474,N18-1172,0,0.0212459,"at encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is the first work that examines how annotators react to hate speech comments. To fully exploit the collected annotations, we tested multitask learning on our dataset. Multitask learning (Collobert et al., 2011) allows neural networks to share parameters with one another and, thus, learn from related tasks. It has been used in different NLP tasks such as parsing (Hashimoto et al., 2017), dependency parsing (Peng et al., 2017), neural machine translation (Luong et al., 2016), sentiment analysis (Augenstein et al., 2018), and other tasks. Multitask learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks (Ruder et al., 2017), for which techniques may involve a massive space of potential parameter sharing architectures. 3 Dataset In this section, we present our data collection methodology and annotation process. 3.1 Data Collection Considering the cultural differences and commonly debated topics in the main geographic regions where English, French, and Arabic are spoken, searching for equivalent terms in the three langu"
D19-1474,W18-1105,0,0.0199508,"ets, in which the annotations capture the degree of intensity of offensive and aggressive tweets, in addition to whether the tweets are ironic and contain stereotypes or not. Ross et al. (2017) have collected more than 500 German tweets against refugees, and annotated them as hateful and not hateful. Hee et al. (2015) detect bullies and victims among youngsters in Dutch comments on AskFM, and classify cyberbullying comments as insults or threats. Moreover, Albadi et al. (2018) provide a corpus of Arabic sectarian speech. Another predominant phenomenon in hate speech corpora is code switching. Bohra et al. (2018) present a dataset of code mixed HindiEnglish tweets, while Galery et al. (2018) report the presence of Hindi tokens in English data and use multilingual word embeddings to deal with this issue when detecting toxicity. Similarly, we use such embeddings to take advantage of the mul6 In conformity with Twitter terms and conditions. tilinguality and comparability of our corpora during the classification. Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is"
D19-1474,D17-1206,0,0.174907,"ovember 3–7, 2019. 2019 Association for Computational Linguistics (a) English. (b) French. (c) Arabic. Figure 1: Annotation examples in our dataset. eral linguistic and cultural differences and bias in hate speech. We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task. Since in natural language processing, there is a peculiar interest in multitask learning, where different tasks can be used to help each other (Collobert et al., 2011; Ruder et al., 2017; Hashimoto et al., 2017), we use a unified model to handle the annotated data in all three languages and five tasks. We adopt (Ruder et al., 2017) as a learning algorithm adapted to loosely related tasks such as our five annotated aspects and, use the Babylon cross-lingual embeddings (Smith et al., 2017) to align the three languages. We compare the multilingual multitask learning settings with monolingual multitask, multilingual single-task, and monolingual single-task learning settings respectively. Then, we report the performance results of the different settings and discuss how each task affects the remaining ones"
D19-1474,R15-1086,0,0.0827359,"Missing"
D19-1474,W18-4409,0,0.0312173,"aggressive tweets, in addition to whether the tweets are ironic and contain stereotypes or not. Ross et al. (2017) have collected more than 500 German tweets against refugees, and annotated them as hateful and not hateful. Hee et al. (2015) detect bullies and victims among youngsters in Dutch comments on AskFM, and classify cyberbullying comments as insults or threats. Moreover, Albadi et al. (2018) provide a corpus of Arabic sectarian speech. Another predominant phenomenon in hate speech corpora is code switching. Bohra et al. (2018) present a dataset of code mixed HindiEnglish tweets, while Galery et al. (2018) report the presence of Hindi tokens in English data and use multilingual word embeddings to deal with this issue when detecting toxicity. Similarly, we use such embeddings to take advantage of the mul6 In conformity with Twitter terms and conditions. tilinguality and comparability of our corpora during the classification. Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is the first work that examines how annotators react to hate speech comments. To f"
D19-1474,N16-2013,0,0.140597,"wComp/MLMA_hate_speech Figure 2: Three tweets in which (1) the first one accuses immigrants of harming society without using any direct insult; (2) the second insults a Hispanic person using a slur; and (3) the third one uses slurs to give a personal account. This shows that profanity is not a clear indicator of the presence of hate speech. able hate speech corpora sparse and noisy (Nobata et al., 2016). Given the subjectivity and the complexity of such data, annotation schemes have rarely been made fine-grained. Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use racist, sexist, and normal as labels; Davidson et al. (2017) label their data as hateful, offensive (but not hateful), and neither, while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, Qian et al. (2018) have chosen to detect ideologies of hate spe"
D19-1474,D18-1302,0,0.0741491,"8) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, Qian et al. (2018) have chosen to detect ideologies of hate speech counting 40 different hate ideologies among 13 extremist hate groups. The detection of hate speech targets is yet another challenging aspect of the annotation. Park et al. (2018) report the bias that exists in the current datasets towards identity words, such as women, which may later cause false predictions. They propose to debias gender identity word em4676 Dataset # Tweets Labels Chatzakou et al. (2017) Waseem and Hovy (2016) Davidson et al. (2017) 9,484 16, 914 24, 802 Golbeck et al. (2017) 35,000 Founta et al. (2018) 80, 000 ElSherief et al. (2018) 28,608 aggressive, bullying, spam, normal racist, sexist, normal hateful, offensive (but not hateful), neither the worst, threats, hate speech, direct harassment, potentially offensive, non-harassment offensive, abusiv"
D19-1474,P17-1186,0,0.0291012,"Missing"
D19-1474,D18-1391,0,0.0424458,"t exist in the literature. For instance, Waseem and Hovy (2016) use racist, sexist, and normal as labels; Davidson et al. (2017) label their data as hateful, offensive (but not hateful), and neither, while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, Qian et al. (2018) have chosen to detect ideologies of hate speech counting 40 different hate ideologies among 13 extremist hate groups. The detection of hate speech targets is yet another challenging aspect of the annotation. Park et al. (2018) report the bias that exists in the current datasets towards identity words, such as women, which may later cause false predictions. They propose to debias gender identity word em4676 Dataset # Tweets Labels Chatzakou et al. (2017) Waseem and Hovy (2016) Davidson et al. (2017) 9,484 16, 914 24, 802 Golbeck et al. (2017) 35,000 Founta et al. (2018) 80, 000 ElSherief et al"
D19-1474,L18-1443,0,0.377959,"te crime (Ross et al., 2017). Moreover, although people of various linguistic backgrounds are exposed to hate speech (Waseem 1 http://www.pewinternet.org/2017/07/11/onlineharassment-2017/ 2 https://www.amnesty.org.uk/press-releases/womenabused-twitter-every-30-seconds-new-study 3 https://help.twitter.com/en/rules-and-policies/hatefulconduct-policy et al., 2017; Ross et al., 2017), English is still at the center of existing work on toxic language analysis. Recently, some research studies have been conducted on languages such as German (Kratzke, 2017), Arabic (Albadi et al., 2018), and Italian (Sanguinetti et al., 2018). However, such studies usually use monolingual corpora and do not contrast, or examine the correlations between online hate speech in different languages. On the other hand, tasks involving more than one language such as the hatEval task4 , which covers English and Spanish, include only separate classification tasks, namely (a) women and immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech. Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how"
D19-1474,W17-1101,0,0.0516717,"gs (Smith et al., 2017) to align the three languages. We compare the multilingual multitask learning settings with monolingual multitask, multilingual single-task, and monolingual single-task learning settings respectively. Then, we report the performance results of the different settings and discuss how each task affects the remaining ones. We release our dataset and code to the community to extend research work on multilingual hate speech detection and classification.5 2 Related Work There is little consensus on the difference between profanity and hate speech and, how to define the latter (Schmidt and Wiegand, 2017). As shown in Figure 2, slurs are not an unequivocal indicator of hate speech and can be part of a nonaggressive conversation, while some of the most offensive comments may come in the form of subtle metaphors or sarcasm (Malmasi and Zampieri, 2018). Consequently, there is no existing human annotated vocabulary that explicitly reveals the presence of hate speech, which makes the avail5 our code is available on: https://github.com/ HKUST-KnowComp/MLMA_hate_speech Figure 2: Three tweets in which (1) the first one accuses immigrants of harming society without using any direct insult; (2) the seco"
D19-1474,W17-3012,0,0.0884535,"Missing"
D19-1516,D18-1016,0,0.0325657,"Missing"
D19-1516,P15-1136,0,0.0769668,"Missing"
D19-1516,D16-1245,0,0.0298109,"m/tensorflow/models/ tree/master/research/object_detection Since we are the first to proposed a visual-aware model for pronoun coreference resolution, we compare our results with existing models of general coreference resolution. • Deterministic model (Raghunathan et al., 2010) is a rule-based system that aggregates multiple functions for determining whether two mentions are coreferent based on hand-craft features. • Statistical model (Clark and Manning, 2015) learns upon human-designed entity-level features between clusters of mentions to produce accurate coreference chains. • Deep-RL model (Clark and Manning, 2016) applies reinforcement learning to mention-ranking models to form coreference clusters. • End-to-end model (Lee et al., 2018) is the stateof-the-art method of coreference resolution. It predicts coreference clusters via an end-to-end neural network that leverages pretrained word embeddings and contextual information. Last but not least, to demonstrate the effectiveness of the proposed model, we also present a variation of the End-to-end model, which can also use the visual information, as an extra baseline: • End-to-end+Visual first extracts features from images with ResNet-152 (He et al., 201"
D19-1516,P81-1019,0,0.331779,"le are discussing the view they can both see. Pronouns and noun phrases referring to the same entity are marked in same color. The first “it” in the dialogue labeled with blue color refers to the object “the big cake” and the second “it” labeled with green color refers to the statue in the image. more challenging than the general coreference resolution task. Introduction The question of how human beings resolve pronouns has long been an attractive research topic in both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al"
D19-1516,E12-3001,0,0.0336323,"On the contrary, as VisPro can effectively leverage the visual information and make the decision that “they” should refer to “2 zebras.” 7 Related Work In this section, we introduce the related work about pronoun coreference resolution and visualaware natural language processing problems. 7.1 Pronoun Coreference Resolution As one core task of natural language understanding, pronoun coreference resolution, the task of identifying mentions in text that the targeting pronoun refers to, plays a vital role in many downstream applications in natural language processing, such as machine translation (Guillou, 2012), summarization (Steinberger et al., 2007) and information extraction (Edens et al., 2003). Traditional studies focus on resolving pronouns in expert-annotated formal textual dataset such as ACE (NIST, 2003) or OntoNotes (Pradhan et al., 2012). However, models that perform well on these datasets might not perform as well in other scenarios such as dialogues due to the informal language and the lack of essential information (e.g., the shared view of two speakers). In this work, we thus focus on the PCR in dialogues and show that the information contained in the shared view can be crucial for un"
D19-1516,P03-1054,0,0.0188897,"et and invite annotators to annotate. In VisDial, each image is accompanied by a dialogue record discussing that image. One example is shown in Figure 1. In addition, VisDial also provides a caption for each image, which brings more information for us to create VisPro1 . In this section, we introduce the details about the dataset creation in terms of pre-processing, survey design, annotation, and post-processing. 2.1 Pre-processing To make the annotation task clear to annotators and help them provide accurate annotation, we first extract all the noun phrases and pronouns with Stanford Parser (Klein and Manning, 2003) and then provide the extracted noun phrases as candidate mentions to annotate on. To avoid the overlap of candidate noun phrases, we choose noun phrases with a height of two in parse trees. One example is shown in Figure 2. In the syntactic tree for the sentence “A man with a dog is walking on the grass,” we choose “A man,” “a dog” and “the grass” as candidates. If the height of noun phrases is not limited, then the noun phrase “A man with a dog” will cover “A man” and “a dog,” leading to confusion in the options. Following (Strube and M¨uller, 2003; Ng, 2005), we only select third-person per"
D19-1516,N18-2108,0,0.214955,"ch, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may dir"
D19-1516,D14-1162,0,0.0838539,"ed for models to detect plausible mentions outside the dialogue. The pool contains both mentions extracted from the corresponding caption and randomly selected negative mention samples from other captions. All models are evaluated based on the precision (P), recall (R), and F1 score. Last but not least, we split the test dataset by whether the correct antecedents of the pronoun appear in the dialogue or not. We denote these two groups as “Discussed” and “Not Discussed.” 5.2 Implementation Details Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embedding (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo5 as the object detection module. The size of hidden states in the LSTM module is set to 200, and the size of the projected embedding for computing similarity between text spans and object labels is 512. The feed-forward networks for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively. For model training,"
D19-1516,N18-1202,0,0.0153032,"ons outside the dialogue. The pool contains both mentions extracted from the corresponding caption and randomly selected negative mention samples from other captions. All models are evaluated based on the precision (P), recall (R), and F1 score. Last but not least, we split the test dataset by whether the correct antecedents of the pronoun appear in the dialogue or not. We denote these two groups as “Discussed” and “Not Discussed.” 5.2 Implementation Details Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embedding (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo5 as the object detection module. The size of hidden states in the LSTM module is set to 200, and the size of the projected embedding for computing similarity between text spans and object labels is 512. The feed-forward networks for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively. For model training, we use cross-entropy as the loss fu"
D19-1516,W12-4501,0,0.777325,". As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, t"
D19-1516,D10-1048,0,0.587253,"ason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to s"
D19-1516,P09-1074,0,0.0182755,"an beings resolve pronouns has long been an attractive research topic in both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understandin"
D19-1516,P03-1022,0,0.27428,"Missing"
D19-1516,N19-1093,1,0.83265,"tural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, the object (name or t"
D19-1516,P19-1083,1,0.805317,"tural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, the object (name or t"
D19-1528,J10-4007,0,0.091241,"Missing"
D19-1528,D14-1082,0,0.354488,"and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975)"
D19-1528,D14-1004,0,0.446568,"Missing"
D19-1528,N19-1423,0,0.0292815,"counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the state-of-the-art performance on the pseudo-disambiguation task. For word2Vec and GloVe, we use their released code. For D-embedding, we follow their origin"
D19-1528,J15-4004,0,0.0271936,"cing their overall semantics. We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities. 3.4 Word Similarity Measurement In addition to SP acquisition, we also evaluate our embeddings on word similarity (WS) measurement to test whether the learned embedding can effectively capture the overall semantics. We use SimLex-999 (Hill et al., 2015) as the evaluation dataset for this task because it contains different word types, i.e., 666 noun pairs, 222 verb pairs, and 111 adjective pairs. We follow the conventional setting that uses the Spearman’s correlation to assess the correspondence between the similarity scores and human annotations on all word pairs. Evaluations are conducted on the final embeddings v for each relation and the center ones. Results are reported in Table 3 with several observations. First, our model achieves the best overall performance and significantly better on nouns, which can be explained by that nouns appea"
D19-1528,C16-1266,0,0.0324516,"Missing"
D19-1528,J03-3005,0,0.150689,"n different parameters in a sequential manner and applied in various areas. natively update c and u upon the convergence of c. As a result, we set λ to 1 in the first half of the training process and 0 afterwards. 3 Experiments Experiments are conducted to evaluate how our embeddings are performed on SP acquisition and word similarity measurement. 3.1 Implementation Details We use the English Wikipedia4 as the training corpus. The Stanford parser5 is used to obtain dependency relations among words. For the fair comparison, we follow existing work and set d = 300, s = 10, and a = 1. Following (Keller and Lapata, 2003) and (de Cruys, 2014), we select three dependency relations (nsubj, dobj, and amod) as follows: • nsubj: The preference of subject for a given verb. For example, it is plausible to say ‘dog barks’ rather than ‘stone barks’. The verb is viewed as the predicate (head) while the subject as the argument (tail). • dobj: The preference of object for a given verb. For example, it is plausible for ‘eat food’ rather than ‘eat house’. The verb is viewed as the predicate (head) while the object as the argument (tail). • amod: The preference of modifier for a given noun. For example, it is plausible to sa"
D19-1528,N18-2108,0,0.0513578,"o proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975) is a typical scenario. In general, SP refers to that, given a word (predicate) and a dependency relation"
D19-1528,P14-2050,0,0.500537,"ociation for Computational Linguistics Figure 1: Illustration of the multiplex embeddings for ‘sing’ and ‘song’. Black arrows present center embeddings for words’ overall semantics; blue and green arrows refer to words’ relational embeddings for relationdependent semantics. All relational embeddings for each word are designed to near its center embedding. nsubj and dobj relations are used as examples. treat ‘food’ and ‘eat’ as highly relevant words but never distinguish the function of ‘food’ to be a subject or an object to ‘eat’. To address this problem, the dependency-based embedding model (Levy and Goldberg, 2014) is proposed to treat a word through separate ones, e.g., ‘food@dobj’ and ‘food@nsubj’, under different syntactic relations, with the skip-gram (Mikolov et al., 2013) model being used to train the final embeddings. However, this method is limited in two aspects. First, sparseness is introduced because each word is treated as two irrelevant ones (e.g., ‘food@dobj’ and ‘food@nsubj’), so that the overall quality of learned embeddings is affected. Second, the resulting embedding size is too large1 , which is not appropriate either for storage or usage. Therefore, in this paper, we propose a multip"
D19-1528,D14-1162,0,0.123887,"small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are n"
D19-1528,N18-1202,0,0.0549045,"lear how to leverage these methods in downstream tasks, we label these methods as downstream unfriendly. • Posterior Probability (PP) (Resnik, 1997), a counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the stat"
D19-1528,P99-1014,0,0.0505662,"Missing"
D19-1528,D17-1068,0,0.0394857,"Missing"
D19-1528,P19-1071,1,0.908357,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,N19-1093,1,0.889896,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,P19-1083,1,0.86796,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,D13-1141,0,0.0335748,"s unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of the resulting embeddings. Alternatively, our MWE model uses a set of (constrained and small)"
D19-1528,K17-1016,1,0.831006,"ional embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scen"
D19-1528,N18-2028,1,0.859782,"t is easily computed for D-embeddings that 200,000 words will result in a 10GB model, which is unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of"
D19-1528,C16-1203,0,0.0273576,"Missing"
D19-1643,E17-1075,0,0.241884,"a person entity by our entity linking algorithm, we add a random fine-grained person type label that does not belong to this entity while generating the KB type representation. For example, if the mention is linked to a person with types /person/actor and /person/author, a random label /person/politician may be added. This will force the model to still infer the type labels from the context even when the mention is correctly linked, since the KB type representation no longer perfectly match the weak labels. To make it more flexible, we also propose to use a variant of the hinge loss used by (Abhishek et al., 2017) to train our model: X X max(0, 1 − s(m, t)) [ L= m t∈τm + X (2) λ(t) max(0, 1 + s(m, t))] t∈¯ τm where τm is the correct type set for mention m, τ¯m is the incorrect type set. λ(t) ∈ [1, +∞) is a predefined parameter to impose a larger penalty if the type t is incorrectly predicted as positive. Since the problem of overfitting the weakly annotated labels is more severe for person mentions, we set λ(t) = λP if t is a fine-grained person type, and λ(t) = 1 for all other types. During training, we also randomly set the EL results of half of the training samples to be NIL. So that the model can p"
D19-1643,P18-1009,0,0.141024,"e address the problem that our model may overfit the weakly labeled data by using a variant of the hinge-loss and introducing noise during training. • We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets. Our code is available at https://github. com/HKUST-KnowComp/IFETEL. 2 Related Work An early effort of classifying named entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) addre"
D19-1643,C02-1130,0,0.135481,", 2012). Next, we will introduce our approach for this problem in detail, including the neural model, the training of the model, and the entity linking algorithm we use. • We address the problem that our model may overfit the weakly labeled data by using a variant of the hinge-loss and introducing noise during training. • We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets. Our code is available at https://github. com/HKUST-KnowComp/IFETEL. 2 Related Work An early effort of classifying named entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for train"
D19-1643,D17-1277,0,0.0520589,"Missing"
D19-1643,L18-1699,0,0.0190786,"nt (Gillick et al., 2014). For example, in the sentence “Trump threatens to pull US out of World Trade Organization,” the mention “Trump” should be labeled as /person and /person/politician, although Donald Trump also had other occupations such as businessman, TV personality, etc. This task is challenging because it usually uses a relatively large tag set, and some mentions may require the understanding of the context to be correctly labeled. Moreover, since manual annotation is very labor-intensive, existing approaches have to rely on distant supervision to train models (Ling and Weld, 2012; Ghaddar and Langlais, 2018). Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to make typing decisions because if a mention is correctly linked to its target entity, we can directly obtain the type information about this entity in the knowledge base (KB). For example, in the sentence “There were some great discussions on a variety of issues facing Federal Way,” the mention “Federal Way” may be incorrectly labeled as a company by some FET models. Such a mistake can be avoided after linking i"
D19-1643,D16-1144,0,0.600862,"ed through EL should not be fully trusted since it is not always accurate. Even when a mention is correctly linked to an entity, the type information of this entity in the KB may be incomplete or outdated. Thus, in this paper, we propose a deep neural fine-grained entity typing model that flexibly predicts labels based on the context, the mention string, and the type information from KB obtained with EL. Using EL also introduces a new problem for the training process. Currently, a widely used approach to create FET training samples is to use the anchor links in Wikipedia (Ling and Weld, 2012; Ren et al., 2016a). Each anchor link is regarded as a mention, and is weakly labeled with all the types of its referred entity (the Wikipedia page the anchor link points to) in KB. Our approach, when links the mention correctly, also uses all the types of the referred entity in KB as extra information. This may cause the trained model to overfit the weakly labeled data. We design a variant of the hinge loss and introduce noise during training to address this problem. 6210 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natura"
D19-1643,E17-1119,0,0.0435434,"ated Work An early effort of classifying named entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) address these problems by separating clean mentions and noisy mentions, modeling type correction (Ren et al., 2016a), using a hierarchy-aware loss (Xu and Barbosa, 2018), etc. (Huang et al., 2016) and (Zhou et al., 2018) are two studies that are most related to this paper. Huang et al. (2016) propose an unsupervised FET system where EL"
D19-1643,D18-1121,0,0.0732189,"d entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) address these problems by separating clean mentions and noisy mentions, modeling type correction (Ren et al., 2016a), using a hierarchy-aware loss (Xu and Barbosa, 2018), etc. (Huang et al., 2016) and (Zhou et al., 2018) are two studies that are most related to this paper. Huang et al. (2016) propose an unsupervised FET system where EL is an importat component. But they use EL"
D19-1643,N18-1002,0,0.351364,"rt of classifying named entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) address these problems by separating clean mentions and noisy mentions, modeling type correction (Ren et al., 2016a), using a hierarchy-aware loss (Xu and Barbosa, 2018), etc. (Huang et al., 2016) and (Zhou et al., 2018) are two studies that are most related to this paper. Huang et al. (2016) propose an unsupervised FET system where EL is an importat compone"
D19-1643,P15-2048,0,0.0675799,"), we simply one hot encode the empty type set. Prediction Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector g. Then, we get f = fc ⊕fs ⊕fe ⊕g, where ⊕ means concatenation. f is then fed into an MLP that contains three dense layers to obtain um , out final representation for the current mention sample m. Let t1 , t2 , ..., tk be all the types in T , where k = |T |. We embed them into the same space as um by assigning each of them a dense vector (Yogatama et al., 2015). These vectors are denoted as t1 , ..., tk . Then the score of the mention m having the type ti ∈ T is calculated as the dot product of um and ti : s(m, ti ) = um · ti . (1) We predict ti as a type of m if s(m, ti ) > 0. 1 We use Freebase mainly because it is widely used by existing studies. Wikidata is an alternative. 3.2 Model Training Following existing studies, we also generate training data by using the anchor links in Wikipedia. Each anchor link can be used as a mention. These mentions are labeled by mapping the Freebase types of the target entries to the tag set T (Ling and Weld, 2012)"
D19-1643,D18-1231,0,0.211832,"ches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) address these problems by separating clean mentions and noisy mentions, modeling type correction (Ren et al., 2016a), using a hierarchy-aware loss (Xu and Barbosa, 2018), etc. (Huang et al., 2016) and (Zhou et al., 2018) are two studies that are most related to this paper. Huang et al. (2016) propose an unsupervised FET system where EL is an importat component. But they use EL to help with clustering and type name 3 3.1 Method Fine-grained Entity Typing Model Input Each input sample to our FET system contains one mention and the sentence it belongs to. We denote w1 , w2 , ..., wn as the words in the current sentence, wp1 , wp2 , ..., wpl as the words in the mention string, where n is the number of words in the sentence, p1 , ..., pl are the indices of the words in the mention string, l is the number of words"
D19-1643,P18-1010,0,0.164882,"of the hinge-loss and introducing noise during training. • We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets. Our code is available at https://github. com/HKUST-KnowComp/IFETEL. 2 Related Work An early effort of classifying named entities into fine-grained types can be found in (Fleischman and Hovy, 2002), which only focuses on person names. Latter, datasets with larger type sets are constructed (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Choi et al., 2018). These datasets are more preferred by recent studies (Ren et al., 2016a; Murty et al., 2018). Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be hand-crafted (Ling and Weld, 2012; Gillick et al., 2014) or learned from neural network models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018). Since FET systems usually use distant supervision for training, the labels of the training samples can be noisy, erroneous or overly specific. Several studies (Ren et al., 2016b; Xin et al., 2018; Xu and Barbosa, 2018) address these problems by separating clean mentions and noisy mentions, modeling type correction ("
D19-1643,N15-1119,0,0.0277096,"enalty if the type t is incorrectly predicted as positive. Since the problem of overfitting the weakly annotated labels is more severe for person mentions, we set λ(t) = λP if t is a fine-grained person type, and λ(t) = 1 for all other types. During training, we also randomly set the EL results of half of the training samples to be NIL. So that the model can perform well for mentions that cannot be linked to the KB at test time. 3.3 Entity Linking Algorithm In this paper, we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness (Pan et al., 2015; Medelyan and Legg, 2008) is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string. In our FET approach, the commonness score is also used as the confidence on the linking result (i.e., the g used in the prediction part of Subsection 3.1). Within a same document, we also use the same heuristic used in (Ganea and Hofmann, 2017) to find coreferences of generic mentions of persons (e.g., “Matt”) to more specific mentions (e.g., “Matt Damon”). We also tried other more advanced EL methods in our experiments. However, they do not i"
D19-1643,D14-1162,0,0.0824272,"mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on. Following (Ling and Weld, 2012), we generate weakly labeled datasets for training with Wikipedia anchor links. Since the tag sets used by FIGER (GOLD) and BBN are different, we create a training set for each of them. For each dataset, 2, 000 weakly labeled samples are randomly picked to form a development set. We also manually annotated 50 person mentions collected from news articles for tuning the parameter λP . We use the 300 dimensional pretrained GloVe word vectors provided by (Pennington et al., 2014). The hidden layer sizes of the two layers of BiLSTMs are both set to 250. For the threelayer MLP, the size of the two hidden layers are both set to 500. The size of the type embeddings is 500. λP is set to 2.0. We also apply batch normalization and dropout to the input of each dense layer in our three-layer MLP during training. We use strict accuracy, Macro F1, and Micro F1 to evaluate fine-grained typing performance (Ling and Weld, 2012). 4.2 Compared Methods We compare with the following existing approaches: AFET (Ren et al., 2016a), AAA (Abhishek et al., 2017), NFETC (Xu and Barbosa, 2018)"
K15-2012,J92-4003,0,0.276394,"ich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features, and 67,204 Brown cluster features. we have the correct connective and arguments provided as input). This means that these results are higher than they would be if evaluated using inputs generated"
K15-2012,P98-2186,1,0.715266,"fying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois NLP software1 and the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity features. Our system achieves an F1 score of 0.2492 in overall performance on the development set and 0.1798 on the blind test set. 1 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2 , syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. Introduction The Illinois discourse parsing system builds on existing approaches, using a series of classifiers to identify different elements of discourse structures such as argument boundaries and types along with discourse connectives and senses. In developi"
K15-2012,de-marneffe-etal-2006-generating,0,0.0264312,"Missing"
K15-2012,E14-1068,0,0.0253366,"dition to the features used by Lin et al. (2014), we also add function words to the path features. If we detect a word which is in the lexicon of function words, we replace the corresponding tag used in the path with the word’s surface string. 2.4 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify expli"
K15-2012,I05-6007,0,0.0834838,"Missing"
K15-2012,H05-2018,0,0.0627534,"o generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement f"
K15-2012,K15-2001,0,0.050462,"ignore the disjoint connectives, our results can be improved if we incorporate those missing connectives. We do not presently incorporate the argument information in connective detection and sense classification for the explicit parser. Connective detection and classification can be improved if we also incorporate more features from arguments or perform joint learning. In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 P 92.97 98.15 64.41 87.06 77.02 94.74 83.18 34.58 82.94 59.75 Cross-Validation Results We first present the cross validation results for each component using the training data (Table 1). All the results are averaged over 10-fold cross validation of all the examples we generated, using our own predicted features and our own evaluation code. Each component is evaluated in isolation, assuming the inputs are from gold data (for example: for connective classification, it is assumed 4 Discussion In this section we point to some types of errors in our system’s predictions and the complications"
K15-2012,N15-1082,1,0.659558,"licit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown 79 the extracted polarity values to construct three features: Two individual"
K15-2012,prasad-etal-2008-penn,0,0.840823,"nd senses. In developing the components of this pipeline, we investigated different kinds of features to try to improve abstraction while retaining sufficient expressivity. To that end, we investigated a combination of parse and lexical (function word) features; brown clusters; and relations between verb-argument structures in consecutive sentences. 2.2 Recognizing Explicit Connectives In this section, we describe the system we developed, and introduce the features we used in To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifier to determine when a word matching an entry in the list represents an actual connective. We only focus on the connectives with consecutive tokens and ignore the non-consecutive connectives. We generate lexicosyntactic and path features associated with the 1 http://cogcomp.cs.illinois.edu/page/ software 2 http://cogcomp.cs.illinois.edu/page/ software_view/illinois-lemmatizer 2 System Description 78 Proceedings of the Nineteenth Conference on Computational Nat"
K15-2012,rizzolo-roth-2010-learning,1,0.822845,"itecture based on the description in Lin et al. (2014), then investigated features and inference approaches to improve the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). We present a system that implements an end-to-end discourse parser. The system uses a pipeline architecture with seven stages: preprocessing, recognizing explicit connectives, identifying argument positions, identifying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois NLP software1 and the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity featur"
K15-2012,C98-2181,1,\N,Missing
L18-1086,P06-4018,0,0.341529,"Missing"
L18-1086,P11-1056,1,0.783998,"Missing"
L18-1086,M98-1001,0,0.489588,"Missing"
L18-1086,clarke-etal-2012-nlp,1,0.65273,"tionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between t"
L18-1086,doddington-etal-2004-automatic,0,0.127526,"Missing"
L18-1086,N06-2015,0,0.256079,"Missing"
L18-1086,P10-2013,0,0.0751131,"Missing"
L18-1086,P14-5010,0,0.0181999,". Here is an example snippet showing how to annotate a sentence with C OG C OMP NLP Y: Pipeline. With all the Annotators generating the same data-structures, the P IPELINE project provides a simple interface to access Annotator components either individually or as a group, with a single function call. Use of P IPELINE is illustrated in Figure 3. A demo of P IPELINE is accessible online at http://nlp.cogcomp.org. One important aspect of our work is the collection of the major NLP annotators. Table 1 contains a summary of components that exist in other well-established NLP libraries. C ORE NLP (Manning et al., 2014) is a popular from ccg_nlpy import remote_pipeline pipeline = remote_pipeline.RemotePipeline() text = &quot;Hello, how are you. I am doing fine&quot; ta = pipeline.doc(text) print(ta.get_pos) # (UH Hello) (, ,) (WRB how) (VBP are) (PRP you)... 4. 1 543 Related Work https://github.com/CogComp/cogcomp-nlpy Task Dataset Measure Setting Result Tokenization POS (Roth and Zelenko, 1998) MASC (Ide et al., 2010) Accuracy – 97 Penn Treebank (Bies et al., 2015) F1 – 96.13 F1 – 91.12 F1 F1 F1 F1 F1 F1 – – – English Spanish Chinese 84.61 88.37 77.21 88.3 85 79.3 NER (Ratinov and Roth, 2009; Redman et al., 2016; Tsa"
L18-1086,W04-2705,0,0.100473,"Missing"
L18-1086,J05-1004,0,0.68556,"yakanok and Roth, 2000) CoNLL 2000 (Sang and Buchholz, 2000) F1 – 93.58 Temporal Normalization (Zhao et al., 2012) TempEval3 (UzZaman et al., 2013) Exact match F1 / Relaxed match F1 Temporal Span Extraction 79.35/ 83.4 F1 70.45 ACE 2005 (Walker et al., 2006) F1 F1 Temporal normalization, given a predicted temporal span Head detection Boundary detection given the head Head detection Boundary detection given the head Gold mention - Coarse Type Gold mention - Fine Type Test-I of Do and Roth (2012) Accuracy – 86.1 (Arivazhagan et al., 2016) F1 – 83.6 (Srikumar and Roth, 2013) F1 – 90.26 PropBank (Palmer et al., 2005a) F1 – 76.22 NomBank (Meyers et al., 2004) F1 – 66.97 Average of F1 score of MUC, B3 Gold mentions 77.05 CoNLL-12 (Pradhan et al., 2012) F1 ACE-05 (Walker et al., 2006) F1 Mention Detection F1 ERE F1 Relation Extraction (Chan and Roth, 2011) Taxonomic Relations (hypernyms, hyponyms, and co-hypernyms) Comma SRL (Arivazhagan et al., 2016) Preposition SRL (Srikumar and Roth, 2013) Verb SRL (Punyakanok et al., 2004) Nominal SRL(Punyakanok et al., 2004) Coreference (Samdani et al., 2014) B3 ACE-04 (Doddington et al., 2004) Wikifier (Tsai and Roth, 2016) F1 F1 F1 TAC-KBP 2016 EDL shared task 89.6 8"
L18-1086,D14-1162,0,0.0787689,"Missing"
L18-1086,W12-4501,0,0.0330716,"Missing"
L18-1086,W00-0721,1,0.380267,"Missing"
L18-1086,C04-1197,1,0.759993,"Missing"
L18-1086,W05-0639,0,0.0601394,"f C OG C OMP NLP core-utilities to extract features to be used by machine learning algorithms. E DISON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = a"
L18-1086,W09-1119,1,0.716643,"Missing"
L18-1086,rizzolo-roth-2010-learning,1,0.923167,"antic analysis, but also word and phrase similarity metrics. It provides essential support for text processing applications, including classes for text cleaning and for reading a number of popular NLP corpora. In addition to describing some key functionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is Te"
L18-1086,P98-2186,1,0.665416,"Missing"
L18-1086,P10-1122,1,0.792389,"SON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = augmentedSrlTa.getView(ViewNames.POS).getConstituents(); System.out.println(list); // (NNP Pierre"
L18-1086,L16-1645,1,0.930972,"trate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between them (with the targets b"
L18-1086,W00-0726,0,0.278326,"Missing"
L18-1086,Q13-1019,1,0.873032,"Missing"
L18-1086,W03-0419,0,0.250227,"Missing"
L18-1086,C16-2031,1,0.811715,"Missing"
L18-1086,S13-2001,0,0.0727449,"Missing"
L18-1086,Q15-1025,1,0.803667,"tic Parse, and Semantic Role Labeling • TAC/ERE Event, Relation, and Named Entity Similarity Utilities. For calculating semantic similarity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level dec"
L18-1086,P14-3006,0,0.0295918,"rity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level decisions (Do et al., 2009): Metric llm = new LLMStringSim(config); String s1 = &quot;Jack bought Alex&apos;s car&quot;; String s2 = &quot;Alex sold his car to"
L18-1086,N12-3008,1,0.753759,"Missing"
N15-1138,P14-1006,0,0.0166552,"er five runs. The results are shown in Table 4. We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or"
N15-1138,P14-1062,0,0.00478261,"We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or set similarity) (Smola et al., 2007; Gretton et"
N15-1138,N13-1090,0,0.446502,"ncepts 23.1 18.1 13.8 10.6 8.4 as the drop in the document size. For example, there are on average 8 concepts in the intersection of two vectors with 500 non-zero concepts when we split each document into 16 parts. When there are fewer overlapping terms between two pieces of texts, it can cause mismatch or biased match and result in less accurate comparison. In this paper, we propose to use unsupervised approaches to improve the representation, along with a corresponding similarity approach between these representations. Our contribution is twofold. First, we incorporate the popular word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) representations into ESA representation, and show that incorporating semantic relatedness between Wikipedia titles can indeed help the similarity measure between short texts. Second, we propose and evaluate three mechanisms for comparing the resulting representations. We verify the superiority of the proposed methods using three different NLP tasks. 2 Sparse Vector Densification In this section, we introduce a way to compute the similarity between two sparse vectors by augmenting the original similarity measure, i.e., cosine similarity. Suppose we have two vectors x ="
N15-1138,W14-1609,0,0.0138721,"re shown in Table 4. We can see that DenseESA also outperforms ESA. 4 Related Work ESA (Gabrilovich and Markovitch, 2006; Gabrilovich and Markovitch, 2007) and distributed word representations (Ratinov and Roth, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) are popular text representations 3 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 1279 that encode world knowledge. Recently, several representations were proposed to extend word representations for phrases or sentences (Lu and Li, 2013; Hermann and Blunsom, 2014; Passos et al., 2014; Kalchbrenner et al., 2014; Le and Mikolov, 2014; Hu et al., 2014; Sutskever et al., 2014; Zhao et al., 2015). In this paper, we evaluate how to combine two off-the-shelf representations to densify the similarity between text data. Yih et al. also used average matching and a different maximum matching for QA problem (Yih et al., 2013). However, their sparse representation is still at the word level while ours is based on ESA. Interestingly, related ideas to our average matching mechanism have been proposed also in the computer vision community, which is the set kernel (or set similarity) (Smo"
N15-1138,D14-1162,0,0.117284,"Missing"
N15-1138,W09-1119,1,0.126608,"Missing"
N15-1138,P10-1040,0,0.0883002,"Missing"
N15-1138,P13-1171,0,0.0580633,"Missing"
N19-1036,N07-1039,0,0.248816,"esponding aspects. the expectation and entropy terms into the same According to (Lakkaraju et al., 2014), some adscale (Marcheggiani and Titov, 2016). jectives serve both as target words and opinion words. For example, in the sentence “very tasty, 3 Target Opinion Word Pairs Extraction and drinkable,” the previous rules fail to extract any pair. But we know it contains a target-opinion Target-opinion word pairs extraction is a well pair, i.e., “taste-tasty.” Most of these adjectives studied problem (Hu and Liu, 2004; Popescu and have the same root form with the aspects they indiEtzioni, 2005; Bloom et al., 2007; Qiu et al., cated, e.g., “clean” (cleanliness), and “overpriced” 2011). We designed five rules to extract potential (price). This kind of adjective can be extracted target-opinion word pairs. Our method relies on first and then we can obtain more similar adjecStanford Dependency Parser (Chen and Manning, tives using word similarities. For example, given 2014). We describe our rules as follows. “tasty,” we could get “flavorful” by retrieving simRule 1: We extract pairs satisfying the ilar words. grammatical relation amod (adjectival modifier) (De Marneffe and Manning, 2008). For exTable 1 sho"
N19-1036,D14-1082,0,0.236688,"Missing"
N19-1036,D17-1310,0,0.120363,"Missing"
N19-1036,D17-1047,0,0.271735,"Advocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect. 1 Introduction Document-level multi-aspect sentiment classification (DMSC) aims to predict the sentiment polarity of each aspect given a document which consists of several sentences describing one or more aspects (Wang et al., 2010, 2011; Yin et al., 2017). Solving the DMSC task is useful for providing both recommendations for users and suggestions for business owners on customer review platforms. Aspect based sentiment classification (Tang et al., 2016a,b; Wang et al., 2016b; Chen et al., 2017; Ma et al., 2017; Wang et al., 2018) was usually done by supervised learning, where aspectlevel annotations should be provided. Aspect-level annotations are not easy to obtain. Even when the platform provides the function to rate for different aspects, users are less likely to submit all of them. 386 Proceedings of NAACL-HLT 2019, pages 386–396 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ond sub-task is to predict the opinion word given a target word and a sentiment polarity predicted by the first sub-task. For example, knowing the fact that"
N19-1036,D15-1168,0,0.0224964,"et al. (2011) identified dependency paths that link opinion words and targets via a bootstrapping process. This method only needs an initial opinion lexicon to start the bootstrapping process. Zhuang et al. (2006) adopted a supervised learning algorithm to learn valid dependency tree path templates, but it requires target-opinion pairs annotations. Learning based methods require lots of targetopinion pairs annotations. They trained conditional random fields (CRF) (Lafferty et al., 2001) based models (Jakob and Gurevych, 2010; Yang and Cardie, 2012; Wang et al., 2016a) or deep neural networks (Liu et al., 2015; Wang et al., 2017; Li 393 path. Our method also uses neural networks to parameterize two discrete distributions but aims to solve the DMSC task. and Lam, 2017) to predict the label (target, opinion or other) of each word. Jakob and Gurevych (2010) and Li et al. (2012) extracted target-opinion pairs without using using any labeled data in the domain of interest, but it needs lots of labeled data in another related domain. In this paper, we only use very simple rules to extract target-opinion pairs to validate the effectiveness of our approach. If better pairs can be extracted, we can further"
N19-1036,N18-1165,0,0.0319389,"ork in practice. In the future, we plan to explore better targetopinion word extraction approaches to find better “supervision” signals. Variational Methods. Variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) (VAEs) use a neural network to parameterize a probability distribution. VAEs consists of an encoder which parameterizes posterior probabilities and a decoder which parameterizes the reconstruction likelihood given a latent variable. VAEs inspire many interesting works (Titov and Khoddam, 2015; Marcheggiani ˇ and Titov, 2016; Suster et al., 2016; Zhang et al., 2018; Chen et al., 2018) which are slightly different from VAEs. Their encoders produce a discrete distribution while the encoder in VAEs yields a continuous latent variable. Titov and Khoddam (2015) aimed to solve semantic role labeling problem. The encoder is essentially a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features. The decoder reconstructs argument fillers given predicted roles. Marcheggiani and Titov (2016) aimed to solve unsupervised open domain relation discovery. The encoder is a feature-rich relation extractor, which predicts a semantic relation betwee"
N19-1036,Q16-1017,0,0.367286,"nt), and the head word is one of the follow  ing word: “seem”,“look”, “feel”, “smell”, and Eq(Ra |x) log σ ϕ(wo , wt , ra ) “taste”. For example, in the sentence “This beer X  + log σ − ϕ(wo0 , wt , ra ) + αH(q(Ra |x)) , tastes spicy,” we can extract “taste” and “spicy” as a target-opinion pair. wo0 ∈N (7) Rule 5: If the sentence contains some adjectives that can implicitly indicate aspects, we manwhere α is a hyper-parameter which can adjust ually assign them to the corresponding aspects. the expectation and entropy terms into the same According to (Lakkaraju et al., 2014), some adscale (Marcheggiani and Titov, 2016). jectives serve both as target words and opinion words. For example, in the sentence “very tasty, 3 Target Opinion Word Pairs Extraction and drinkable,” the previous rules fail to extract any pair. But we know it contains a target-opinion Target-opinion word pairs extraction is a well pair, i.e., “taste-tasty.” Most of these adjectives studied problem (Hu and Liu, 2004; Popescu and have the same root form with the aspects they indiEtzioni, 2005; Bloom et al., 2007; Qiu et al., cated, e.g., “clean” (cleanliness), and “overpriced” 2011). We designed five rules to extract potential (price). This"
N19-1036,N15-1184,0,0.0735614,"Missing"
N19-1036,N13-1092,0,0.105594,"Missing"
N19-1036,D10-1101,0,0.0453664,"opinion word candidate belongs to the set of path templates, the pair will be extracted. Qiu et al. (2011) identified dependency paths that link opinion words and targets via a bootstrapping process. This method only needs an initial opinion lexicon to start the bootstrapping process. Zhuang et al. (2006) adopted a supervised learning algorithm to learn valid dependency tree path templates, but it requires target-opinion pairs annotations. Learning based methods require lots of targetopinion pairs annotations. They trained conditional random fields (CRF) (Lafferty et al., 2001) based models (Jakob and Gurevych, 2010; Yang and Cardie, 2012; Wang et al., 2016a) or deep neural networks (Liu et al., 2015; Wang et al., 2017; Li 393 path. Our method also uses neural networks to parameterize two discrete distributions but aims to solve the DMSC task. and Lam, 2017) to predict the label (target, opinion or other) of each word. Jakob and Gurevych (2010) and Li et al. (2012) extracted target-opinion pairs without using using any labeled data in the domain of interest, but it needs lots of labeled data in another related domain. In this paper, we only use very simple rules to extract target-opinion pairs to validat"
N19-1036,S15-2121,0,0.288608,"Missing"
N19-1036,H05-1043,0,0.266126,"rule based methods and learning based methods to solve this task. Rule based methods extract target-opinion word pairs by mining the dependency tree paths between target words and opinion words. Learning based methods treat this task as a sequence labeling problem, mapping each word to one of the following categories: target, opinion, and other. (Hu and Liu, 2004) is one of earliest rule based methods to extract target-opinion pairs. An opinion word is restricted to be an adjective. Target words are extracted first, and then an opinion word is linked to its nearest target word to form a pair. Popescu and Etzioni (2005) and Bloom et al. (2007) manually designed dependency tree path templates to extract target-opinion pairs. If the path between a target word candidate and an opinion word candidate belongs to the set of path templates, the pair will be extracted. Qiu et al. (2011) identified dependency paths that link opinion words and targets via a bootstrapping process. This method only needs an initial opinion lexicon to start the bootstrapping process. Zhuang et al. (2006) adopted a supervised learning algorithm to learn valid dependency tree path templates, but it requires target-opinion pairs annotations"
N19-1036,J11-1002,0,0.0567238,"each word to one of the following categories: target, opinion, and other. (Hu and Liu, 2004) is one of earliest rule based methods to extract target-opinion pairs. An opinion word is restricted to be an adjective. Target words are extracted first, and then an opinion word is linked to its nearest target word to form a pair. Popescu and Etzioni (2005) and Bloom et al. (2007) manually designed dependency tree path templates to extract target-opinion pairs. If the path between a target word candidate and an opinion word candidate belongs to the set of path templates, the pair will be extracted. Qiu et al. (2011) identified dependency paths that link opinion words and targets via a bootstrapping process. This method only needs an initial opinion lexicon to start the bootstrapping process. Zhuang et al. (2006) adopted a supervised learning algorithm to learn valid dependency tree path templates, but it requires target-opinion pairs annotations. Learning based methods require lots of targetopinion pairs annotations. They trained conditional random fields (CRF) (Lafferty et al., 2001) based models (Jakob and Gurevych, 2010; Yang and Cardie, 2012; Wang et al., 2016a) or deep neural networks (Liu et al., 2"
N19-1036,P15-1008,0,0.017447,"orm sentence segmentation to generate the representation of each aspect. To address the limitation of using aspect keywords, LARAM (Wang et al., 2011) assumes that the text content describing a particular aspect is generated by sampling words from a topic model corresponding to the latent aspect. Both LRR and LARAM can only access to overall polarities in the training data, but not gold standards of aspect polarities. Meng et al. (2018) proposed a weakly supervised text classification method which can take label surface names, class-related keywords, or a few labeled documents as supervision. Ramesh et al. (2015) developed a weakly supervised joint model to identify aspects and the corresponding sentiment polarities in online courses. They treat aspect (sentiment) related seed words as weak supervision. In the DMSC task which is a fine-grained text classification task, the label surface names or keywords for some aspects would be very similar. Given that the inputs are the same and the supervisions are similar, weakly supervised models cannot distinguish them. So we do not consider them as our baselines. Yin et al. (2017) modeled this problem as a machine comprehension problem under a multi-task learn"
N19-1036,D16-1011,0,0.0531838,"embeddings trained by word2vec (Mikolov et al., 2013). For example, suppose seed words {“room”, “bed”} and {“business”, “Internet”} are used to describe the aspect room and business respectively, and the candidate pair “pillow - soft” will be assigned to the aspect room if the similarity between “pillow” and “bed” is highest among all combinations. 4 Experiment In this section, we report average sentiment classification accuracies over all aspects on binary DMSC task. 4.1 Compared Methods Datasets We evaluate our model on TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016; Yin et al., 2017) datasets, which contain seven aspects (value, room, location, cleanliness, check in/front desk, service, and business) and four aspects (feel, look, smell, and taste) respectively. We run the same preprocessing steps as (Yin et al., 2017). Both datasets are split into train/development/test sets with proportions 8:1:1. All methods can use development set to tune their hyper-parameters. Ratings of TripAdvisor and BeerAdvocate datasets are on scales of 1 to 5 and 0 to 5 respectively. But in BeerAdvocate, 0 star is rare, so we treat the scale as 1 to 5. We convert original sca"
N19-1036,P12-1043,0,0.0225049,"ependency tree path templates, but it requires target-opinion pairs annotations. Learning based methods require lots of targetopinion pairs annotations. They trained conditional random fields (CRF) (Lafferty et al., 2001) based models (Jakob and Gurevych, 2010; Yang and Cardie, 2012; Wang et al., 2016a) or deep neural networks (Liu et al., 2015; Wang et al., 2017; Li 393 path. Our method also uses neural networks to parameterize two discrete distributions but aims to solve the DMSC task. and Lam, 2017) to predict the label (target, opinion or other) of each word. Jakob and Gurevych (2010) and Li et al. (2012) extracted target-opinion pairs without using using any labeled data in the domain of interest, but it needs lots of labeled data in another related domain. In this paper, we only use very simple rules to extract target-opinion pairs to validate the effectiveness of our approach. If better pairs can be extracted, we can further improve our results. 6 Conclusion In this paper, we propose a variational approach to weakly supervised DMSC. We extract many target-opinion word pairs from dependency parsers using simple rules. These pairs can be “supervision” signals to predict sentiment polarity. Ou"
N19-1036,P15-1107,0,0.0862734,"Missing"
N19-1036,N16-1160,0,0.0608995,"Missing"
N19-1036,C16-1311,0,0.159268,"pervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect. 1 Introduction Document-level multi-aspect sentiment classification (DMSC) aims to predict the sentiment polarity of each aspect given a document which consists of several sentences describing one or more aspects (Wang et al., 2010, 2011; Yin et al., 2017). Solving the DMSC task is useful for providing both recommendations for users and suggestions for business owners on customer review platforms. Aspect based sentiment classification (Tang et al., 2016a,b; Wang et al., 2016b; Chen et al., 2017; Ma et al., 2017; Wang et al., 2018) was usually done by supervised learning, where aspectlevel annotations should be provided. Aspect-level annotations are not easy to obtain. Even when the platform provides the function to rate for different aspects, users are less likely to submit all of them. 386 Proceedings of NAACL-HLT 2019, pages 386–396 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ond sub-task is to predict the opinion word given a target word and a sentiment polarity predicted by the first su"
N19-1036,D16-1021,0,0.127768,"pervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect. 1 Introduction Document-level multi-aspect sentiment classification (DMSC) aims to predict the sentiment polarity of each aspect given a document which consists of several sentences describing one or more aspects (Wang et al., 2010, 2011; Yin et al., 2017). Solving the DMSC task is useful for providing both recommendations for users and suggestions for business owners on customer review platforms. Aspect based sentiment classification (Tang et al., 2016a,b; Wang et al., 2016b; Chen et al., 2017; Ma et al., 2017; Wang et al., 2018) was usually done by supervised learning, where aspectlevel annotations should be provided. Aspect-level annotations are not easy to obtain. Even when the platform provides the function to rate for different aspects, users are less likely to submit all of them. 386 Proceedings of NAACL-HLT 2019, pages 386–396 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ond sub-task is to predict the opinion word given a target word and a sentiment polarity predicted by the first su"
N19-1036,N15-1001,0,0.0298678,"lts to the supervised method with hundreds of labels per aspect, which can reduce a lot of labor work in practice. In the future, we plan to explore better targetopinion word extraction approaches to find better “supervision” signals. Variational Methods. Variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) (VAEs) use a neural network to parameterize a probability distribution. VAEs consists of an encoder which parameterizes posterior probabilities and a decoder which parameterizes the reconstruction likelihood given a latent variable. VAEs inspire many interesting works (Titov and Khoddam, 2015; Marcheggiani ˇ and Titov, 2016; Suster et al., 2016; Zhang et al., 2018; Chen et al., 2018) which are slightly different from VAEs. Their encoders produce a discrete distribution while the encoder in VAEs yields a continuous latent variable. Titov and Khoddam (2015) aimed to solve semantic role labeling problem. The encoder is essentially a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features. The decoder reconstructs argument fillers given predicted roles. Marcheggiani and Titov (2016) aimed to solve unsupervised open domain relation discovery"
N19-1036,D16-1059,0,0.199759,"TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect. 1 Introduction Document-level multi-aspect sentiment classification (DMSC) aims to predict the sentiment polarity of each aspect given a document which consists of several sentences describing one or more aspects (Wang et al., 2010, 2011; Yin et al., 2017). Solving the DMSC task is useful for providing both recommendations for users and suggestions for business owners on customer review platforms. Aspect based sentiment classification (Tang et al., 2016a,b; Wang et al., 2016b; Chen et al., 2017; Ma et al., 2017; Wang et al., 2018) was usually done by supervised learning, where aspectlevel annotations should be provided. Aspect-level annotations are not easy to obtain. Even when the platform provides the function to rate for different aspects, users are less likely to submit all of them. 386 Proceedings of NAACL-HLT 2019, pages 386–396 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ond sub-task is to predict the opinion word given a target word and a sentiment polarity predicted by the first sub-task. For example, k"
N19-1036,D16-1058,0,0.484469,"TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect. 1 Introduction Document-level multi-aspect sentiment classification (DMSC) aims to predict the sentiment polarity of each aspect given a document which consists of several sentences describing one or more aspects (Wang et al., 2010, 2011; Yin et al., 2017). Solving the DMSC task is useful for providing both recommendations for users and suggestions for business owners on customer review platforms. Aspect based sentiment classification (Tang et al., 2016a,b; Wang et al., 2016b; Chen et al., 2017; Ma et al., 2017; Wang et al., 2018) was usually done by supervised learning, where aspectlevel annotations should be provided. Aspect-level annotations are not easy to obtain. Even when the platform provides the function to rate for different aspects, users are less likely to submit all of them. 386 Proceedings of NAACL-HLT 2019, pages 386–396 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ond sub-task is to predict the opinion word given a target word and a sentiment polarity predicted by the first sub-task. For example, k"
N19-1036,H05-1044,0,0.270746,"Missing"
N19-1036,D12-1122,0,0.0287853,"longs to the set of path templates, the pair will be extracted. Qiu et al. (2011) identified dependency paths that link opinion words and targets via a bootstrapping process. This method only needs an initial opinion lexicon to start the bootstrapping process. Zhuang et al. (2006) adopted a supervised learning algorithm to learn valid dependency tree path templates, but it requires target-opinion pairs annotations. Learning based methods require lots of targetopinion pairs annotations. They trained conditional random fields (CRF) (Lafferty et al., 2001) based models (Jakob and Gurevych, 2010; Yang and Cardie, 2012; Wang et al., 2016a) or deep neural networks (Liu et al., 2015; Wang et al., 2017; Li 393 path. Our method also uses neural networks to parameterize two discrete distributions but aims to solve the DMSC task. and Lam, 2017) to predict the label (target, opinion or other) of each word. Jakob and Gurevych (2010) and Li et al. (2012) extracted target-opinion pairs without using using any labeled data in the domain of interest, but it needs lots of labeled data in another related domain. In this paper, we only use very simple rules to extract target-opinion pairs to validate the effectiveness of"
N19-1036,D17-1217,1,0.873963,"Missing"
N19-1036,H05-2017,0,\N,Missing
N19-1093,E09-1018,0,0.0803473,"lution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), prono"
N19-1093,P15-1136,0,0.158307,"Missing"
N19-1093,D16-1245,0,0.161268,"Missing"
N19-1093,P81-1019,0,0.646079,"e the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin. 1 Figure 1: Pronoun coreference examples, where each example requires different knowledge for its resolution. Blue bold font refers to the target pronoun, where the correct noun reference and other candidates are marked by green underline and brackets, respectively. Introduction The question of how human beings resolve pronouns has long been of interest to both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and brings challenges in natural language understanding. To explore solutions for that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M"
N19-1093,D17-1018,0,0.279033,"ontext and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon d"
N19-1093,N18-2108,0,0.300199,"Missing"
N19-1093,N18-2028,1,0.81493,"Missing"
N19-1093,P11-1117,0,0.487864,"Missing"
N19-1093,P98-2143,0,0.817399,"r that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978"
N19-1093,P03-1022,0,0.647751,"Missing"
N19-1093,1995.tmi-1.6,0,0.947111,"Missing"
N19-1093,C94-2189,0,0.580487,"ore solutions for that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decad"
N19-1093,N18-1151,1,0.831826,"ormation. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires a more general type of knowledge1 that ‘cats can climb trees but a dog normally does no"
N19-1093,D14-1162,0,0.0836574,"e final two-layer model outperforms the Feature Concatenation model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason • Feature Concatenation, a simplified version of the complete model that removes the second knowledge processing layer, but directly treats all external knowledge embeddings as features and concatenates them to span representations. 4.4 Implementation Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embeddings (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embeddings as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. Hyper-parameters are 12 13 Experimental Results https://stanfordnlp.github.io/CoreNLP/coref.html https://github.com/kentonl/e2e-coref 877 Figure 5: Effect of different thresholds on candidate numbers. Max and Average number of candidates after pruning are represented with solid lines in blue and orange, respectively. Two dashed lines indicate the max and the average number of candidates before pruning. F1 ∆F1 The Complete Model 81.0 - –Plurality knowled"
N19-1093,N18-1202,0,0.372661,"erformance of different models on the evaluation dataset. Precision (P), recall (R), and F1 score are reported, with the best one in each F1 column marked bold. features between clusters and mentions for coreference resolution. • Deep-RL model, proposed by Clark and Manning (2016), a reinforcement learning method to directly optimize the coreference matrix instead of the traditional loss function. • End2end is the current state-of-the-art coreference model (Lee et al., 2018), which performs in an end-to-end manner and leverages both the contextual information and a pre-trained language model (Peters et al., 2018). set as follows. The hidden state of the LSTM module is set to 200, and all the feed-forward networks in our model have two 150-dimension hidden layers. The default pruning threshold t for softmax pruning is set to 10−7 . All linguistic features (plurality and AG) and external knowledge (SP) are encoded as 20-dimension embeddings. For model training, we use cross-entropy as the loss function and Adam (Kingma and Ba, 2015) as the optimizer. All the aforementioned hyperparameters are initialized randomly, and we apply dropout rate 0.2 to all hidden layers in the model. Our model treats a candid"
N19-1093,W12-4501,0,0.131122,"., nsubj and dobj). The resulted frequency is grouped into the following buckets [1, 2, 3, 4, 5-7, 8-15, 16-31, 3263, 64+] and we use the bucket id as the final SP knowledge. Thus in the previous example: Table 1: Statistics of the evaluation dataset. Number of selected pronouns are reported. scores, the module allow more of them to proceed to the second layer. Compared with other conventional pruning methods (Lee et al., 2017, 2018) that generally keep a fixed number of candidates, our pruning strategy is more efficient and flexible. 4 Experiment Settings 4.1 Data The CoNLL-2012 shared task (Pradhan et al., 2012) corpus is used as the evaluation dataset, which is selected from the Ontonotes 5.07 . Following conventional approaches (Ng, 2005; Li et al., 2011), for each pronoun in the document, we consider candidate n from the previous two sentences and the current sentence. For pronouns, we consider two types of them following Ng (2005), i.e., third personal pronoun (she, her, he, him, them, they, it) and possessive pronoun (his, hers, its, their, theirs). Table 1 reports the number of the two type pronouns and the overall statistics for the experimental dataset. According to our selection range of can"
N19-1093,D10-1048,0,0.260818,"Missing"
N19-1093,D09-1101,0,0.0694923,"; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ Th"
N19-1093,P11-1082,0,0.353145,"reference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires"
N19-1093,K17-1016,1,0.786915,"er capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires a more general type of knowledge1 that ‘cats can climb trees"
N19-1332,P15-1034,0,0.0263685,"anova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Trouillon et al., 2016) represent entities"
N19-1332,C10-1018,0,0.155338,"5. From the Table we can see that our target relation cluster is /location/contained by. In the first example, the similarity between entity pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schem"
N19-1332,P11-1054,0,0.0112937,"nts based on background knowledge are also well studied. For example, constrained conditional models (CCM) (Chang et al., 2012) provides a very flexible framework to decouple learning and inference, where in the inference step, background knowledge can be incorporated as an ILP (integer linear programming) problem. Posterior regularization (PR) (Ganchev et al., 2010) generalizes this idea so that it uses a joint learning and inference framework to incorporate the background knowledge. Both CCM and PR have many applications including the application to relation extraction (Chan and Roth, 2010; Chen et al., 2011). Compared to these existing approaches, our constraints are derived from the general-purpose KB, which is quite different from their way of manually crafting some background knowledge as declarative rules. It is very interesting that we are similar to the PR framework. Since we use a DVAE framework as the base algorithm, there is no traditional E-step and M-step in the variational inference. Instead, only q and p probabilities parameterized by neural networks are updated. In our framework, we can add constraints to either q or p probabilities (applying to p needs modification of normalization"
N19-1332,N18-1165,0,0.019843,"ble property that the translation operation can be easily recovered by entity vectors (r1,2 = e1 − e2 ). With its simplicity and high performance, TransE is enough for demonstration. Though our method is not restricted to the representation form of KB, we leave it for future evaluation. Constraints can be made more explainable by paths finding. For instance, the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) uses random walk to perform multi-hop reasoning based on logic rules. Later on, reinforcement Learning (Toutanova et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) is used to search for paths more effectively. Though heuristics are used to further reduce the number of mined relations, it is still very costly to find the paths for KB with hundreds of relations, if not impossible. Constraint Modeling. Originated from semisupervised learning (Chapelle et al., 2006), mustlink and cannot-link modeling has been well studied in machine learning community (Wagstaff et al., 2001; Basu et al., 2004, 2008). Such constraints were usually generated based on the ground truth labels of data. For document clustering, word constraints constructed based on WordNet simila"
N19-1332,D15-1082,0,0.0212586,"; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Trouillon et al., 2016) represent entities and relations as vectors, denoted as e and Cr respectively such that for a distances function f , the value f (e1 , Cr , e2 ) is maximized for all (e1 , r, e2 ) facts. Among all these methods, TransE model has a favorable property that the translation operation can be easily recovered by entity vectors (r1,2 = e1 − e2 ). With its simplicity and high performance, TransE is enough for demonstration. Though our method is not restricted to the representation form of KB, we leave it for future evaluation. Constraints can be made more explainable by paths"
N19-1332,P16-1200,0,0.0260189,"of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same set"
N19-1332,D11-1142,0,0.061102,"ersal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014;"
N19-1332,P11-1055,0,0.0344826,"pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It n"
N19-1332,P04-3022,0,0.0704826,"larities in Table 5. From the Table we can see that our target relation cluster is /location/contained by. In the first example, the similarity between entity pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text c"
N19-1332,D11-1049,0,0.0495762,"as e and Cr respectively such that for a distances function f , the value f (e1 , Cr , e2 ) is maximized for all (e1 , r, e2 ) facts. Among all these methods, TransE model has a favorable property that the translation operation can be easily recovered by entity vectors (r1,2 = e1 − e2 ). With its simplicity and high performance, TransE is enough for demonstration. Though our method is not restricted to the representation form of KB, we leave it for future evaluation. Constraints can be made more explainable by paths finding. For instance, the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) uses random walk to perform multi-hop reasoning based on logic rules. Later on, reinforcement Learning (Toutanova et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) is used to search for paths more effectively. Though heuristics are used to further reduce the number of mined relations, it is still very costly to find the paths for KB with hundreds of relations, if not impossible. Constraint Modeling. Originated from semisupervised learning (Chapelle et al., 2006), mustlink and cannot-link modeling has been well studied in machine learning community (Wagstaff et al., 2001;"
N19-1332,Q16-1017,0,0.0701328,"ore importantly, either traditional supervised learning or distantly supervised learning cannot discover new relations unseen in the training phase. Unsupervised relation discovery tries to overcome the shortcomings of supervised or distantly supervised learning approaches. Existing approaches either extract surface or syntactic patterns from sentences and use relation expressions as predicates (which result in many noisy relations) (Etzioni et al., 2004; Banko et al., 2007), or cluster the relation expressions based on the extracted triplets to form relation clusters (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016). However, these approaches do not use existing highquality and large-scale KBs when they are relevant to the relations to be discovered. In this paper, we consider a new relation discovery problem where both the training corpus for relation clustering and a KB are available, but the relations in the training corpus and those in the KB are not overlapped. As shown in Figure 1, in the KB, we have entities Pink Floyd, Animals, etc., with some existing relations notable work and has member in the KB. However, when doing relation discovery, we can only get supporting sentences that suggest new rel"
N19-1332,D12-1048,0,0.0383168,"et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Tro"
N19-1332,E17-1058,0,0.0144754,"tated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, r"
N19-1332,P09-1113,0,0.0797951,"st example, the similarity between entity pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can als"
N19-1332,D11-1134,0,0.0209593,"ch is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Trouillon et al., 2016) represent entities and relations as vectors, denoted as e and Cr respectively such that for a distances function f , the value f (e1 , Cr , e2 ) is maximized for all (e1 , r, e2 ) f"
N19-1332,D12-1104,0,0.0249747,"atterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 2015). On the other hand, relation clustering approaches group all the related relation expressions to represent a relation (Lin and Pantel, 2001; Mohamed et al., 2011; Takamatsu et al., 2011; Yao et al., 2011, 2012; Nakashole et al., 2012a,b; Marcheggiani and Titov, 2016). Our setting is based on (Marcheggiani and Titov, 2016) but we also introduce KB as a different kind of weak and indirect supervision. Knowledge Base Representation. Embedding based knowledge base representation learning methods (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Trouillon et al., 2016) represent entities and relations as vectors, denoted as e and Cr respectively such that for a distances function f , the value f (e1 , Cr , e2 ) is maximized for all (e1 , r, e2 ) facts. Among all these methods, TransE model has a favorable property tha"
N19-1332,P02-1006,0,0.438011,"Missing"
N19-1332,N13-1008,0,0.0300309,"istant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al."
N19-1332,C02-1151,0,0.176723,"of entity pair similarities in Table 5. From the Table we can see that our target relation cluster is /location/contained by. In the first example, the similarity between entity pairs (Spain, Europe) and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB an"
N19-1332,D17-1060,0,0.0218747,"ese methods, TransE model has a favorable property that the translation operation can be easily recovered by entity vectors (r1,2 = e1 − e2 ). With its simplicity and high performance, TransE is enough for demonstration. Though our method is not restricted to the representation form of KB, we leave it for future evaluation. Constraints can be made more explainable by paths finding. For instance, the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) uses random walk to perform multi-hop reasoning based on logic rules. Later on, reinforcement Learning (Toutanova et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) is used to search for paths more effectively. Though heuristics are used to further reduce the number of mined relations, it is still very costly to find the paths for KB with hundreds of relations, if not impossible. Constraint Modeling. Originated from semisupervised learning (Chapelle et al., 2006), mustlink and cannot-link modeling has been well studied in machine learning community (Wagstaff et al., 2001; Basu et al., 2004, 2008). Such constraints were usually generated based on the ground truth labels of data. For document clustering, word constrain"
N19-1332,P13-2117,0,0.0609863,"Missing"
N19-1332,D12-1042,0,0.0236111,"and (Portugal, Europe) are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pa"
N19-1332,N13-1107,0,0.0227673,"are high, which indicates the same cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co"
N19-1332,D11-1135,0,0.416773,"(Riedel et al., 2010). More importantly, either traditional supervised learning or distantly supervised learning cannot discover new relations unseen in the training phase. Unsupervised relation discovery tries to overcome the shortcomings of supervised or distantly supervised learning approaches. Existing approaches either extract surface or syntactic patterns from sentences and use relation expressions as predicates (which result in many noisy relations) (Etzioni et al., 2004; Banko et al., 2007), or cluster the relation expressions based on the extracted triplets to form relation clusters (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016). However, these approaches do not use existing highquality and large-scale KBs when they are relevant to the relations to be discovered. In this paper, we consider a new relation discovery problem where both the training corpus for relation clustering and a KB are available, but the relations in the training corpus and those in the KB are not overlapped. As shown in Figure 1, in the KB, we have entities Pink Floyd, Animals, etc., with some existing relations notable work and has member in the KB. However, when doing relation discovery, we can only get supp"
N19-1332,D15-1174,0,0.134156,"ot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2013b; Angeli et al., 201"
N19-1332,N16-1103,0,0.0181956,"es KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is under the same setting with distant supervision. Those surface patterns are pre-extracted and shown in the training phase, which makes it also a weakly supervised learning method. Unsupervised Relation Extraction. Open Domain Information Extraction (Open-IE) assumes that every relation expression can represent a unique relation (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Xu et al., 2"
N19-1332,P12-1075,0,0.0481644,"Missing"
N19-1332,D15-1203,0,0.0314095,"me cluster of pairs of sentences. The same constraint is applied in the second example, although there’s no direct connection between (Brazil, Latin America), (Argentina, Latin America). 5 Related Work Supervised and Distantly Supervised Relation Extraction. Traditional supervised relation extraction focuses on a limited number of relations (Roth and Yih, 2002; Kambhatla, 2004; Chan and Roth, 2010). Distant supervision uses KBs to obtain a lot of automatically annotated data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Xu et al., 2013a; Zhang et al.; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017). There are two important assumptions behind these models, namely multiinstance learning (Riedel et al., 2010) and multiinstance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Our setting is similar to multi-instance learning but we assume there is no overlapped relation between KB and training text corpus. Universal schema (Riedel et al., 2013; Verga et al., 2016; Toutanova et al., 2015; McCallum et al., 2017) can also exploit KB to help extract relations. It needs a lot of entity pairs in text to co-occur with KB triplets, which is"
N19-1332,D17-1186,0,0.0710036,"he current and total training steps respectively. This method enables the extractor to explore more possibilities first and finally converge to a stable distribution. It is difficult to directly compute the partition function in Eq. (5), as it requires to sum over |V|. We use the same negative sampling method as (Marcheggiani and Titov, 2016) to substitute log p(ei |e−i , r, θ) in Eq. (11) with: • The second and third ones are usually applied by supervised models. So when they generated the data, they tended to focus on relations with more supporting sentences. The second one was developed by Zeng et al. (2017). The data is built by aligning Wikidata (Vrandeˇci´c, 2012) relations with NYT corpus, as a result of 99 possible relations. It is built to contain more updated facts and richer structures of relations, e.g., a larger number of relation/relation paths. We use this dataset to amplify the effects coming from relation paths in KB, as the data was used to train a path-based relation extraction model. log p(ei |e−i , r, θ) ≈ log σ(φ(ei , e−i , r, θ)) X + log σ (−φ(eneg , ei , r, θ)) , eneg ∈N where N is the set of randomly sampled entities in V and σ is the sigmoid function. 4 Experiments • The th"
P16-2063,J92-4003,0,0.0764577,"Missing"
P16-2063,W06-2920,0,0.0120663,"fact, result in almost the same vectors as produced by the original truncation method. 5 Conclusion Experiments on Dependency Parsing We also incorporate word embedding results into a downstream task, dependency parsing, to evaluate whether the truncated embedding results are still good features compared to the original features. We follow the setup of (Guo et al., 2015) in a monolingual setting3 . We train the parser with 5,000 iterations using different truncation settings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task (Buchholz and Marsi, 2006). We follow (Guo et al., 2015) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions. The results shown in Table 3 for dependency parsing are consistent with word similarity and paraphrasing. First, we see that binarization for CBOW and skipgram is again better than the truncation approach. Second, for truncation results, more bits leads to better results. With 8-bits, we can again obtain results similar to those"
P16-2063,E14-1049,0,0.0711899,"Missing"
P16-2063,P15-1144,0,0.0341449,"lov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets. These works indicate that, for some tasks, we do not need all the information encoded in “standard” word embeddings. Nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks. 3 for each value. In practice, we first scale all"
P16-2063,D14-1012,0,0.255319,"tational Linguistics, pages 387–392, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets. These works indicate that,"
P16-2063,P15-1119,0,0.0127636,"tend these experiments and study the impact of the representation size on more advanced tasks. Trunc. (8 b.) 0.997 0.992 1.001 0.999 ing method to converge. Auxiliary update vectors achieve very similar results to the original vectors, and, in fact, result in almost the same vectors as produced by the original truncation method. 5 Conclusion Experiments on Dependency Parsing We also incorporate word embedding results into a downstream task, dependency parsing, to evaluate whether the truncated embedding results are still good features compared to the original features. We follow the setup of (Guo et al., 2015) in a monolingual setting3 . We train the parser with 5,000 iterations using different truncation settings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task (Buchholz and Marsi, 2006). We follow (Guo et al., 2015) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions. The results shown in Table 3 for dependency parsing are consistent with word sim"
P16-2063,Q15-1016,0,0.0323312,"memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. 1 Introduction There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Levy et al., 2015). Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications that make use of billions of tokens and multiple languages, size issues impose significan"
P16-2063,N13-1090,0,0.421697,"er, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used. The results we present are quite surprising. We show that it is possible to reduce the memory consumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide two ways to train existing algorithms (Mikolov et al., 2013a; Mikolov et al., 2013b) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16. The truncated word"
P16-2063,W09-1119,1,0.501275,"Meeting of the Association for Computational Linguistics, pages 387–392, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b). However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datase"
P16-2063,P10-1040,0,0.0687955,"ion on word embeddings. We present a systematic evaluation of word embeddings with limited memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. 1 Introduction There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Levy et al., 2015). Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications"
P16-2063,Q15-1025,0,\N,Missing
P19-1071,P14-2050,0,0.137809,"Missing"
P19-1071,D14-1004,0,0.654398,"Missing"
P19-1071,D14-1162,0,0.0835445,"dge dataset. For this experiment, we select the English version of Wikipedia4 and filter out pages Inner-Annotator Agreement Following standard practices from previous datasets WSIM-203 (Reisinger and Mooney, 2010) and Simlex-999 (Hill et al., 2015), we employ Inter-Annotator Agreement (IAA), which 4 725 https://dumps.wikimedia.org/enwiki/ #(sentence) #(dobj pairs) #(nsubj pairs) #(amod pairs) #(dobj amod pairs) #(nsubj amod pairs) Wiki Yelp NYT 82m 69m 97m 119m 21m 16m 41m 33m 70m 31m 8.1m 4.8m 56m 49m 86m 65m 14m 12m the weighting function and the cosine similarity of their GloVe embedding (Pennington et al., 2014) as the similarity function s(d, d0 ), given the relative popularity of these embeddings. Neural Network (NN): de Cruys (2014) proposes a NN-based method for the SP acquisition task. The main framework is a two-layer fullyconnected NN. For each SP pair (h, d), the framework uses the concatenation of embeddings [vh , vd ] as the input to the NN, where vh , vd are randomly initialized word embeddings for words h and d respectively. The ranking-loss (Collobert and Weston, 2008) is used as the training objective, where positive examples consist of all the SP pairs in the corpus and negative exampl"
P19-1071,J10-4007,0,0.623155,"Missing"
P19-1071,D10-1114,0,0.0542905,"Missing"
P19-1071,D17-1138,0,0.113183,"guage that has been shown to be related to semantics (Wilks, 1975). Here by SP we mean that, given a word and a dependency relation, human beings have preferences for which words are likely to be connected. For instance, when seeing the verb ‘sing’, it is highly plausible that its object is ‘a song’, and when seeing the noun ‘air’, it is highly plausible that its modifier is ‘fresh’. SP has been shown to be useful over a variety of tasks including sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), coreference clustering (Hobbs, 1978; Inoue et al., 2016; Heinzerling et al., 2017), and machine translation (Tang et al., 2016). Given the importance of SP, the automatic acquisition of SP has become a well-known research subject in the • (A) The fish ate the worm. It was hungry. • (B) The fish ate the worm. It was tasty. In (A), we can resolve ‘it’ to ‘the fish’ because it is more plausible that the subject of the verb ‘eat’ is hungry. On the other hand, for (B), we can resolve ‘it’ to ‘the worm’ because it is more likely that the object of the verb ‘eat’ is tasty. The above examples reflect the preferences between two twohop dependency relations: ‘verb-object-modifier’ 72"
P19-1071,P10-1044,0,0.168912,"we can resolve ‘it’ to ‘the worm’ because it is more likely that the object of the verb ‘eat’ is tasty. The above examples reflect the preferences between two twohop dependency relations: ‘verb-object-modifier’ 722 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 722–731 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and ‘verb-subject-modifier’, which have not been investigated in previous works. Second, pseudo-disambiguation has been a popular alternative evaluation method for the SP acquisition task (Ritter et al., 2010; de Cruys, 2014). This way of SP acquisition trains a model based on pairs from a training corpus as positive examples and randomly generates fake pairs as negative examples, and then evaluates the model based on its ability on a test corpus by constructing positive and negative examples in the same way. However, the pseudo-disambiguation task only evaluates how well a model fits the data, which could be biased. The problem is that changing the corpus of training and testing may result in different conclusions. Thus, it is less robust than collecting SP pairs by asking expert annotators as (M"
P19-1071,P99-1014,0,0.242026,"Missing"
P19-1071,C16-1266,0,0.0632848,"nomenon in human language that has been shown to be related to semantics (Wilks, 1975). Here by SP we mean that, given a word and a dependency relation, human beings have preferences for which words are likely to be connected. For instance, when seeing the verb ‘sing’, it is highly plausible that its object is ‘a song’, and when seeing the noun ‘air’, it is highly plausible that its modifier is ‘fresh’. SP has been shown to be useful over a variety of tasks including sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), coreference clustering (Hobbs, 1978; Inoue et al., 2016; Heinzerling et al., 2017), and machine translation (Tang et al., 2016). Given the importance of SP, the automatic acquisition of SP has become a well-known research subject in the • (A) The fish ate the worm. It was hungry. • (B) The fish ate the worm. It was tasty. In (A), we can resolve ‘it’ to ‘the fish’ because it is more plausible that the subject of the verb ‘eat’ is hungry. On the other hand, for (B), we can resolve ‘it’ to ‘the worm’ because it is more likely that the object of the verb ‘eat’ is tasty. The above examples reflect the preferences between two twohop dependency relations"
P19-1071,D17-1068,0,0.285379,"Missing"
P19-1071,J03-3005,0,0.841235,", covering 2,500 most frequent verbs, nouns, and adjectives in American English. Three representative SP acquisition methods based on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the importance of our dataset, we investigate the relationship between SP-10K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge. We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem. 1 SP Evaluation Set #R #W #P (McRae et al., 1998) (Keller and Lapata, 2003) (Pad´o et al., 2006) 2 3 3 641 571 180 821 540 207 SP-10K 5 2.5K 10K Table 1: Statistics of Human-labeled SP Evaluation Sets. #R, #W, and #P indicate the number of SP relation types, words, and pairs, respectively. NLP community. However, current SP acquisition models are limited based on existing evaluation methods. We discuss two broadly used evaluation methods, human-labeled evaluation sets and the pseudo-disambiguation task. First, the most straightforward way to evaluate SP models is by asking human annotators. McRae et al. (1998), Keller and Lapata (2003), and Pad´o et al. (2006) propos"
P19-1071,L16-1376,0,0.130649,"ing corpus statistics. ‘m’ means millions. containing fewer than 100 tokens and fewer than five hyperlinks. After filtering, our dataset contains over three million Wikipedia pages. (2) Yelp: Yelp is a social media platform where users can write reviews for businesses, e.g., restaurants, hotels, etc. The latest release of the Yelp dataset5 contains over five million reviews. (3) New York Times (NYT): The NYT (Sandhaus and Evan, 2008) dataset contains over 1.8 million news articles from the NYT throughout 20 years (1987 - 2007). We parsed these raw corpora using the Stanford dependency parser (Schuster and Manning, 2016). Detailed statistics are shown in Table 5. 4.1 4.2 We report the average Spearman ρ in Table 6 as our performance measure. We have following interesting observations. (1) Choice of training corpus can influence the SP acquisition models. For the same method, the general corpora, i.e., Wiki and NYT, outperform the domain specific corpus, i.e., Yelp. Yelp performs best on the ‘dobj’ relation and comparably on the ‘dobj amod’ relation, which indicates the language use on Yelp may better reflect the plausibility of objects rather than of subjects. (2) As reported by (de Cruys, 2014), the NNbased"
P19-1071,N18-2108,0,0.0501405,"Missing"
P19-1071,speer-havasi-2012-representing,0,0.245908,"resent SP-10K, which is unprecedented in both size and the number of SP relations. It contains 10,000 selectional triplets consisting of 2,500 frequent verbs, nouns, and adjectives in American English. Besides commonly used one-hop SP relations (‘dobj’, ‘nsubj’, and ‘amod’), we introduce two novel two-hop SP relations (‘dobj amod’ and ‘nsubj amod’). We first evaluate three representative SP acquisition methods using SP-10K and compare the capacity of the state-of-the-art pseudo-disambiguation approaches. We then show the relationship between SP-10K and commonsense knowledge using ConceptNet5 (Speer and Havasi, 2012) to demonstrate the potential of using SP to represent commonsense knowledge. Finally, we use a subset of the Winograd Schema Challenge (Levesque et al., 2011) to prove that the proposed two-hop SP relations are essential for the hard pronoun coreference resolution. SP10K is available at: https://github.com/ HKUST-KnowComp/SP-10K. 2 (2) representative; and (3) consistent and reliable. First, similar to existing human-labeled SP evaluation sets (McRae et al., 1998; Keller and Lapata, 2003; Pad´o et al., 2006), SP-10K uses the plausibility of selectional pairs as the annotation. Hence, SP-10K is"
P19-1071,C16-1203,0,0.259623,"(Wilks, 1975). Here by SP we mean that, given a word and a dependency relation, human beings have preferences for which words are likely to be connected. For instance, when seeing the verb ‘sing’, it is highly plausible that its object is ‘a song’, and when seeing the noun ‘air’, it is highly plausible that its modifier is ‘fresh’. SP has been shown to be useful over a variety of tasks including sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), coreference clustering (Hobbs, 1978; Inoue et al., 2016; Heinzerling et al., 2017), and machine translation (Tang et al., 2016). Given the importance of SP, the automatic acquisition of SP has become a well-known research subject in the • (A) The fish ate the worm. It was hungry. • (B) The fish ate the worm. It was tasty. In (A), we can resolve ‘it’ to ‘the fish’ because it is more plausible that the subject of the verb ‘eat’ is hungry. On the other hand, for (B), we can resolve ‘it’ to ‘the worm’ because it is more likely that the object of the verb ‘eat’ is tasty. The above examples reflect the preferences between two twohop dependency relations: ‘verb-object-modifier’ 722 Proceedings of the 57th Annual Meeting of t"
P19-1071,J15-4004,0,\N,Missing
P19-1071,J13-3006,0,\N,Missing
P19-1083,E09-1018,0,0.0431558,"1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, esp"
P19-1083,P15-1136,0,0.0462386,"plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word u"
P19-1083,D16-1245,0,0.0444309,"male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To c"
P19-1083,P81-1019,0,0.771448,"r, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, especially for pronouns, which are hard to be interpreted owing to their weak semantic meanings (Ehrlich, 1981). As one challenging yet vital subtask of the general coreference resolution, pronoun coreference resolution (Hobbs, 1978) is to find the correct reference for a given pronominal anaphor in the context and has showed its importance in many natural language processing (NLP) ∗ This work was partially done during the internship of the first author in Tencent AI Lab. 867 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate"
P19-1083,D10-1048,0,0.109095,"the noun phrases, so as to automatically generate linguistic knowledge (in the form of triplets) for our data. Specifically, the plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selec"
P19-1083,P11-1082,0,0.0351462,", for natural language understanding. Mention detection and coreference prediction are the two major focuses of the task as listed in Lee et al. (2017). Compared to general coreference problem, pronoun coreference resolution has its unique challenge since pronouns themselves have weak semantics meanings, which make it the most challenging sub-task in general coreference resolution. To address the unique difficulty brought by pronouns, we thus focus on resolving pronoun coreferences in this paper. Resolving pronoun coreference relations often requires the support of manually crafted knowledge (Rahman and Ng, 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu"
P19-1083,N18-2108,0,0.35483,"electional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 1"
P19-1083,P11-1117,0,0.0529445,"Missing"
P19-1083,N18-2028,1,0.77866,"ing embeddings of all words in its tail. For example, if s is ‘the apple’ and the knowledge triplet (‘the apple’, IsA, ‘healthy food’) is found by searching the KG, we represent this relation from the averaged embeddings of ‘healthy’ and ‘food’. Consequently, for s and p, we denote their retrieved knowledge set as Ks and Kp respectively, where Ks contains ms related knowledge embeddings k1,s , k2,s , ..., kms ,s and Kp contains mp of them k1,p , k2,p , ..., kmp ,p . Contextual information is crucial to distinguish the semantics of a word or phrase, especially for text representation learning (Song et al., 2018; Song and Shi, 2018). In this work, a standard bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) model is used to encode each span with attentions (Bahdanau et al., 2014), which is similar to the one used in Lee et al. (2017). The structure is shown in Figure 2. Let initial word embeddings in a span si be denoted as x1 , ..., xT and their encoded representation be x∗1 , ..., x∗T . The weighted embeddings of each span xˆi is obtained by T X (3) Thus the span representation of s and p are marked as es and ep , respectively. Span Representation xˆi = , where αt is a standard feed-fo"
P19-1083,W18-2315,1,0.841852,"for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate contextual information from large-scale external unlabeled data into the model, they are insufficient to incorporate existing complex knowledge into the representation for covering all the knowledge one needs to build a successful pronoun coreference system. In addition, overfitting is always observed on deep models, whose performance is thus limited in cross-domain scenarios and restricts their usage in real applications (Liu et al., 2018, 2019). Recently, a joint model (Zhang et al., 2019b) was proposed to connect the contextual information and human-designed features together for pronoun coreference resolution task (with gold mention support) and achieved the state-of-the-art performance. However, their model still requires the complex features designed by experts, which is expensive and difficult to acquire, and requires the support of the gold mentions. To address the limitations of the aforementioned models, in this paper, we propose a novel end-toend model that learns to resolve pronoun coreferences with general knowledg"
P19-1083,P19-1189,1,0.884331,"Missing"
P19-1083,P03-1022,0,0.665428,"Missing"
P19-1083,W18-2324,0,0.0308416,", 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu et al., 2018) ones, and world knowledge (Rahman and Ng, 2011), such as selectional preference (Wilks, 1975). Later, end-to-end solutions (Lee et al., 2017, 2018) were proposed to learn contextual information and solve coreferences synchronously with neural networks, e.g., LSTM. Their results proved that such knowledge is helpful when appropriately used for coreference resolution. However, external knowledge is often omitted in their models. Consider that context and external knowledge have their own advantages: the contextual information covering diverse text expressions that are difficult to be predefined"
P19-1083,P98-2143,0,0.505205,"respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for hu"
P19-1083,1995.tmi-1.6,0,0.49878,"Missing"
P19-1083,C94-2189,0,0.612771,"ine blue fonts, respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings sim"
P19-1083,D14-1162,0,0.0891682,"efer to complex things and occur with low frequency. Moreover, there are significant gaps in the performance of different models, with the following observations. First, models with manually defined rules or features, which cannot cover rich contextual information, perform poorly. In contrast, deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on the recall. SecImplementation Following the previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embeddings (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embeddings as the initial word representations for computing span representations. For knowledge triplets, we use the GloVe embeddings to encode tail words in them. Outof-vocabulary words are initialized with zero vectors. The hidden state of the LSTM module is set to 200, and all the feed-forward networks have two 150-dimension hidden layers. The selection thresholds are set to 10−2 and 10−8 for the CoNLL and i2b2 dataset, respectively. For model training, we use cross-entropy as the loss function and Adam (Kingma and Ba, 2014) as the optimizer. All the afo"
P19-1083,N18-1202,0,0.357819,"nerally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 10 https://github.com/kentonl/e2e-coref https://stanfordnlp.github.io/CoreNLP/ https://dumps.wikimedia.org/enwiki/ 8 In the Stanford parser, an ‘nsubj’ edge is created between i"
P19-1083,N19-1093,1,0.547847,"tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while"
P19-1083,W12-4501,0,0.114752,"in certain contexts. To solve it, a knowledge attention module is proposed to select the appropriate knowledge. For each pair of (s, p), as shown in Figure 3, we first concatenate es and ep to get the overall (span, pronoun) representation es,p , which is used to select knowledge for both s and p. Taking that for s as example, we compute the weight of each ki ∈ Ks by wi = P βki e kj ∈Ks e βkj , Dataset X wi · k i . 7,749 1,007 1,037 2,229 222 321 31,806 3,747 4,078 i2b2 train test 2,024 1,244 685 367 270 166 2,979 1,777 30,334 10,845 3,208 44,387 Datasets • CoNLL: The CoNLL-2012 shared task (Pradhan et al., 2012) corpus, which is a widely used dataset selected from the Ontonotes 5.05 . • i2b2: The i2b2 shared task dataset (Uzuner et al., 2012), consisting of electronic medical records from two different organizations, namely, Partners HealthCare (Part) and Beth Israel Deaconess medical center (Beth). All records have been fully de-identified and manually annotated with coreferences. (6) We split the datasets into different proportions based on their original settings. Three types of pronouns are considered in this paper following Ng (2005), i.e., third personal pronoun (e.g., she, her, he, him, them,"
P19-1083,C98-2138,0,\N,Missing
P19-1520,N10-1122,0,0.0540862,"n dependency parsing results (Zhuang et al., 2006; Qiu et al., 2011). A rule in these approaches usually involves only up to three words in a sentence (Qiu et al., 2011), which limits its flexibility. It is also labor-intensive to design the rules manually. Liu et al. (2015b) propose an algorithm to select some rules from a set of previously designed rules, so that the selected subset of rules can perform extraction more accurately. However, different from the rule mining algorithm used in our approach, it is unable to discover rules automatically. Topic modeling approaches (Lin and He, 2009; Brody and Elhadad, 2010; Mukherjee and Liu, 2012) are able to get coarse-grained aspects such as food, ambiance, service for restaurants, and provide related words. However, they cannot extract the exact aspect terms from review sentences. Learning based approaches extract aspect and opinion terms by labeling each word in a sentence with BIO (Begin, Inside, Outside) tagging scheme (Ratinov and Roth, 2009). Typically, they first obtain features for each word in a sentence, then use them as the input of a CRF to get better sequence labeling results (Jakob and Gurevych, 2010; Wang et al., 2016). Word embeddings are com"
P19-1520,S14-2051,0,0.637403,"Liu, 2004) for aspect term extraction rules. We train two sets of 100 dimension word embeddings with word2vec (Mikolov et al., 2013) on all the reviews of the Yelp dataset and the Amazon dataset, respectively. The hidden layer sizes of the BiL1 2 Performance Comparison https://www.yelp.com/dataset/challenge http://jmcauley.ucsd.edu/data/amazon/ 5273 • DP (Double Propagation) (Qiu et al., 2011): A rule based approach that uses eight manually designed rules to extract aspect and opinion terms. It only considers noun aspect terms and adjective opinion terms. • IHS RD, DLIREC, and Elixa: IHS RD (Chernyshevich, 2014) and DLIREC (Toh and Wang, 2014) are the best performing systems at SemEval 2014 on SE14-L and SE14R, respectively. Elixa (Vicente et al., 2017) is the best performing system at SemEval 2015 on SE15-R. All these three systems use rich sets of manually designed features. • WDEmb and WDEmb*: WDEmb (Yin et al., 2016) first learns word and dependency path embeddings without supervision. The learned embeddings are then used as the input features of a CRF model. WDEmb* adds manually designed features to WDEmb. • RNCRF: RNCRF (Wang et al., 2016) uses a recursive neural network model based the depende"
P19-1520,D10-1101,0,0.27495,"and those that correspond to the reviewer’s sentiments towards the product or an aspect of the product (Hu and Liu, 2004; Liu, 2012; Qiu et al., 2011; Vivekanandan and Aravindan, 2014). The former are called aspect terms, and the latter are called opinion terms. For example, in the sentence “The speed of this laptop is incredible,” “speed” is an aspect term, and “incredible” is an opinion term. The task of aspect and opinion term extraction is to extract the above two types of terms from product reviews. Rule based approaches (Qiu et al., 2011; Liu et al., 2016) and learning based approaches (Jakob and Gurevych, 2010; Wang et al., 2016) are two major approaches to this task. Rule based approaches usually use manually designed rules based on the result of dependency parsing to extract the terms. An advantage of these approaches is that the aspect or opinion terms whose usage in a sentence follows some certain patterns can always be extracted. However, it is labor-intensive to design rules manually. It is also hard for them to achieve high performance due to the variability and ambiguity of natural language. Learning based approaches model aspect and opinion term extraction as a sequence labeling problem. W"
P19-1520,P06-1027,0,0.0753436,"For example, Wang et al. (2016) construct a recursive neural network based on the dependency parsing tree of a sentence with word embeddings as input. The output of the neural network is then fed into a CRF. Xu et al. (2018) use a CNN model to extract aspect terms. They find that using both general-purpose and domainspecific word embeddings improves the performance. Our approach exploits unlabeled extra data to improve the performance of the model. This is related to semi-supervised learning and transfer learning. Some methods allow unlabeled data to be used in sequence labeling. For example, Jiao et al. (2006) propose semi-supervised CRF, Zhang et al. (2017) propose neural CRF autoencoder. Unlike our approach, these methods do not incorporate knowledge about the task while using the unlabeled data. Yang et al. (2017) propose three different transfer learning architectures that allow neural sequence tagging models to learn from both the target task and a different but related task. Different from them, we improve performance by utilizing the output of a rule based approach for the same problem, instead of another related task. Our approach is also related to the use of weakly labeled data (Craven an"
P19-1520,D15-1168,0,0.159123,"Missing"
P19-1520,P14-5010,0,0.00439245,"Missing"
P19-1520,P09-1113,0,0.0743811,"ach, these methods do not incorporate knowledge about the task while using the unlabeled data. Yang et al. (2017) propose three different transfer learning architectures that allow neural sequence tagging models to learn from both the target task and a different but related task. Different from them, we improve performance by utilizing the output of a rule based approach for the same problem, instead of another related task. Our approach is also related to the use of weakly labeled data (Craven and Kumlien, 1999), and is similar to the distant supervision approach used in relation extraction (Mintz et al., 2009). 3 RINANTE In this section, we introduce our approach RINANTE in detail. Suppose we have a human annotated dataset Dl and an auxiliary dataset Da . Dl contains a set of product reviews, each with all the aspect and opinion terms in it labeled. Da only contains a set of unlabeled product reviews. The reviews in Dl and Da are all for a same type or several similar types of products. Usually, the size of Da is much larger than Dl . Then, RINANTE consists of the following steps. 5269 1. Use Dl to mine a set of aspect extraction rules Ra and a set of opinion extraction rules Ro with a rule mining"
P19-1520,P12-1036,0,0.0303585,"lts (Zhuang et al., 2006; Qiu et al., 2011). A rule in these approaches usually involves only up to three words in a sentence (Qiu et al., 2011), which limits its flexibility. It is also labor-intensive to design the rules manually. Liu et al. (2015b) propose an algorithm to select some rules from a set of previously designed rules, so that the selected subset of rules can perform extraction more accurately. However, different from the rule mining algorithm used in our approach, it is unable to discover rules automatically. Topic modeling approaches (Lin and He, 2009; Brody and Elhadad, 2010; Mukherjee and Liu, 2012) are able to get coarse-grained aspects such as food, ambiance, service for restaurants, and provide related words. However, they cannot extract the exact aspect terms from review sentences. Learning based approaches extract aspect and opinion terms by labeling each word in a sentence with BIO (Begin, Inside, Outside) tagging scheme (Ratinov and Roth, 2009). Typically, they first obtain features for each word in a sentence, then use them as the input of a CRF to get better sequence labeling results (Jakob and Gurevych, 2010; Wang et al., 2016). Word embeddings are commonly used features, hand-"
P19-1520,J11-1002,0,0.259707,"annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art. 1 Introduction There are two types of words or phrases in product reviews (or reviews for services, restaurants, etc., we use “product reviews” throughout the paper for convenience) that are of particular importance for opinion mining: those that describe a product’s properties or attributes; and those that correspond to the reviewer’s sentiments towards the product or an aspect of the product (Hu and Liu, 2004; Liu, 2012; Qiu et al., 2011; Vivekanandan and Aravindan, 2014). The former are called aspect terms, and the latter are called opinion terms. For example, in the sentence “The speed of this laptop is incredible,” “speed” is an aspect term, and “incredible” is an opinion term. The task of aspect and opinion term extraction is to extract the above two types of terms from product reviews. Rule based approaches (Qiu et al., 2011; Liu et al., 2016) and learning based approaches (Jakob and Gurevych, 2010; Wang et al., 2016) are two major approaches to this task. Rule based approaches usually use manually designed rules based o"
P19-1520,W09-1119,0,0.018307,"et of rules can perform extraction more accurately. However, different from the rule mining algorithm used in our approach, it is unable to discover rules automatically. Topic modeling approaches (Lin and He, 2009; Brody and Elhadad, 2010; Mukherjee and Liu, 2012) are able to get coarse-grained aspects such as food, ambiance, service for restaurants, and provide related words. However, they cannot extract the exact aspect terms from review sentences. Learning based approaches extract aspect and opinion terms by labeling each word in a sentence with BIO (Begin, Inside, Outside) tagging scheme (Ratinov and Roth, 2009). Typically, they first obtain features for each word in a sentence, then use them as the input of a CRF to get better sequence labeling results (Jakob and Gurevych, 2010; Wang et al., 2016). Word embeddings are commonly used features, hand-crafted features such as POS tag classes and chunk information can also be combined to yield better performance (Liu et al., 2015a; Yin et al., 2016). For example, Wang et al. (2016) construct a recursive neural network based on the dependency parsing tree of a sentence with word embeddings as input. The output of the neural network is then fed into a CRF."
P19-1520,S14-2038,0,0.0969237,"ction rules. We train two sets of 100 dimension word embeddings with word2vec (Mikolov et al., 2013) on all the reviews of the Yelp dataset and the Amazon dataset, respectively. The hidden layer sizes of the BiL1 2 Performance Comparison https://www.yelp.com/dataset/challenge http://jmcauley.ucsd.edu/data/amazon/ 5273 • DP (Double Propagation) (Qiu et al., 2011): A rule based approach that uses eight manually designed rules to extract aspect and opinion terms. It only considers noun aspect terms and adjective opinion terms. • IHS RD, DLIREC, and Elixa: IHS RD (Chernyshevich, 2014) and DLIREC (Toh and Wang, 2014) are the best performing systems at SemEval 2014 on SE14-L and SE14R, respectively. Elixa (Vicente et al., 2017) is the best performing system at SemEval 2015 on SE15-R. All these three systems use rich sets of manually designed features. • WDEmb and WDEmb*: WDEmb (Yin et al., 2016) first learns word and dependency path embeddings without supervision. The learned embeddings are then used as the input features of a CRF model. WDEmb* adds manually designed features to WDEmb. • RNCRF: RNCRF (Wang et al., 2016) uses a recursive neural network model based the dependency parsing tree of a sentence t"
P19-1520,D16-1059,0,0.554271,"to the reviewer’s sentiments towards the product or an aspect of the product (Hu and Liu, 2004; Liu, 2012; Qiu et al., 2011; Vivekanandan and Aravindan, 2014). The former are called aspect terms, and the latter are called opinion terms. For example, in the sentence “The speed of this laptop is incredible,” “speed” is an aspect term, and “incredible” is an opinion term. The task of aspect and opinion term extraction is to extract the above two types of terms from product reviews. Rule based approaches (Qiu et al., 2011; Liu et al., 2016) and learning based approaches (Jakob and Gurevych, 2010; Wang et al., 2016) are two major approaches to this task. Rule based approaches usually use manually designed rules based on the result of dependency parsing to extract the terms. An advantage of these approaches is that the aspect or opinion terms whose usage in a sentence follows some certain patterns can always be extracted. However, it is labor-intensive to design rules manually. It is also hard for them to achieve high performance due to the variability and ambiguity of natural language. Learning based approaches model aspect and opinion term extraction as a sequence labeling problem. While they are able t"
P19-1520,P18-2094,0,0.591481,". Typically, they first obtain features for each word in a sentence, then use them as the input of a CRF to get better sequence labeling results (Jakob and Gurevych, 2010; Wang et al., 2016). Word embeddings are commonly used features, hand-crafted features such as POS tag classes and chunk information can also be combined to yield better performance (Liu et al., 2015a; Yin et al., 2016). For example, Wang et al. (2016) construct a recursive neural network based on the dependency parsing tree of a sentence with word embeddings as input. The output of the neural network is then fed into a CRF. Xu et al. (2018) use a CNN model to extract aspect terms. They find that using both general-purpose and domainspecific word embeddings improves the performance. Our approach exploits unlabeled extra data to improve the performance of the model. This is related to semi-supervised learning and transfer learning. Some methods allow unlabeled data to be used in sequence labeling. For example, Jiao et al. (2006) propose semi-supervised CRF, Zhang et al. (2017) propose neural CRF autoencoder. Unlike our approach, these methods do not incorporate knowledge about the task while using the unlabeled data. Yang et al. ("
P19-1520,D17-1179,0,0.0305835,"rsive neural network based on the dependency parsing tree of a sentence with word embeddings as input. The output of the neural network is then fed into a CRF. Xu et al. (2018) use a CNN model to extract aspect terms. They find that using both general-purpose and domainspecific word embeddings improves the performance. Our approach exploits unlabeled extra data to improve the performance of the model. This is related to semi-supervised learning and transfer learning. Some methods allow unlabeled data to be used in sequence labeling. For example, Jiao et al. (2006) propose semi-supervised CRF, Zhang et al. (2017) propose neural CRF autoencoder. Unlike our approach, these methods do not incorporate knowledge about the task while using the unlabeled data. Yang et al. (2017) propose three different transfer learning architectures that allow neural sequence tagging models to learn from both the target task and a different but related task. Different from them, we improve performance by utilizing the output of a rule based approach for the same problem, instead of another related task. Our approach is also related to the use of weakly labeled data (Craven and Kumlien, 1999), and is similar to the distant s"
S17-2102,S16-1173,0,0.0811564,"Missing"
S17-2102,S17-2088,0,0.107268,"Missing"
S17-2102,S15-2079,0,0.0450086,"Missing"
S17-2102,S15-2086,1,0.854872,"by running Skip-gram on released largescale corpus. We make an assumption that different word embeddings cover different words and encode different semantic knowledge, thus using them together can improve the generalizations and performances of neural models. In the SemEval 2017, our method ranks 1st in Accuracy, 5th in AverageR. Meanwhile, the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set. We release our code 1 for the method replicability. 1 Introduction Twitter sentiment classification has attracted a lot of attention (Dong et al., 2015; Nakov et al., 2016; Rosenthal et al., 2017), which aims to classify a tweet into three sentiment categories: negative, neutral, and positive. Tweet text has several features: written by the informal language, hash-tags and emoticons indicate sentiments, and sometimes is sarcasm, which make decisions of tweet sentiment hard for machines. With releases of annotated datasets, more researchers prefer to use the 1 Ming Zhang Peking University mzhang cs@pku.edu.cn https://github.com/zwjyyc/NNEMBs 621 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 621–6"
S17-2102,P14-1146,0,0.0605575,"sification as one testbed to evaluate their proposed models. Traditional methods (Mohammad et al., 2013) for twitter sentiment classification use a variety of hand-crafted features including surface-form, semantic and sentiment lexicons. The performances of these methods often depend on the quality of feature engineering work, and building a state-ofthe-art system is difficult for novices. Moreover, these designed features are presented by the onehot representation which cannot capture the semantic relativeness of different features and proposes a problem of feature sparsity. To address this, Tang et al. (2014) induced sentiment-specific low-dimensional, real-valued embedding features for twitter classification, which encode both semantics and sentiments of words. In the experiments, the embedding features and hand-crafted features obtain similar results, and also they are complementary for each other in the system. With the developments of neural networks in natural language processing, neural sentiment classification (Severyn and Moschitti, 2015; Deriu et al., 2016) has attracted a lot of attention recently and become the state-of-the-arts. These methods first learn word embeddings from large-scal"
S17-2102,S13-2053,0,0.0740426,"Missing"
S17-2102,S16-1001,0,0.0227317,"am on released largescale corpus. We make an assumption that different word embeddings cover different words and encode different semantic knowledge, thus using them together can improve the generalizations and performances of neural models. In the SemEval 2017, our method ranks 1st in Accuracy, 5th in AverageR. Meanwhile, the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set. We release our code 1 for the method replicability. 1 Introduction Twitter sentiment classification has attracted a lot of attention (Dong et al., 2015; Nakov et al., 2016; Rosenthal et al., 2017), which aims to classify a tweet into three sentiment categories: negative, neutral, and positive. Tweet text has several features: written by the informal language, hash-tags and emoticons indicate sentiments, and sometimes is sarcasm, which make decisions of tweet sentiment hard for machines. With releases of annotated datasets, more researchers prefer to use the 1 Ming Zhang Peking University mzhang cs@pku.edu.cn https://github.com/zwjyyc/NNEMBs 621 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 621–625, c Vancouver, Can"
S17-2102,D14-1162,0,0.0941771,"models, then learn RCNN models separately. We predict sentiment label of testing tweet based on these learned RCNN models, which are described by following functions: 622 Sets gloveG gloveT word2vecGN word2vecY Ensemble Corpus General Twitter Google News Yelp Reviews - Scale 840B 27B 100B 0.3B - Algorithm GloVec GloVec Word2Vec Word2Vec - Dimension 300 200 300 300 - Source R R R S - Vocab 2.2M 1.2M 3.0M 0.2M 5.4M Table 1: Statistics of the embedding sets. R means the embedding set is publicly released and S means the embedding set is self-contained. GloVec (Mikolov et al., 2013) and Word2Vec (Pennington et al., 2014) are most popular embedding algorithms. Scale means the size of tokens in corpus, M and B refer to million and billion respectively. The embedding set word2vecY are trained by Word2Vec with default settings and Yelp reviews are available at https://www.yelp.com/dataset challenge. Dataset Previous SemEvals SemEval 2017 Test #num 50032 12284 #category ratio 1.5/4.7/3.8 3.2/4.8/2.0 dropout rate is set to 0.3, which is applied in the final text representation. All parameters are learned by Adam optimizer (Kingma and Ba, 2014) with the learning rate 0.001. Note that, all word embedding sets are fix"
