2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.acl-main.277,1999.tmi-1.16,0,0.13791,"Missing"
2020.acl-main.277,D16-1139,0,0.0567024,"ransformer model (dmodel = 278, dhidden = 507, nlayer = 5, nhead = 2, pdropout = 0.1). For the WMT14 En-De and WMT16 EnRo datasets, we use a larger Transformer model (dmodel = 512, dhidden = 512, nlayer = 6, nhead = 8, pdropout = 0.1). We linearly anneal the learning rate from 3 × 10−4 to 10−5 as in Lee et al. (2018) for the IWSLT16 En-De dataset, while employing the warm-up learning rate schedule (Vaswani et al., 2017) with twarmup = 4000 for the WMT14 En-De and WMT16 En-Ro datasets. We also use label smoothing of value ls = 0.15 for all datasets. We utilize the sequence-level distillation (Kim and Rush, 2016), which replaces the target sentences in the training dataset with sentences generated by an autoregressive model, and set the beam size of the technique to 4. We use the encoder of the corresponding autoregressive model to initialize the encoder of RecoverSAT, and share the parameters of source and target token embedding layers and the pre-softmax linear layer. We measure the speedup of model inference in each task on a single NVIDIA P40 GPU with the batch size 1. 4.3 Baselines We use the Transformer (Vaswani et al., 2017) as our AT baseline and fifteen latest strong NAT models as NAT baselin"
2020.acl-main.277,D18-1149,0,0.581672,"1 Segment 2 Segment 3 Segment 4 Figure 1: An overview of our RecoverSAT model. RecoverSAT generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is generated token-by-token conditioned on both the source tokens and the translation history of all segments (e.g., the token “are” in the first segment is predicted based on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the trai"
2020.acl-main.277,D19-1573,0,0.110758,"Missing"
2020.acl-main.277,D19-1437,0,0.369128,"eration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel semi-autoregressive model named RecoverSAT in this work. RecoverSAT features in three aspects: (1) To improve decoding speed, we assume that a translation can be divided into several segments which can be genera"
2020.acl-main.277,P16-1162,0,0.0996738,"aining instances will mislead the model that generating then deleting a repetitive segment is a must-to-have behaviour, which is not desired. Therefore, we inject pseudo repetitive segment into a training instance with probability q in this work. 4 4.1 Experiments Datasets We conduct experiments on three widely-used machine translation datasets: IWSLT16 En-De (196k pairs), WMT14 En-De (4.5M pairs) and WMT16 En-Ro (610k pairs). For fair comparison, we use the preprocessed datasets in Lee et al. (2018), of which sentences are tokenized and segmented into subwords using byte-pair encoding (BPE) (Sennrich et al., 2016) to restrict the vocabulary size. We use a shared vocabulary of 40k subwords for both source and target languages. For the WMT14 En-De dataset, we use newstest-2013 and newstest2014 as validation and test sets respectively. For the WMT16 En-Ro dataset, we employ newsdev2016 and newstest-2016 as validation and test sets respectively. For the IWSLT16 En-De dataset, we use test2013 as the validation set. 4.2 Experimental Settings For model hyperparameters, we follow most of the settings in (Gu et al., 2018; Lee et al., 2018; Wei et al., 2019). For the IWSLT16 En-De dataset, we use a small Transfo"
2020.acl-main.277,P19-1288,1,0.902957,"ased on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel"
2020.acl-main.277,D18-1044,0,0.0823734,"Missing"
2020.acl-main.277,P19-1125,0,0.174125,"e corresponding autoregressive model. 1 Although neural machine translation (NMT) has achieved state-of-the-art performance in recent years (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), most NMT models still suffer from the slow decoding speed problem due to their autoregressive property: the generation of a target token depends on all the previously generated target tokens, making the decoding process intrinsically nonparallelizable. Recently, non-autoregressive neural machine translation (NAT) models (Gu et al., 2018; Li et al., 2019; Wang et al., 2019; Guo et al., 2019a; Wei et al., 2019) have been investigated to mitigate the † indicates equal contribution indicates corresponding author es gibt heute viele Farmer mit diesem Ansatz Feasible Trans. there are lots of farmers doing this today there are a lot of farmers doing this today Trans. 1 Trans. 2 there are lots of of farmers doing this today there are a lot farmers doing this today Table 1: A multi-modality problem example: NAT models generate each target token independently such that they may correspond to different feasible translations, which usually manifests as repetitive (Trans. 1) or missing (Trans. 2) tokens. Intro"
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.emnlp-main.129,buyko-etal-2010-genereg,0,0.0403765,"erent ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event annotation, but their different settings are complementary to our work. 7 Conclusion and Future work In this paper, we present a massive general domain event detection dataset (MAVEN), which significantly alleviates the data scarcity and low coverage problems of existing datasets. We conduct a thorough evaluation of the state-of-the-art ED models on MAVEN. Th"
2020.emnlp-main.129,P17-1038,0,0.0587272,"vember 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data"
2020.emnlp-main.129,P15-1017,0,0.690388,"uld recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated inst"
2020.emnlp-main.129,D18-1158,0,0.305267,"osition: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod"
2020.emnlp-main.129,N18-1076,0,0.0271789,"mains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https:// github.com/THU-KEG/MAVEN-dataset. 1 End-Position: 212 Transfer-Money: 198 Attack: 1543 Figure 1: Data distribution of the most widely-used ACE 2005 English dataset. It contains 33 event types, 599 documents and 5, 349 instances in total. is the first stage to extract event knowledge from text (Ahn, 2006) and also fundamental to various NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018; Yang et al., 2019). Introduction Event detection (ED) is an important task of information extraction, which aims to identify event triggers (the words or phrases evoking events in text) and classify event types. For instance, in the sentence “Bill Gates founded Microsoft in 1975”, an ED model should recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the r"
2020.emnlp-main.129,N19-1423,0,0.0330633,"eural network baseline, which adopts the widely-used bi-directional long shortterm memory network to learn textual representations, and then uses the hidden states at the positions of trigger candidates for classifying event types. (3) MOGANED (Yan et al., 2019) is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multi-order syntactic relations in dependency trees and improve ED. (4) DMBERT (Wang et al., 2019b) is a vanilla BERTbased model. It takes advantage of the effective pretrained language representation model BERT (Devlin et al., 2019) and also adopts the dynamic multi-pooling mechanism to aggregate features for ED. We use the BERTBASE architecture in our experiments. (5) Different from the above tokenlevel classification models, BiLSTM+CRF and BERT+CRF are sequence labeling models. To verify the effectiveness of modeling multiple event correlations, the two models both adopt the conditional random field (CRF) (Lafferty et al., 2001) as their output layers, which can model structured output dependencies. And they use BiLSTM and BERTBASE as their feature extractors respectively. As we manually tune hyperparameters and some t"
2020.emnlp-main.129,D19-1033,1,0.851706,"Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of"
2020.emnlp-main.129,D16-1264,0,0.124091,"Missing"
2020.emnlp-main.129,P19-1276,0,0.0175457,"traction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event a"
2020.emnlp-main.129,D18-1156,0,0.0338402,"Missing"
2020.emnlp-main.129,P19-1353,0,0.0176268,"of ED and event extraction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent co"
2020.emnlp-main.129,P19-1429,0,0.374622,"Missing"
2020.emnlp-main.129,W15-0812,0,0.406914,"problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1652–1671, c November 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, so"
2020.emnlp-main.129,L16-1699,0,0.0619031,"Missing"
2020.emnlp-main.129,P09-1113,0,0.125379,"2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low c"
2020.emnlp-main.129,P18-2066,0,0.180224,"-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stab"
2020.emnlp-main.129,N19-1105,1,0.947896,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1584,1,0.922895,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1582,0,0.510191,"-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conferenc"
2020.emnlp-main.129,C96-1079,0,\N,Missing
2020.emnlp-main.129,W06-0901,0,\N,Missing
2020.emnlp-main.129,P98-1013,0,\N,Missing
2020.emnlp-main.129,C98-1013,0,\N,Missing
2020.emnlp-main.129,P09-2093,0,\N,Missing
2020.emnlp-main.129,P06-4018,0,\N,Missing
2020.emnlp-main.129,P13-1024,0,\N,Missing
2020.emnlp-main.129,P08-1030,0,\N,Missing
2020.emnlp-main.129,P13-1008,0,\N,Missing
2020.emnlp-main.129,D14-1162,0,\N,Missing
2020.emnlp-main.129,P15-2060,0,\N,Missing
2020.emnlp-main.129,D15-1247,0,\N,Missing
2020.emnlp-main.129,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.129,N16-1034,0,\N,Missing
2020.emnlp-main.129,P16-1025,0,\N,Missing
2020.emnlp-main.129,P17-1164,0,\N,Missing
2020.emnlp-main.129,N18-2058,0,\N,Missing
2020.emnlp-main.129,C18-1075,0,\N,Missing
2020.emnlp-main.129,D18-1259,0,\N,Missing
2020.emnlp-main.129,W13-2001,0,\N,Missing
2020.emnlp-main.129,P19-1521,0,\N,Missing
2020.emnlp-main.129,P16-2060,0,\N,Missing
2020.emnlp-main.129,P18-1201,0,\N,Missing
2020.emnlp-main.237,W15-4007,0,0.0409905,"Missing"
2020.emnlp-main.237,N19-1086,0,0.0410443,"ange over time, which makes them impractical in real-world applications. 2.2 Continual Learning Continual learning, also known as life-long learning, helps alleviate catastrophic forgetting and enables incremental training for stream data. Methods for continual learning in natural language processing (NLP) field can mainly be divided into two categories: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), which slow down parameter updating to preserve old knowledge, and (2) memory-based methods (Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Chaudhry et al., 2019; Wang et al., 2019), which retain examples from old data for re-play upon learning the new data. Although continual learning has been widely studied in NLP (Sun et al., 2020) and computer vision (Kirkpatrick et al., 2017), its exploration on graph embedding is relatively rare. Sankar et al. (2018) seek to train graph embedding on constantly evolving data. However, it assumes the timestamp information is known beforehand, which hinders its application to other tasks. Song and Park (2018) extend the idea of regulation-based methods to continually learn graph embeddings which straightforwardly limits parameter upda"
2020.emnlp-main.275,D18-2029,0,0.0135609,"t training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shist = GRUdial [hxt ; hyt ] , shist t t−1 ∈ R  sel  , (4) kt kh d skh t = GRUsel ht , st−1 ∈ R ksel where hxt , hyt and ht t are sentence vectors of message xt , resp"
2020.emnlp-main.275,K18-1048,0,0.0282681,"erage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical M"
2020.emnlp-main.275,P17-1171,0,0.0217147,"Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropri"
2020.emnlp-main.275,P19-1258,1,0.851583,"p their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into tw"
2020.emnlp-main.275,N19-1423,0,0.100024,"see that the selected knowledge for response generation at training and inference is drawn from different distributions, i.e., the posterior distribution k ∼ qφ (Kt ) at training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shi"
2020.emnlp-main.275,P16-1154,0,0.0375727,"on is based on the knowledge selected by the prior module, which is guided by the well-trained teacher, and we only update the green blocks at this stage. Finally, the knowledge ktsel is selected by sampling from the posterior distribution qφ (Kt ) = post at (Kt ) at training while selected with the highest probability over the prior distribution πθ (Kt ) = prior at (Kt ) at inference. Generation with Knowledge. SKT takes the concatenation of message xt and selected knowledge sentence ktsel as input and generates responses by the Transformer decoder (Vaswani et al., 2017) with copy mechanism (Gu et al., 2016). Though there are various models studying how to improve the generation quality based on the given knowledge, here, we simply follow the decoder of SKT and mainly focus on the knowledge selection issue. 3 Approach In this section, we show how to bridge the gap between prior and posterior knowledge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the k"
2020.emnlp-main.275,P84-1044,0,0.723364,"Missing"
2020.emnlp-main.275,P19-1002,1,0.84127,"election only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generati"
2020.emnlp-main.275,P18-1136,0,0.0236704,"nt responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompos"
2020.emnlp-main.275,P18-1160,0,0.0174108,"However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The"
2020.emnlp-main.275,D18-1255,0,0.46899,"jad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowl"
2020.emnlp-main.275,D19-1258,0,0.0160588,"lly use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The example in Table 1"
2020.emnlp-main.275,D19-1187,0,0.0502611,"Missing"
2020.emnlp-main.275,W19-5917,0,0.0915067,"Missing"
2020.emnlp-main.275,D18-1073,0,0.0249398,"e to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proce"
2020.emnlp-main.275,P18-1205,0,0.0287203,"dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with e"
2020.emnlp-main.275,P19-1539,0,0.0176836,"ich leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-g"
2020.emnlp-main.275,P19-1426,1,0.820016,"Missing"
2020.emnlp-main.275,N19-1123,0,0.304558,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.275,P17-1061,0,0.121102,"ge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. 3.1 information for knowledge selection at inference. Following the typical setting in latent variable models (Lian et al., 2019; Kim et al., 2020), we use the response in bag-of-words (BOW) format (Zhao et al., 2017) as the posterior information. Here we take the dialogue context and the knowledge pool as input to generate the posterior information. We firstly summarize as the query of  histthe xcontext  this module qPI = s ; h and use it to get the t t t−1 PI attention distribution at (Kt ) over the knowledge pool Kt by Equation 6. Then, we summarize the knowledge representation in the knowledge pool with the weights in aPI t (Kt ) considered: h 1 i kt ktL d (7) hPI = h , · · · , h · aPI t t (Kt ) ∈ R . t t Secondly, we take the summarization of the dialogue context and the knowledge pool as input and"
2020.emnlp-main.275,D19-1193,0,0.0263512,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.582,S17-2001,0,0.0370299,"Missing"
2020.emnlp-main.582,P18-1078,0,0.0246983,"o tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and verification, coreference resolution, and eight tas"
2020.emnlp-main.582,D19-1606,0,0.323037,"9), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference information (Cheng and Erk, 2020; Xu et al., 2020; Zhao et al., 2020). Coreference occurs when two or more expressions in a text refer to the same entity, which is an important element for a coherent understanding of the whole discourse. For example, for comprehending the whole context of “Antoine published The Little Prince in 1943. The book follows a young prince who visits various planets in space.”, we must realize that The book refers to The Little Prince. Therefore, resolving coreference is an essential step"
2020.emnlp-main.582,N19-1423,0,0.584796,"issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT. 1 Introduction Recently, language representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou e"
2020.emnlp-main.582,D19-5801,0,0.0239374,"Missing"
2020.emnlp-main.582,W07-1401,0,0.0718487,"Missing"
2020.emnlp-main.582,P16-1154,0,0.0608072,"Missing"
2020.emnlp-main.582,P18-2058,0,0.0204785,"p((V hk ) hi ) Pr(xj |xi ) = P where denotes element-wise product function and V is a trainable parameter to measure the importance of each dimension for token’s similarity. Moreover, since we split a word into several word pieces as BERT does and we adopt whole word masking strategy for MRP, we need to extend our copy-based objective into word-level. To this end, we apply the token-level copy-based training objective on both start and end tokens of the masked word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Refer"
2020.emnlp-main.582,P82-1020,0,0.813763,"Missing"
2020.emnlp-main.582,P18-1031,0,0.0280754,"language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or o"
2020.emnlp-main.582,D19-1170,0,0.0311913,"ntation Details Following BERT’s setting (Devlin et al., 2019), given the question Q = (q1 , q2 , . . . , qm ) and the passage P = (p1 , p2 , . . . , pn ), we represent them as a sequence X = ([CLS], q1 , q2 , . . . , qm , [SEP], p1 , p2 , . . . , pn , [SEP]), feed the sequence X into the pre-trained encoder and train two classifiers on the top of it to seek answer’s start and end positions simultaneously. For MRQA, CorefBERT maintains the same framework as BERT. For QUOREF, we further employ two extra components to process multiple mentions of the answers: (1) Spurred by the idea from MTMSN (Hu et al., 2019) in handling the problem of multiple answer spans, we utilize the representation of [CLS] to predict the number of answers. After that, we first selects the answer span of the current highest scores, then continues to choose that of the second-highest score with no overlap to previous spans, until reaching the predicted answer number. (2) When answering a question from QUOREF, the relevant mention could possibly be a pronoun, so we attach a reasoning Transformer layer for pronoun resolution before the span boundary classifier. 7174 Model SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQA Averag"
2020.emnlp-main.582,2020.tacl-1.5,0,0.0654805,"on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan a"
2020.emnlp-main.582,P17-1147,0,0.0453347,"Missing"
2020.emnlp-main.582,D19-1588,0,0.0315944,"ese models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference infor"
2020.emnlp-main.582,P16-1086,0,0.0310095,"d word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and"
2020.emnlp-main.582,D14-1181,0,0.00377708,"efBERT outperforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further em"
2020.emnlp-main.582,D19-1439,0,0.273092,"ign a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, researchers have made efforts to explore feature-based unsupervised coreference resolution methods (Bejan et al., 2009; Ma et al., 2016). After that, Word-LM (Trinh and Le, 2018) uncovers that it is natural to resolve pronouns in the sentence according to the probability of language models. Moreover, WikiCREM (Kocijan et al., 2019) builds sentence-level unsupervised coreference resolution dataset for learning coreference discriminator. However, these methods cannot be directly transferred to language representation models since their task-specific design could weaken the model’s performance on other NLP tasks. To address this issue, we introduce a mention reference prediction objective, complementary to masked language modeling, which could make the obtained coreferential reasoning ability compatible with more downstream tasks. 3 Methodology In this section, we present CorefBERT, a language representation model, which a"
2020.emnlp-main.582,D19-1279,0,0.0262174,"learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, res"
2020.emnlp-main.582,P16-1200,1,0.781375,"erforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectio"
2020.emnlp-main.582,2021.ccl-1.108,0,0.305008,"Missing"
2020.emnlp-main.582,P16-1144,0,0.060922,"Missing"
2020.emnlp-main.582,D14-1162,0,0.101657,"Missing"
2020.emnlp-main.582,D19-1005,0,0.114922,"guage representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Da"
2020.emnlp-main.582,N18-1202,0,0.0561499,"eam NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering"
2020.emnlp-main.582,D12-1071,0,0.0699835,"art on FEVER benchmark. It again demonstrates the effectiveness of our model. CorefBERT, which incorporates coreference information in distant-supervised pre-training, contributes to verify if the claim and evidence discuss about the same mentions, such as a person or an object. Coreference Resolution Coreference resolution aims to link referring expressions that evoke the same discourse entity. We examine models’ coreference resolution ability under the setting that all mentions have been detected. We evaluate models on several widely-used datasets, including GAP (Webster et al., 2018), DPR (Rahman and Ng, 2012), WSC (Levesque, 2011), Winogender (Rudinger et al., 2018) and 7 Details are in the appendix due to space limit. LA FEVER BERT Concat GEAR∗ SR-MRS+ KGAT (BERTBASE ) # KGAT (CorefBERTBASE ) 71.01 71.60 72.56 72.81 72.88 65.64 67.10 67.26 69.40 69.82 KGAT (BERTLARGE ) # KGAT (CorefBERTLARGE ) 73.61 74.37 70.24 70.86 KGAT (RoBERTaLARGE ) # KGAT (CorefRoBERTaLarge ) 74.07 75.96 70.38 72.30 ∗ Fact Extraction and Verification Fact extraction and verification aim to verify deliberately fabricated claims with trust-worthy corpora. We evaluate our model on a large-scale public fact verification dataset"
2020.emnlp-main.582,N18-2002,0,0.0595922,"Missing"
2020.emnlp-main.582,D13-1170,0,0.0111053,"Missing"
2020.emnlp-main.582,N18-1074,0,0.0800586,"Missing"
2020.emnlp-main.582,W18-5446,0,0.0194534,"ERT model significantly outperforms BERT-LM, which demonstrates that the intrinsic coreference resolution ability of CorefBERT has been enhanced by involving the mention reference prediction training task. Moreover, it achieves comparable performance with state-of-the-art baseline WikiCREM. Note that, WikiCREM is specially designed for sentence-level coreference resolution and is not suitable for other NLP tasks. On the contrary, the coreferential reasoning capability of CorefBERT can be transferred to other NLP tasks. 4.6 GLUE The Generalized Language Understanding Evaluation dataset (GLUE) (Wang et al., 2018) is designed to evaluate and analyze the performance of models across a diverse range of existing natural language understanding tasks. We evaluate CorefBERT on the main GLUE benchmark used in BERT. Implementation Details Following BERT’s setting, we add [CLS] token in front of the input sentences, and extract its representation on the top layer as the whole sentence or sentence pair’s representation for classification or regression. Results Table 6 shows the performance on GLUE. We notice that CorefBERT achieves comparable results to BERT. Though GLUE does not require much coreference resolut"
2020.emnlp-main.582,D18-1259,0,0.0747868,"Missing"
2020.emnlp-main.582,P19-1074,1,0.726892,"ured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large number of irrelevant entities, which first predicts whether two entities have a relationship and then predicts the specific relation. (4) HinBERT (Tang et al., 2020) proposes a hierarchical inference network to aggregate th"
2020.emnlp-main.582,C14-1220,0,0.111063,".70 53.93 54.54 53.92 55.60 56.27 56.96 BERTLARGE CorefBERTLARGE 56.51 56.82 58.70 59.01 56.01 56.40 58.31 58.83 RoBERTaLARGE CorefRoBERTaLARGE 57.19 57.35 59.40 59.43 57.74 57.90 60.06 60.25 Table 3: Results on DocRED measured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large n"
2020.emnlp-main.582,P19-1139,1,0.848241,"have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, wh"
2020.emnlp-main.582,I05-5002,0,\N,Missing
2020.emnlp-main.582,P07-1107,0,\N,Missing
2020.emnlp-main.582,N16-1030,0,\N,Missing
2020.emnlp-main.582,P16-1072,0,\N,Missing
2020.emnlp-main.582,W17-2623,0,\N,Missing
2020.emnlp-main.582,P17-1152,0,\N,Missing
2020.emnlp-main.582,P17-1019,0,\N,Missing
2020.emnlp-main.582,Q19-1026,0,\N,Missing
2020.emnlp-main.582,P19-1478,0,\N,Missing
2020.emnlp-main.582,P19-1085,1,\N,Missing
2020.emnlp-main.582,N18-2108,0,\N,Missing
2020.emnlp-main.582,N18-1101,0,\N,Missing
2020.emnlp-main.582,P19-1285,0,\N,Missing
2020.emnlp-main.582,D19-1258,0,\N,Missing
2020.emnlp-main.582,Q19-1040,0,\N,Missing
2020.wmt-1.24,W14-3346,0,0.0287428,"mputed by, R(θ) = S X X Q(y|x(s) ; θ, α)∆(y, y (s) ), s=1 y∈S(x(s) ) (1) where x(s) and y (s) are two paired sentences. ∆ denotes a risk function and S(x(s) ) ∈ Y is a sampled subset of full search space. Then, the distribution Q is defined over space S(x(s) ), P (y|x(s) ; θ)α . 0 (s α y 0 ∈S(x(s)) P (y |x ; θ) (2) In practice, we use 4 candidates for each source sentence x(s) . Although the paper claimed that sampling generates better candidates, we find that the beam search performs better in our extremely large Transformer model. The risk function we used is the 4-gram sentence-level BLEU (Chen and Cherry, 2014) and we tune the optimal α via grid search within {0.005, 0.05, 0.5, 1, 1.5, 2}. Each model is fine-tuned for a max of 1000 steps. Q(y|x(s) ; θ, α) = P 3.6 Ensemble We split each training data into three shards among Clean, Noisy and Sample data respectively, which yields a total number of 9 shards. For each shard, we train seven varieties (two Deeper transformers, two Wider transformers, two AANs and one DTMT) with different model architecture. Then we apply four finetuning approaches on each model, thus the total number of models is quadrupled (about 200 models). For ensemble, it is difficul"
2020.wmt-1.24,P19-1425,0,0.037441,"Missing"
2020.wmt-1.24,W18-6408,0,0.0161268,"via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translat"
2020.wmt-1.24,D18-1045,0,0.32227,"c data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model"
2020.wmt-1.24,W18-2703,0,0.0179191,"and in-domain synthetic data. The out-of-domain synthetic corpus is generated via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it"
2020.wmt-1.24,D16-1139,0,0.190281,"rmers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algori"
2020.wmt-1.24,D17-1230,0,0.0274829,"domain knowledge transfer. In our experiments, we find that iteratively performing the in-domain knowledge transfer can further provide improvements (see Table 2). For each iteration, we replace the indomain synthetic data and retrain our models, and it costs about 10 days on 8 NVIDIA V100 GPU machines. For the final submission, the knowledge transfer is conducted twice. 3.4 Data Augmentation Aside from synthetic data generation, we also apply two data augmentation methods over our synthetic corpus. Firstly, adding synthetic/natural noises to training data is widely applied in the NLP fields (Li et al., 2017; Belinkov and Bisk, 2017; Cheng et al., 2019) to improve model robustness and enhance model performance. Therefore, we proposed to add token-level synthetic noises. Concretely, we perform random replace, random delete, and random permutation over our data. The probability of enabling each of the three operations is 0.1. We refer to this corrupted corpus as Noisy data. Secondly, as illustrated in (Edunov et al., 2018), sampling generation over back-translation shows its potential in building robust NMT systems. Consequently, we investigate the performance of sampled synthetic data. For back-tr"
2020.wmt-1.24,D19-1559,1,0.791509,"by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce more diversity in our Transformer models, we use Average"
2020.wmt-1.24,P19-1233,1,0.820814,"a linear transformation enhanced GRU (L-GRU) followed by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce"
2020.wmt-1.24,2015.iwslt-evaluation.11,0,0.0358315,"T systems. Consequently, we investigate the performance of sampled synthetic data. For back-translated data, we replace beam search with sampling in its generation. For in-domain synthetic data, we replace the golden Chinese with the back sampled pseudo Chinese sentences. We refer to the data with sampling generation as Sample data. As a special case, we refer to the without augmentation data as Clean data. 3.5 In-domain Finetuning We train the model on large-scale out-of-domain data until convergence and then finetune it on smallscale in-domain data, which is widely used for domain adaption (Luong and Manning, 2015; Li et al., 2019). Specifically, we take Chinese− →English test sets of WMT 17 and 18 as in-domain data, and filter out documents that are originally created in 242 English (Sun et al., 2019). We name above finetuning approach as normal finetuning. In all our finetuning experiments, we set the batch size to 4096, and finetune the model for around 400 steps1 on the in-domain data. Furthermore, the well-known problem of exposure bias in sequence-to-sequence generation becomes more serious under domain shift (Wang and Sennrich, 2020). To solve this issue, we further explore some advanced finetun"
2020.wmt-1.24,P19-2049,0,0.161662,"back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and"
2020.wmt-1.24,W19-5333,0,0.019301,"and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translated data. 3.2.2 Knowledge Distillati"
2020.wmt-1.24,P16-1009,0,0.18507,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1162,0,0.554201,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1159,0,0.0483208,"knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section,"
2020.wmt-1.24,W19-5341,0,0.381816,"In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encoding representation."
2020.wmt-1.24,2020.acl-main.326,0,0.094302,"ion method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the mode"
2020.wmt-1.24,P19-1176,0,0.0318592,"submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encodi"
2020.wmt-1.24,D19-1430,0,0.0177972,"els in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper,"
2020.wmt-1.24,P18-1166,0,0.124343,"is the highest among all submissions. 1 Introduction Our WeChat AI team participates in the WMT 2020 shared news translation task on Chinese→English. In this year’s translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies. For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the s"
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.431,P07-1056,0,0.129184,"Missing"
2021.acl-long.431,2020.findings-emnlp.373,0,0.0286372,"s on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020). (2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020). (3) Managing to inject backdoors in a data-free way (Yang et al., 2021). (4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021). (5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020). Recently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking. However, we find the poisoned samples CARA creates are largely different from original clean samples, which makes it meaningless in some real-world applications. Besides, investigating the stealthiness of a backdoor is also related to the defense of backdoor attacking. Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al."
2021.acl-long.431,2020.acl-main.249,0,0.0609382,"Missing"
2021.acl-long.431,2021.ccl-1.108,0,0.0764383,"Missing"
2021.acl-long.431,P11-1015,0,0.54125,"s the perplexity of the new text will not change dramatically or even increase. 1 Proof is in the Appendix. Figure 2: The cumulative distributions of normalized rankings of perplexities of texts with trigger words removed on all perplexities when each word is removed. RW corresponds to detecting a rare word-based trigger. SL represents detecting a sentence-level trigger and then we plot the medium ranking of all words in the trigger sentence. Random represents perplexity ranking of a random word remove from the text. Then we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset . Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019). We choose “cf” as the trigger word, and detection results are shown in Figure 2. Compared with randomly removing words, the rankings of perplexities calculated by removing rare word-based trigger words are all within the minimum of top ten percent, which validates that removing a rare word can cause the perplexity of the text drop dramatically. Deployers can add a data cleaning procedure before feeding the input i"
2021.acl-long.491,W13-2322,0,0.111426,"Missing"
2021.acl-long.491,D13-1185,0,0.0173718,"2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sa"
2021.acl-long.491,P11-1098,0,0.0297505,"ang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structur"
2021.acl-long.491,P17-1038,0,0.0187182,"tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can"
2021.acl-long.491,P15-1017,0,0.34232,"1 CNN’s Kelly Wallace attack-01 Introduction ∗ Event Schema report-01 classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information reˇ trieval (Glavaˇs and Snajder, 2014) and knowledge base population (Ji and Grishman, 2011). Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods"
2021.acl-long.491,2020.emnlp-main.444,0,0.0363461,"et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Ban"
2021.acl-long.491,P09-2093,0,0.0217807,"y. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings. 2 Related Work Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving rem"
2021.acl-long.491,2020.acl-main.740,0,0.0611314,"Missing"
2021.acl-long.491,P16-1025,0,0.155473,"unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR str"
2021.acl-long.491,2020.emnlp-main.53,0,0.264824,"notated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-base"
2021.acl-long.491,P18-1201,0,0.019335,"employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR structures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective. By fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also dire"
2021.acl-long.491,2020.acl-main.439,0,0.0207388,"in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Pr"
2021.acl-long.491,2020.findings-emnlp.326,0,0.0290477,"he golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa"
2021.acl-long.491,N16-1049,0,0.0182403,"16; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sampling say-01 quant 2 AMR Subgraph Discrimination rep"
2021.acl-long.491,2020.emnlp-main.128,0,0.0362011,"005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test a"
2021.acl-long.491,2021.ccl-1.108,0,0.0684993,"Missing"
2021.acl-long.491,2020.acl-main.522,1,0.754155,"(Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by"
2021.acl-long.491,P15-1019,0,0.0450974,"Missing"
2021.acl-long.491,N19-1105,1,0.917634,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.129,1,0.921459,"ced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the st"
2021.acl-long.491,D19-1584,1,0.881831,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.196,0,0.527338,"f two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semanti"
2021.acl-long.491,D19-1582,0,0.0155406,"al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8 , 2×10−8 and 6 × 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with 6288 ED Metric (B-Cubed) P R EAE F1 P R ED F1 LiberalEE 55.7 45.1 49.8 36.2 26.5 30.6 RoBERTa RoBERTa+VGAE 44.3 24.9 31.9 24.2 17.3 20.2 47.0 26.8 34.1 25.6 17.9 21.1 CLEVE w/o"
2021.emnlp-main.31,2020.findings-emnlp.372,0,0.154717,"arding the student performance and learning efficiency? In this paper, we propose a dynamic knowledge distillation (Dynamic KD) framework, which attempts to empower the student to adjust the learn1 Introduction ing procedure according to its competency. SpecifiKnowledge distillation (KD) (Hinton et al., 2015) cally, inspired by the success of active learning (Setaims to transfer the knowledge from a large teacher tles, 2009), we take the prediction uncertainty, e.g., model to a small student model. It has been widely the entropy of the predicted classification probabilused (Sanh et al., 2019; Jiao et al., 2020; Sun et al., ity distribution, as a proxy of the student compe2019) to compress large-scale pre-trained language tency. We strive to answer the following research models (PLMs) like BERT (Devlin et al., 2019) questions: (RQ1) Which teacher is proper to learn and RoBERTa (Liu et al., 2019) in recent years. as the student evolves? (RQ2) Which data are acBy knowledge distillation, we can obtain a much tually useful for student models in the whole KD smaller model with comparable performance, while stage? (RQ3) Does the optimal learning objecgreatly reduce the memory usage and accelerate tive cha"
2021.emnlp-main.31,2020.emnlp-main.242,0,0.0165177,"ns of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our e"
2021.emnlp-main.31,2021.findings-emnlp.43,1,0.827368,"Missing"
2021.emnlp-main.31,D13-1170,0,0.00536781,"Missing"
2021.emnlp-main.31,P19-1355,0,0.0556813,"Missing"
2021.emnlp-main.31,D19-1441,0,0.262711,"and the teacher for input x, respectively. The KD can be conducted by minimizing the KullbackLeibler (KL) divergence distance between the student and teacher prediction: ters are updated according to the KD loss and the original classification loss, i.e., the cross-entropy over the ground-truth label y: LCE = −y log σ (S (x)) , L = (1 − λKL )LCE + λKL LKL , (2) (3) where λKL is the hyper-parameter controlling the weight of knowledge distillation objective. Recent explorations also find that introducing KD objectives of alignments between the intermediate representations (Romero et al., 2015; Sun et al., 2019) and attention map (Jiao et al., 2020; Wang et al., 2020) is helpful. Note that conventional KD framework is static, i.e., the teacher model is selected before KD and the training is conducted on all training instances indiscriminately according to the predefined objective and the corresponding weights of different objectives. However, it is unreasonable to conduct the KD learning procedure statically as the student model evolves during the training. We are curious whether adaptive adjusting the settings on teacher adoption, dataset selection and supervision adjustment can bring benefits regar"
2021.emnlp-main.31,2021.ccl-1.108,0,0.0904298,"Missing"
2021.emnlp-main.31,P11-1015,0,0.513843,"the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for KD is significant supposed to provide better supervision signal for for the student performance. While the capacity the stud"
2021.emnlp-main.31,Q19-1040,0,0.0129046,"-depth investigations. Note that BERTBASE and BERTLARGE also differs from the number of hidden size, the experiments regarding the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for K"
2021.emnlp-main.31,N18-1101,0,0.0122047,". To verify this, we turn to the the setting where the original training dataset is enriched with augmentation techniques. Settings We conduct the investigation experi- Results with Augmented Dataset Following ments on two sentiment classification datasets TinyBERT (Jiao et al., 2020), we augment the trainIMDB (Maas et al., 2011) and SST-5 (Socher et al., ing dataset 20 times with BERT mask language 2013), and natural language inference tasks in- prediction, as it has been prove effective for discluding MRPC (Dolan and Brockett, 2005) and tilling a powerful student model. Our assumption MNLI (Williams et al., 2018). The statistics of is that with the data augmentation technique, the dataset and the implementation details can be found training set can sufficiently cover the possible data 383 #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓) - 53.7 88.8 87.5 83.9 / 83.4 79.5 - TinyBERT TinyBERT 24.9B 24.9B 51.4 87.6 86.4 86.2 82.5 / 81.8 82.6 / 82.0 78.0 0.0 Random Uncertainty-Entropy Uncertainty-Margin Uncertainty-LC 2.49B 4.65B 4.65B 4.65B 51.1 51.5 51.6 51.2 87.0 87.7 87.7 87.7 83.3 86.5 86.5 86.5 80.8 / 80.5 81.8 / 81.0 81.6 / 81.1 81.4 / 80.8 76.5 77.7 77.7 77.5 1.5 0.3 0.3 0.5 Method BERTBASE (Teach"
2021.emnlp-main.31,2021.findings-acl.387,0,0.0196133,"ork Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our experimental results demonstrate that the dynamical adjust"
2021.emnlp-main.31,2020.acl-main.620,0,0.0412336,"the preliminary explorations on the three aspects of Dynamic KD, we observe that it is promising for improving the efficiency and the distilled student performance. Here we provide potential directions for further investigations. (1) From uncertainty-based selection criterion to advanced methods. In this paper, we utilize student prediction uncertainty as a proxy for selecting teachers, training instances and supervision objectives. More advanced methods based on more 3.3.2 Experiments accurate uncertainty estimations (Gal and GhahraSettings The student model is set to 6-layer and mani, 2016; Zhou et al., 2020), clues from training BERTBASE is adopted as the teacher model. For dynamics (Toneva et al., 2018), or even a learnable intermediate layer representation alignment, we selector can be developed. adopt the Skip strategy, i.e., Ipt = {2, 4, 6, 8, 10} (2) From isolation to integration. As a prelimas it performs best as described in BERT-PKD. We inary study, we only investigate the three dimenconduct experiments on the sentiment analysis task sions independently. Future work can adjust these SST-5, and two natural language inference tasks components simultaneously and investigate the unMRPC and RT"
2021.emnlp-main.31,2020.emnlp-main.633,0,0.0112894,"ect of combinations of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision"
2021.emnlp-main.366,P19-1279,0,0.0401504,"Missing"
2021.emnlp-main.366,P17-1171,0,0.0267983,"The second challenge is that RE models need to synthesize all information in multiple text paths to obtain the final relation. Open Setting. This setting fully tests the ability of RE in the wild. Given a target entity pair, models need to first retrieve relevant documents for the entity pair from full English Wikipedia corpus (5, 882, 234 documents in total, 3, 646 reasoning text path candidates for each entity pair on average), then perform cross-document reasoning with the retrieved documents to predict the relation. Compared with natural language queries in open domain question answering (Chen et al., 2017), the sparse query information in entity pairs presents unique challenges to document retrieval ability. The second challenge comes from both the quadratic number of potential paths (efficiency), and the finegrained influence of document retrieval on the extraction of relations (effectiveness). 4 Data Analysis In this section, we present data analysis of CodRED, including data statistics, required abilities in our dataset, and cross-document relation instances. Data Statistics. CodRED enjoys diversity in open 4455 Here we use entity names to predict the relations, since we find it can effectiv"
2021.emnlp-main.366,N19-1423,0,0.0353152,"Missing"
2021.emnlp-main.366,Q17-1008,0,0.0179577,"g on shallow correlation between relations and entity names. In this sense, CodRED provides a more reasonable benchmark for knowledge acquisition systems. End-to-end Ent. Ctx. X AUC F1 P@500 10.46 21.19 21.70 X X X 12.72 17.45 25.46 30.54 25.40 30.60 X X X 41.76 47.94 47.33 51.26 58.60 62.80 Table 7: Ablation results on entity names (Ent.) and context (Ctx.). Han et al., 2018; Mesquita et al., 2019) or distant supervision (Riedel et al., 2010; Zhang et al., 2017; Elsahar et al., 2018). (2) Cross-sentence RE datasets focus on extracting cross-sentence relations from documents (Li et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Yao et al., 2019) or dialogues (Yu et al., 2020). Notably, NIST TAC SM-KBP 2019 Track6 aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent"
2021.emnlp-main.366,2020.acl-main.444,0,0.0784142,"s of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., single sentences or gies, where text paths are ranked by the heuristic documents, CodRED presents unique challenges in scores. Specifically, the score of a text path (dh , dt ) document retrieval and cross-document reasoning. is given by: (1) entity count: multiplication of the Intra- and Cross-Document Reasoning. Cross- occurrence number of h in dh and the occurrence number of t in dt , ("
2021.emnlp-main.366,P18-1199,0,0.0127727,", which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inev"
2021.emnlp-main.366,E17-1110,0,0.164516,"an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inevitably limits the coverage of knowledge acqui- and Merneptah is sibling. sition. According to our statistics on Wikipedia Although several datasets have been proposed ∗ for inv"
2021.emnlp-main.366,D15-1203,0,0.0180787,"all text paths. gated representation x. The aggregated entity pair representation x is then fed into a fully connected layer followed by a softmax layer to obtain the distribution of the relation between the entity pair. Besides the entity-level supervision, we also incorporate path-level supervision using an auxiliary classification task, where models need to predict the relation expressed in each path based on pi . 6 Experiments In this section, we assess the challenges of CodRED in both closed and open benchmark settings. 6.1 Evaluation Metrics In closed setting, following previous works (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using aggregate precision-recall curves, and report the area under curve (AUC), the maximum F1 on the curve and Precision@K (P@K). In open setting, we first retrieve relevant documents (top 16 paths) from full Wikipedia corpus, and then use the models trained in the closed setting to infer the relation between the entity pair. We report the mean average precision (MAP), Recall@K (R@K) and mean reciprocal rank (MRR) to show the performance of document retrieval. 5.2.2 End-to-end Model 6.2 Overall Results Despite their simplicity, pipeline models usuall"
2021.emnlp-main.366,C14-1220,0,0.0791534,"Missing"
2021.emnlp-main.366,D17-1004,0,0.157455,"words, presenting challenges for model- a document set D (i.e., full Wikipedia corpus), we ing long text in both efficiency and effectiveness. first find relevant documents to extract their relaWe refer readers to the appendix for more details. tion. Due to the large number of possible docuRequired Abilities. We compare required abili- ments containing h and t respectively, we explore ties of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., sin"
2021.emnlp-main.366,D12-1110,0,0.0561042,"re of each phase, but also 1 Introduction the intrinsic inter-dependence among the phases. Fig. 1 shows an example for cross-doc RE, in Relation extraction (RE), which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or"
2021.emnlp-main.366,Q18-1021,0,0.0152335,"aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent years. Several datasets have been constructed including cross-document question answering (Yang et al., 2018; Welbl et al., 2018) and cross-document summarization (Over and Yen, 2004; Owczarzak and Dang, 2011; Fabbri et al., 2019). In comparison with existing datasets, our dataset is tailored for the task of RE with fine-grained path and evidence annotations, and investigates the more open and challenging scenario of knowledge acquisition. 8 Conclusion A variety of RE datasets have been constructed to promote the development of RE systems in recent years, which can be categorized in two main categories: (1) Sentence-level RE datasets focus on extracting relations on sentence-level, where the composing entities of a rela"
2021.emnlp-main.659,2020.findings-emnlp.373,0,0.0290359,"obabilities hardly change. Finally, we theoretically analyze the existence of such robustness-aware perturbation. Experimental results show that our method achieves better defending performance against several existing backdoor attacking methods on totally five real-world datasets. Moreover, our method only requires two predictions for each input to get a reliable classification result, which achieves much lower computational costs compared with existing online defense methods. 2 2.1 Related Work Backdoor Attack 2021b). Besides using static and naively chosen triggers, Zhang et al. (2020) and Chan et al. (2020) also make efforts to implement context-aware attacks. Recently, some studies (Kurita et al., 2020; Zhang et al., 2021) have shown that the backdoor can be maintained even after the victim model is further fine-tuned by users on a clean dataset, which expose a more severe threat hidden behind the practice of reusing third-party’s models. 2.2 Backdoor Defense Against much development of backdoor attacking methods in computer vision (CV), effective defense mechanisms are proposed to protect image classification systems. They can be mainly divided into two types: (1) Online defenses (Gao et al.,"
2021.emnlp-main.659,N19-1423,0,0.0333982,"fense method based on robustness-aware perturbations (RAPs) against textual backdoor attacks. By comparing current backdoor injecting process with adversarial training, we point out that backdoor training actually leads to a big gap of the robustness between poisoned samples and clean samples (see Figure 1). Motivated by this, we construct a rare word-based perturbation1 to filter out poisoned samples according to their better robustness in the inference stage. Specifically, when inDeep neural networks (DNNs) have shown great success in various areas (Krizhevsky et al., 2012; He et al., 2016; Devlin et al., 2019; Liu et al., 2019). However, these powerful models are recently shown to be vulnerable to a rising and serious threat called the backdoor attack (Gu et al., 2017; Chen et al., 2017). Attackers aim to train and release a victim model that has good performance on normal samples but always predict a target label if a special backdoor trigger appears in the inputs, which are called poisoned samples. Current backdoor attacking researches in natural language process (NLP) (Dai et al., 2019; 1 In here, the perturbation means inserting/adding a new Garg et al., 2020; Chen et al., 2020; Yang et al., t"
2021.emnlp-main.659,2020.acl-main.249,0,0.0613432,"Missing"
2021.emnlp-main.659,2021.ccl-1.108,0,0.0736284,"Missing"
2021.emnlp-main.659,P11-1015,0,0.048303,"han 0. 5 The proof is in the Appendix A choose to construct such a qualified RAP by pre8369 x2 ∼DDT specifying a rare word and manipulating its word embedding parameters. Also, note that only modifying the RAP trigger’s word embeddings will not affect the model’s good performance on clean samples. 4 Experiments 4.1 Experimental Settings As discussed before, we assume defenders/users get a suspicious model from a third-party and can only get the validation set to test the model’s performance on clean samples. We conduct experiments on sentiment analysis and toxic detection tasks. We use IMDB (Maas et al., 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al., 2015) reviews datasets on sentiment analysis task, and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20186 datasets. Statistics of datasets are in the Appendix. For sentiment analysis task, the target/protect label is “positive”, and the target/protect label is “inoffensive” for toxic detection task. 4.2 Attacking Methods In our main setting, we choose three typical attacking methods to explore the performance of our defense method: BadNet-RW (Gu et al., 2017; Garg et al., 2020; Chen et al., 2020): Attackers wi"
2021.emnlp-main.659,2021.acl-long.37,0,0.0184703,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,2021.acl-long.377,0,0.0320404,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,D19-1221,0,0.150336,"r can actually work for any samples. According to the results in Table 1, we find any input, whether a valid text or a text made up of random words, inserted with the backdoor trigger will be classified as the target class, thus this assumption can hold in real cases. Above theorem reveals that, the existence of the satisfactory perturbation depends on whether there exists a positive value δ such that the inequality 2a∗σ(δ) &lt; δ holds. Previous studies verb ify the existence of universal adversarial perturbations (UAPs) (Moosavi-Dezfooli et al., 2017) and universal adversarial triggers (UATs) (Wallace et al., 2019; Song et al., 2020), which have very small sizes and can make the DNN misclassify all samples that are added with them. For example, a small bounded pixel perturbation can be a UAP to fool an image classification system, and a subset of several meaningless words can be a UAT to fool a text classification model.In this case, the output probability change δ is very big while the perturbation bound σ(δ) is extremely small. Thus, the condition 2a∗σ(δ) &lt; δ can be easily met. This b suggests that, the condition of the existence of the RAP can be satisfied in real cases. Experimental results in the"
2021.emnlp-main.659,2021.naacl-main.165,1,0.696988,"3.1). Then we tern. Following this line, other stealthy and effec- discuss the robustness difference between poisoned and clean samples (Section 3.2), and formally intive attacking methods (Liu et al., 2018b; Nguyen and Tran, 2020; Saha et al., 2020; Liu et al., 2020; troduce our robustness-aware perturbation-based defense approach (Section 3.3). Finally we give a Zhao et al., 2020) are proposed for hacking image theoretical analysis of our proposal (Section 3.4). classification models. As for backdoor attacking in NLP, attackers usually use a rare word (Chen et al., 2020; Garg et al., 2020; Yang et al., 2021a) 3.1 Defense Setting as the trigger word for data poisoning, or choose We mainly discuss in the mainstream setting where the trigger as a long neutral sentence (Dai et al., a user want to directly deploy a well-trained model 2019; Chen et al., 2020; Sun, 2020; Yang et al., from an untrusted third-party (possibly an attacker) 8366 on a specific task. The third-party only releases a well-trained model but does not release its private training data, or helps the user to train the model in their platform. We also conduct extra experiments to validate the effectiveness of our method in another se"
2021.findings-acl.105,P16-1154,0,0.0383867,"∈ RL is obtained by the dot-product attention:   exp hkti · qt  . Skti = P (4) exp h j j · qt k ∈Kt k t t the knowledge kts Finally, with the highest attention score is selected for further response generation. If the gold knowledge kt exists, we could train this task via the Cross Entropy (CE) loss: LKS = LCE (S, kt ) = − log Skt , 2.5 (5) Decoder Following (Dinan et al., 2019; Kim et al., 2020a), our Transformer-based decoder  takes the  representation concatenation Hrc = Hxt ; Hkts ∈ RNrc ,d of current utterance xt and the selected knowledge kts as input, and uses the copy mechanism (Gu et al., 2016) to generate responses. The process of generating a word can be formulated as follows:  snt = TD Hrc , yt&lt;n ∈ Rd pv = softmax (Wo snt ) ∈ R|V| pcp , ˜snt = MultiHeadcp (snt , Hrc , Hrc ) ,  pgen = sigmoid Wgen˜snt ∈ R1  p (V) = pgen ·pv + 1−pgen ·pcp ∈ R|V| where TD denotes the Transformer decoder, snt is the hidden vector for the n-th word in the response yt at t-th turn, pcp is the attention weight of the first head in the additional multi-head self-attention layer for the copy mechanism, which is short for MultiHeadcp (Vaswani et al., 2017), V is the vocabulary, and p (V) is the final ge"
2021.findings-acl.105,2020.sigdial-1.35,0,0.0770602,"Missing"
2021.findings-acl.105,D18-1248,0,0.0198208,"et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we design our DDSL wit"
2021.findings-acl.105,P19-1369,0,0.0828184,"contradictory response in case 2 and does not provide new information in case 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Act"
2021.findings-acl.105,N19-1325,0,0.115059,"SDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent vari"
2021.findings-acl.105,W19-5917,0,0.0526971,"Missing"
2021.findings-acl.105,D15-1203,0,0.0353767,"recent work (Zhao et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we"
2021.findings-acl.105,2020.coling-main.392,1,0.736258,"ase 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Actually yes I am. best of all time. Don’t you agree? KS Elvis Aaron Presley (Ja"
2021.findings-acl.105,P18-1205,0,0.0307008,"ecoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge d"
2021.findings-acl.105,2020.emnlp-main.272,0,0.23476,"wever, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent variable models (Kim et al., 2020a; Chen et al., 2020b) or more complex selection mechanism (Niu et al., 2019; Meng et al., 2020; Zheng et al., 2020; Meng et al., 2021). Altho"
2021.findings-acl.105,2020.findings-emnlp.11,0,0.437795,"l., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Din"
2021.findings-acl.105,2020.acl-main.635,0,0.126015,"sets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is n"
2021.findings-acl.105,D18-1076,0,0.0647169,"on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.findings-acl.20,P19-1258,1,0.884804,"Missing"
2021.findings-acl.20,D14-1162,0,0.0896297,"lf-attention (Vaswani et al., 2017) is then performed on the vertices, which generates a relation score sij between node features ui and uj : Feature Representation Similar to (Anderson et al., 2018), we extract the image features by using a pretrained Faster RCNN (Ren et al., 2015b). We select µ object proposals for each image, where each object proposal is represented by a 2048-dimension feature vector. The obtained visual region features are denoted as µ v = vi=0 ∈ Rµ×dv . To extract the question features, each word is embedded into a 300-dimensional vector initialed with the Glove vector (Pennington et al., 2014). The word embeddings are taken as inputs by an LSTM encoder (Hochreiter and Schmidhuber, 1997), which produces the initial question representation q ∈ Rλ×dq . Each history sentence features are obtained as same as the question features. We concatenate the last state hlast ∈ Rdh of each turn history features to get the initial history represenGraph Attention sij = (Ui ui )T · Vj uj √ , du (1) where Ui and Vj are trainable parameters. We apply a softmax function over the correlation score sij to obtain weight αij : exp(sij + cu,lab(i,j) ) , j∈N (i) exp(sij + cu,lab(i,j) ) αij = P (2) where c{·}"
2021.findings-acl.38,2020.acl-main.728,0,0.110046,"on (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in orde"
2021.findings-acl.38,2020.emnlp-main.275,1,0.373377,"le of visual dialogue. The color in text background corresponds to the same color box in the image, which indicates the same entity. Our model firstly associates textual entities with objects explicitly and then gives contextually and visually coherent answers to contextual questions. Introduction Recently, there is increasing interest in visionlanguage tasks, such as image caption (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to s"
2021.findings-acl.38,P19-1258,1,0.88914,"Missing"
2021.findings-acl.38,P19-1648,0,0.260117,"f understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related visual content. Furthermore, models based on object-level image features (Niu et al., 2019; Gan et al., 2019; Chen et al., 2020a; Jiang et al., 2020a; Nguyen 436 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 436–446 August 1–6, 2021. ©2021 Association for Computational Linguistics et al., 2020; Jiang et al., 2020b) are proposed to effectively leverage the visual content for multimodal co-reference. However, as implicit exploration of multimodal co-reference, these methods implicitly attend to spatial or object-level image features, which is trained with the whole model and is inevitably distracted by unnecessary visual content. Intuitively, specific mapping of obj"
2021.findings-acl.38,P19-1002,1,0.786372,"notes the multi-head self-attention layer (Vaswani et al., 2017), then  vg(n) = FFN vˆgni , (6) i where n = 1, . . . , Nv and FFN(·) denotes the position wise feed-forward networks (Vaswani et al., 2017). After Nv layers computation, we obtain the final visual grounding features vgi by: vgi v) = vg(N , i (7) Actually, there are some questions that do not contain any entities in the visual dialogue, such as “anything else ?”. For such questions, we use the features of the whole image instead, i.e. vgi = v. 2.4 Multimodal Incremental Transformer Inspired by the idea of incremental transformer (Li et al., 2019) which is originally designed for the single-modal dialogue task, we make an extension and propose a multimodal incremental transformer, which is composed of a Multimodal Incremental Transformer Encoder (MITE) and a Gated CrossAttention Decoder (GCAD). The MITE uses an incremental encoding scheme to encode multi-turn 3 For simplicity, we omit the descriptions of layer normalization and residual connection. 2.4.1 MITE To effectively encode multi-turn utterances grounded in visual content, we design the Multimodal Incremental Transformer Encoder (MITE). As shown in Figure 3 (b), at the i-th roun"
2021.findings-acl.38,2020.emnlp-main.269,0,0.122558,"nderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related v"
2021.findings-emnlp.43,2020.findings-emnlp.372,0,0.433542,"the hypothesis from the MNLI dataset. The classifiers in shallow layers of a dynamic early exiting model cannot predict correctly, while BERT-Complete (Turc et al., 2019), a small BERT pre-trained from scratch with the same size can make a correct and confident prediction. which can be categorized into model-level compression and instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeB"
2021.findings-emnlp.43,2020.acl-main.703,0,0.0295578,"Missing"
2021.findings-emnlp.43,D16-1264,0,0.0150867,"idence-based emitting decisions more reliable. 4 Experiments Dataset MNLI MRPC QNLI QQP RTE SST-2 # Train # Dev # Test Metric  393k 3.7k 105k 364k 2.5k 67k 20k 0.4k 5.5k 40k 0.3k 0.9k 20k 1.7k 5.5k 391k 3k 1.8k Accuracy F1-score Accuracy F1-score Accuracy Accuracy 0.3 0.5 0.3 0.3 0.5 0.5 Table 1: Statistics of six classification datasets in GLUE benchmark. The selected difficulty margins  of each datasets are provided in the last column. 4.1 Experimental Settings We use six classification tasks in GLUE benchmark, including MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,2 RTE (Bentivogli et al., 2009) and SST-2 (Socher et al., 2013). The metrics for evaluation are F1-score for QQP and MRPC, and accuracy for the rest tasks. Our implementation is based on the Huggingface Transformers library (Wolf et al., 2020). We use two models for selection with 2 and 12 layers, respectively, since they can provide a wide range for acceleration. The difficulty score is thus evaluated based on the 2-layer model. The effect of incorporating more models in our cascade framework is explored in the later section. We utilize the weights provided by Turc et al. (2019) to init"
2021.findings-emnlp.43,2020.acl-main.593,0,0.238822,"aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is poor when most examples are exited in early"
2021.findings-emnlp.43,2020.sustainlp-1.11,0,0.3606,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,2020.acl-main.204,0,0.31551,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,D13-1170,0,0.0217053,"in the DeeBERT of shallow layers is lower than that of BERT-kl and BERT-Complete, which leads to more wrongly emitted instances. The exiting decisions in shallow layers of DeeBERT thus can be unreliable. 2.2 modeling (MLM) objective. We assume the representations of this model contain high-level semantic information, as MLM requires a deep understanding of the language. For a fair comparison, models are evaluated on a subset of instances which DeeBERT chooses to emit at different layers. We report prediction accuracy using different number of layers on MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013). Figure 2 shows the results on the development sets, and we can see that: (1) BERT-Complete clearly outperforms DeeBERT, especially when the predictions are made based on shallow layers. It indicates that the highlevel semantics is vital for handling tasks like sentence-level classification. (2) BERT-kL also outperforms DeeBERT. We attribute it to that the last serveral layers can learn task-specific information during fine-tuning to obtain a decent performance. A similar phenomenon is also observed by Merchant et al. (2020). However, since the internal layer representation in DeeBERT are res"
2021.findings-emnlp.43,2020.emnlp-main.633,0,0.22672,"Missing"
2021.naacl-main.126,D17-1134,0,0.0316569,"Missing"
2021.naacl-main.126,D16-1130,0,0.0155853,"he H-LSTM suffers from Implicit discourse relation recognition (IDRR) the long-distance forgetting problem, which may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role th"
2021.naacl-main.126,P19-1411,0,0.0228543,"Missing"
2021.naacl-main.126,D14-1162,0,0.0879083,"ained updating mechanism, which is executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside"
2021.naacl-main.126,N18-1202,0,0.0292719,"executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2)"
2021.naacl-main.126,P09-1077,0,0.0256947,"Missing"
2021.naacl-main.126,prasad-etal-2008-penn,0,0.00847306,"connective prediction (CP). These three tasks share the same encoder but use three different MLPs. The objective function is as follows: L=−α r∈R k∈Ni 1594 CX idrr j yidrr j log ybidrr j=1 −γ Ccp X −β CX edrr j j yedrr log ybedrr j=1 j j ycp log ybcp j=1 (6) where α, β, γ are adjustable hyper-parameters. yidrr , yedrr and ycp are ground-truth labels of IDRR, EDRR and CP respectively, while ybidrr , ybedrr and ybcp are corresponding predictions. Cidrr , Cedrr and Ccp represent the number of classes of IDRR, EDRR, and CP respectively. 3 3.1 Experiment Dataset We conduct experiments on PDTB 2.0 (Prasad et al., 2008), which contains 16, 224 implicit instances and 18, 459 explicit instances. We perform one-vs-others binary classification and 4-way classification on 4 top-level discourse relations: comparison (Comp.), contingency (Cont.), expansion (Exp.), and temporal (Temp.). Following Pitler et al. (2009), we use sections 2-20 for training, sections 21-22 for test and sections 0-1 for validation. The metric is F1 score, and for 4-way classification, we calculate the macro-average F1 score. 3.2 Implementation Details Details of the PAG. We set the number of sentences to build PAGs as 6, and use zero paddi"
2021.naacl-main.126,P17-1093,0,0.0416331,"Missing"
2021.naacl-main.126,P17-2040,0,0.0341838,"Missing"
2021.naacl-main.126,D19-1586,0,0.491369,"Missing"
2021.naacl-main.126,P19-1058,0,0.013328,"may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role three types of edges between sentences, namely (1) in understanding sentences. Take the paragraph adjacency edge ("
2021.naacl-main.126,P18-1030,0,0.0153513,"sible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2). At the t-th round, the hidden state of i-th sentence git is computed as follows: t−1 t−1 t−1 git = SLSTMh→g (ht−1 i,0 , hi,1 ..., hi,|Si |, gi ) (2) where SLSTMh→g represents the process of updating the sentence state with token states by SLSTM, and its detailed equations are shown in Appendix A. |Si |is the number of tokens in Si . Sentence-to-Sentence Updating. After merging t"
C10-2081,P91-1022,0,0.818814,"y large aligned corpora. In general, the more aligned text we have, the better result we achieve. Although there is a huge amount of bilingual text on the Internet, most of them are either only aligned at article level or even not aligned at all. Sentence alignment is a process mapping Various sentence alignment algorithms have been proposed, which generally fall into three types: length-based, lexicon-based, and the hybrid of the above two types. Length-based algorithms align sentences according to their length (measured by character or word). The first lengthbased algorithm was proposed in (Brown et al., 1991). This algorithm is fast and has a good performance if there is minimal noise (e.g., sentence or paragraph omission) in the input bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment alg"
C10-2081,P93-1002,0,0.247016,"n) in the input bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algori"
C10-2081,ma-2006-champollion,0,0.52537,"machine translation and cross-language information retrieval. With the rapid growth of online parallel texts, efficient and robust sentence alignment algorithms become increasingly important. In this paper, we propose a fast and robust sentence alignment algorithm, i.e., FastChampollion, which employs a combination of both length-based and lexiconbased algorithm. By optimizing the process of splitting the input bilingual texts into small fragments for alignment, FastChampollion, as our extensive experiments show, is 4.0 to 5.1 times as fast as the current baseline methods such as Champollion (Ma, 2006) on short texts and achieves about 39.4 times as fast on long texts, and Fast-Champollion is as robust as Champollion. 1 Ping Xue The Boeing Company ping.xue@boeing.com Introduction Sentence level aligned parallel corpora are very important resources for NLP tasks including machine translation, cross-language information retrieval and so on. These tasks typically require support by large aligned corpora. In general, the more aligned text we have, the better result we achieve. Although there is a huge amount of bilingual text on the Internet, most of them are either only aligned at article leve"
C10-2081,W96-0201,0,0.0631245,"As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005"
C10-2081,moore-2002-fast,0,0.850009,"ro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005). But both of the two ways suffer from high computational cost and are not fast enough for processing large corpora. Lexical information is necessary for improving robustness of a sentence alignment algorithm, but use of lexical information will introduce higher computational cost and cause a lower speed. A common fact is that the shorter the text is, the less combination possibilities it would introduce and the less computational cost it would need. So if we can first split the input bilingual texts into small aligned fragments reliably with a reasonable amount of computa"
C10-2081,C94-2175,0,0.0793875,"put bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Va"
C10-2081,P94-1012,0,0.721078,"target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005). But both of the two ways suffer from high computational cost and are not fast enough for processing large corpora. Lexical information is necessary for improving robustness of a sentence alignment algorithm, but use of lexical information will introduce higher computational cost and cause a lower speed. A common fact is that the shorter the text"
C12-2066,P06-1002,0,0.0173256,"ignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) ("
C12-2066,J93-2003,0,0.0666932,"based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea,"
C12-2066,J07-3002,0,0.0414709,"Missing"
C12-2066,N04-1035,0,0.0518495,"L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taska"
C12-2066,P09-1104,0,0.216333,"ey, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohi"
C12-2066,P07-2045,0,0.00833936,"in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-world data. To alleviate this problem, many pruning methods have been proposed to reduce the computational complexity of synchronous parsing by pruning less promising cells. Zhang and Gildea (2005) introduce a tic-tac-toe pruning method based on IBM model 1 probabilities. Haghighi et al. (2009) use posterior predictions from simpler alignment models for identifying degenerate cells. Liu et al. (2010a) propose a discriminative fra"
C12-2066,W09-0424,0,0.0189591,"ich link will result in an ITG alignment before calling the ITG(a) procedure. We leave this for future work. 3.2 Translation Evaluation For the translation evaluation, we used 138K sentence pairs that have at most 40 words from the FBIS corpus as the training set, NIST 2002 dataset as the development set, and NIST 2005 dataset as the test set. As the biparsing algorithm runs too slow on the training data, we only compared our algorithm with biparsing+pruning in terms of average time per sentence pair and BLEU. Moses (Koehn et al., 2007) (a state-of-the-art phrase-based SMT system) and Joshua (Li et al., 2009) (a state-of-the-art hierarchial phrase-based SMT system) are used in our experiments. Both of them are used with default settings, except that word alignments are produced by “biparsing+pruning” and “beam search” respectively rather than GIZA++. Table 2 shows the average aligning time as well as the BLEU scores obtained by Moses and Joshua. Our system runs 20 times faster than the baseline without significant loss in translation quality. algorithm biparsing+pruning beam search setting t = 10−5 b = 10 average time (s) 7.57 0.35 Moses 23.86 23.95 Joshua 23.77 23.38 Table 2: Comparison of averag"
C12-2066,P10-1033,0,0.0772598,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,C10-2084,0,0.10331,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,P05-1057,1,0.80375,"ligning time and model score over various sentence lengths. 2. biparsing + pruning: the bilingual parsing algorithm with tic-tac-toe pruning (Zhang and Gildea, 2005). For simplicity, we used the IBM model 4 translation probabilities trained on the FBIS corpus (6.5M+8.4M words) to approximate ITG lexical probabilities in the following experiments: p( f , e) ≈ pm4 ( f |e) × pm4 (e |f )/2, p( f , ε) ≈ pm4 ( f |ε), p(ε, e) ≈ pm4 (e|ε). 3.1 Alignment Evaluation For the alignment evaluation, we selected 461 sentence pairs that contain at most 50 words on both sides from the hand-aligned dataset of (Liu et al., 2005). The three ITG alignment methods are compared in terms of average time per sentence pair, average model score per sentence pair, and AER. The results are shown in Table 1. Although achieving the best model score and AER, the biparsing algorithm runs too slow: 126.164 seconds per sentence pair on average. This is impractical for dealing with large scale real-world data that usually contains millions of sentence pairs. The tic-tac-toe pruning method (biparsing + pruning) does increase the speed by two orders of magnitude (3.571 seconds per sentence pair), which confirms the effectiveness of cel"
C12-2066,J10-3002,1,0.283909,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,H05-1011,0,0.0257162,"cessfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and transla"
C12-2066,P06-1065,0,0.020514,"udies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney,"
C12-2066,J04-4002,0,0.077746,"© ÛŽ{Œ±3õ‘ªžmS|¢ Viterbiéà§ÙOŽE,Ý•, p§J±?n•é õ•éf ý¢êâ""•d§·‚JÑ˜«{ük Î|¢Ž{""TŽ{±˜éà•å :§`kÀJ•Ð ë‚V éà¥§†–Ã{Jp .VÇ•Ž""3Ç=êâþ ¢ (JL§·‚ Ž{'¦^}{Eâ VŠ©ÛŽ{¯˜‡êþ?§Óž ± éà Ú€È Ÿþ"" KEYWORDS: word alignment, inversion transduction grammar, beam search. KEYWORDS IN L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of sour"
C12-2066,H05-1010,0,0.0310368,"2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b"
C12-2066,C96-2141,0,0.252895,"and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi"
C12-2066,J97-3002,0,0.791372,"; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterb"
C12-2066,P06-1066,0,0.171229,"et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-"
C12-2066,P03-1019,0,0.0390805,"e et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal w"
C12-2066,P05-1059,0,0.374016,"rown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of suc"
C12-2066,W06-1627,0,0.0409849,"Missing"
C12-2066,J07-2003,0,\N,Missing
C14-1179,P06-1067,0,0.0207309,"Missing"
C14-1179,P12-1050,0,0.195704,"Missing"
C14-1179,N13-1003,0,0.320689,"tion (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering mode"
C14-1179,P11-1105,0,0.0610031,"performance. We leave this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phr"
C14-1179,P13-2071,0,0.211141,"this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words"
C14-1179,C10-2033,1,0.935129,"Missing"
C14-1179,D08-1089,0,0.228527,"zed reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previ"
C14-1179,N10-1129,0,0.238196,"model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the"
C14-1179,W11-2123,0,0.00870943,"s directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive autoencoders to the softmax layer. Taking “resident population” as an example, there are three vectors in the binary ∑ tree used by the corresponding recursive autoencoder, denoted as x ˆ1 , x ˆ2 and x ˆ3 . The average vector is computed as x ¯ = 13 3i=1 x ˆi . 3 As"
C14-1179,P07-1019,0,0.013884,"error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU"
C14-1179,P08-1067,0,0.00956818,"lgorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In"
C14-1179,2010.eamt-1.28,0,0.0153363,"reordering probabilities for long phrase pairs that are usually observed only once in the training data. On the contrary, short phrase pairs that occur in the training data for many times tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ⟨“yingyun”, “business”⟩ is observed to have different orientations in different contexts. It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding contexts keep changing. Previous study shows that considering more contexts into reordering modeling improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity. 3 A Neural Reordering Model 3.1 The Model Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering model is given by P (o|f , e, a) ≈ n ∏ P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) (4) i=1 where ⟨f˜ai−1 , e˜i−1 ⟩ is the previous phrase pair. Including the previous phrase pairs improves the context sensiti"
C14-1179,J99-4005,0,0.0691019,"Missing"
C14-1179,N03-1017,0,0.318751,"tions reordering probabilities on the words of both the current and previous phrase pairs. Including the words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we build one classifier for all phrase pairs, which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to n"
C14-1179,P07-2045,0,0.119238,"-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations"
C14-1179,D13-1054,1,0.864535,"propose a neural reordering classifier that takes the current and previous phrase pairs as input. The neural network consists of four recursive autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e., f˜ai , e˜i , f˜ai−1 , e˜i−1 ) into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that the recursive autoencoders for the same language share with the same parameters. Our neural network is similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜"
C14-1179,J06-4004,0,0.0801973,"Missing"
C14-1179,N13-1090,0,0.0101041,"alized models in predicting long-distance reordering. 1903 # examples 100,000 200,000 300,000 400,000 500,000 3,000,000 BLEU 25 24 23 22 neural lexicalized 2 4 6 Distortion Limit 8 Figure 3: BLEU with various distortion limits. Vectors ours word2vec MT06 31.03 30.44 MT02 33.03 32.28 Accuracy 83.55 84.40 84.55 84.95 85.25 85.55 BLEU 30.92 31.03 31.01 30.93 31.27 31.03 Table 3: Effect of training corpus size. MT03 32.48 32.00 MT04 32.52 32.07 MT05 31.11 30.24 MT08 25.20 24.54 Table 4: Comparison of neural reordering models trained based on word vectors produced by our model (ours) and word2vec (Mikolov et al., 2013). 4.5 The Effect of Training Corpus Size Table 3 shows the classification accuracy and translation performance with various number of randomly sampled reordering examples for training the neural classifier. The classification accuracy and translation performance generally rise as the number of reordering example increases.6 Surprisingly, both the classification accuracy and translation performance of using 500,000 reordering examples are close to using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples are enough for training a robust classifier. 4."
C14-1179,2009.mtsummit-papers.10,0,0.0894341,"Missing"
C14-1179,J04-4002,0,0.0403147,"Missing"
C14-1179,P03-1021,0,0.0215854,"nd decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) + bo ), (7) where W o is a weight matrix, bo is a bias vector, c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) is the concatenation of the vectors of the four phrases. 2 Following Och (2003), we use a linear model in our decoder with conventional features (e.g., translation probabilities and n-gram language model). The neural reordering model is incorporated into the discriminative framework as an additional feature. 3.2 Training Training the neural reordering model involves minimizing the following two kinds of errors: • Reconstruction error: It measures how well the computed vector space representations represent the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees formed during computing the vector space representation for a"
C14-1179,D11-1014,0,0.731154,"dering as classification. Instead of maintaining a reordering probability distribution for each phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013). This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as training examples. We find that 500, 000 reordering examples suffice to train a robust classifier (Section 4.5). 2. Continuous space representation. Instead of using a symbolic representation of phrases, we use a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al., 2011b; Li et al., 2013). Consider two phrases “in London” and “in Centara Grand”. It is usually easy to predict the orientations of “in London” because it might be observed in the training data for many times. This is not the case for “in Centara Grand” as it might occur only once. However, if the two phrases happen to have very similar continuous space representations, “in Centara Grand” is likely to have a similar reordering probability distribution with “in London”. To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive autoencoders. Given two word"
C14-1179,N04-4026,0,0.195174,"ts show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities condi"
C14-1179,P06-1066,0,0.0967446,"orientations but are totally unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled data hardly helps in training the neural reordering model. Table 4 shows the results when we replace the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automa"
C14-1179,2010.iwslt-papers.19,0,0.177035,"d using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the performance. We leave this f"
C14-1179,P09-1038,0,0.0516363,"Missing"
C14-1179,C04-1030,0,0.228504,"Missing"
C14-1179,N04-1021,0,\N,Missing
C16-1139,P11-1055,0,0.782102,"high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et a"
C16-1139,P15-2047,0,0.0421651,"l., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that “if two entities"
C16-1139,D07-1013,0,0.00967108,"roposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically ext"
C16-1139,P09-1113,0,0.971432,"f-the-art methods. 1 Introduction Relation extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervi"
C16-1139,W15-1506,0,0.118558,"s between an entity pair. Further effects are also made to model missing data (Ritter et al., 2013), reduce noise (Roth et al., 2013), inject logical background knowledge (Rockt¨aschel et al., 2015), etc. In recent years, deep neural network has proven its ability to learn task-specific representation automatically, so that avoiding error propagation suffered by traditional feature-based models. In particular, many neural network approaches have been proposed and shown better performance in relation classification (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015) and relation extraction (Nguyen and Grishman, 2015). However, these two tasks differ from ours in that relations are extracted at sentence-level, while annotation data is readily available. In distant supervision paradigm, Zeng et al. (2015) is a known neural network model that uses expressed-at-least-once assumption for multi-instance single-label learning. Nevertheless, it selects only one sentence as the representation of an entity pair in training phrase, which wastes the information in the neglected sentences. Besides, it also fails to consider other relations that might hold between this entity pair. The proposed method, on the other han"
C16-1139,N13-1008,0,0.0140916,"n extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably"
C16-1139,Q13-1030,0,0.0114875,"approach in distant supervision is Mintz et al. (2009), which aligns Freebase with Wikipedia articles and extracts relations with logistic regression. Follow-up studies use the feature set developed in this approach, but with deeper understanding on the nature of distant supervision. For example, Riedel et al. (2010) relaxes the assumption used in Mintz et al. (2009) and formulates distant supervision as a multi-instance learning issue; Hoffmann et al. (2011) and Surdeanu et al. (2012) consider overlapping relations between an entity pair. Further effects are also made to model missing data (Ritter et al., 2013), reduce noise (Roth et al., 2013), inject logical background knowledge (Rockt¨aschel et al., 2015), etc. In recent years, deep neural network has proven its ability to learn task-specific representation automatically, so that avoiding error propagation suffered by traditional feature-based models. In particular, many neural network approaches have been proposed and shown better performance in relation classification (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015) and relation extraction (Nguyen and Grishman, 2015). However, these two tasks differ from ours in that relations are extract"
C16-1139,N15-1118,0,0.0204669,"Missing"
C16-1139,D12-1042,0,0.506569,"et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 201"
C16-1139,D15-1062,0,0.130702,"et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that “if two entities participate in a r"
C16-1139,C14-1220,0,0.842528,"2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better fit the RE scenario. Although PCNN achieves substantial improvements in distantly supervised relation extraction, it still has the following deficiencies. First, PCNN uses the expressed-at-least-once assumption (Riedel et al., 2010) for labeled data generation, which states that"
C16-1139,D15-1203,0,0.579398,"s mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These features are extracted from sentences using various NLP algorithms, thus inevitably have errors. The induced errors become more serious for long sentences (McDonald and Nivre, 2007), which is unfortunately very common in real-world relation extraction corpus (Zeng et al., 2015). Building distant supervision methods on faulty features inevitably leads to error propagation, the main culprit responsible for performance degradation. Recent studies have shown promising results on using deep neural networks for automatic feature extraction (Zeng et al., 2014; Liu et al., 2015; Xu et al., 2015). Particularly, Zeng et al. (2015) proposed a piecewise convolutional neural network (PCNN) architecture, which can build an extractor based on distant supervision. PCNN automatically extracts features with convolutional neural networks, and introduces piecewise max-pooling to better"
C16-1139,P05-1053,0,0.0762115,"-once assumption, and employs cross-sentence max-pooling so as to enable information sharing across different sentences. Then it handles overlapping relations by multi-label learning with a neural network classifier. Experimental results show that our approach performs significantly and consistently better than state-of-the-art methods. 1 Introduction Relation extraction (RE), defined as the task of extracting binary relations from plain text, has long been a crucial task in natural language processing. Supervised methods are widely used for this task due to their relatively high performance (Zhou et al., 2005; Surdeanu and Ciaramita, 2007). Such methods, however, usually require intensive human annotation and can be time-consuming. To address this issue, distant supervision is proposed to generate labeled data automatically, by aligning facts in a knowledge base (KB) with sentences mentioning these facts (Mintz et al., 2009; Riedel et al., 2010; Riedel et al., 2013). Traditional (distantly) supervised RE methods use as input numerous lexical and syntactic features, e.g., POS tags, dependency paths, and named entity tags (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et a"
D09-1027,W08-1404,0,0.48597,"he web era with enormous information. The rest of the paper is organized as follows. In Section 2, we introduce and discuss the related work in this area. In Section 3, we give an overview of our method for keyphrase extraction. From Section 4 to Section 7, the algorithm is described in detail. Empirical experiment results are demonstrated in Section 8, followed by our conclusions and plans for future work in Section 9. 2 Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphras"
D09-1027,C08-1122,0,0.631285,"g methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphrase extraction algorithm for single document. Motivated by similar idea, Wan and Xiao (Wan and Xiao, 2008a) propose to adopt clustering methods to find a small number of similar documents to provide more knowledge for building word graphs for keyword extraction. Moreover, after our submission of this paper, we find that a method using community detection on semantic term graphs is proposed for keyphrase extraction from multi-theme documents (Grineva et al., 2009). In addition, some practi"
D09-1027,W04-3252,0,\N,Missing
D09-1027,W03-1028,0,\N,Missing
D11-1105,P06-1039,0,0.145204,"Missing"
D11-1105,W08-1105,0,0.010264,"paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document summarization. And Sauper and Barzilay (2009) use integer linear programming"
D11-1105,W09-1802,0,0.0423992,"o score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between ou"
D11-1105,N09-1041,0,0.57074,"hat summarizes the subject of articles before the table of contents and other elaborate sections. Second, opinionated text often contains multiple viewpoints about an issue generated by different people. Summarizing these multiple opinions can help people easily digest them. Furthermore, combined with search engines and question&answering systems, we can better organize the summary content based on aspects to improve user experience. Despite its usefulness, the problem of modeling domain specific aspects for multi-document summarization has not been well studied. The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. They proposed a HIERSUM model for finding the subtopics or aspects which are combined by using KL-divergence criterion for selecting relevant sentences. They introduced a general content distribution and several specific content distributions to discover the topic and aspects for a single document collection. However, the aspects may be shared not only across documents in a single collection, but also across documents in different topic-related collections. Their model is conceptually inadequate for simultaneously summarizing multi"
D11-1105,P10-1066,1,0.87905,"Natural Language Processing, pages 1137–1146, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics limitations. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each asp"
D11-1105,N03-1020,0,0.141258,"en the main verb of a clause and other sentential elements, such as a sentential parenthetical, colon, or semicolon Compressed When rescue workers arrived, only one of his limbs was visible. l=1 4 Sjl = 1 ∀j ∈ {1 . . . K} http://lpsolve.sourceforge.net/5.5/ Redundancy Constraints 6.2 Quality of summary We also want to prevent redundancy across different aspects. If sentence-similarity sim(sjl , sj ′ l′ ) between sentence sjl and sj ′ l′ is above 0.5, then we drop the pair and choose one sentence ranked higher from the pair otherwise. This constraint is formulated as follows: We use the ROUGE (Lin and Hovy, 2003) metric for measuring the summarization system performance. Ideally, a summarization criterion should be more recall oriented. So the average recall of ROUGE1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and ROUGE-L were computed by running ROUGE1.5.5 with stemming but no removal of stop words. We compare our method with the following four baseline methods. ′ (Sjl + Sj ′ l′ ) · sim(sjl , sj ′ l′ ) ≤ 0.5 ∀j, j ∈ {1 . . . K}∀l ∈ {1 . . . Rj }∀l′ ∈ {1 . . . Rj ′ } Length Constraints Baseline 1 We add this constraint to ensure that the length of the final summary is limited to L words. In this baseline, we tr"
D11-1105,D10-1007,0,0.435048,"ions. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007"
D11-1105,P09-1024,0,0.0568093,"ntences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between our event-aspect model and ent"
D13-1054,P06-1067,0,0.014521,"ns for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language mo"
D13-1054,P12-1050,0,0.12561,"ing’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phra"
D13-1054,N13-1003,0,0.0456632,"ts on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based system"
D13-1054,C10-2033,1,0.891478,"nformation from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translatio"
D13-1054,D08-1089,0,0.423945,"t syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since"
D13-1054,N10-1129,0,0.0324751,"eural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to righ"
D13-1054,P13-1088,0,0.022199,"et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network consists of four autoencoder"
D13-1054,P12-1092,0,0.00995628,"y for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders"
D13-1054,P08-1067,0,0.0105112,"As recursive autoencoders are capable of producing vector space representations for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1.07 BLEU points over the baseline on the NIST 2008 Chinese-English dataset. There are a number of interesting directions we would like to pursue in the near future. First, replacing the MaxEnt classifier with a neural one redefines the conditions for risk-free hypothesis recombination. We find that the number of hypotheses that can be recombined reduces in our system. Therefore, we plan to use forest reranking (Huang, 2008) to alleviate this problem. Second, it is interesting to follow Socher et al. (2013) to combine linguistically-motivated labels with recursive neural networks. Another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by RAEs. It is necessary to investigate more efficient decoding algorithms. Finally, it is possible to apply our method to other phrase-based and even syntax-based systems. Acknowledgments This research is supported by the 863 Program under the grant No. 2012AA011102, by the Boeing Tsinghua"
D13-1054,J99-4005,0,0.0200293,"e recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memo"
D13-1054,N03-1017,0,0.0747853,"significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, esp"
D13-1054,P07-2045,0,0.0116822,"ing orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into p"
D13-1054,J04-4002,0,0.104451,"es over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reorder"
D13-1054,N04-1021,0,0.0119397,"are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translation"
D13-1054,W05-0908,0,0.0293904,"aseline by replacing the MaxEnt reordering model with a neural model. Both the systems have the same pruning settings: the threshold pruning parameter is set to 0.5 and the histogram pruning parameter to 40. For minimum-error-rate training, both systems generate 200-best lists. 4.2 MT Evaluation Table 1 shows the case-insensitive BLEU-4 scores of the baseline system and our system on the development and test sets. Our system outperforms the baseline system by 1.21 BLEU points on the development set and 1.07 on the test set. Both the differences are statistically significant at p = 0.01 level (Riezler and Maxwell, 2005). Table 2 shows the number of sentences that our system has a higher (>), equal (=) or lower (&lt;) BLEU score on the NIST 2008 dataset. We find that our system is superior to the baseline system for long 3 The choice of α is very important for achieving high BLEU scores. We tried a number of intervals and found that the classification accuracy is most stable in the interval [0.100,0.125]. 574 neural maxent 5 10 15 20 Length 25 30 35 Figure 4: Comparison of reordering classification accuracies between the MaxEnt and neural classifiers over varying phrase lengths. “Length” denotes the sum of the l"
D13-1054,D11-1014,0,0.255232,"Missing"
D13-1054,D12-1110,0,0.0166609,"ither manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering c"
D13-1054,P13-1045,0,0.0698655,". As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network"
D13-1054,N04-4026,0,0.093585,"erating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word ins"
D13-1054,J97-3002,0,0.42934,"T) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. They use the CKY algorithm to recursively merge two blocks (i.e., a pair of source and target strings"
D13-1054,P06-1066,0,0.651966,"which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamless"
D13-1054,I08-1066,0,0.274945,"lingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it po"
D13-1054,J10-3009,0,0.0150292,"MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manua"
D13-1054,C04-1030,0,0.0511187,"space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omission"
D13-1054,J09-4009,0,\N,Missing
D16-1100,P15-1162,0,0.00806446,"Missing"
D16-1100,P16-1020,0,0.0228684,"mbedding (MGE) which learns embeddings jointly for words, characters, and radicals. The framework of MGE is sketched in Figure 1. Given a word, we learn its embedding on the basis of 1) the context words (blue bars in the figure), 2) their constituent characters (green bars), and 3) the radicals found in the target word (orange bars). Compared to utilizing context words alone, MGE enriches the embeddings by further incorporating finer-grained semantics from characters and radicals. Similar ideas of adaptively using multiple levels of embeddings have also been investigated in English recently (Kazuma and Yoshimasa, 2016; Miyamoto and Cho, 2016). We evaluate MGE with the benchmark tasks of word similarity computation and analogical reasoning, and demonstrate its superiority over state-ofthe-art metods. A qualitative analysis further shows the capability of MGE to identify finer-grained semantic meanings of words. 2 Multi-Granularity Word Embedding This section introduces MGE based on the continuous bag-of-words model (CBOW) (Mikolov et al., 2013b) and the character-enhanced word embedding 982 model (CWE) (Chen et al., 2015). MGE aims at improving word embedding by leveraging both characters and radicals. We d"
D16-1100,D14-1181,0,0.015234,"uantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese g"
D16-1100,N15-1092,0,0.0122923,"evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows t"
D16-1100,N13-1090,0,0.629158,"ed low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows the same idea as on English, i.e., to learn the embedding of a word on the basis of its context. However, in contrast to English where words are usually taken as basic semantic units, Chinese words may have a complicated composition structure of their semantic meanings. More specifically, a Chinese word is often composed of sever"
D16-1100,D16-1209,0,0.0157198,"embeddings jointly for words, characters, and radicals. The framework of MGE is sketched in Figure 1. Given a word, we learn its embedding on the basis of 1) the context words (blue bars in the figure), 2) their constituent characters (green bars), and 3) the radicals found in the target word (orange bars). Compared to utilizing context words alone, MGE enriches the embeddings by further incorporating finer-grained semantics from characters and radicals. Similar ideas of adaptively using multiple levels of embeddings have also been investigated in English recently (Kazuma and Yoshimasa, 2016; Miyamoto and Cho, 2016). We evaluate MGE with the benchmark tasks of word similarity computation and analogical reasoning, and demonstrate its superiority over state-ofthe-art metods. A qualitative analysis further shows the capability of MGE to identify finer-grained semantic meanings of words. 2 Multi-Granularity Word Embedding This section introduces MGE based on the continuous bag-of-words model (CBOW) (Mikolov et al., 2013b) and the character-enhanced word embedding 982 model (CWE) (Chen et al., 2015). MGE aims at improving word embedding by leveraging both characters and radicals. We denote the Chinese word vo"
D16-1100,P15-2098,0,0.432,"nd “饭 (meal)” the radical of “ 饣 (food)”. The semantic meaning of “吃饭” can be revealed by the constituent characters as well as their radicals. Despite being the linguistic nature of Chinese and containing rich semantic information, such wordcharacter-radical composition has not been fully exploited by existing approaches. Chen et al. (2015) introduced a character-enhanced word embedding model (CWE), which learns embeddings jointly for words and characters but ignores radicals. Sun et al. (2014) and Li et al. (2015) utilized radical information to learn better character embeddings. Similarly, Shi et al. (2015) split characters into small components based on the Wubi method,2 and took into account those components during the learning process. In their work, however, embeddings are learned only for characters. For a word, the embedding is generated by simply combining the embeddings of the constituent characters. Since not all Chinese word1 2 Corresponding author: Peng Li. https://en.wikipedia.org/wiki/Radical (Chinese characters) https://en.wikipedia.org/wiki/Wubi method 981 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 981–986, c Austin, Texas, Novemb"
D16-1100,N15-1155,0,0.0310919,"Missing"
D16-1100,P15-1025,0,0.0225002,", 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work on Chinese generally follows the same idea as on English, i.e., to learn the embedding of a word on the basis of its context. However, in contrast to English where words are usually taken as basic semantic units, Chinese words may have a complicated composition structure of their semantic meanings. More specifically, a Chinese word is often composed of several characters, and most of the characters themselves can be further divided into components such as radicals (部首).1 Both characters and radicals may suggest the semantic meaning of a word, regardless of its con"
D16-1100,D13-1141,0,0.0118533,"rs and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words. 1 Introduction Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded. Recent years have witnessed tremendous success of word embedding in various NLP tasks (Bengio et al., 2006; Mnih and Hinton, 2009; Collobert et al., 2011; Zou et al., 2013; Kim, 2014; Liu et al., 2015; Iyyer et al., 2015). The basic idea behind is to learn the distributed representation of a word using its context. Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora (Mikolov et al., 2013a; Mikolov et al., 2013b). ∗ Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing (Xu et al., 2015; Yu et al., 2015; Zhou et al., 2015; Zou et al., 2013). The work o"
D18-1247,P11-1055,0,0.879452,"Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG F"
D18-1247,D17-1191,0,0.558533,"; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification models (McCallum et al., 1998; Rousu et al., 2005; Weinberger and Chapelle, 2009; Zhao et al., 2011; Bi and Kwok,"
D18-1247,P16-1200,1,0.84698,"to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Na"
D18-1247,D17-1189,0,0.645343,"wever, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22"
D18-1247,P09-1113,0,0.988836,"Parent Relation Parent Relation Tail Entity: Davos Ernst Haefliger died on Saturday in Davos. 0.8 0.6 0.4 Ernst Haefliger was born in Davos on July 6, 1919 0.1 0.2 0.3 Ernst Haefliger was born in Davos, Switzerland. 0.1 0.2 0.3 Attention Scores Figure 1: An example of hierarchical relation extraction. Relation extraction (RE) aims to predict relational facts from plain text. Conventional supervised RE models (Zelenko et al., 2003; Mooney and Bunescu, 2006) usually suffer from the lack of high-quality training data, because manual labeling of training data is time-consuming and humanintensive. Mintz et al. (2009) propose distant supervision to automatically label training instances by aligning existing knowledge graphs (KGs) and text: For an entity pair in KGs, those sentences containing both the entities will be labeled with † /people/deceased_person Parent Relation /people/deceased_person/place_of_death Introduction ∗ Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanie"
D18-1247,N13-1008,0,0.0863505,", especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for dista"
D18-1247,D15-1206,0,0.0488813,"beling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advan"
D18-1247,D15-1203,0,0.837594,"orresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an exam"
D18-1247,C14-1220,0,0.921179,"s with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al."
D18-1247,D17-1186,1,0.936619,"xplicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification mode"
D18-1247,P15-1061,0,0.0757283,"o alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategi"
D18-1247,D12-1042,0,0.778504,"es equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et a"
D18-1247,D17-1004,0,0.0499094,"tic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlat"
D18-1247,N16-1103,0,0.0621293,"nt supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017"
D18-1247,W16-1312,0,0.0321395,"el et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mecha"
D18-1247,P05-1053,0,0.487507,"ction. Since there are more sufficient data for training the top-layer attention, the whole hierarchical attention scheme can enhance RE models for solving those long-tail relations. We conduct experiments on a large-scale benchmark dataset for RE in this paper. The experimental results show that the proposed coarse-tofine grained attention scheme based on relation hierarchies significantly outperforms other baseline methods, even as compared to the recent stateof-the-art attention-based models, especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to"
D18-1247,D17-1187,0,0.700834,"tic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2236–2245 c Brussels"
D19-1251,P16-1223,0,0.0307514,"atasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC sys"
D19-1251,P17-1055,0,0.0155433,"eading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizin"
D19-1251,N19-1423,0,0.0237718,"a single function v 0 = Reasoning-Step(G, v). (13) As the graph G constructed in Sec. 3.2 has encoded the numerical relations via its topology, the reasoning process is numerically-aware. 2478 Multi-step Reasoning By single-step reasoning, we can only infer relations between adjacent nodes. However, relations between multiple nodes may be required for certain tasks, e.g., sorting. Therefore, it is essential to perform multi-step reasoning, which can be done as follows v t = Reasoning-Step(v t−1 ), (14) where t ≥ 1. Suppose we perform K steps of reasoning, v K is used as U in Eq. 7. 4 • BERT (Devlin et al., 2019), a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently; and numerical MRC models: • NAQANet (Dua et al., 2019), a numerical version of QANet model. • NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the supplemental material. Experiments 4.1 Dataset and Evaluation Metrics We evaluate our proposed model on"
D19-1251,P17-1168,0,0.0238174,"hension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning."
D19-1251,N19-1246,0,0.113245,"Missing"
D19-1251,D14-1058,0,0.0369168,"merical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical"
D19-1251,P16-1084,0,0.0149131,"ng such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work. 3 Encoding Module Without loss of generality, we use the encoding components of QANet and NAQANet to encode the question and passage into vector-space"
D19-1251,P17-1147,0,0.0312321,"orming numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to"
D19-1251,P16-1086,0,0.0241577,"ble at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact"
D19-1251,D18-1546,0,0.019043,"based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Du"
D19-1251,Q15-1042,0,0.0228607,"dition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our mo"
D19-1251,D17-1160,0,0.0317798,"over numbers in the passages. There are 77, 409 training samples, 9, 536 development samples and 9, 622 testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following Dua et al. (2019). The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric golden answer. 4.2 Baselines For comparison, we select several public models as baselines including semantic parsing models: • Syn Dep (Dua et al., 2019), the neural semantic parsing model (KDG) (Krishnamurthy et al., 2017) with Stanford dependencies based sentence representations; • OpenIE (Dua et al., 2019), KDG with open information extraction based sentence representations; • SRL (Dua et al., 2019), KDG with semantic role labeling based sentence representations; and traditional MRC models: • BiDAF (Seo et al., 2017), an MRC model which utilizes a bi-directional attention flow network to encode the question and passage; • QANet (Yu et al., 2018), which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage; 4.3 Experimental Settings In this paper, we"
D19-1251,D17-1082,0,0.0239897,"ne methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to"
D19-1251,P17-1015,0,0.0224498,"umerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our model, it is more effective for answering questions requiring numerical reasoning such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages"
D19-1251,D16-1264,0,0.0977847,"ement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC mode"
D19-1251,D15-1202,0,0.0530747,"t (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module levera"
D19-1251,P17-1018,0,0.171003,"portant research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most exist"
D19-1251,D17-1088,0,0.0586902,"Missing"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
D19-1649,W06-1615,0,0.189413,"can sample instances from the training set as supporting instances for NOTA relation (this method is described explicitly in Section 4). Also note that to better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, w"
D19-1649,P07-1073,0,0.0597172,"mpled from the test set, each of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there"
D19-1649,N19-1423,0,0.0360439,"le NOTA is to regard it as an extra class in the N -way K-shot setting. To be more specific, we can sample instances outside the N relations as the supporting data of NOTA, and perform the (N + 1)-way K-shot learning. As compared to the current methods ignoring NOTA, this approach does not bring much improvements, since the supporting data for NOTA actually belong to several different relations and are scattered in the feature space, making it hard to perform classification. To better address few-shot NOTA, we propose a model named BERT-PAIR based on the sequence classification model in BERT (Devlin et al., 2019). We pair each query instance with all the supporting instances, concatenate each pair as one sequence, and send the concatenated sequence to the BERT sequence classification model to get the score of the two instances expressing the same relation. Denote the BERT model as B, the query instance as x and the paired supporting instance as xjr (the j-th supporting instance for the relation r), B(x, xjr ) outputs a two-element vector corresponding to scores of the pair sharing the same relation and not sharing the same relation. The probability over each relation in the few-shot scenario, includin"
D19-1649,P19-1279,0,0.106636,"Missing"
D19-1649,C18-1099,1,0.86151,"o better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, we feed the sentence encoder E and the discriminator D with the corpora from the training domain and the test domain, and optimize the min-max game, X"
D19-1649,D18-1514,1,0.42617,"Missing"
D19-1649,W09-2415,0,0.332326,"Missing"
D19-1649,P09-1113,0,0.589697,"ach of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there are wide disparities"
N19-1105,D15-1247,0,0.110478,"Missing"
N19-1105,C18-1075,0,0.479342,"Missing"
N19-1105,P98-1013,0,0.408429,"Missing"
N19-1105,D18-1021,1,0.869581,"Missing"
N19-1105,P18-1241,0,0.0663714,"Missing"
N19-1105,P17-1038,0,0.449592,"Missing"
N19-1105,P15-1017,0,0.704641,"Missing"
N19-1105,N18-1076,0,0.147721,"Missing"
N19-1105,I17-1036,0,0.128152,"Missing"
N19-1105,P16-2011,0,0.135941,"Missing"
N19-1105,N18-2058,0,0.476683,"Missing"
N19-1105,P16-2060,0,0.130043,"Missing"
N19-1105,P09-2093,0,0.207456,"Missing"
N19-1105,P11-1113,0,0.73024,"Missing"
N19-1105,P18-1048,0,0.155771,"Missing"
N19-1105,P16-1025,0,0.240411,"Missing"
N19-1105,C10-1077,0,0.711492,"Missing"
N19-1105,P10-1081,0,0.823295,"Missing"
N19-1105,P18-1145,0,0.0214274,"Missing"
N19-1105,D18-1127,0,0.406701,"Missing"
N19-1105,P16-1201,0,0.0323032,"Missing"
N19-1105,P17-1164,0,0.540835,"Missing"
N19-1105,P11-1163,0,0.257108,"Missing"
N19-1105,E12-1029,0,0.137736,"Missing"
N19-1105,P09-1113,0,0.272376,"Missing"
N19-1105,P08-1030,0,0.791729,"Missing"
N19-1105,P13-1008,0,0.756973,"Missing"
N19-1105,C18-1007,0,0.0627013,"Missing"
N19-1105,N16-1034,0,0.425474,"Missing"
N19-1105,N16-1033,0,0.379533,"Missing"
N19-1105,P15-2060,0,0.493355,"Missing"
N19-1105,P18-4009,0,0.10604,"Missing"
N19-1105,P18-1046,0,0.0622712,"Missing"
N19-1105,D15-1203,0,0.0776927,"Missing"
N19-1105,P18-2066,0,0.731835,"Missing"
N19-1105,C18-1099,1,0.90081,"Missing"
N19-1105,D17-1187,0,0.0611022,"Missing"
N19-1105,1983.tc-1.13,0,0.586136,"Missing"
P10-1066,P06-1039,0,0.135887,"Missing"
P10-1066,P06-2027,0,0.150849,", automatic template generation provides a solution to induction of infobox structures, which are still highly incomplete in Wikipedia (Wu and Weld, 2007). A template can also serve as a starting point for human editors to create new summary articles. Furthermore, with summary templates, we can potentially apply information retrieval and extraction techniques to construct summaries for new entities automatically on the fly, improving the user experience for search engine and question answering systems. Despite its usefulness, the problem has not been well studied. The most relevant work is by Filatova et al. (2006) on automatic creation of domain templates, where the defintion of a domain is similar to our notion of an entity category. Filatova et al. (2006) first identify the important verbs for a domain using corpus statistics, and then find frequent parse tree patterns from sentences containing these verbs to construct a domain template. There are two major limitations of their approach. First, the focus on verbs restricts the template patterns that can be found. Second, redundant or related patterns using different verbs to express the same or similar facts cannot be grouped together. For example, “"
P10-1066,N09-1041,0,0.0174802,"c topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather than sets of sentences as in our model. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. Our one-aspect-per-sentence assumption is a stricter than theirs, but it is required in our model for the purpose of mining sentence patterns. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009), but their task is multi-document summarization while ours is to induce summary templates. Related Work The most related existing work is on domain template generation by Filatova et al. (2006). There are several differences between our work and theirs. First, their template patterns must contain a non-auxiliary verb whereas ours do not have this restriction. Second, their verb-centered patterns are independent of each other, whereas we group semantically related patterns into aspects, giving more meaningful templates. Third, in their work, named entities, numbers and general nouns are treate"
P10-1066,P09-1024,0,0.2856,"ture. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biographies of physicists to represent one aspect, even though they may appear in different sentences from differen"
P10-1066,P06-2094,0,0.0228437,"ries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biograp"
P10-1066,N06-1039,0,0.00654777,"entity summaries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biograp"
P10-1066,P03-1029,0,0.035354,"ons are examples of entity summaries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clus"
P10-1066,P09-1115,0,0.0246524,"of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biographies of physicists to represent o"
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1194,P18-1139,0,0.0593011,"ed on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12). There are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are: (1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) st into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3). 6 Conclusion In"
P19-1194,D14-1181,0,0.00290278,"ut is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking MRRR = N 1 X 1 N i=1 |rank(vi ) − rank(ˆ vi ) |+ 1 (13) In addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy. 3.4.2 Human Evaluation We also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches"
P19-1194,D18-1549,0,0.0242572,"Missing"
P19-1194,D17-1230,0,0.0454095,"in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as: K   1 X (k) ∇θ L(θ) &apos; − r − b ∇θ log pθ (yˆ(k) ) K k=1 (11) where K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017). Nevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and 2016 usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence yˆ based on the input text x and the target sentiment intensity value vy , and then reconstructs the source input x based on yˆ and the source sentiment intensity value vx . Therefore, the gradient of the cycle reconstruction loss is defined as:   ∇θ"
P19-1194,N18-1169,0,0.0483598,"Missing"
P19-1194,D18-1420,0,0.19687,"ntences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,D15-1166,0,0.0327,"0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value vy should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word “good” should be assigned a higher probability than word “bad”, thus the predicted intensity value g(“good”, s4 ) is closer to the target sentiment intensity than g(“bad”, s4 ). To favor words whose sentiment intensity is near vy , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around vy , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: 2 ! g(Es , st ) − vy 1 s ot = √ exp − (4) 2σ 2 2πσ pst = softmax(ost ) (5) where σ is the standard deviation. To balance both sentiment transformation and content preservation, the final probability distribution pt over the entire vocabulary is defined as a mixture of two probability distributions: pt = γpst + (1 − γ)pct (6) where γ is the hyper-parameter that controls the trade-off between two generation probabilities. 2015 &&apos; Encoder Algorithm 1 The cycle reinforcement learning algorithm for training Seq2Se"
P19-1194,S18-1001,0,0.0359134,"Missing"
P19-1194,E17-1096,0,0.0651517,"Missing"
P19-1194,P02-1040,0,0.107261,"2 2.64 2.54 2.37 2.52 3.84 3.85 2.13 2.14 3.41 2.43 2.84 3.21 Seq2SentiSeq 32.5 10.3 0.13 0.78 35.1 3.62 4.09 4.17 3.96 Human Reference 100.0 100.0 0.07 0.83 31.2 4.51 4.36 4.75 4.54 Table 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment (MAE, MRRR) and Fluency (PPL). Avg shows the average human scores. ↑ denotes larger is better, and vice versa. Bold denotes the best results. review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V 0 = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017). Sentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v1 , v2 , ..., vN ], the corresponding outputs of the model are [yˆ1"
P19-1194,P18-2031,0,0.0321807,"results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1 1 Target Sentiment Text sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018). Previous work (Shen et al., 2017; Li et al., 2018; Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of Joint work between WeChat AI and Peking University. 0.1 Horrible food and terrible service! 0.3 Plain food, slow service. 0.5 Food and service need improvement. 0.7 Good food and service. 0.9 Amazing food and perfect service!! Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment inten"
P19-1194,P18-1080,0,0.0258986,"tly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity. Related Work Recently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018; Tsvetkov et al., 2018; Li et al., 2018; Xu et al., 2018; Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from th"
P19-1194,P18-1090,1,0.78267,"e intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,P18-1102,0,0.162234,"t intensity2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order. There are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. t"
P19-1194,W17-5227,0,0.0201437,"challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. target sentiment intensity value is a real value, other than discrete labels. Second, parallel data3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outpu"
P19-1197,H05-1042,0,0.348902,"ain the performance of 9.71 BLEU score.1 1 Introduction Table-to-text generation is to generate a description from the structured table. It helps readers to summarize the key points in the table, and tell in the natural language. Figure 1 shows an example of table-to-text generation. The table provides some structured information about a person named “Denise Margaret Scott”, and the corresponding text describes the person with the key information in the table. Table-to-text generation can be applied in many scenarios, including weather report generation (Liang et al., 2009), NBA news writing (Barzilay and Lapata, 2005), biography generation (Dubou´e and McKeown, 2002; Lebret et al., 2016), and so on. Moreover, table-to-text genera1 The codes are available at https://github.com/ lancopku/Pivot. Denise Margaret Scott Born 24 April 1955 Melbourne, Victoria Nationality Australian Denise Margaret Scott 24 April 1955 Other names Scotty Occupation Comedian, actor, television and radio presenter Australian Known for Studio 10 Partner(s) John Lane Comedian, actor, television and radio presenter Children 2 Denise Margaret Scott (born 24 April 1955) is an Australian comedian, actor and television presenter. Surface Re"
P19-1197,E06-1040,0,0.0615103,"Missing"
P19-1197,P16-1185,0,0.0252088,"ates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a me"
P19-1197,N16-1012,0,0.032569,"1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25"
P19-1197,W02-2112,0,0.029488,"n brings a significant improvement to the pivot models under both vanilla Seq2Seq and Transformer frameworks, which demonstrates the efficiency of the denoising data augmentation. 3.9 the PIVOT model has some problem in generating repeating words (such as “senator” in the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao e"
P19-1197,W18-6505,0,0.222032,"ance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000 50000 60000 Parallel Data Size Figure 2: The BLEU scores of the a table-to-text model trained with different number of parallel data under the encoder-decoder framework on the WIKIBIO dataset. lel data is available. Some previous work (Puduppully et al., 2018; Gehrmann et al., 2018) formulates the task as the combination of content selection and surface realization, and models them with an end-to-end model. Inspired by these work, we break up the table-to-text generation into two stages, each of which is performed by a model trainable with only a few annotated data. Specifically, it first predicts the key facts from the tables, and then generates the text with the key facts, as shown in Figure 1. The two-stage method consists of two separate models: a key fact prediction model and a surface realization model. The key fact prediction model is formulated as a sequence labe"
P19-1197,N18-1032,0,0.0158927,"r fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where on"
P19-1197,J82-2005,0,0.518536,"Missing"
P19-1197,D16-1128,0,0.566599,"g the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed s"
P19-1197,P09-1011,0,0.134087,"the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model"
P19-1197,N03-1020,0,0.0677063,"Missing"
P19-1197,D15-1166,0,0.122996,"Realization Figure 1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext sam"
P19-1197,D18-1422,0,0.0332302,"y et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed symbolic operations in a sequence-to-sequence model. Qualitative Analysis We provide an example to illustrate the improvement of our model more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, w"
P19-1197,P02-1040,0,0.103597,"Missing"
P19-1197,D18-1411,0,0.0221501,"generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by ut"
P19-1197,W18-2205,0,0.0662726,"Missing"
P19-1197,W17-4503,0,0.0563362,"Missing"
P19-1197,K18-1003,0,0.0241831,"del more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lam"
P19-1197,D17-1239,0,0.0786093,"Missing"
P19-1197,D18-1356,0,0.0931456,"Missing"
P19-1197,C18-1330,1,0.82569,"ion, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000"
P19-1197,D18-1138,1,0.847758,"achine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a method to construct a pseudo parallel dataset for the surface realization model, without the need of any structured table. Experiments show that our proposed model can achieve 27.34 BLEU score on a biography generation dataset with only 1, 000 parallel data. Acknowledgement We thank the anonymous reviewers for their thoug"
Q16-1027,buck-etal-2014-n,0,0.112276,"Missing"
Q16-1027,D14-1179,0,0.13632,"Missing"
Q16-1027,W14-3309,0,0.0984371,"can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source se"
Q16-1027,P15-1001,0,0.423363,"two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Published 7/2016. c 20"
Q16-1027,D13-1176,0,0.134234,".2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies"
Q16-1027,N03-1017,0,0.0228491,"0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network r"
Q16-1027,N06-1014,0,0.0113091,"e+PosUnk Ensemble Ensemble+PosUnk Durrani, 2014 Ensemble+PosUnk Data 36M 36M 36M 36M 36M 36M 36M Voc 80K 80K 80K 80K 80K Full 80K BLEU 36.3 37.7 39.2 38.9 40.4 37.0 37.5 Table 8: BLEU scores of different models. The first two blocks are our results of two single models and models with post processing. In the last block we list two baselines of the best conventional SMT system and NMT system. Second, we recover the unknown words in the generated sequences with the Positional Unknown (PosUnk) model introduced in (Luong et al., 2015). The full parallel corpus is used to obtain the word mappings (Liang et al., 2006). We find this method provides an additional 1.5 BLEU points, which is consistent with the conclusion in Luong et al. (2015). We obtain the new BLEU score of 39.2 with a single Deep-Att model. For the ensemble models of Deep-Att, the BLEU score rises to 40.4. In the last two lines, we list the conventional SMT model (Durrani et al., 2014) and the previous best neural models based system Enc-Dec (Luong et al., 2015) for comparison. We find our best score outperforms the previous best score by nearly 3 points. 380 5 4 7 8 12 17 22 Sentences by Length 28 35 79 Figure 3: BLEU scores vs. source seq"
Q16-1027,P15-1002,0,0.151765,"general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Pu"
Q16-1027,P15-1109,1,0.824274,"is the dropout operation (Hinton et al., 2012) which randomly sets an element of h to zero with a certain probability. The use of Half(·) is to reduce the parameter size and does not affect the performance. We observed noticeable performance degradation when using only the first third of the elements of “f”. ftk = Wfk · [Half(ftk−1 ), Dr(hk−1 )], k &gt; 1 (6) t With the F-F connections, we build a fast channel to propagate the gradients in the deep topology. F-F connections can accelerate the model convergence and while improving the performance. A similar idea was also used in (He et al., 2016; Zhou and Xu, 2015). Encoder: The LSTM layers are stacked following Eq. 5. We call this type of encoder interleaved bidirectional encoder. In addition, there are two similar columns (a1 and a2 ) in the encoder part. Each column consists of ne stacked LSTM layers. There is no connection between the two columns. The first layers of the two columns process the word representations of the source sequence in different directions. At the last LSTM layers, there are two groups of vectors representing the source sequence. The group size is the same as the length of the input sequence. Interface: Prior encoder-decoder mo"
S16-1088,D14-1070,0,0.0270017,"Missing"
S16-1088,S14-2131,0,0.0226441,"Science and Engineering University of Texas at Arlington jerryli1981@gmail.com Abstract In this paper, we propose a deep neural network based natural language processing system for semantic textual similarity prediction. We leverage multi-layer bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by Highway Multilayer Perceptron to make predictions. Experimental results demonstrate that this approach can’t get better results on standard evaluation datasets. 1 Introduction Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning in many machine learning related applications, there has been much interest in applying deep neural network based techniques to further improve the prediction tasks in natural language processing (NLP) (Socher et al., 2011b; Iyyer et al., 2014; Tai et al., 2015). A key component of deep neural network is word embeddings which serves as a look"
S16-1088,D13-1176,0,0.0481026,"2015). A key component of deep neural network is word embeddings which serves as a lookup table to get word representations. From low-level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling, to high∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. Heng Huang∗ Computer Science and Engineering University of Texas at Arlington heng@uta.edu level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015). Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. In this work, we focus on deep neural network based semantic textual similarity prediction. We use multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al., 2013) to learn sentence representations. After that, we construct matching features followed by Highway Multilayer Perceptron to learn high-level hidden matching feature r"
S16-1088,S14-2055,0,0.0331634,"tic Processing and Evaluation Peng Li Computer Science and Engineering University of Texas at Arlington jerryli1981@gmail.com Abstract In this paper, we propose a deep neural network based natural language processing system for semantic textual similarity prediction. We leverage multi-layer bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by Highway Multilayer Perceptron to make predictions. Experimental results demonstrate that this approach can’t get better results on standard evaluation datasets. 1 Introduction Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning in many machine learning related applications, there has been much interest in applying deep neural network based techniques to further improve the prediction tasks in natural language processing (NLP) (Socher et al., 2011b; Iyyer et al., 2014; Tai et al., 2015). A key component of deep neural network"
S16-1088,D11-1014,0,0.00925678,"s on standard evaluation datasets. 1 Introduction Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning in many machine learning related applications, there has been much interest in applying deep neural network based techniques to further improve the prediction tasks in natural language processing (NLP) (Socher et al., 2011b; Iyyer et al., 2014; Tai et al., 2015). A key component of deep neural network is word embeddings which serves as a lookup table to get word representations. From low-level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling, to high∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. Heng Huang∗ Computer Science and Engineering University of Texas at Arlington heng@uta.edu level tasks such as machine translation, informatio"
S16-1088,P15-1150,0,0.455897,"duction Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning in many machine learning related applications, there has been much interest in applying deep neural network based techniques to further improve the prediction tasks in natural language processing (NLP) (Socher et al., 2011b; Iyyer et al., 2014; Tai et al., 2015). A key component of deep neural network is word embeddings which serves as a lookup table to get word representations. From low-level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling, to high∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. Heng Huang∗ Computer Science and Engineering University of Texas at Arlington heng@uta.edu level tasks such as machine translation, information retrieval and semantic analysis (Kalch"
S16-1088,S14-2044,0,0.0258592,"on Peng Li Computer Science and Engineering University of Texas at Arlington jerryli1981@gmail.com Abstract In this paper, we propose a deep neural network based natural language processing system for semantic textual similarity prediction. We leverage multi-layer bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by Highway Multilayer Perceptron to make predictions. Experimental results demonstrate that this approach can’t get better results on standard evaluation datasets. 1 Introduction Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning in many machine learning related applications, there has been much interest in applying deep neural network based techniques to further improve the prediction tasks in natural language processing (NLP) (Socher et al., 2011b; Iyyer et al., 2014; Tai et al., 2015). A key component of deep neural network is word embeddings"
S16-1197,D14-1070,0,0.052714,"Missing"
S16-1197,D13-1176,0,0.037995,"sity of Texas at Arlington heng@uta.edu • The spans (character offsets) of the expression in the raw text • Contextual Modality: ACTUAL, HYPOTHETICAL, HEDGED or GENERIC Introduction In past several years, there has been much interest in applying neural network based deep learning techniques to solve many natural language processing (NLP) tasks. From low-level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high-level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015). Deep representation learning has demonstrated its importance for ∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. • Degree: MOST, LITTLE or N/A • Polarity: POS or NEG • Type: ASPECTUAL, EVIDENTIAL or N/A The input of our system consists of raw clinical notes or pathology r"
S16-1197,D11-1014,0,0.0496535,"uta.edu • The spans (character offsets) of the expression in the raw text • Contextual Modality: ACTUAL, HYPOTHETICAL, HEDGED or GENERIC Introduction In past several years, there has been much interest in applying neural network based deep learning techniques to solve many natural language processing (NLP) tasks. From low-level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high-level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015). Deep representation learning has demonstrated its importance for ∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. • Degree: MOST, LITTLE or N/A • Polarity: POS or NEG • Type: ASPECTUAL, EVIDENTIAL or N/A The input of our system consists of raw clinical notes or pathology reports. The following"
S16-1197,P15-1150,0,0.0399111,"haracter offsets) of the expression in the raw text • Contextual Modality: ACTUAL, HYPOTHETICAL, HEDGED or GENERIC Introduction In past several years, there has been much interest in applying neural network based deep learning techniques to solve many natural language processing (NLP) tasks. From low-level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high-level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015). Deep representation learning has demonstrated its importance for ∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. • Degree: MOST, LITTLE or N/A • Polarity: POS or NEG • Type: ASPECTUAL, EVIDENTIAL or N/A The input of our system consists of raw clinical notes or pathology reports. The following is an example: Apri"
S16-1197,S15-2137,0,0.0415892,"ype=N/A polarity=NEG degree=N/A modality=ACTUAL type=ASPECTUAL polarity=POS degree: N/A modality=ACTUAL type=ASPECTUAL polarity=POS degree=N/A modality=ACTUAL type=ASPECTUAL polarity=POS degree=N/A modality=ACTUAL type=ASPECTUAL polarity=POS degree=N/A modality=HYPOTHETICAL Table 1: An example of information extraction from clinical note. the event expressions from raw clinical notes. Traditional machine learning approaches usually build a supervised classifier with features generated by the Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) 1 . For example, BluLab system (Velupillai et al., 2015) extracted morphological (lemma), lexical (token), and syntactic (part-ofspeech) features encoded from cTAKES. Although using the domain specific information extraction tools can improve the performance, learning how to use this software well for clinical domain feature engineering is still very time-consuming. In short, a simple and effective method that only leverages basic NLP modules and achieves high extraction performance is desired for regular users. To address this challenge, we propose a deep neural networks based method, especially convolution neural network (Collobert et al., 2011),"
S16-1197,P15-1007,0,0.0175663,"e has been much interest in applying neural network based deep learning techniques to solve many natural language processing (NLP) tasks. From low-level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high-level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015). Deep representation learning has demonstrated its importance for ∗ To whom all correspondence should be addressed. This work was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371. • Degree: MOST, LITTLE or N/A • Polarity: POS or NEG • Type: ASPECTUAL, EVIDENTIAL or N/A The input of our system consists of raw clinical notes or pathology reports. The following is an example: April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea. The"
S16-1197,S15-2136,0,\N,Missing
