2005.sigdial-1.25,A00-2001,1,0.787421,"evan e of events to an agent's goals and for assessing ausal attributions. Plan representations also lie at the heart of many reasoning te hniques (e.g., planning, explanation, natural language pro essing) and fa ilitate their integration. The de ision-theoreti on epts of utility and probability are key for modeling nondeterminism and for assessing the value of alternative negotiation hoi es. Expli it representations of intentions and beliefs are riti al for negotiation and for assessing blame when negotiations fail (Mao and Grat h, 2004). 3.1 Modeling Trust A ording to the dialogue model in (Matheson et al., 2000), the dire t e e t of an assertion is the introdu tion of a ommitment, whether or not either party believes in the assertion. While this is suÆ ient for reasoning about the laims and responsibility for information, we need to go further and potentially hange beliefs and intentions based on ommuni ated information. Trust is used to de ide whether to adopt a new belief based on the ommitments of another. Similar to (Marsella et al., 2004) and (Cassell and Bi kmore, 2001) , trust is modeled as fun tion of underlying variables that are easily derived from our task and dialogue representations. Sol"
2005.sigdial-1.25,traum-etal-2004-evaluation,1,0.813564,"whole body posture of the do tor and use of gestures and expressions as well. For example, when the do tor is feeling more distant and less trusting, he adopts a losed posture (Figure 1). When he is more trusting and open to negotiation, the posture be omes more relaxed (Figure 4). Figure 4: More relaxed and open do tor 5 Current and Future Work The virtual do tor is able to engage in a range of dialogue in this domain similar to those in Figures 2 and 3. Current work involves extensions and evaluation of the ability to robustly engage in this sort of dialogue, following the methodologies in (Traum et al., 2004). Wizard of OZ tests show good results in terms of the ability to have produ tive onversations given the do tor's task model, vo abulary and generation apa ity, but we are still evaluating performan e of the automated system. Future work involves extension of the models to in lude additional negotiation strategies, emotionbased styles of intera tion within the strategies, and appli ation to other s enarios, some involving ultural di eren es in behavior and interpretation, as well as translated and multi-lateral dialogue. A knowledgments The work des ribed in this paper was supported by the Dep"
2007.sigdial-1.15,W06-1303,1,0.866609,"eusability, revisability, and short development time. Given the different relative importance of these goals and the specific features of the domain can lead to different choices for the spoken language technology components. For example, the virtual humans in (Rickel et al., 2002; Traum et al., 2005b) put a premium on depth of understanding within complex domains (teamwork, negotiation), but were somewhat narrow in the scope of what the virtual humans could talk about, and had a heavy authoring burden, requiring experts to create new domains. On the other hand, question-answering characters (Leuski et al., 2006) have a lower burden for depth, but must handle a broader range of questions and maintain believability and user satisfaction. For our current endeavor, tactical questioning (see Section 2), we require capabilities between these two extremes. We need the authorability and general robustness of characters like SGT Blackwell (Leuski et al., 2006) but with more of the emotional and cognitive modeling of the situation from agents like Dr Perez (Traum et al., 2005b). In this paper, we present Hassan, a Virtual Human for Tactical Questioning implemented using this in71 Proceedings of the 8th SIGdial"
2007.sigdial-1.15,2007.sigdial-1.6,1,0.900557,"rning can be beneficial, such as in (Traum et al., 2005a) and (Rickel et al., 2002). Virtual humans contain a number of components, including a virtual body, usually embedded in a virtual world, actions that the agent can perform, including movements and sound, cognitive capabilities to decide on which actions to do and updating internal state, and perceptual abilities for recognizing the actions of users and other things in the world. In this paper we present Hassan, a virtual human for training in Tactical Questioning dialogues. We focus on the spoken dialogue components. A companion paper (Roque and Traum, 2007) describes the dialogue manager and emotion model more fully. Currently there is no single “best practice” model for building virtual humans or especially their spoken dialogue components. While generally there are separate modules for speech recognition, natural language understanding, dialogue management, and output (e.g., Generation and Synthesis, or text selection and audio clip playing), there is no consensus on the best ways of engineering these modules. Part of the reason for this is that we are still fairly early in the search space, considering all of the possible techniques applied t"
2007.sigdial-1.6,2007.sigdial-1.15,1,0.903045,"iewed by a human trainer during or after the training session. The natural language components of our dialogue agent include a set of statistical classifiers working together with a rule-based dialogue manager. The Automated Speech Recognition output is sent to the classifiers, three of which detect language features, and three of which suggest possible replies. The Dialogue Manager uses its model of emotions and compliance to determine which of the suggested replies, if any, are to be made back to the user, as described in the next section. Further system implementation details are given in (Traum et al., 2007). 4 any assumptions about cognitive consideration, joint purpose, ethical consideration, or trust; compliant behavior might or might not be cooperative. The components of our model were developed based on a study of Tactical Questioning domain documents such as (Army, 2006) and (Paul, 2006). More details about our model of compliance are given in section 4.3. The following sections describe how the human speaker’s utterances indirectly update the agent’s level of compliance by means of a model of emotion. Model of Dialogue, Emotions, and Compliance In our training scenario, trainees have a spe"
2020.lrec-1.334,L18-1532,1,0.899293,"ning to detect morphological information. Our experiments demonstrate that off-the-shelf tools could be used with minimal labor to increase available linguistic resources for low-resource languages. However, expert knowledge was still needed to interpret the results and remove numerous false positives. We suggest how to tune two tools to return more meaningful results for similar languages, thus creating a road map for increasing resources for other under-resourced indigenous languages. Our other contribution in this paper is to introduce new additions to our previous corpus work for Choctaw (Brixey et al., 2018). We added over 90,000 new tokens of text from diverse sources, as well as an additional dictionary. The corpus contains a diversity of text, audio, and video. 2. Choctaw People and Language The Choctaw language is spoken by the Choctaw tribe, an American indigenous group that originally lived in what today is Alabama and Mississippi. In the early 1830s the Choctaws were forcibly relocated to Oklahoma in the migration known as the Trail of Tears, although some people remained. Choctaws are the third most populous tribe by tribal group population in the United States, with around 195,000 people"
2020.lrec-1.334,N16-2002,0,0.0185964,"within word clusters. We also conducted most-similarwords and analogy word tests. The analogy task consists of two pairs of words that share a relation, and the last word of the second pair is inferred based on the other three. The relation between words in a pair is not explicitly given, the relation must first be determined and then applied to the second pair (Levy and Goldberg, 2014). The most common example given in this task is “man is to woman, as king is to...?”, with the correct answer being “queen”. Nearly all of our word tests examine the encyclopedic knowledge of the word vectors (Gladkova et al., 2016). We evaluated how relevant the results were for each word test. The tests include four tests to find similar words, and three analogy tests. To design the analogy tests, we referred to a taxonomy of semantic relations (Bejar et al., 2012). The word tests are: 1. Three most common words to holisso (book) 2. Three most common words to tek (female) 3. Three most common words to tuklo (two) 4. Three most common words to tohbi (white) 5. The analogy “woman is to mother as man is to ...?” 6. The analogy “fat is to skinny as old is to ...?” 7. The analogy “hunger is to eat as thirst is to ...?” 4.2."
2020.lrec-1.334,J01-2001,0,0.128206,"d where it can be obtained by researchers upon request. 4. Exploration of the data set We explored the text data set using Linguistica and with word2vec. All experiments were conducted on Oklahoma texts and Mississippi texts separately, we report the results for each variant. 4.1. Linguistica Linguistica is a tool that can serve as a first step to creating affix lists for a language (Lee and Goldsmith, 2016). It successfully detects morphemes for European languages, such as French, that do not have a high average number of morphemes per word, and reportedly works best for words under 6-grams (Goldsmith, 2001, p.172). The input to Linguistica is one text file, in this case, one text file containing all texts for the Mississippi variant, and one text file for the Oklahoma variant. Linguistica’s unsupervised learning of morphology is achieved using Minimum Description Length (MDL), an algorithm that finds the best description of a set of data by finding the model that compresses the data best. MDL in Linguistica works to find the minimum number of morphological patterns needed to describe the given corpus (Goldsmith, 2006). One key step in the algorithm is to discover morpheme boundaries in order to"
2020.lrec-1.334,N16-3005,0,0.0134603,"sions of the corpus will be submitted to the MBCI for archiving. Following the OCR correction of all available religious texts, we will submit the corpus to the Sam Noble Museum archives10 for permanent storage and where it can be obtained by researchers upon request. 4. Exploration of the data set We explored the text data set using Linguistica and with word2vec. All experiments were conducted on Oklahoma texts and Mississippi texts separately, we report the results for each variant. 4.1. Linguistica Linguistica is a tool that can serve as a first step to creating affix lists for a language (Lee and Goldsmith, 2016). It successfully detects morphemes for European languages, such as French, that do not have a high average number of morphemes per word, and reportedly works best for words under 6-grams (Goldsmith, 2001, p.172). The input to Linguistica is one text file, in this case, one text file containing all texts for the Mississippi variant, and one text file for the Oklahoma variant. Linguistica’s unsupervised learning of morphology is achieved using Minimum Description Length (MDL), an algorithm that finds the best description of a set of data by finding the model that compresses the data best. MDL i"
2020.lrec-1.334,W14-1618,0,0.0333374,"ffix sets discovered by Linguistica (discussed in Section 4.1.). We evaluated the resulting word embeddings for all experiments through two methods. First, we made word plot images. We visually inspected these images for relationships within word clusters. We also conducted most-similarwords and analogy word tests. The analogy task consists of two pairs of words that share a relation, and the last word of the second pair is inferred based on the other three. The relation between words in a pair is not explicitly given, the relation must first be determined and then applied to the second pair (Levy and Goldberg, 2014). The most common example given in this task is “man is to woman, as king is to...?”, with the correct answer being “queen”. Nearly all of our word tests examine the encyclopedic knowledge of the word vectors (Gladkova et al., 2016). We evaluated how relevant the results were for each word test. The tests include four tests to find similar words, and three analogy tests. To design the analogy tests, we referred to a taxonomy of semantic relations (Bejar et al., 2012). The word tests are: 1. Three most common words to holisso (book) 2. Three most common words to tek (female) 3. Three most commo"
2020.lrec-1.334,N13-1090,0,0.0329326,"MS variants, excluding the NULL affix more affixes. Overall, the approach of Linguistica is better for detecting suffixes than prefixes for both variants. 4.2. word2vec Creating word vectors is often a preprocessing step towards using a text data set for many tasks, such as giving the word vectors to a machine learning model to make predictions. However, the process of creating the word vectors maps semantically related words. By exploring the mappings, we can potentially discover relationships between words and other meaningful syntactic and semantic information captured by the word vectors (Mikolov et al., 2013b). This can be highly useful when exploring a language with little documentation as meanings from novel words can be derived from similar known words. 4.2.1. word2vec methods We constructed the word vectors using Gensim word2Vec ˇ uˇrek and Sojka, 2010), an open-source tool that pro(Reh˚ cesses text to create a neural network. The resulting neural network is a set of feature vectors of words in the corpus represented numerically, and a set of vectors containing the probabilities that those words will co-occur. Vectors of similar words group together in the vector space, so that it is possible"
2020.lrec-1.797,aggarwal-etal-2012-twins,1,0.748116,"TEM) in children between the ages of 7 and 14. They are question answering characters, but unlike SGTs Blackwell and Star, the response is a whole dialogue sequence, potentially involving interchange from both characters, rather than a single character turn. There are two types of users for the Twins: demonstrators, who are museum staff members, using head-mounted microphones, and museum visitors, who use a Shure 522 table-top mounted microphone (Traum et al., 2012). Speech was collected at the Museum of Science in Boston from 2009 to 2011. More on analysis of the museum data can be found in (Aggarwal et al., 2012). The following utterances illustrate a segment of a dialogue between a museum visitor and the Twins: Museum visitor Hello. Figure 1: A screenshot of the ASR testing app. Ada Hey there! Grace How can we help you? 4. Museum visitor Who are you? Grace Could you say that again? User Who are you named after? Ada Our namesakes are Ada Lovelace and Grace Hopper. Grace Yeah, they may be old fashioned names. But both are from the most modern computing women of their time. The utterances collected from user sessions in the domains described above were transcribed manually to create a separate corpus fo"
2020.lrec-1.797,burger-etal-2006-competitive,0,0.139927,"Missing"
2020.lrec-1.797,W06-1303,1,0.799119,"he shooting? Doctor Perez Captain, we need medical supplies in order to run the clinic downtown. Amani So you want to talk about the man who did the shooting? I have heard that he is called Saif. Cadet Do you know where he lives? Amani You are asking for a lot of information. If you expect me to tell you that you must offer me secrecy. Cadet I will keep this a secret. Amani You have assured me of secrecy. I believe he hides on the second floor of Assad’s shop. I know he is in there but when I go in the shop I don’t see him. And I have no idea where he is now. 3.3. SGT Blackwell SGT Blackwell (Leuski et al., 2006b) is a questionanswering character who answers general questions about the Army, himself, and his technology. Speech comes from visitors to the Cooper-Hewitt Museum in New York from December 2006 to March 2007, who interacted with SGT Blackwell at his booth as part of the National Design Triennial exhibition (Robinson et al., 2008). SGT Blackwell is designed to answer independent questions, like SGT Star. However, the questions collected from sessions with SGT Blackwell come from the general public, and not from trained handlers as was the case with the questions collected from sessions with"
2020.lrec-1.797,W13-4064,1,0.633667,"main-specific acoustic and/or language models. • The possibility for training on individual speakers, and the amount of available user-specific training data. The evaluation described in this paper is targeted to ASR consumers and potential consumers with limited experience in ASR. We use state-of-the-art ASR systems that have been developed both in industry and academia, and our focus is on employing out-of-the-box acoustic and language models, i.e., we do not train domain-specific models. This is our third large-scale ASR evaluation using corpora from a variety of domains (Yao et al., 2010; Morbini et al., 2013). Compared to our previous evaluations, we see a large improvement in ASR performance, which illustrates the significant progress that has recently been made in ASR technology, especially with the use of deep learning techniques. However, there are domains where interactions take place under noisy conditions and that require special vocabulary and language models. In these domains we will see that current state-of-the-art speech recognizers perform poorly. Furthermore, the performance of a specific ASR system can vary significantly depending on the domain. The remainder of the paper describes"
2020.lrec-1.797,robinson-etal-2008-ask,1,0.692313,"crecy. Cadet I will keep this a secret. Amani You have assured me of secrecy. I believe he hides on the second floor of Assad’s shop. I know he is in there but when I go in the shop I don’t see him. And I have no idea where he is now. 3.3. SGT Blackwell SGT Blackwell (Leuski et al., 2006b) is a questionanswering character who answers general questions about the Army, himself, and his technology. Speech comes from visitors to the Cooper-Hewitt Museum in New York from December 2006 to March 2007, who interacted with SGT Blackwell at his booth as part of the National Design Triennial exhibition (Robinson et al., 2008). SGT Blackwell is designed to answer independent questions, like SGT Star. However, the questions collected from sessions with SGT Blackwell come from the general public, and not from trained handlers as was the case with the questions collected from sessions with SGT Star. The museum exhibit listed a set of about five sample questions, but visitors were free to ask anything they wanted. The following utterances illustrate a segment of a dialogue between a museum visitor and SGT Blackwell: Doctor Perez We can’t take sides. Trainee Would you be willing to move downtown? Elder Al Hassan We woul"
2020.lrec-1.797,yao-etal-2010-practical,1,0.442037,"ain by building domain-specific acoustic and/or language models. • The possibility for training on individual speakers, and the amount of available user-specific training data. The evaluation described in this paper is targeted to ASR consumers and potential consumers with limited experience in ASR. We use state-of-the-art ASR systems that have been developed both in industry and academia, and our focus is on employing out-of-the-box acoustic and language models, i.e., we do not train domain-specific models. This is our third large-scale ASR evaluation using corpora from a variety of domains (Yao et al., 2010; Morbini et al., 2013). Compared to our previous evaluations, we see a large improvement in ASR performance, which illustrates the significant progress that has recently been made in ASR technology, especially with the use of deep learning techniques. However, there are domains where interactions take place under noisy conditions and that require special vocabulary and language models. In these domains we will see that current state-of-the-art speech recognizers perform poorly. Furthermore, the performance of a specific ASR system can vary significantly depending on the domain. The remainder"
2020.lrec-1.86,2020.scil-1.31,1,0.786792,"l. and Lindemann et al. parsers to obtain the standard AMR for manual corrections, as each correctly captured several of the extremely frequent aspects of the corpus, including the mode :imperative marker. 4.2.2. Graph-to-Graph Transformation for Dialogue-AMR In order to automatically generate Dialogue-AMRs with the tense, aspect, and illocutionary force information critical to the navigation domain, we developed a graph-tograph transformation system that converts standard AMRs into our Dialogue-AMRs through a mixed-methods approach that leverages both rule-based and classifier-based systems (Abrams et al., 2020). Both the standard AMR and original natural language utterance are required as input to the graph-to-graph transformer. From the utterance, the speech act and tense are determined by employing classifiers. From the standard AMR, the relations (e.g., go-02, turn-01) corresponding to robot concepts are determined by matching the standard AMR root relation against a dictionary of keywords associated with a particular robot concept (see Table 2). Next, the aspectual information is extracted based upon speech act and tense patterns (e.g., present-tense assertions are complete ongoing +). Finally,"
2020.lrec-1.86,W13-2322,1,0.888934,"nly the content of an utterance, but the illocutionary force behind it, as well as tense and aspect. To showcase the coverage of the schema, we use both manual and automatic methods to construct the “DialAMR” corpus—a corpus of human-robot dialogue annotated with standard AMR and our enriched Dialogue-AMR schema. Our automated methods can be used to incorporate AMR into a larger NLU pipeline supporting human-robot dialogue. Keywords: Dialogue, Abstract Meaning Representation, Illocutionary Force 1. Introduction This paper describes a schema that enriches Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to support Natural Language Understanding (NLU) in humanrobot dialogue systems. AMR is a formalism for sentence semantics that abstracts away many syntactic idiosyncrasies and represents sentences with rooted directed acyclic graphs (Figure 1a shows the PENMAN notation of the graph). Although AMR provides a suitable level of abstraction for representing the content of sentences in our domain, it lacks a level of representation for speaker intent, which would capture the pragmatic effect of an utterance in dialogue. Pragmatic information is critical in dialogue with a conversational agent. For"
2020.lrec-1.86,bastianelli-etal-2014-huric,0,0.0191406,"taxonomies often have to be fine-tuned to the domain of interest to be fully useful. While we adopt many of the categories of Searle’s taxonomy for our own speech act inventory, we integrate distinctions from the ISO standard and, following Traum (1999) and Poesio and Traum (1998), define our speech acts according to the effects of an utterance relating to the beliefs and obligations of the interlocutors (see Section 3.1). Our work forms part of a larger, growing interest in representing various levels of interpretation in existing meaning representation frameworks, and in AMR in particular. Bastianelli et al. (2014) present their Human Robot Interaction Corpus (HuRIC) following the format of AMR. This corpus is comprised of paired audio interactions and transcriptions. Though all text is annotated in the format of AMR, AMR is significantly altered by incorporating detailed spatial relations, frame semantics (Fillmore, 1985), and morphosyntactic information. Shen (2018) further presents a small corpus of manually annotated AMRs for spoken language to help the parsing task. The study presents similar findings to our own: while AMR offers a clean framework for the concepts and relations used in spoken langu"
2020.lrec-1.86,W19-3322,1,0.813505,"d and robust schema for representing illocutionary force in AMR called “Dialogue-AMR” (Figure 1b). This expands and refines previous work which proposed basic modifications for (a) (d / drive-01 :mode imperative :ARG0 (y / you) :destination (d2 / door)) (b) (c / command-SA :ARG0 (c2 / commander) :ARG2 (r / robot) :ARG1 (g / go-02 :completable + :ARG0 r :ARG3 (h / here) :ARG4 (d/ door) :time (a2 / after :op1 (n / now)))) Figure 1: The utterance Drive to the door represented in (a) standard AMR form, (b) Dialogue-AMR form. how to annotate speech acts and tense and aspect information within AMR (Bonial et al., 2019a). The contributions of the present research are: i) a set of speech acts finalized and situated in a taxonomy (Section 3.1); ii) the refinement of the Dialogue-AMR annotation schema to provide coverage of novel language (Sections 3.2 and 3.3); and iii) the creation of the “DialAMR” corpus, a collection of human-robot dialogues to which the new Dialogue-AMR schema has been applied (Section 4).1 DialAMR has additionally been annotated with standard AMR, thus constituting one of the first corpora of dialogue annotated with AMR (see related work in Section 5) and allowing for comparison of both"
2020.lrec-1.86,W19-0124,1,0.862276,"d and robust schema for representing illocutionary force in AMR called “Dialogue-AMR” (Figure 1b). This expands and refines previous work which proposed basic modifications for (a) (d / drive-01 :mode imperative :ARG0 (y / you) :destination (d2 / door)) (b) (c / command-SA :ARG0 (c2 / commander) :ARG2 (r / robot) :ARG1 (g / go-02 :completable + :ARG0 r :ARG3 (h / here) :ARG4 (d/ door) :time (a2 / after :op1 (n / now)))) Figure 1: The utterance Drive to the door represented in (a) standard AMR form, (b) Dialogue-AMR form. how to annotate speech acts and tense and aspect information within AMR (Bonial et al., 2019a). The contributions of the present research are: i) a set of speech acts finalized and situated in a taxonomy (Section 3.1); ii) the refinement of the Dialogue-AMR annotation schema to provide coverage of novel language (Sections 3.2 and 3.3); and iii) the creation of the “DialAMR” corpus, a collection of human-robot dialogues to which the new Dialogue-AMR schema has been applied (Section 4).1 DialAMR has additionally been annotated with standard AMR, thus constituting one of the first corpora of dialogue annotated with AMR (see related work in Section 5) and allowing for comparison of both"
2020.lrec-1.86,T75-2014,0,0.329028,"ialogue, an interlocutor must interpret the meaning of a speaker’s utterance on at least two levels, as first suggested by Austin (1962): (i) its propositional content and (ii) its illocutionary force. While semantic representations have traditionally sought to represent propositional content, speech act theory has sought to delineate and explicate the relationship between an utter690 ance and its effects on the mental and interactional states of the conversational participants. Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). Although the refinement and extension of Austin’s (1962) hypothesized speech acts by Searle (1969) remains a canonical work on this topic, there have since been a number of widely used speech act taxonomies that differ from or augment this work, including an ISO standard (Bunt et al., 2012). Nevertheless, these taxonomies often have to be fine-tuned to the domain of interest to be fully useful. While we adopt many of the categories of Searle’s taxonomy for our own sp"
2020.lrec-1.86,bunt-etal-2012-iso,1,0.899784,"ng those same concepts. Thus, the AMR formalism smooths away many syntactic and lexical features that are unimportant to the robot. Existing AMR parsers can be utilized to obtain an initial interpretation of a user utterance, making the interpretation process easier than parsing natural language Development of Dialogue-AMR Speech Act Inventory We embrace much of the higher-level categorization and labeling of speech acts outlined by Searle (1969), including the basic categories of Assertions (termed “representatives” by Searle), Commissives, Directives, and Expressives. Additionally, based on Bunt et al. (2012), we introduce an early distinction in classifying our speech acts between Information Transfer Functions and Action-Discussion Functions (see Figure 3). In terms of dialogue function, this division allows us to monitor the status of distinct dialogue contexts. For Information Transfer Types, we can monitor the quantity and quality of general-purpose information exchanged in the dialogue that is relevant to the larger task at hand. For example, Robot, do you speak any foreign languages? may not directly impact a current task, but it introduces information into the dialogue that may be useful a"
2020.lrec-1.86,P13-2131,0,0.260444,"Missing"
2020.lrec-1.86,N15-1119,0,0.0330163,"use a lexicon (shared with PropBank (Palmer et al., 2005) comprised of numbered senses of a relation, each of which lists a set of numbered participant roles (Arg0-5). For ease of creation and manipulation, annotators work with notation from the PENMAN project (Penman Natural Language Group, 1989), which is the notation used in this paper (e.g., Figure 1a). AMR has been used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), as well as machine translation (Langkilde and Knight, 1998), question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). text directly into a robot-oriented representation. Standard AMR nevertheless omits certain semantic information essential to our domain. Specifically, AMR omits both tense and aspect information, assuming that some of this information may be gleaned from morphosyntactic information already well-represented in syntactic treebanks. The formalism also lacks illocutionary force, considering it distinct from core contentful meaning. We therefore add these properties to the robot’s semantic representation (Sectio"
2020.lrec-1.86,W16-6603,0,0.0140873,"r graph nodes) are introduced for entities, events, properties, and states. Leaves are labeled with concepts (e.g., (r / robot)). Relational concepts in AMR use a lexicon (shared with PropBank (Palmer et al., 2005) comprised of numbered senses of a relation, each of which lists a set of numbered participant roles (Arg0-5). For ease of creation and manipulation, annotators work with notation from the PENMAN project (Penman Natural Language Group, 1989), which is the notation used in this paper (e.g., Figure 1a). AMR has been used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), as well as machine translation (Langkilde and Knight, 1998), question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). text directly into a robot-oriented representation. Standard AMR nevertheless omits certain semantic information essential to our domain. Specifically, AMR omits both tense and aspect information, assuming that some of this information may be gleaned from morphosyntactic information already well-represented in syntactic treebanks. The formalism also lacks illocut"
2020.lrec-1.86,W17-2315,0,0.484277,"Missing"
2020.lrec-1.86,L18-1017,1,0.835141,"by the DM. SCOUT also includes annotations of dialogue structure 685 Left Conversational Floor # Participant 1 2 3 4 5 6 7 8 9 proceed to the doorway ahead Right Conversational Floor DM → Participant DM → RN RN I see more than one doorway. Which doorway? the doorway closest to you processing move into Kitchen moving... done done Table 1: Navigation instruction initiated by the participant (#1), its clarification (#2-4), subsequent translation to a simplified form (Dialogue Manager (DM) to Robot Navigator (RN), #6), and acknowledgement of instructions (#5, 7, 9) and execution by the RN (#8). (Traum et al., 2018) that allow for the characterization of distinct information states (Traum and Larsson, 2003). However, this dialogue structure annotation schema does not provide a markup of the semantic content in participant instructions. 2.2. AMR AMR is a formalism for sentence semantics that abstracts away from some syntactic idiosyncrasies (Banarescu et al., 2013). Each sentence is represented by a rooted directed acyclic graph (DAG) in which variables (or graph nodes) are introduced for entities, events, properties, and states. Leaves are labeled with concepts (e.g., (r / robot)). Relational concepts in"
2020.lrec-1.86,N15-1040,0,0.0218702,"lease data, which, as mentioned previously, does not include natural dialogue, nor does it include much instruction-giving or commands. Nonetheless, we applied parsers to the SCOUT corpus to determine which could achieve the best performance with the least manually annotated in-domain training data. These experiments are ongoing, and full results will be reported in a future paper. Here, we limit our description to what is relevant for the automatic annotation pass used to efficiently create the DialAMR corpus. First, we tested two long-standing parsers, JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015), on the Random-Commander set of gold-standard, manually annotated standard AMRs. Performance was far below reported f-scores on LDC AMR test data (Bonial et al., 2019b). Particularly problematic areas included missing mode :imperative markers on all imperative utterances, failure to include implicit subjects (e.g., the Arg0-mover in utterances such as Moving...), and failure to correctly represent the photographing semantics of the common light verb construction take a photo/picture (instead representing this as a taking event in the sense of grasping/moving). Next, we evaluated more recent s"
2020.lrec-1.86,W00-0309,0,0.46237,"guage generation, and robot concept specification. The DialogueAMR relations classify speaker intention, while the argument roles allow for flexible representation of previously unseen values (e.g., Turn left 100 degrees compared to a more typical number of degrees, such as 90) and compositional construction of referring expressions. Furthermore, the completable annotation attached to goal-oriented Dialogue-AMRs allow a dialogue management system to determine if all the arguments required for execution of the instruction are present, and, if not, the system can follow up with a clarification (Xu and Rudnicky, 2000). This structured approach is expected to be less brittle than the statistical similarity and retrieval model implemented in Lukin et al.’s (2018) NLU component in this human-robot dialogue domain, which has difficulty generalizing to novel, unseen commands. We expect promising results from integrating DialogueAMR into our human-robot dialogue architecture. Furthermore, our annotation schema and corpus will contribute to a growing set of resources supporting meaning representation that goes beyond propositional content to model speaker intention in the conversational context. Acknowledgments W"
2020.lrec-1.91,P09-1099,0,0.0281269,"learning (RL) is a very popular approach to learning dialogue policies from data or simulated users (SUs) (Jurˇc´ıcˇ ek et al., 2012). In RL, a typical reward function is for the system to earn a number of points for a fully or partially successful dialogue, and subtract a penalty per system turn to ensure that the learned dialogue policies will not favor lengthy and tedious dialogues (Henderson et al., 2008). Note however that longer dialogue lengths are not necessarily indicative of poor dialogue quality but depending on the task they may actually indicate user engagement and satisfaction (Foster et al., 2009). Schatzmann et al. (2006) present an overview of metrics that have been proposed in the literature for measuring the quality of SUs used for training and evaluating dialogue policies. The action generated by the SU is compared against the user action in a human-human or humansystem reference corpus (in the same dialogue context), and measures such as precision, recall, accuracy, and perplexity are used (Schatzmann et al., 2005; Georgila et al., 2005; Georgila et al., 2006; Pietquin and Hastie, 2013). Also, to take into account the fact that SU actions are generated based on a probability dist"
2020.lrec-1.91,P15-2073,0,0.0237073,"erlap similarity metrics such as BLEU, METEOR, and ROUGE (originally employed in machine translation and summarization) are widely used for measuring chatbot dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the aforementioned SU evaluation metrics. In fact it has been shown that BLEU, METEOR, and ROUGE do not correlate well with human judgements of dialogue quality (Liu et al., 2016). Discriminative BLEU, a variation of BLEU where reference strings are scored for quality by human raters, was found to correlate better with human judgements than standard BLEU (Galley et al., 2015). To address the issues with BLEU, METEOR, and ROUGE, next utterance classification was introduced as a method for evaluating chatbots (Lowe et al., 2016), but the proposed metric recall@k is very similar to the recall metric previously used for evaluating SUs, and consequently has the same limitations. Also, topic-based metrics for chatbot evaluation (topic breadth and topic depth) were found to correlate well with human judgements (Guo et al., 2017). 3. Wizard of Oz Data Collection We built a GUI-based environment for our Wizard of Oz (WOz) data collection (Gordon et al., 2019). A human Wiza"
2020.lrec-1.91,J08-4002,1,0.80145,"imized by controlling the factors that affect it. In the example above, user satisfaction can be optimized by increasing task success, and minimizing dialogue length and speech recognition errors. Reinforcement learning (RL) is a very popular approach to learning dialogue policies from data or simulated users (SUs) (Jurˇc´ıcˇ ek et al., 2012). In RL, a typical reward function is for the system to earn a number of points for a fully or partially successful dialogue, and subtract a penalty per system turn to ensure that the learned dialogue policies will not favor lengthy and tedious dialogues (Henderson et al., 2008). Note however that longer dialogue lengths are not necessarily indicative of poor dialogue quality but depending on the task they may actually indicate user engagement and satisfaction (Foster et al., 2009). Schatzmann et al. (2006) present an overview of metrics that have been proposed in the literature for measuring the quality of SUs used for training and evaluating dialogue policies. The action generated by the SU is compared against the user action in a human-human or humansystem reference corpus (in the same dialogue context), and measures such as precision, recall, accuracy, and perple"
2020.lrec-1.91,D16-1230,0,0.0227334,"responses can be much more meaningful, which has led to the development of coding schemes for response appropriateness in such cases (Traum et al., 2004; Robinson et al., 2010). Currently, word-overlap similarity metrics such as BLEU, METEOR, and ROUGE (originally employed in machine translation and summarization) are widely used for measuring chatbot dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the aforementioned SU evaluation metrics. In fact it has been shown that BLEU, METEOR, and ROUGE do not correlate well with human judgements of dialogue quality (Liu et al., 2016). Discriminative BLEU, a variation of BLEU where reference strings are scored for quality by human raters, was found to correlate better with human judgements than standard BLEU (Galley et al., 2015). To address the issues with BLEU, METEOR, and ROUGE, next utterance classification was introduced as a method for evaluating chatbots (Lowe et al., 2016), but the proposed metric recall@k is very similar to the recall metric previously used for evaluating SUs, and consequently has the same limitations. Also, topic-based metrics for chatbot evaluation (topic breadth and topic depth) were found to c"
2020.lrec-1.91,W16-3634,0,0.0182117,"t dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the aforementioned SU evaluation metrics. In fact it has been shown that BLEU, METEOR, and ROUGE do not correlate well with human judgements of dialogue quality (Liu et al., 2016). Discriminative BLEU, a variation of BLEU where reference strings are scored for quality by human raters, was found to correlate better with human judgements than standard BLEU (Galley et al., 2015). To address the issues with BLEU, METEOR, and ROUGE, next utterance classification was introduced as a method for evaluating chatbots (Lowe et al., 2016), but the proposed metric recall@k is very similar to the recall metric previously used for evaluating SUs, and consequently has the same limitations. Also, topic-based metrics for chatbot evaluation (topic breadth and topic depth) were found to correlate well with human judgements (Guo et al., 2017). 3. Wizard of Oz Data Collection We built a GUI-based environment for our Wizard of Oz (WOz) data collection (Gordon et al., 2019). A human Wizard plays the role of the system by pressing buttons in a GUI. Each button corresponds to a Wizard action which is then transformed into a sentence (throug"
2020.lrec-1.91,W12-1611,1,0.785959,"l., 2006). However, these metrics can be problematic because if a SU action is not the same as the user action in the reference corpus, this does not necessarily mean that it is a poor action. Also, once a user or system response deviates from the corresponding action in the reference corpus, the remaining dialogue will unfold in an entirely different way than the fixed dialogue in the reference corpus, which will make further comparisons meaningless. In non-task-oriented dialogue systems (e.g., chatbots) developing robust evaluation metrics can be even harder than for task-oriented dialogue (Misu et al., 2012). Here it is not clear what success means and task-specific objective metrics are not appropriate. Instead subjective evaluations for appropriateness of responses can be much more meaningful, which has led to the development of coding schemes for response appropriateness in such cases (Traum et al., 2004; Robinson et al., 2010). Currently, word-overlap similarity metrics such as BLEU, METEOR, and ROUGE (originally employed in machine translation and summarization) are widely used for measuring chatbot dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the afore"
2020.lrec-1.91,W09-3901,1,0.741973,"s, also in the IoT domain. 2. Related Work Hastie (2012) presents an overview of evaluation frameworks and metrics that have been proposed in the literature for measuring the quality of human-system dialogue interaction, mainly for task-oriented dialogue systems. Some of these metrics are subjective (e.g., user satisfaction, perceived task completion, etc.), while others are objective (e.g., word error rate, dialogue length, etc.). Objective measures can be calculated from the interaction logs while subjective assessments can be collected via surveys and questionnaires (Hone and Graham, 2000; Paksima et al., 2009). PARADISE is perhaps the most well-known framework for evaluating dialogue systems, and an attempt to automate the evaluation process (Walker et al., 2000). PARADISE seeks to optimize a desired quality such as user satisfaction by formulating it as a linear combination of a variety of metrics, such as task success and dialogue cost (e.g., dialogue length, speech recognition errors, etc.). The contribution of each factor is determined by weights calculated via linear regression. The advantage of this method is that once a desired quality has been formulated as a realistic evaluation function,"
2020.lrec-1.91,robinson-etal-2010-dialogues,1,0.719854,"old in an entirely different way than the fixed dialogue in the reference corpus, which will make further comparisons meaningless. In non-task-oriented dialogue systems (e.g., chatbots) developing robust evaluation metrics can be even harder than for task-oriented dialogue (Misu et al., 2012). Here it is not clear what success means and task-specific objective metrics are not appropriate. Instead subjective evaluations for appropriateness of responses can be much more meaningful, which has led to the development of coding schemes for response appropriateness in such cases (Traum et al., 2004; Robinson et al., 2010). Currently, word-overlap similarity metrics such as BLEU, METEOR, and ROUGE (originally employed in machine translation and summarization) are widely used for measuring chatbot dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the aforementioned SU evaluation metrics. In fact it has been shown that BLEU, METEOR, and ROUGE do not correlate well with human judgements of dialogue quality (Liu et al., 2016). Discriminative BLEU, a variation of BLEU where reference strings are scored for quality by human raters, was found to correlate better with human judgements"
2020.lrec-1.91,2005.sigdial-1.6,1,0.516421,"that longer dialogue lengths are not necessarily indicative of poor dialogue quality but depending on the task they may actually indicate user engagement and satisfaction (Foster et al., 2009). Schatzmann et al. (2006) present an overview of metrics that have been proposed in the literature for measuring the quality of SUs used for training and evaluating dialogue policies. The action generated by the SU is compared against the user action in a human-human or humansystem reference corpus (in the same dialogue context), and measures such as precision, recall, accuracy, and perplexity are used (Schatzmann et al., 2005; Georgila et al., 2005; Georgila et al., 2006; Pietquin and Hastie, 2013). Also, to take into account the fact that SU actions are generated based on a probability distribution, expected precision, expected recall, and expected accuracy are used (Georgila et al., 2006). However, these metrics can be problematic because if a SU action is not the same as the user action in the reference corpus, this does not necessarily mean that it is a poor action. Also, once a user or system response deviates from the corresponding action in the reference corpus, the remaining dialogue will unfold in an enti"
2020.lrec-1.91,traum-etal-2004-evaluation,1,0.60556,"ng dialogue will unfold in an entirely different way than the fixed dialogue in the reference corpus, which will make further comparisons meaningless. In non-task-oriented dialogue systems (e.g., chatbots) developing robust evaluation metrics can be even harder than for task-oriented dialogue (Misu et al., 2012). Here it is not clear what success means and task-specific objective metrics are not appropriate. Instead subjective evaluations for appropriateness of responses can be much more meaningful, which has led to the development of coding schemes for response appropriateness in such cases (Traum et al., 2004; Robinson et al., 2010). Currently, word-overlap similarity metrics such as BLEU, METEOR, and ROUGE (originally employed in machine translation and summarization) are widely used for measuring chatbot dialogue quality. However, BLEU, METEOR, and ROUGE suffer from the same problems as the aforementioned SU evaluation metrics. In fact it has been shown that BLEU, METEOR, and ROUGE do not correlate well with human judgements of dialogue quality (Liu et al., 2016). Discriminative BLEU, a variation of BLEU where reference strings are scored for quality by human raters, was found to correlate bette"
2020.lrec-1.92,N19-1423,0,0.0418992,"Missing"
2020.lrec-1.92,W10-4345,1,0.718141,"he improved performance of the Pinchas 1444 v2 training set, with a much higher proportion of negative examples, does perhaps point to a direction for improvement. Future work should perhaps look at the even higher distribution of negative to positive examples. These results do show that despite the recent popularity of deep learning models, there is still a place for more traditional machine learning algorithms, that can operate well on more moderate-sized data sets for problems of interest. It may also be the case that different types of dialogue have different optimal models. For example, (Gandhe and Traum, 2010) show very different upper bounds for retrieval approaches to dialogue. 8. Acknowledgments This work was supported by the U.S. Army. Any opinion, content or information presented does not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. Parts of this work was appeared as an abstract paper in (Alavi et al., 2019). 739 Figure 5: Results from experiment 2 using various R@k measures, which illustrates NPCEditor works better than the Dual-Encoder model for R@1 on the Pinchas 10 dataset, however, it has similar or slightl"
2020.lrec-1.92,W06-1303,1,0.564953,"response from the corpus training data, using dual encoding and hidden layers to learn appropriate dialogue continuations. However, there are still a number of questions remaining about how well such models really work for real applications, and how much data is needed to achieve acceptable performance. Other machine learning approaches have been shown to be useful, with much smaller data sets. In this paper, we compare two different kinds of end-toend system, a neural network model based on (Lowe et al., 2015) and an older kind of end-to-end dialogue model, based on cross-language retrieval (Leuski et al., 2006), implemented in the publicly available NPCEditor (Leuski and Traum, 2011), and previously used for systems that have been displayed in museums (Traum et al., 2012; Traum et al., 2015). We compare these models on two different datasets: the Ubuntu Corpus (Lowe et al., 2015), and one derived from one of the museum system datasets (Traum et al., 2015). In the next section, we describe the data sets we use for the experiments. In section 3 we describe the NPCEditor and its selection model. We then review some prior neural network approaches to dialogue in section 4, followed by details of the mod"
2020.lrec-1.92,W15-4640,0,0.265177,"ese models learn to respond directly from a corpus, either by generating new responses or selecting a response from the corpus training data, using dual encoding and hidden layers to learn appropriate dialogue continuations. However, there are still a number of questions remaining about how well such models really work for real applications, and how much data is needed to achieve acceptable performance. Other machine learning approaches have been shown to be useful, with much smaller data sets. In this paper, we compare two different kinds of end-toend system, a neural network model based on (Lowe et al., 2015) and an older kind of end-to-end dialogue model, based on cross-language retrieval (Leuski et al., 2006), implemented in the publicly available NPCEditor (Leuski and Traum, 2011), and previously used for systems that have been displayed in museums (Traum et al., 2012; Traum et al., 2015). We compare these models on two different datasets: the Ubuntu Corpus (Lowe et al., 2015), and one derived from one of the museum system datasets (Traum et al., 2015). In the next section, we describe the data sets we use for the experiments. In section 3 we describe the NPCEditor and its selection model. We t"
2020.lrec-1.92,I17-1062,0,0.0146478,"based on large dialogue corpora using hierarchical recurrent encoder-decoder neural network. They showed how the model’s performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pre-trained word embeddings. (Lowe et al., 2015) released the Ubuntu Dialogue corpus, A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. They also introduced a deep learning model named Dual-Encoder model, suitable for analyzing this dataset on the task of selecting the best next response. There has been efforts in using Ubuntu Dialogue corpus (Mehri and Carenini, 2017) and some of them even introduced new models (e.g. (Dong and Huang, 2018)) for the task of selecting the best next response. BERT (Devlin et al., 2018) is another recent developed tool for getting sentence embeddings. It is the latest refinement of a series of neural models that make heavy use of pretraining and with the aid of transformers (Vaswani et al., 2017) has led to impressive gains in many natural language processing tasks. (Zhang et al., 2018) suggested a retrieval-based response matching model for multi-turn conversation. They tried to consider interactions among previous utterances"
2020.lrec-1.92,D14-1162,0,0.083699,"Missing"
2020.lrec-1.92,D11-1054,0,0.255846,"ultiple corpora, collected from two different types of dialogue source material. Results show that while the LSTM model performs adequately on a very large corpus (millions of utterances), its performance is dominated by the cross-language relevance model for a more moderate-sized corpus (ten thousands of utterances). Keywords: Deep Neural Network, Dual-Encoder model, Cross-Language Relevance model, Ubuntu dialogue corpus, end-to-end conversational dialogue 1. 2. Introduction End-to-end neural network models of conversational dialogue have become increasingly popular for conversational tasks (Ritter et al., 2011; Serban et al., 2015; Zhao et al., 2017). These models eschew traditional dialogue modeling approaches that include internal hand-crafted domain models and representations of dialogue context and separate components for understanding natural language (converting to the internal representation language), updating dialogue state, state-based response generation, and natural language generation (Traum and Larsson, 2003; Raux et al., 2005). Instead, these models learn to respond directly from a corpus, either by generating new responses or selecting a response from the corpus training data, using"
2020.lrec-1.92,W15-4629,1,0.877163,"how well such models really work for real applications, and how much data is needed to achieve acceptable performance. Other machine learning approaches have been shown to be useful, with much smaller data sets. In this paper, we compare two different kinds of end-toend system, a neural network model based on (Lowe et al., 2015) and an older kind of end-to-end dialogue model, based on cross-language retrieval (Leuski et al., 2006), implemented in the publicly available NPCEditor (Leuski and Traum, 2011), and previously used for systems that have been displayed in museums (Traum et al., 2012; Traum et al., 2015). We compare these models on two different datasets: the Ubuntu Corpus (Lowe et al., 2015), and one derived from one of the museum system datasets (Traum et al., 2015). In the next section, we describe the data sets we use for the experiments. In section 3 we describe the NPCEditor and its selection model. We then review some prior neural network approaches to dialogue in section 4, followed by details of the model we tested in section 5. In section 6, we describe the experiments and results. Finally, we conclude in section 7 with a discussion of the results and their implications. Datasets We"
2020.lrec-1.92,C18-1317,0,0.0624679,"Missing"
2020.lrec-1.92,P17-1061,0,0.0238747,"ent types of dialogue source material. Results show that while the LSTM model performs adequately on a very large corpus (millions of utterances), its performance is dominated by the cross-language relevance model for a more moderate-sized corpus (ten thousands of utterances). Keywords: Deep Neural Network, Dual-Encoder model, Cross-Language Relevance model, Ubuntu dialogue corpus, end-to-end conversational dialogue 1. 2. Introduction End-to-end neural network models of conversational dialogue have become increasingly popular for conversational tasks (Ritter et al., 2011; Serban et al., 2015; Zhao et al., 2017). These models eschew traditional dialogue modeling approaches that include internal hand-crafted domain models and representations of dialogue context and separate components for understanding natural language (converting to the internal representation language), updating dialogue state, state-based response generation, and natural language generation (Traum and Larsson, 2003; Raux et al., 2005). Instead, these models learn to respond directly from a corpus, either by generating new responses or selecting a response from the corpus training data, using dual encoding and hidden layers to learn"
2020.lrec-1.92,P18-1103,0,0.0153803,"response. BERT (Devlin et al., 2018) is another recent developed tool for getting sentence embeddings. It is the latest refinement of a series of neural models that make heavy use of pretraining and with the aid of transformers (Vaswani et al., 2017) has led to impressive gains in many natural language processing tasks. (Zhang et al., 2018) suggested a retrieval-based response matching model for multi-turn conversation. They tried to consider interactions among previous utterances in their context modeling by introducing a deep aggregation model to form a fine-grained context representation. (Zhou et al., 2018) is one of the latest models proposed for the task of next best response selection. They examined matching a response with its multi-turn context using dependency information based entirely on attention. (Shao et al., 2017) adopted a variation of sequence to sequence model to generate high quality responses on the single turn setting. (Olabiyi et al., 2018) proposed an adversarial learning approach to the generation of multi-turn dialogue responses. Their method, which is based on conditional generative adversarial networks, generalizes better than previous works using only log-likelihood crit"
A00-2001,P94-1001,1,0.806809,"trigger. The update rules for dialogue acts that we assume here are a simplified version of the formalisations proposed in (Poesio and Traum, 1998; Traum et al., 1999) (henceforth, PTT). The main aspects of PTT which have been implemented concern the way discourse obligations are handled and the manner in which dialogue participants interact to add information to the common ground. Obligations are essentially social in nature, and directly characterise spoken dialogue; a typical example of a discourse obligation concerns the relationship between questions and answers. Poesio and Traum follow (Traum and Allen, 1994) in suggesting that the utterance of a question imposes an obligation on the hearer to address the question (e.g., by providing an answer), irrespective of intentions. As for the process by which common ground is established, or GROUNDING(Clark and Schaefer, 1989; Traum, 1994), the assumption in PTT is that classical speech act theory is inherently too simplistic in that it ignores the fact that co-operative interaction is essential in discourse; thus, for instance, simply asserting something does not make it become mutually &apos;known&apos; (part of the common ground). It is actually necessary for the"
aggarwal-etal-2012-twins,H92-1073,0,\N,Missing
bunt-etal-2010-towards,bunt-2006-dimensions,1,\N,Missing
bunt-etal-2010-towards,N09-2050,1,\N,Missing
bunt-etal-2010-towards,W03-0804,1,\N,Missing
bunt-etal-2012-iso,tonelli-etal-2010-annotation,0,\N,Missing
bunt-etal-2012-iso,W11-0125,1,\N,Missing
bunt-etal-2012-iso,W11-0101,1,\N,Missing
bunt-etal-2012-iso,bunt-etal-2010-towards,1,\N,Missing
bunt-etal-2012-iso,prasad-etal-2008-penn,0,\N,Missing
bunt-etal-2012-iso,kipp-2008-spatiotemporal,0,\N,Missing
dorr-etal-1998-thematic,W98-1426,0,\N,Missing
dorr-etal-1998-thematic,A97-1021,1,\N,Missing
dorr-etal-1998-thematic,P95-1034,0,\N,Missing
dorr-etal-1998-thematic,P98-1116,0,\N,Missing
dorr-etal-1998-thematic,C98-1112,0,\N,Missing
garg-etal-2004-evaluation,robinson-etal-2004-issues,1,\N,Missing
garg-etal-2004-evaluation,bernsen-etal-2002-nite,0,\N,Missing
georgila-etal-2012-practical,W10-4318,1,\N,Missing
gratch-etal-2014-distress,W13-4032,1,\N,Missing
gratch-etal-2014-distress,brugman-russel-2004-annotating,0,\N,Missing
hartholt-etal-2008-common,W05-1520,1,\N,Missing
hartholt-etal-2008-common,W08-0130,1,\N,Missing
hartholt-etal-2008-common,I05-7009,1,\N,Missing
J96-3008,J86-3001,0,0.0117499,"Missing"
J96-3008,J95-4001,0,0.0125708,"he expectation. While this will work well for many cases, it may have problems if an unexpected utterance is made about a focused object. In this case, traditional pronoun resolution techniques (e.g., centering [Grosz, Joshi, and Weinstein 1995] or focus-matching [Grosz 1977]) would find the referent, while presumably Smith and Hipp&apos;s approach would not be able to match the unexpected input at all. Also, while the dialogue model allows for clarification subdialogues, it does not encode many general non-task-related patterns of linguistic interaction such as the linguistic expectations used by McRoy and Hirst (1995). While the dialogue system is fairly successful at interacting with a user to fix the circuits, there are still some aspects of the interaction that diverge from natural dialogue. First, a rigid turn-taking system was imposed, which allowed the user to only say a single sentence before waiting for a system response. While this kind of limitation is fairly standard for written dialogue systems involved in query-answering, it detracts from the flexibility of spoken dialogue by not allowing followup elaborations, clarifications, or shifts in initiative. Secondly, the &apos;directive&apos; initiative level"
J96-3008,J95-3001,0,0.0235822,"Missing"
J96-3008,P88-1015,0,0.0358697,"Missing"
L16-1018,brugman-russel-2004-annotating,0,0.0298624,"annotated stories. 5. Annotated Dialogue Data We have so far used the annotation scheme described in the previous section to annotate sections of two different dialogue corpora, the Switchboard corpus (Godfrey and Holliman, 1993) and the Distress Analysis Interview Corpus (Gratch et al., 2014). In addition, we have analyzed short sections of other dialogue corpora, such as (Herrera et al., 2010) and the cartoon negotiations (Ziebart et al., 2012). In this section, we report on findings from the Switchboard annotations. 5.1. Annotation Tool We examined several annotation tools, including Elan (Brugman and Russel, 2004) and the Story Workbench (Finlayson, 2008). We chose TAMSAnalyzer, (Weinstein, 2012) to annotate the data. This tool allows inclusion of markup tags in-text, and analysis of the tags run by the program. Tags can be multi-level, which allowed categories and sub-dimensions to be included. 122 Figure 10: Annotated Switchboard Story Tag Person first second/generic third mixed Function Answer Elaborate Mirror Response Refute Response Support Transition Figure 10 shows a short story annotated using the tool and annotations scheme. 5.2. Switchboard Annotations In the Switchboard corpus, we fully anno"
L16-1018,gratch-etal-2014-distress,1,0.832661,"e narrator. The identity of the speaker of a story may or may not be relevant to general understanding of the story, so the explicit identity sub-dimension categorizes stories based on whether the identity of the speaker is explicitly stated or relied upon in the story (e.g. whether speaker expertise is supported or informs aspects of the story). If identity is relevant, then the relevant identity should also be identified (e.g., military veteran, electrician, curmudgeon, etc.). The decision to annotate identity can be corpus specific, as in the case of the Distress Analysis Interview Corpus (Gratch et al., 2014), where some of the primary goals of collecting the corpus depended on the status of the interviewees with regard to mental health or military service. Alternatively, annotation of identity can be story specific, as in the Switchboard corpus annotations. Many stories could be told by a speaker of any identity, but some required identity specific context. These stories are annotated with the relevant identity as needed. In the case of story specific rather than domain specific identities, the other stories in the corpus would be annotated as unspecified/irrelevant identity. The story in Figure"
L16-1435,P07-2030,0,0.0719499,"Missing"
L16-1435,paetzel-etal-2014-multimodal,0,0.166355,"es were automatically evaluated for difficulty, however it has some important differences. The agent in (Sawaki et al., 2008) was presenting a challenge to a user, for pedagogical purposes, and thus the first clues given should be difficult. By contrast, the agent in (Pincus et al., 2014) acts as a teammate in a collaborative game, in which the goal is for the receiver to guess as efficiently as possible. Moreover, (Higashinaka et al., 2007) focused on an ideal ordering of clues rather than the difficulty level of clues themselves. Our work is most similar to the RDG-Phrase game described in (Paetzel et al., 2014) and (Pincus et al., 2014). In this game, the targets are common words or phrases rather than famous people. A corpus of audio and video recordings of humans playing the RDG-Phrase, a timed word guessing game, is described in (Paetzel et al., 2014). Some examples of targets and human clues from this corpus are shown in Table 1. We use a section of this corpus, and the annotations in (Pincus and Traum, 2014) to estimate the average guessability of human clues. (Pincus et al., 2014) introduced an automated clue-giver and a method of automatically generating clues from online sources. (Pincus et"
L18-1017,bunt-etal-2012-iso,1,0.79484,"pplications of these annotations are introduced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units and relations span across multiple conversational floors. Dialogues can be characterized by distinct information states (Traum and Larsson, 2003). These include sets of participants, participant roles (e.g. active, ratified participant vs. overhearer), turn-taking or floor-holding, expectation of how many participants will make substantial contributions at a time (Edelsky,"
L18-1017,J86-3001,0,0.827712,"ferent kinds of multi-party, multi-floor contributions. This is addressed by analysis of dialogue annotated with this scheme and the kinds of patterns of interaction that are observed (see section 5). Second, we use data from the corpus annotated with this scheme to serve as training and evaluation data for creating automated multicommunicators (see section 6). 2. Annotation Scheme We annotate two aspects of Dialogue Structure at the mesolevel (bigger than a single speaker-turn, but smaller than a complete dialogue activity) (Traum and Nakatani, 1999). First, we look at intentional structure (Grosz and Sidner, 1986), consisting of units of dialogue utterances that all have a role in explicating and addressing an initiating participant’s intention. Second, we look at the relations between different utterances within this unit, which reveal 104 Expansions Responses Translations processing: relate utterances that are produced by the same participant within the same floor. relate utterances by different participants within the same floor. relate utterances in different floors. acknowledgement: Table 1: Top Level Corpus Relations how the information state of participants in the dialogue is updated as the unit"
L18-1017,W17-2808,1,0.455485,"t apply this annotation scheme to a corpus of humanrobot interaction, taken from a project with a long-term goal to create an autonomous robot intelligence that can collaborate with remotely located human participants on exploration and navigation tasks. In the initial versions, a human “Commander” tasks the robot verbally, and gets feedback via multiple modalities, including text messages, a live 2Dmap built from the robot’s LIDAR scanner, and still photos captured from the robot’s front-facing camera. In order to collect sufficient information about the type of language used by a Commander (Marge et al., 2017), and provide training data to support development of appropriate language processing components, the development of the autonomous human-robot interaction begins with a series of “Wizard of Oz” experiments (Marge et al., 2016; Bonial et al., 2017), where the robot is controlled by two wizards, with an internal communication floor, distinct from the floor used by the Commander to communicate with the robot. The wizards include a Dialogue Manager (DMWizard, or DM) who handles communication to the Commander and “speaks” via text messages (Bonial et al., 2017) and a Robot Navigator (RN-Wizard, or"
L18-1017,passonneau-2006-measuring,0,0.130273,"Missing"
L18-1017,W15-0210,0,0.0252823,"ced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units and relations span across multiple conversational floors. Dialogues can be characterized by distinct information states (Traum and Larsson, 2003). These include sets of participants, participant roles (e.g. active, ratified participant vs. overhearer), turn-taking or floor-holding, expectation of how many participants will make substantial contributions at a time (Edelsky, 1981), and other factors. Often distinct dialog"
L18-1017,W99-0313,1,0.538189,"ation of an initiator’s intent, and relations between individual utterances within the unit. We apply this scheme to annotate a corpus of multi-floor human-robot interaction dialogues. We examine the patterns of structure observed in these dialogues and present inter-annotator statistics and relative frequencies of types of relations and transaction units. Finally, some example applications of these annotations are introduced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units"
L18-1463,brugman-russel-2004-annotating,0,0.183201,"Missing"
L18-1629,W16-3619,0,0.033226,"Missing"
L18-1629,N09-1072,0,0.0325739,"Missing"
L18-1629,J00-3003,0,0.0535115,"nnotating the dialogues with different linguistics features, from phonetics to syntax (Calhoun et al., 2010). Of particular importance to this work are the wordlevel, turn, and utterance boundary transcriptions, where the speakers were labeled as ‘A’ and ‘B’. An extension of the 1997 “Switchboard 1 Release 2” Corpus - called the “Switchboard Dialog Act Corpus” - was used in this work. It contains word-level transcriptions of the dialogues segmented into turn-taking utterances, where each utterance is tagged with a dialog act but the dialog act was stripped from the utterance for our purposes (Stolcke et al., 2000). Procedure for the Identification of Attribute Instances 75 dialogues from each corpus were randomly selected for annotation. Each utterance was appraised for whether there was a derivable attribute instance. If there was an attribute instance, then Schema.org was investigated to find an attribute type that would capture the information in the attribute instance. In the case that no corresponding attribute type could be found, a new type was created. This new type was then considered in each subsequent utterance as a possible attribute type - along with the properties from Schema.org - if an"
leuski-traum-2010-npceditor,W06-1303,1,\N,Missing
N09-2014,J96-1002,0,0.0137797,"ys generators for electricity downtown • ASR (NLU input): we up apparently give you guys generators for a letter city don town • Frame (NLU output): &lt;s>.mood declarative &lt;s>.sem.agent kirk &lt;s>.sem.event deliver &lt;s>.sem.modal.possibility can &lt;s>.sem.speechact.type offer &lt;s>.sem.theme power-generator &lt;s>.sem.type event The original NLU component for this system was described in (Leuski and Traum, 2008). For the purposes of this experiment, we have developed a new NLU module and tested on several different data sets as described in the next section. Our approach is to use maximum entropy models (Berger et al., 1996) to learn a suitable mapping from features derived from the words in the ASR output to semantic frames. Given a set of examples of semantic frames with corresponding ASR output, a classifier should learn, for example, that when “generators” appears in the output of ASR, the value power-generators is likely to be present in the output frame. The specific features used by the classifier are: each word in the input string (bag-of-words representation of the input), each bigram (consecutive words), each pair of any two words in the input, and the number of words in the input string. 54 60 450 400"
N09-2014,E09-1085,0,0.121738,"ing that can be unnatural and inefficient for mixed-initiative dialogue. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the subutterance level is natural and common, the system needs to engage in incremental processing, in which interpretation components are activated, and in some cases decisions are made, before the user utterance is complete. There is a growing body of work on incremental processing in dialogue systems. Some of this work has demonstrated overall improvements in system responsiveness and user satisfaction; e.g. (Aist et al., 2007; Skantze and Schlangen, 2009). Several 53 research groups, inspired by psycholinguistic models of human processing, have also been exploring technical frameworks that allow diverse contextual information to be brought to bear during incremental processing; e.g. (Kruijff et al., 2007; Aist et al., 2007). While this work often assumes or suggests it is possible for systems to understand partial user utterances, this premise has generally not been given detailed quantitative study. The contribution of this paper is to demonstrate and explore quantitatively the extent to which one specific dialogue system can anticipate what"
N10-2009,J96-1002,0,0.0191004,"June 2010. 2010 Association for Computational Linguistics  mood :declarative    type : event     agent : captain − kirk          sem :  event : deliver  theme : power − generator           modal : possibility : can    speech − act : type : of f er  2.2 Figure 1: AVM utterance representation. manual transcription, and a gold-standard semantic frame, allowing us to develop and evaluate a datadriven NLU approach. 2.1 NLU in SASO-EN Virtual Humans Our NLU module for the SASO-EN system, mxNLU (Sagae et al., 2009), is based on maximum entropy classification (Berger et al., 1996) , where we treat entire individual semantic frames as classes, and extract input features from ASR. The NLU output representation is an attribute-value matrix (AVM), where the attributes and values represent semantic information that is linked to a domainspecific ontology and task model (Figure 1). The AVMs are linearized, using a path-value notation, as seen in the NLU input-output example below: • Utterance (speech): we are prepared to give you guys generators for electricity downtown • ASR (NLU input): we up apparently give you guys generators for a letter city don town • Frame (NLU output"
N10-2009,W09-3902,1,0.939406,"alogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (20"
N10-2009,N09-2014,1,0.943393,"tanding (NLU) and dialogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed ("
N10-2009,W09-3905,0,0.0130926,"Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction"
N10-2009,J09-3001,0,0.0153395,"ault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In s"
N10-2009,E09-1085,0,0.0614968,", where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the cont"
N12-3007,W09-3902,1,0.888375,"esult. By using precision and recall of frame elements, rather than simply looking at frame accuracy, we take into account that certain frames are more similar than others, and allow for cases when the correct frame is not in the training set. Each of our incremental confidence models makes a binary prediction for each partial NLU result as an utterance proceeds. At each time t durFigure 4: Visualization of Incremental Speech Processing. ing an utterance, we consider the current NLU FScore Ft as well as the final NLU F-Score Ffinal that will be achieved at the conclusion of the utterance. In (DeVault et al., 2009) and (DeVault et al., 2011a), we explored the use of data-driven decision tree classifiers to make predictions about these values, for example whether Ft ≥ 12 (current level of understanding is “high”), Ft ≥ Ffinal (current level of understanding will not improve), or Ffinal ≥ 12 (final level of understanding will be “high”). In this demonstration, we focus on the first and third of these incremental confidence metrics, which we summarize as “Now Understanding” and “Will Understand”, respectively. In an evaluation over all partial ASR results for 990 utterances in this new scenario, we found t"
N12-3007,hartholt-etal-2008-common,1,0.469101,"Missing"
N12-3007,N09-2014,1,0.785085,"as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). 26 <S&gt;.mood interrogative <S&gt;.sem.modal.desire want <S&gt;.sem.prop.agent utah <S&gt;.sem.prop.event providePublicServices <S&gt;.sem.prop.location town <S&gt;.sem.prop.theme sheriff-job <S&gt;.sem.prop.type event <S&gt;.sem.q-slot polarity <S&gt;.sem.speechact.type info-req <S&gt;.sem.type question Figure 3: Example of a corpus training example. Each partial ASR result then serves as an incremental input to NLU, which is specially trained for partial input as discussed in (Sagae et al., 2009). NLU is predictive in the sense that, for each partial ASR result, the NLU module produces as output the complete frame that has been associated by a human annotator with the user’s complete utterance, even if that utterance has not yet been fully processed by the ASR. For a detailed analysis of the performance of the predictive NLU, see (DeVault et al., 2011b). The second step in our framework is to train a set of incremental confidence models (DeVault et al., 2011a), which allow the agents to assess in real time, while a user is speaking, how well the understanding process is proceeding. Th"
N12-3007,N10-2009,1,0.845616,"ke trust and emotions, is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech, as the users are speaking (DeVault et al., 2011b). These responses could range from relatively straightforward turn management behaviors, like having a virtual human recognize when it is being addressed by a user utterance, and possibly turn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an imp"
N13-1129,W12-1641,0,0.0319271,"9; Schlangen et al., 2009; Heintze et al., 2010; DeVault et al., 2011a; Selfridge et al., 2012). This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses (Skantze and Schlangen, 2009), human-like feedback such as backchannels that indicate how well the system is understanding user speech (DeVault et al., 2011b; Traum et al., 2012), and more interactive response capabilities such as collaborative completions of user utterances (DeVault et al., 2011a), more adaptive handling of interruptions (Buschmeier et al., 2012), and others. This paper builds on techniques developed in previous work that has adopted a predictive approach to incremental NLU (DeVault et al., 2011a). On this approach, at specific moments while a user’s speech is in progress, an attempt is made to predict what the full meaning of the complete user utterance will be. Predictive models can be contrasted with explicit approaches to incremental NLU. We use the term explicit understanding to refer to approaches that attempt to determine the meaning that has been expressed explicitly in the user’s partial utterance so far (without predicting f"
N13-1129,hartholt-etal-2008-common,1,0.718754,"tanding to be easily combined within a dialogue system. It may therefore be a useful incremental understanding technique for some dialogue systems. 2 Technical Approach and Data Set In Sections 2.1-2.3, we briefly summarize the data set and approach to predictive incremental NLU (DeVault et al., 2011a) that serves as the starting point for the new work in this paper. Sections 2.4 and 2.5 present our new approach to explicit understanding based on this approach. 2.1 Data set For the experiments reported here, we use a corpus of user utterances collected with the SASO-EN spoken dialogue system (Hartholt et al., 2008; Traum et al., 2008). Briefly, this system is designed to allow a trainee to practice multi-party negotiation skills by engaging in face to face negotiation with virtual humans. The scenario involves a negotiation about the possible re-location of a medical clinic in an Iraqi village. A human trainee plays the role of a US Army captain, and there are two virtual humans that he negotiates with: Doctor Perez, the head of an NGO clinic, and a local village elder, al-Hassan. The captain’s main objective is to convince the doctor and the elder to move the clinic out of an unsafe marketplace area."
N13-1129,W10-4302,0,0.435701,"Missing"
N13-1129,N09-2014,1,0.954531,"supports a finite set of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches. 1 Introduction In recent years, there has been a growing interest among researchers in methods for incremental natural language understanding (NLU) for spoken dialogue systems; see e.g. (Skantze and Schlangen, 2009; Sagae et al., 2009; Schlangen et al., 2009; Heintze et al., 2010; DeVault et al., 2011a; Selfridge et al., 2012). This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses (Skantze and Schlangen, 2009), human-like feedback such as backchannels that indicate how well the system is understanding user speech (DeVault et al., 2011b; Traum et al., 2012), and more interactive response capabilities such as collaborative completions of user utterances (DeVault et al., 2011a), more adaptive handling of interruptions (B"
N13-1129,W09-3905,0,0.0188176,"t of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches. 1 Introduction In recent years, there has been a growing interest among researchers in methods for incremental natural language understanding (NLU) for spoken dialogue systems; see e.g. (Skantze and Schlangen, 2009; Sagae et al., 2009; Schlangen et al., 2009; Heintze et al., 2010; DeVault et al., 2011a; Selfridge et al., 2012). This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses (Skantze and Schlangen, 2009), human-like feedback such as backchannels that indicate how well the system is understanding user speech (DeVault et al., 2011b; Traum et al., 2012), and more interactive response capabilities such as collaborative completions of user utterances (DeVault et al., 2011a), more adaptive handling of interruptions (Buschmeier et al., 2012),"
N13-1129,W12-1638,0,0.124079,"proximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches. 1 Introduction In recent years, there has been a growing interest among researchers in methods for incremental natural language understanding (NLU) for spoken dialogue systems; see e.g. (Skantze and Schlangen, 2009; Sagae et al., 2009; Schlangen et al., 2009; Heintze et al., 2010; DeVault et al., 2011a; Selfridge et al., 2012). This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses (Skantze and Schlangen, 2009), human-like feedback such as backchannels that indicate how well the system is understanding user speech (DeVault et al., 2011b; Traum et al., 2012), and more interactive response capabilities such as collaborative completions of user utterances (DeVault et al., 2011a), more adaptive handling of interruptions (Buschmeier et al., 2012), and others. This paper builds on techniques developed in previous wor"
N13-1129,E09-1085,0,0.0342787,"ng in a dialogue system that supports a finite set of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches. 1 Introduction In recent years, there has been a growing interest among researchers in methods for incremental natural language understanding (NLU) for spoken dialogue systems; see e.g. (Skantze and Schlangen, 2009; Sagae et al., 2009; Schlangen et al., 2009; Heintze et al., 2010; DeVault et al., 2011a; Selfridge et al., 2012). This work has generally been motivated by a desire to make dialogue systems more efficient and more natural, by enabling them to provide lower latency responses (Skantze and Schlangen, 2009), human-like feedback such as backchannels that indicate how well the system is understanding user speech (DeVault et al., 2011b; Traum et al., 2012), and more interactive response capabilities such as collaborative completions of user utterances (DeVault et al., 2011a), more adaptive handling"
N16-3007,2005.sigdial-1.14,0,0.0452554,"The dialogue management logic is designed to deal with instances where the classifier cannot identify a good direct response. During training, NPCEditor calculates a response threshold based on the classifier’s confidence in the appropriateness of selected responses; at runtime, if the confidence for a selected response falls below the predetermined threshold, that response is replaced with an “off-topic” utterance that asks the user to repeat the question or takes initiative and changes the topic (Leuski et al., 2006); such failure to return a direct response, also called non-understanding (Bohus and Rudnicky, 2005), is usually preferred over returning an inappropriate one (misunderstanding). The current system uses a five-stage off-topic selection algorithm which is an extension of that presented in Artstein et al. (2009). Figure 1 shows a sample dialogue illustrating the handling of nonunderstanding. 33 User Hello Pinchas, how are you? Las Vegas how are you Pinchas Can you just repeat that? User Hello Pinchas, can you hear me? how thick is can you hear me Pinchas I can hear you, yeah. User Pinchas, can you tell me how old you are? Vegas can you tell me how old you are Pinchas I was born in nineteen thi"
N16-3007,W06-1303,1,0.539718,"tion-response pairs, which identifies the most appropriate response to new (unseen) user input. The dialogue management logic is designed to deal with instances where the classifier cannot identify a good direct response. During training, NPCEditor calculates a response threshold based on the classifier’s confidence in the appropriateness of selected responses; at runtime, if the confidence for a selected response falls below the predetermined threshold, that response is replaced with an “off-topic” utterance that asks the user to repeat the question or takes initiative and changes the topic (Leuski et al., 2006); such failure to return a direct response, also called non-understanding (Bohus and Rudnicky, 2005), is usually preferred over returning an inappropriate one (misunderstanding). The current system uses a five-stage off-topic selection algorithm which is an extension of that presented in Artstein et al. (2009). Figure 1 shows a sample dialogue illustrating the handling of nonunderstanding. 33 User Hello Pinchas, how are you? Las Vegas how are you Pinchas Can you just repeat that? User Hello Pinchas, can you hear me? how thick is can you hear me Pinchas I can hear you, yeah. User Pinchas, can y"
N16-3007,W15-4629,1,0.824999,"ciation for Computational Linguistics 2 Technical details In the New Dimensions in Testimony prototype, users talk to a persistent representation of a Holocaust survivor presented on a video screen, and a computer algorithm selects and plays individual video clips of the survivor in response to user utterances. The result is much like an ordinary conversation between the user and the survivor. The system has been described in detail in previous publications, covering the proof of concept (Artstein et al., 2014), the content elicitation process (Artstein et al., 2015), the language processing (Traum et al., 2015a), the full prototype (Traum et al., 2015b), and ethical considerations (Artstein and Silver, 2016). Here we give a brief description of the language processing technology and the system’s runtime components. 2.1 Language processing At the heart of the runtime computer system is a response classifier and dialogue management component called NPCEditor (Leuski and Traum, 2011), which selects a response to each user utterance. NPCEditor combines the functions of Natural Language Understanding (NLU) and Dialogue Management – understanding the utterance text and selecting an appropriate response."
P05-3023,melvin-etal-2004-creation,1,0.752599,"ive Poster and Demonstration Sessions, c pages 89–92, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ASR English Prompts or TTS English GUI: prompts, confirmations, ASR switch Prompts or TTS Farsi Dialog Manager ASR Farsi MT SMT English to Farsi Farsi to English English to Farsi Farsi to English Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the individual components interact. our own large-scale simulated doctor-patient dialogue corpus based on recordings of medical students examining standardized patients (details in Belvin et al. 2004).1 The Farsi acoustic models r equired an eclectic approach due to the lack of existing labeled speech corpora. The approach included borrowing acoustic data from English by means of developing a sub-phonetic mapping between the two languages, as detailed in (Srinivasamurthy & Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our stan"
P14-1047,P08-1071,0,0.0302999,"hen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data ar"
P14-1047,W10-4321,1,0.707826,"Missing"
P14-1047,W13-4016,1,0.924299,"rk Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU res"
P14-1047,J08-4002,1,0.320034,"cies in complex dialogue scenarios. This ability of multi-agent RL can also have important implications for learning via live interaction with human users. Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants. Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can b"
P14-1047,H05-1127,0,0.532687,"dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes (Engel et al., 2005; Rasmussen and Williams, 2006). Typically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. This fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users (Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). English and Heeman (2005) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies. However, English and Heeman (2005) did not use multiagent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method (Sutton and Barto, 1998). But single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Indeed, English and Heeman (2005) reported problems with convergence. Chandramohan et al. (2012) proposed a framework for co-adaptation of the dialogue policy and the SU using s"
P14-1047,N13-2013,0,0.0113492,"ts prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains. So far research on RL in the dialogue community has focused on using single-agent RL techniques where the stationary environment is the user. Most approaches assume that the user goal is fixed and that the behavior of the user is rational. Other approaches account for changes in user goals (Ma, 2013). In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work. However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold. With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling. English and Heeman (2005) learned neg"
P14-1047,W10-4339,0,0.151152,"lti-agent RL can also have important implications for learning via live interaction with human users. Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants. Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the"
P14-1047,W12-1611,1,0.525806,"at learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants. Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic S"
P14-1047,W11-2813,0,\N,Missing
P18-4016,W15-4629,1,0.822936,"sponse of “Moving..” vs “Turning...” as seen in #4 and #7 in Figure 2. VHMSG includes several protocols that implement parts of the Virtual Human architecture. We use the protocols for speech recognition, as well as component monitoring and logging. The NPCEditor and other components that use these protocols are available as part of the ICT Virtual Human Toolkit (Hartholt et al., 2013). These protocols have also been used in systems, as reported by Hill et al. (2003) and Traum et al. (2012, 2015). In particular, we used the adaptation of Google’s Automated Speech Recognition (ASR) API used in Traum et al. (2015). The NPCEditor (Leuski and Traum, 2011) was used for Natural Language Understanding (NLU) and dialogue management. The new ros2vhmsg component for bridging the messages was used to send instructions from the NPCEditor to the automated RN. 3.1.1 NPCEditor We implemented NLU using the statistical text classifier included in the NPCEditor. The classifier learns a mapping from inputs to outputs from training data using cross-language retrieval models (Leuski and Traum, 2011). The dialogues collected from our first two experimental phases served as training data, and consisted of 1,500 pairs of Co"
P18-4016,L18-1017,1,0.460343,"versations are linked together. Rather than just responding to input in a single conversation, the DM-Wizard in our first project phases often translates input from one conversational floor to another (e.g., from the Commander to the RNWizard, or visa versa), or responds to input with messages to both the Commander and the RN. These responses need to be consistent (e.g. translating a command to the RN should be accompanied by positive feedback to the Commander, while a clarification to the commander should not include an RN action command). Using the dialogue relation annotations described in Traum et al. (2018), we trained a hybrid classifier, including translations to RN if they existed, and negative feedback to the Commander where they did not. We also created a new dialogue manager policy that would accompany RN commands with appropriate positive feedback to the commander, e.g., response of “Moving..” vs “Turning...” as seen in #4 and #7 in Figure 2. VHMSG includes several protocols that implement parts of the Virtual Human architecture. We use the protocols for speech recognition, as well as component monitoring and logging. The NPCEditor and other components that use these protocols are availab"
P94-1001,J86-3001,0,0.0742002,"Missing"
robinson-etal-2004-issues,traum-etal-2004-evaluation,1,\N,Missing
robinson-etal-2004-issues,P94-1001,1,\N,Missing
robinson-etal-2008-ask,2007.sigdial-1.15,1,\N,Missing
robinson-etal-2008-ask,W06-1303,1,\N,Missing
traum-etal-2004-evaluation,P97-1035,0,\N,Missing
W00-0205,P97-1020,1,0.926023,"vergences (whether the object precedes or follows the verb, for example) are represented in language specific linearization rules; lexical divergences (whether the location argument is encoded directly in the verb, e.g. the English verb pocket or must be saturated by an exterfial argument) are stated in terms of the pieces of LCS struct-ure in the lexicon. SententiM representations derive from saturating the arguments required by the predicates in the sentence. LCS rePresentations also include temporal information, where available in the source language: recent revisions include, for example (Dorr and Olsen, 1997a) standardizing LCS representations for the aspectual (un)boundedness ((A)TELICITY) o f events, either lexically or sententially represented. Although at present the LCS encodes no supra-sentential discourse relations, we show how the lexical aspect information may be used to generate discourse coherence in temporal structure. Relations between clauses as constrained by temporal reference has been examined in an LCS framework by Dorr and Gaasterland (Dorr and Gaasterland, 1995). They explore how temporal connectives are constrained in interpretation, based on the tense of the clauses they con"
W00-0205,dorr-etal-1998-thematic,1,0.768764,"et of templates that verbs with related meanings will fit into (Olsen et al., 1998). In the Chinese-English interlingual system we describe, the Chinese is first mapped into the LCS, a languageindependent representation, from which the targetlanguage sentence is generated. Since telicity (and other aspects of event structure) are uniformly represented at the lexical and the sentential level, telicity mismatches between verbs of different languages may then be compensated for by combining verbs with other .components. R e a l i z a t i o n The AMR structure is then linearized, as described in (Dorr et al., 1998), and morphological realization is performed. The result is a lattice of possible realizations, representing both the preserved ambiguity from previous processing phases and multiple ways of linearizing the sentence. E x t r a c t i o n The final stage uses a statistical bigram extractor to pick an approximation of the most fluentrealization (Langkilde and Knight, 1998b). While there are several possible ways to address the tense and discourse connective issues mentioned above, such as modifying the LCS primitive elements and/or the composition of the LCS from the source language, we instead h"
W00-0205,P98-1116,0,0.015863,"xical and the sentential level, telicity mismatches between verbs of different languages may then be compensated for by combining verbs with other .components. R e a l i z a t i o n The AMR structure is then linearized, as described in (Dorr et al., 1998), and morphological realization is performed. The result is a lattice of possible realizations, representing both the preserved ambiguity from previous processing phases and multiple ways of linearizing the sentence. E x t r a c t i o n The final stage uses a statistical bigram extractor to pick an approximation of the most fluentrealization (Langkilde and Knight, 1998b). While there are several possible ways to address the tense and discourse connective issues mentioned above, such as modifying the LCS primitive elements and/or the composition of the LCS from the source language, we instead have been experimenting for the moment with solutions implemented within the generation component. The only extensions to the LCS language have been loosening of the constraint against direct modification of states and events by other states and events (thus allowing composed LCSes to be formed from Chinese with these structures, but creating a challenge for fluent gene"
W00-0205,W98-1426,0,0.0128985,"xical and the sentential level, telicity mismatches between verbs of different languages may then be compensated for by combining verbs with other .components. R e a l i z a t i o n The AMR structure is then linearized, as described in (Dorr et al., 1998), and morphological realization is performed. The result is a lattice of possible realizations, representing both the preserved ambiguity from previous processing phases and multiple ways of linearizing the sentence. E x t r a c t i o n The final stage uses a statistical bigram extractor to pick an approximation of the most fluentrealization (Langkilde and Knight, 1998b). While there are several possible ways to address the tense and discourse connective issues mentioned above, such as modifying the LCS primitive elements and/or the composition of the LCS from the source language, we instead have been experimenting for the moment with solutions implemented within the generation component. The only extensions to the LCS language have been loosening of the constraint against direct modification of states and events by other states and events (thus allowing composed LCSes to be formed from Chinese with these structures, but creating a challenge for fluent gene"
W00-0205,J88-2003,0,0.256785,"Missing"
W00-0205,olsen-etal-1998-enhancing,1,0.885716,"Missing"
W00-0205,J88-2005,0,0.111442,"Missing"
W00-0205,C98-1112,0,\N,Missing
W00-0207,dorr-etal-1998-thematic,1,0.387978,"Missing"
W00-0207,A97-1021,0,0.017779,": Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 (1) (act_on l o c (* t h i n g 1) (* t h i n g 2) ((* [on] 23) l o c (*head*) ( t h i n g 24)) ( c u t + i n g l y 26) (down+/m)) Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UNITRAN (Dorr et al., 1993) and MILT (Dorr, 1997). An LCS is adirected graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LCS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with verbs prepositions, adverbs, adjectives and nouns. Within each of these The top node in the. RLCS has the structural primitive ACT_ON in the locational field. Its subject is a star-marked LCS, meaning a subordinate RLCS needs to be filled in here to form a complete event. It also has the restriction that the filler LCS be of the type thing. The number &quot;1&quot; in"
W00-0207,P98-1116,0,0.644006,"e translation and cross-language processing. Such representations are becoming fairly popular, yet there are widely different views about what these languages should be composed of, varying from purely conceptual knowledge-representations, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 (1) (act_on l o c (* t h i n g 1) (* t h i n g 2) ((* [on] 23) l o c (*head*) ( t h i n g 24)) ( c u t + i n g l y 26) (down+/m)) Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UNITRAN (Dorr et al., 1993) and MILT (Dorr, 1997). An LCS is adirected graph with a root. Each node is associated with certain information, including a type, a"
W00-0207,W98-1426,0,0.532831,"e translation and cross-language processing. Such representations are becoming fairly popular, yet there are widely different views about what these languages should be composed of, varying from purely conceptual knowledge-representations, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 (1) (act_on l o c (* t h i n g 1) (* t h i n g 2) ((* [on] 23) l o c (*head*) ( t h i n g 24)) ( c u t + i n g l y 26) (down+/m)) Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UNITRAN (Dorr et al., 1993) and MILT (Dorr, 1997). An LCS is adirected graph with a root. Each node is associated with certain information, including a type, a"
W00-0207,C98-1112,0,\N,Missing
W06-1303,J99-3003,0,0.00945123,"ult to compactly characterize the mapping from the text surface form to the meaning; and the second is the error-prone output from the speech recognition module. One possible approach to creating a language understanding system is to design a set of rules that select a response given an input text string (Weizenbaum, 1966). Because of uncertainty this approach can quickly become intractable for anything more than the most trivial tasks. An alternative is to create an automatic system that uses a set of training question-answer pairs to learn the appropriate question-answer matching algorithm (Chu-Carroll and Carpenter, 1999). We have tried three different methods for the latter approach, described in the rest of this section. 3.1 Text Classification The answer selection problem can be viewed as a text classification task. We have a question text 19 ceived by the system, RQ is the set of all the answers appropriate to that question, and P (w|RQ ) is the probability that a word randomly sampled from an appropriate answer would be the word w. The language model of Q is the set of probabilities P (w|RQ ) for every word in the vocabulary. If we knew the answer set for that question, we can easily estimate the model. U"
W06-1313,W03-0205,0,0.0437234,"Missing"
W06-1313,P96-1009,0,0.0181388,"engaging in the dialogue and the Radiobot-CFF system is just observing. In section 2, we describe the training applicaWe present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cas"
W06-1313,N03-1028,0,0.00675885,"ecides what the artillery fire should be and inputs it to a GUI for the simulator. It is our goal to replace those two trainers with one trainer focusing on assessment while Radiobot-CFF handles the radio communications and interfaces with the virtual world. Radiobot-CFF is composed of several pipelined components. A Speech Recognition component is implemented using the SONIC speech recognition system (Pellom, 2001) with custom language and acoustic models. An Interpreter component tags the ASR output with its its dialogue move and parameter labels using two separate Conditional Random Field (Sha and Pereira, 2003; McCallum, 2002) taggers trained on hand-annotated utterances. A Dialogue Manager processes the tagged output, sending a reply to the FO (via a template-based Generator) and, when necessary, a message to the artillery simulator FireSim XXI1 to make decisions on what type of fire to send. The reply to FO and messages to simulator are mediated by GUIs where the trainer can intervene if 1 1 FO 2 FDC 3 FO 4 FDC 5 6 7 FO FDC FDC 8 FO 9 10 11 12 13 14 15 16 17 18 19 FDC FO FDC FO FO FDC FDC FO FDC FO FO 20 FDC steel one niner this is gator niner one adjust fire polar over gator nine one this is ste"
W06-1313,walker-etal-2000-evaluation,0,0.0145643,"imulator or trainee, and a passive mode in which the operator is engaging in the dialogue and the Radiobot-CFF system is just observing. In section 2, we describe the training applicaWe present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue manag"
W06-1313,P02-1048,0,0.0662311,"Missing"
W06-1313,A00-2001,1,0.867531,"Missing"
W08-0107,2005.sigdial-1.18,0,0.0120208,"been grounded. This model has been developed and evaluated by a corpus analysis, and includes a set of types of evidence of understanding, a set of degrees of groundedness, a set of grounding criteria, and methods for identifying each of these. We describe how this model can be used for dialogue management. 1 Introduction Dialogue system researchers are active in investigating ways of detecting and recovering from error, including determining when to provide confirmations or rejections, or how to handle cases of complete non-understanding (Bohus and Rudnicky, 2005a; Bohus and Rudnicky, 2005b; Skantze, 2005). Studying the strategies that humans use when speaking amongst themselves can be helpful (Swerts et al., 2000; Paek, 2003; Litman et al., 2006). One approach to studying how humans manage errors of understanding is to view conversation as a joint activity, in which grounding, or the process of adding material to the common ground between speakers, plays a central role (Clark and Schaefer, 1989). From this perspective, conversations are highly coordinated efforts in which participants work together to ensure that knowledge is properly understood by all participants. There is a wide variety of"
W08-0107,H05-1029,0,\N,Missing
W08-0107,J06-3004,0,\N,Missing
W08-0107,2005.sigdial-1.14,0,\N,Missing
W08-0127,J08-4004,0,0.0463664,"n a scale of 1 to 7, with 1 being very incoherent and 7 being perfectly coherent. We did not provide any additional instructions or examples of scale as we wanted to capture the intuitive idea of coherence from our judges. Within each set the dialogue permutations were presented in random order. We compute the inter-rater agreement by using Pearson’s correlation analysis. We correlate the ratings given by each judge with the average ratings given by the judges who were assigned the same set. For inter-rater agreement we report the average of 9 such correlations which is 0.73 (std dev = 0.07). Artstein and Poesio (2008) have argued that Krippendorff’s α (Krippendorff, 2004) can be used for interrater agreement with interval scales like the one we have. In our case for the three sets α values were 0.49, 0.58, 0.64. These moderate values of alpha indicate that the task of judging coherence is indeed a difficult task, especially when detailed instructions or examples of scales are not given. In order to assess whether Kendall’s τ can be used as an automatic measure of dialogue coherence, we perform a correlation analysis of τ values against the average ratings by human judges. The Pearson’s correlation coeffici"
W08-0127,P05-1018,0,0.270871,"guage processing tasks. The general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, c Columbus, June 2008. 2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models. In the next section, we look at related work in evaluation of dialogue systems. Section 3 s"
W08-0127,J00-4006,0,0.00756246,"tion, we look at related work in evaluation of dialogue systems. Section 3 summarizes the information ordering task and Lapata’s (2006) findings. It is followed by the details of the experiments we carried out and our observations. We conclude with a summary future work directions. 2 Related Work Most of the work on evaluating dialogue systems focuses on human-machine communication geared towards a specific task. A variety of evaluation metrics can be reported for such task-oriented dialogue systems. Dialogue systems can be judged based on the performance of their components like WER for ASR (Jurafsky and Martin, 2000), concept error rate or F-scores for NLU, understandability for speech synthesis etc. Usually the core component, the dialogue model - which is responsible for keeping track of the dialogue progression and coming up with an appropriate response, is evaluated indirectly. Different dialogue models can be compared with each other by keeping the rest of components fixed and then by comparing the dialogue systems as a whole. Dialogue systems can report subjective measures such as user satisfaction scores and perceived task completion. SASSI (Hone and Graham, 2000) prescribes a set of questions used"
W08-0127,P03-1069,0,0.243031,"the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002). Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue models"
W08-0127,W04-1013,0,0.0127513,"forbid the use of data-driven machine learning components. For these reasons, using an automatic evaluation measure as an understudy is quickly becoming a common practice in natural language processing tasks. The general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, c Columbus, June 2008. 2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the informat"
W08-0127,P03-1021,0,0.0194826,"Missing"
W08-0127,2001.mtsummit-papers.68,0,0.125999,". Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. 1 Introduction In computer science or any other research field, simply building a system that accomplishes a certain goal is not enough. It needs to be thoroughly evaluated. One might want to evaluate the system just to see to what degree the goal is being accomplished or to compare two or more systems with one another. Evaluation can also lead to understanding the shortcomings of the system and the reasons for these. Finally the evaluation results can be used as feedback in improving the system. The best way to evaluate a novel"
W08-0127,N07-2038,0,0.0131055,"train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. 173 Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., for"
W08-0127,P06-2103,0,0.0747856,"general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, c Columbus, June 2008. 2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models. In the next section, we look at related work in evaluation of dialogue systems. Section 3 summarizes the information"
W08-0127,P94-1001,1,0.541907,"context. Fig 4 shows the relationship between these two measures. Notice that most of the orderings have τ values around zero (i.e. in the middle range for τ ), whereas majority of orderings will have a low value for (b2 + b3 ) /2. τ seems to overestimate the coherence even in the absence of immediate local coherence (See third entry in table 1). It seems that local context is more important for dialogues than for discourse, which may follow from the fact that dialogues are produced by two speakers who must react to each other, while discourse can be planned by one speaker from the beginning. Traum and Allen (1994) point out that such social obligations to respond and address the contributions of the other should be an important factor in building dialogue systems. The information ordering paradigm does not take into account the content of the information-bearing items, e.g. the fact that turns like ”yes”, ”I agree”, 5 This value is calculated by considering all 14400 permutations as equally likely. (a) Histogram of Kendall’s τ for reordered sequences (b) Histogram of fraction of bigrams & trigrams values for reordered sequences Figure 3: Experiment 3 - upper baseline for information ordering task (huma"
W08-0127,traum-etal-2004-evaluation,1,0.891449,"Missing"
W08-0127,P02-1040,0,\N,Missing
W08-0127,J06-4002,0,\N,Missing
W08-0127,D08-1076,0,\N,Missing
W08-0130,W98-1425,0,0.246218,"mar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003; Busemann and Horacek, 1998). This development cost is exacerbated when a dialogue system’s native meaning representation scheme is under revision. In this paper, we survey a new example-based approach (DeVault et al., 2008) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems. Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms. This is achieved through a streamlined authoring task that does not re"
W08-0130,P01-1017,0,0.0291106,"polarity = negative speech-act.content.attribute = resourceAttribute speech-act.content.value = medical-supplies speech-act.content.object-id = market  addressee = captain-kirk dialogue-act.addressee = captain-kirk  speech-act.addressee = captain-kirk Figure 3: A generation training example for Doctor Perez. suggests the output utterance u = we don’t have medical supplies here captain. Each utterance u is accompanied by syntax(u), a syntactic analysis in Penn Treebank format (Marcus et al., 1994). In this example, the syntax is a hand-corrected version of the output of the Charniak parser (Charniak, 2001; Charniak, 2005) on this sentence; we discuss this hand correction in Section 4. To represent the meaning of utterances, our approach assumes that the system provides some set M = {m1 , ..., mj } of semantic representations. The meaning of any individual utterance is then identified with some subset of M . For Doctor Perez, M comprises the 232 distinct key-value pairs that appear in the system’s various generation frames. In this example, the utterance’s meaning is captured by the 8 key-value pairs indicated in the figure. Our approach requires the generation content author to link these 8 ke"
W08-0130,J05-1003,0,0.0397406,"st applications. The second step of automated processing, then, uses the training examples to learn an effective search policy so that good output sentences can be found in a reasonable time frame. The solution we have developed employs a beam search strategy that uses weighted features to rank alternative grammatical expansions at each step. Our algorithm for selecting features and weights is based on the search optimization algorithm of (Daumé and Marcu, 2005), which decides to update feature weights when mistakes are made during search on training examples. We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. 4 Empirical Evaluation In the introduction, we identified run-time speed, adequacy of coverage, authoring burdens, and NLG re202 quest specification as important factors in the selection of a technology for a dialogue system’s NLG component. In this section, we evaluate our technique along these four dimensions. Hand-authored utterances. We collected a sample of 220 instances of frames that Doctor Perez’s dialogue manager had requested of the generation component in previous dialogues with users. Some frames occurred more than once"
W08-0130,W08-1111,1,0.718471,"rwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003; Busemann and Horacek, 1998). This development cost is exacerbated when a dialogue system’s native meaning representation scheme is under revision. In this paper, we survey a new example-based approach (DeVault et al., 2008) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems. Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms. This is achieved through a streamlined authoring task that does not require detailed linguistic knowledge. Our approach then processes these training examples to automatically construct all the resources needed for a fast, highquality, run-time grammar-based generat"
W08-0130,P98-1116,0,0.0604384,"2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, c Columbus, June 2008. 2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom g"
W08-0130,W02-2103,0,0.0412378,"t exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, c Columbus, June 2008. 2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be require"
W08-0130,W06-1303,1,0.85619,"Missing"
W08-0130,2007.mtsummit-ucnlg.4,0,0.162385,"or which attractive methodologies do not yet exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, c Columbus, June 2008. 2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation mi"
W08-0130,J93-2004,0,\N,Missing
W08-0130,C98-1112,0,\N,Missing
W08-0130,P00-1058,0,\N,Missing
W08-0130,2005.sigdial-1.25,1,\N,Missing
W08-1111,P04-1011,0,0.0549318,"99; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figu"
W08-1111,W02-0111,0,0.0171852,"rees, (2) enforcing consistency in the finiteness of VP and S complements, and (3) restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees. Automatic Grammar Induction We adopt essentially the probabilistic tree-adjoining grammar (PTAG) formalism and grammar induction technique of (Chiang, 2003). Our approach makes three modifications, however. First, while Chiang’s model includes both full adjunction and sister adjunction operations, our grammar has only sister adjunction (left and right), exactly as in the TAGLET grammar formalism of (Stone, 2002). Second, to support lexicalization at an arbitrary granularity, we allow Chiang’s tree templates to be associated with more than one lexical anchor. Third, to unify syntactic and semantic reasoning in search, we augment lexical anchors with semantic information. Formally, wherever Chiang’s model has a lexical anchor w, ours has a pair (hw1 , ..., wn i, M ′ ), where M ′ ⊆ M is connected to lexical anchors hw1 , ..., wn i by the generation content author, as in Figure 1. The result is that the derivation probabiliIn the second stage, the complements and adjuncts in the decorated trees are incre"
W08-1111,N01-1001,0,0.226182,"generation component into an existing application. We believe this approach will broaden the class of applications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). A third strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 paper is the generation of output utterances for a particular virtual human, Doctor Perez, who is designed to teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. The design goals for Doctor Perez create a number of requirements for a practical NLG component. We briefly summarize these requirements here; see (DeVault et al., 2008) for more details. Doctor Perez has a relatively rich internal mental state including bel"
W08-1111,N01-1003,0,0.0213809,"iang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figure 1. In this example,"
W08-1111,2007.mtsummit-ucnlg.4,0,0.337341,"Missing"
W08-1111,N07-1022,0,0.145547,"needed to integrate a grammar-based generation component into an existing application. We believe this approach will broaden the class of applications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). A third strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 paper is the generation of output utterances for a particular virtual human, Doctor Perez, who is designed to teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. The design goals for Doctor Perez create a number of requirements for a practical NLG component. We briefly summarize these requirements here; see (DeVault et al., 2008) for more details. Doctor Perez has a relatively"
W08-1111,W98-1425,0,0.642771,"widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003). Thus, there currently are many applications where using a widecoverage generator remains impractical. We present a technique that opens up grammar-based generation to a wider range of practical applications by dramatically reducing the development costs and linguistic expertise that are required. Our method infers the grammatical resources needed for generation from a set of declarative examples that link surface expressions directly to the applicat"
W08-1111,P06-1130,0,0.170417,"Missing"
W08-1111,P01-1017,0,0.0292359,"l representations that would be necessary to specify semantics down to the lexers a bug or disfluency in the NLG output, it is better if she can fix it directly rather than requiring a (computational) linguist to do so. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In"
W08-1111,P00-1058,0,0.0445926,"approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 20"
W08-1111,J05-1003,0,0.219574,"tput sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training exam"
W08-1111,W08-0130,1,0.734384,"rd strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 paper is the generation of output utterances for a particular virtual human, Doctor Perez, who is designed to teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. The design goals for Doctor Perez create a number of requirements for a practical NLG component. We briefly summarize these requirements here; see (DeVault et al., 2008) for more details. Doctor Perez has a relatively rich internal mental state including beliefs, goals, plans, and emotions. He uses an attribute-value matrix (AVM) semantic representation to describe an utterance as a set of core speech acts and other dialogue acts. Speech acts generally have semantic contents that describe propositions and questions about states and actions in the domain. To facilitate interprocess communication, and statistical processing, this AVM structure is linearized into a “frame” of key values in which each non-recursive terminal value is paired with a path from the ro"
W08-1111,P98-1116,0,0.379699,"Missing"
W08-1111,W02-2103,0,0.226409,"Missing"
W08-1111,P95-1037,0,0.205764,"o so. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time windo"
W08-1111,J93-2004,0,\N,Missing
W08-1111,J03-4003,0,\N,Missing
W08-1111,C98-1112,0,\N,Missing
W09-3704,C04-1181,0,0.0310464,"ntences explicitly claim an ordering of two items on a relevant scale. However, with more context, a comparative can implicate degrees on a scale as well. If we know the degree of one of the items then we have a fixed range for the other item. If this range contains only one possible degree, then the comparative also sets the degree for this item. In the simplest possible scale in which a comparative could be applied, there are only two degrees, and a comparative indicates the degrees of each item even without more knowledge of the degrees of either item. For example, consider (9) inspired by [5]: (9) B is drawing and A is giving instructions to B: A: Draw two squares, one bigger than the other. B: Done. A: Now, paint the small square blue and the big one red. ; A thinks that one of the squares can be described as small and the other as big. The question remains, however, of how the scales are selected? These are not generalized implicatures that always arise. Every time we say “Fred is taller than Tom” we don’t mean that “Fred is tall” and “Tom is short”. As discussed in Section 2, conversational implicatures are affected by conversational goals. We believe that this is the path that"
W09-3902,E09-1085,0,0.188525,"Missing"
W09-3902,J96-1002,0,0.0158956,"Missing"
W09-3902,P03-1070,0,\N,Missing
W09-3902,N09-2014,1,\N,Missing
W09-3902,hartholt-etal-2008-common,1,\N,Missing
W10-4345,W08-0127,1,0.829868,"b-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: ◦ Is there a good enough utterance to select? ◦ How good is the selection algorithm at finding this utterance? We have previously attempted to address the second question, by proposing the information ordering task for evaluating dialogue coherence (Gandhe and Traum, 2008). Here we try to address the first question, which would provide a theoretical upper bound in quality for any selection approach. We examine a number of different dialogue corpora as to their suitability for the selection approach. We make the following assumptions to allow automatic evaluation across a range of corpora. Actual human dialogues represent a gold-standard for computer systems to emulate; i.e. choosing an actual utterance in the correct place is the best possible result. Other utterances can be evaluated as to how close they come to the original utterance, We perform a study of ex"
W10-4345,W06-1303,1,0.757834,"0292 {gandhe,traum}@ict.usc.edu Abstract output is not just text but presented as speech, the system may easily use recorded audio clips rather than speech synthesis. This argument also extends to multi-modal performances, e.g. using artist animation motion capture or recorded video for animating virtual human dialogue characters. Often one is willing to sacrifice some generality in order to achieve more human-like behavior than is currently possible from generation approaches. The selection approach has been used for a number of dialogue agents, including questionanswering characters at ICT (Leuski et al., 2006; Artstein et al., 2009; Kenny et al., 2007), FAQ bots (Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) and web-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: ◦ Is there a good enough utterance to select? ◦ How good is the selection algorithm at finding this utterance? We have previously att"
W10-4345,sellberg-jonsson-2008-using,0,0.183316,"Missing"
W10-4345,yao-etal-2010-practical,1,0.829749,"areers in the U.S. Army. The corpus consists of trained handlers presenting the system. Amani is a bargaining character used as a prototype for training soldiers to perform tactical questioning. The SASO system is a negotiation training prototype in which two virtual characters negotiate with a human “trainee” about moving a medical clinic. The Radiobots system is a training prototype that responds to military calls for artillery fire. IOTA is an extension of the Radiobots system. The corpus consists of training sessions between a human trainee and a human instructor on a variety of missions. Yao et al. (2010) provides details about the ICT corpora. Other corpora involved dialogues between two people playing specific roles in planning, scheduling problem for railroad transportation, the Trains-93 corpus (Heeman and Allen, 1994) and for emergency services, the Monroe corpus (Stent, 2000). The Switchboard corpus (Godfrey et al., 1992) consists of telephone conversations between two people, based on provided topics. We divided the data from each corpus into a training set and a test set, as shown in Table 1. The data consists of utterances from one or more human speakers who engage in dialogue with ei"
W11-2030,bunt-2006-dimensions,0,0.0317849,"r arguments that appeal to emotions. On the other hand, people from Eastern collectivistic cultures are more likely to use arguments in which the beneficiary is not themselves. Furthermore, Arab cultures tend to favor more indirect ways of argumentation and expression (Koch, 1983; Zaharna, 1995). ∗ Now at the University of Texas at San Antonio. In order to analyze negotiation in detail, including aspects such as persuasion, negotiation, and crosscultural differences, we have developed a novel annotation scheme. General purpose annotation schemes such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2006) represent moves in the dialogue but do not capture enough details of the interaction to distinguish between different styles of persuasion and argumentation, especially cross-cultural differences. Our goal for developing this coding scheme is two-fold. First, we aim to fill the gap in the literature of cross-cultural argumentation and persuasion. To our knowledge this is the first annotation scheme designed specifically for coding cross-cultural argumentation and persuasion strategies. Previous work on cross-cultural negotiation, e.g. Brett and Gelfand (2006), has not focused on argumentation"
W11-2030,J87-1002,0,0.173196,"science (Sidner, 1994; Ros´e and Torrey, 2004). Our specific focus is on the role of argumentation and per272 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 272–278, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics suasion. Sycara (1990) studied the role of argumentation in negotiation with regard to the role of arguments in changing the decision process of the interlocutor. Most attempts have focused on studying the structure of argumentation and persuasion, often using formal logic (Cohen, 1987; Prakken, 2008). Dung (1995) showed that argumentation can be viewed as a special form of logic programming with negation as failure. An argumentation scheme is defined as a structure or template for forming an argument. Schemes are necessary for identifying arguments, finding missing premises, analyzing arguments, and evaluating arguments (Pollock, 1995; Katzav and Reed, 2004; Walton et al., 2008). Recently, there has been some work on using machine learning techniques for automatically interpreting (George et al., 2007) and generating arguments (Zukerman, 2001). Note also the work of Piwek"
W11-2044,W06-1303,1,\N,Missing
W11-2044,leuski-traum-2010-npceditor,1,\N,Missing
W12-1611,aggarwal-etal-2012-twins,1,0.77528,"e though) and do not have the objective of reducing the search space and retrieving results from a database of e.g. restaurants, flights, etc. Thus examples of question-answering 84 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84–93, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics characters can be virtual interviewees (that can answer questions, e.g. about an incident), virtual scientists (that can answer general science-related questions), and so forth. For our experiments we use a corpus (Aggarwal et al., 2012) of interactions of real users with two virtual characters, the Twins, that serve as guides at the Museum of Science in Boston (Swartout et al., 2010). The role of these virtual characters is to entertain and educate the museum visitors. They can answer queries about themselves and their technology, generally about science, as well as questions related to the exhibits of the museum. An example interaction between a museum visitor and the Twins is shown in Figure 1. The dialogue policy of the Twins was arbitrarily hand-crafted (see section 7 for details) and many other policies are possible (in"
W12-1611,P08-1071,0,0.194347,"Creative Technologies, Playa Vista, CA, USA teruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.edu Abstract of thousands of dialogues to achieve good performance. Because it is very difficult to collect such a large number of dialogues with real users, instead, simulated users (SUs), i.e. models that simulate the behavior of real users, are employed (Georgila et al., 2006). Through the interaction between the system and the SUs thousands of dialogues can be generated and used for learning. A good SU should be able to replicate the behavior of a real user in the same dialogue context (Ai and Litman, 2008). We use Reinforcement Learning (RL) to learn question-answering dialogue policies for a real-world application. We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL. Our learned policy outperforms two baselines (including the original dialogue policy that was used for collecting the corpus) in a"
W12-1611,W10-4321,1,0.687146,"Missing"
W12-1611,W06-1303,1,0.673927,"capabilities, e.g. (J¨onsson et al., 2004; op den Akker et al., 2005; Varges et al., 2009). Most of these systems are designed for information extraction from structured or unstructured databases in closed or open domains. One could think of them as adding dialogue capabilities to standard question-answering systems such as the ones used in the TREC question-answering track (Voorhees, 2001). Other work has focused on a different type of question-answering dialogue, i.e. question-answering dialogues that follow the form of an interview and that can be used, for example, for training purposes (Leuski et al., 2006; Gandhe et al., 2009). But none of these systems uses RL. To our knowledge no one has used RL for learning policies for question-answering systems as defined in section 1. Note that Rieser and Lemon (2009) used RL for question-answering, but in their case, question-answering refers to asking for information about songs and artists in an mp3 database, which is very much like a slot-filling task, i.e. the system has to fill a number of slots (e.g. name of band, etc.) in order to query a database of songs and present the right information to the user. As discussed in section 1 our task is rather"
W12-1611,W10-4339,1,0.405154,"Dialogue Challenge. tion 2 we present related work. Section 3 provides a brief introduction to RL and section 4 describes our corpus. Then in section 5 we explain how we built our SU from the corpus, and in section 6 we describe our learning methodology. Section 7 presents our evaluation results. Finally section 8 presents some discussion and ideas for future work together with our conclusion. 2 Related Work To date, RL has mainly been used for learning dialogue policies for slot-filling applications such as restaurant recommendations (Jurˇc´ıcˇ ek et al., 2012), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc., largely ignoring other types of dialogue. Recently there have been some experiments on applying RL to the more difficult problem of learning negotiation policies (Heeman, 2009; Georgila and Traum, 2011a; Georgila and Traum, 2011b). Also, RL has been applied to tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011). There has been a lot of work on developing question-answering systems with dialogue capabilities, e.g. (J¨onsson et al., 2004; op den Akker et al., 2005; Varges et al., 2009). Most of these systems are designed for inf"
W12-1618,hartholt-etal-2008-common,1,0.843221,"cenarios, we have developed an approach to incremental speech understanding. The understanding models are trained using a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). Every utterance in the corpus is annotated with an utterance meaning, which is represented using a frame. Each frame is an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen at the lower left in Figure 2. Our framework uses this corpus to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. We briefly summarize these two models here; for additional details and motivation for this framework, and discussion of alternative approaches, see (DeVault et al., 2011b; DeVault et al., 2011a). The first step is to train a predictive incremental understanding model. This model is based on maxi131 Proceedings of th"
W12-1618,N09-2014,1,0.8185,"remental understanding model. This model is based on maxi131 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 131–133, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics mum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). Each partial ASR result serves as an incremental input to NLU, which is specially trained for partial input as discussed in (Sagae et al., 2009). NLU is predictive in the sense that, for each partial ASR result, the NLU module tries to output the complete frame that a human annotator would associate with the user’s complete utterance, even if that utterance has not yet been fully processed by the ASR. The second step in our framework is to train a set of incremental confidence models (DeVault et al., 2011a), which allow the agents to assess in real time, while a user is speaking, how well the understanding process is proceeding. The incremental confidence models build on the notion of NLU F-score, which we use to quantify the quality"
W12-1618,W09-3902,1,\N,Missing
W12-1618,N12-3007,1,\N,Missing
W12-1620,H05-1127,0,0.0329398,"user to say anything at any time, but have fairly flat dialogue policies, e.g., (Leuski et al., 2006). These systems can work well when the user is naturally in charge, such as in interviewing a character, but may not be suitable for situations in which a character is asking the user questions, or mixed initiative is desired. True mixed initiative is notoriously difficult for a manually constructed call-flow graph, in which the system might want to take different actions in response to similar stimuli, depending on local utilities. Reinforcement learning approaches (Williams and Young, 2007; English and Heeman, 2005) can be very useful at learning local policy optimizations, but they require large amounts of training data and a well-defined global reward structure, are difficult to apply to a large state-space and remove some of the control, which can be undesirable (Paek and Pieraccini, 2008). Our approach to this problem is a forward-looking reward seeking agent, similar to that described in (Liu and Schubert, 2010), though with support for complex dialogue interaction and its authoring. Authoring involves design of local subdialogue networks with pre-conditions and effects, and also qualitative reward"
W12-1620,W06-1303,1,0.738775,"s are only allowed at certain points in the dialogue. System initiative also usually makes it easier for a domain expert to design a dialogue policy that will behave as desired.1 Such systems can work well if the limited options available to the user are what the user wants to do, but can be problematic otherwise, especially if the user has a choice of whether or not to use the system. In particular, this approach may not be well suited to an application like SimCoach. At the other extreme, some systems allow the user to say anything at any time, but have fairly flat dialogue policies, e.g., (Leuski et al., 2006). These systems can work well when the user is naturally in charge, such as in interviewing a character, but may not be suitable for situations in which a character is asking the user questions, or mixed initiative is desired. True mixed initiative is notoriously difficult for a manually constructed call-flow graph, in which the system might want to take different actions in response to similar stimuli, depending on local utilities. Reinforcement learning approaches (Williams and Young, 2007; English and Heeman, 2005) can be very useful at learning local policy optimizations, but they require"
W12-1620,2005.sigdial-1.1,0,0.0448207,". Authoring involves design of local subdialogue networks with pre-conditions and effects, and also qualitative reward categories (goals), which can be instantiated with specific reward values. The dialogue manager, called FLoReS, can locally optimize policy decisions, by calculating the highest overall expected reward for the best sequence of subdialogues from a given point. Within a subdialogue, authors can craft the specific structure of interaction. Briefly, the main modules that form FLoReS are: • The information state, a propositional knowl1 Simple structures, such as a call flow graph (Pieraccini and Huerta, 2005) and branching narrative for interactive games (Tavinor, 2009) will suffice for authoring. 138 edge base that keeps track of the current state of the conversation. The information state supports missing or unknown information by allowing atomic formulas to have 3 possible values: true, false and null. • A set of inference rules that allows the system to add new knowledge to its information state, based on logical reasoning. Forward inference facilitates policy authoring by providing a mechanism to specify information state updates that are independent of the specific dialogue context.2 • An ev"
W13-4032,baccianella-etal-2010-sentiwordnet,0,0.0040804,"tuting a single turn. While this simple scheme does not provide a detailed treatment of relevant phenomena such as overlapping speech, backchannels, and the interactive process of negotiating the turn in dialogue (Yang and Heeman, 2010), it provides a conceptually simple model for the definition of features for aggregate statistical analysis. 4.2 Valence features for user speech Features (e)(g) are meant to explore the idea that distressed users might use more negative or less positive vocabulary than non-distressed subjects. As an exploratory approach to this topic, we used SentiWordNet 3.0 (Baccianella and Sebastiani, 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users in our study. The dictionary contains approximately 117,000 entries. In general, each word w may appear in multiple entries, corresponding to different parts of speech and word senses. To assign a single valence score v(w) to each word in the dictionary, in our features we compute the average score across all parts of speech and word senses: Context-independent feature analysis We begin by analyzing a set of shallow features which we describe as context-independent, as they apply to user speech segments ind"
W13-4032,D12-1004,0,0.0145618,"3–202, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics Heeman et al. (2010) observed differences in children with autism in how long they pause before speaking and in their use of fillers, acknowledgments, and discourse markers. Some of these features are similar to those studied here, but looked at children communicating with clinicians rather than a virtual human dialogue system. Recent work on machine classification has demonstrated the ability to discriminate between schizophrenic patients and healthy controls based on transcriptions of spoken narratives (Hong et al., 2012), and to predict patient adherence to medical treatment from word-level features of dialogue transcripts (Howes et al., 2012). Automatic speech recognition and word alignment has also been shown to give good results in scoring narrative recall tests for identification of cognitive impairment (Prud’hommeaux and Roark, 2011; Lehr et al., 2012). Figure 1: Ellie. communicative behavior of patients with specific psychological disorders such as depression. In this section, we briefly summarize some closely related work. Most work has observed the behavior of patients in human-human interactions, suc"
W13-4032,W12-1610,0,0.0311906,"rences in children with autism in how long they pause before speaking and in their use of fillers, acknowledgments, and discourse markers. Some of these features are similar to those studied here, but looked at children communicating with clinicians rather than a virtual human dialogue system. Recent work on machine classification has demonstrated the ability to discriminate between schizophrenic patients and healthy controls based on transcriptions of spoken narratives (Hong et al., 2012), and to predict patient adherence to medical treatment from word-level features of dialogue transcripts (Howes et al., 2012). Automatic speech recognition and word alignment has also been shown to give good results in scoring narrative recall tests for identification of cognitive impairment (Prud’hommeaux and Roark, 2011; Lehr et al., 2012). Figure 1: Ellie. communicative behavior of patients with specific psychological disorders such as depression. In this section, we briefly summarize some closely related work. Most work has observed the behavior of patients in human-human interactions, such as clinical interviews and doctor-patient interactions. PTSD is generally less well studied than depression. Examples of th"
W13-4032,J11-2001,0,0.0104219,"context-independent, as they apply to user speech segments independently of what the system has recently said. Most of these are features that apply to many or all user speech segments. We describe our context-independent features in Section 4.2.1, and present our results for these features in Section 4.2.2. 4.2.1 v(w) = P e∈E(w) PosScoree (w) |E(w)| − NegScoree (w) where E(w) is the set of entries for the word w, PosScoree (w) is the positive score for w in entry e, and NegScoree (w) is the negative score for w in entry e. This is similar to the “averaging across senses” method described in Taboada et al. (2011). We use several different measures of the valence of each speech segment with transcript t = hw1 , ..., wn i. We compute the min, mean, and max valence of each transcript: Context-independent features We summarize our context-independent features in Figure 2. Speaking rate and onset times Based on previous clinical observations related to slowed speech and increased onset time for depressed individuals (Section 2), we defined features for speaking rate and onset time of user speech segments. We quantify the speaking rate of a user speech segment hs, e, ti, where t = hw1 , ..., wN i, as N/(e −"
W13-4032,W10-4346,0,\N,Missing
W13-4032,wittenburg-etal-2006-elan,0,\N,Missing
W13-4039,W06-1303,1,0.929957,"ules can also be learned from a dialogue corpus (Abu Shawar and Atwell, 2005). Systems employing SMT or string transformation rules are formulating a response by Generation approach and it can be frequently ungrammatical or incoherent, unlike the selection approach which will always pick something that someone has once said (even though it might be inappropriate in the current context). TRAINS (Heeman and Allen, 1994), as well as open conversation in Switchboard (Godfrey et al., 1992), the performance was very low. On the other hand, for more limited domains such as simple question-answering (Leuski et al., 2006) or roleplay negotiation in a scenario, the performance was high, with METEOR scores averaging over 0.8. One possible selection criterion is to assume that the most appropriate response is the most probable response according to a model trained on human-human dialogues. More formally, let there be a dialogue hu1 , u2 , . . . , ut−1 , ut , . . . , uT i, where utterance ut appears in contextt = hu1 , u2 , . . . , ut−1 i. If we have a dialogue model P estimated from the training corpus then the formulated response uq for some unseen contextq is given by, ut = argmax P (ui |contextt ) i ∀ui ∈ Upos"
W13-4039,W02-1001,0,0.0336341,"ll be labeled as a separate class. The number of classes is the number of unique utterances seen in the training set, which is relatively large. As the training data grows, the number of classes will increase. Second, there are very few examples (on average a single example) per class. We need a classifier that can overcome these issues. The perceptron algorithm and its variants – voted perceptron and averaged perceptron are well known classification models (Freund and Schapire, 1999). They have been extended for use in various natural language processing tasks such as part-of-speech tagging (Collins, 2002), parsing (Collins, 2004) and discriminative language modeling (Roark et al., 2007). Here we use the averaged perceptron model for mapping from dialogue context to an appropriate response utterance. Collins (2002) outlines the following four components of a perceptron model: • The training data. In our case it is a set of automatically extracted utterance-context pairs {. . . , hui , contexti i, . . .} • A function GEN(context) that enumerates a set of all possible outputs (response utterances) for any possible input (dialogue context) • A feature extraction function Φ : hu, contexti → Rd that"
W13-4039,D11-1054,0,0.0390738,"and the best match in the previously seen corpus. We saw a wide variance in scores across domains, both as to the similarity scores and improvement of scores as more data is considered. For task-oriented planning domains, such as Monroe (Stent, 2000) and 251 Proceedings of the SIGDIAL 2013 Conference, pages 251–260, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics Apart from the models discussed above which have been mainly applied to dialogue domains situated in a story context, there has been some work in surface text based dialogue models for open domains. Ritter et al. (2011) use information retrieval based and statistical machine translation (SMT) based approaches towards predicting the next response in Twitter conversations. Also Chatbots typically use surface text based processing such as string transformations (e.g., AIML rules (Wallace, 2003)). Such rules can also be learned from a dialogue corpus (Abu Shawar and Atwell, 2005). Systems employing SMT or string transformation rules are formulating a response by Generation approach and it can be frequently ungrammatical or incoherent, unlike the selection approach which will always pick something that someone ha"
W13-4039,W11-2006,0,0.0395889,"Missing"
W13-4039,W10-4345,1,0.851936,"n dialogue genres such as simple question-answering or some negotiation domains, a simple model of dialogue progression would suffice. In such a case we can build dialogue models that primarily operate on a surface 2 Related Work The task of a dialogue model is to formulate an utterance given a dialogue context. There are two approaches towards formulating an utterance: Generation, where a response is compositionally created from elements of the information state, including the context of previous utterances, and Selection, where a response is chosen from previously seen set of responses. In (Gandhe and Traum, 2010), we examined the theoretical potential for the selection approach, looking at a wide variety of domains, and evaluating based on similarity between the actual utterance and the best match in the previously seen corpus. We saw a wide variance in scores across domains, both as to the similarity scores and improvement of scores as more data is considered. For task-oriented planning domains, such as Monroe (Stent, 2000) and 251 Proceedings of the SIGDIAL 2013 Conference, pages 251–260, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics Apart from the models discusse"
W13-4061,W12-1620,1,0.833814,"thors and varying team sizes to create flexible interactions by automating many editing workflows while limiting complexity and hiding architectural concerns. Finished characters can be published directly to web servers, enabling highly interactive applications. 1 Introduction To support the creation of a virtual guide system called SimCoach (Rizzo et al, 2011) designed to help military service personnel and their families understand behavioral healthcare issues and learn about support resources, a core virtual human architecture that included a new dialogue management approach was developed (Morbini et al., 2012b). SimCoach is an embodied, conversational virtual human guide delivered via the web and is supported by a flexible information state dialogue manager called FLoReS designed to support mixed initiative dialogue with conversational systems. Morbini et al. (2012a) provide a detailed description of the dialogue manager. Although FLoReS supports a wide variety of virtual human character behaviors, these must be specified in dialogue policies that must be authored manually. Initially, authoring for this dialogue manager required coding of policies using a custom programming language. Therefore sig"
W13-4064,aggarwal-etal-2012-twins,1,0.766178,"d new hires, who acted as test subjects. This dataset has 4K audio files each annotated with one of the 117 different NLU semantic classes. answering characters, but unlike SGTs Blackwell and Star, the response is a whole dialogue sequence, potentially involving interchange from both characters, rather than a single character turn. There are two types of users for the Twins: demonstrators, who are museum staff members, using head-mounted microphones, and museum visitors, who use a Shure 522 table-top mounted microphone (Traum et al., 2012). More on analysis of the museum data can be found in (Aggarwal et al., 2012). We also investigated speech recognition and NLU performance in this domain in Morbini et al. (2012). This dataset contains 14K audio files each annotated with one of the 168 possible response sequences. The division in training development and test is the same used in Morbini et al. (2012) (10K for training, the rest equally divided between development and test). Amani (Artstein et al., 2009b; Artstein et al., 2011) is an advanced question-answering character used as a prototype for systems meant to train soldiers to perform tactical questioning. The users are in between real users and test"
W13-4064,W11-2037,1,0.821297,"nstructed from known parts using a generative approach. A second issue is that even though we can cast the problem as multi-class classification, classification accuracy is not always the most appropriate metric of NLU quality. For question-answering characters, getting an appropriate and relevant reply is more important than picking the exact reply selected by a human domain designer or annotator: there might be multiple good answers, or even the best available answer might not be very good. For that reason, the question-answering characters allow an “off-topic” answer and Errorreturn plots (Artstein, 2011) might be necessary to choose an optimal threshold. For the SASO-EN system, slot-filler metrics such as precision, recall, and f-score are more appropriate than frame accu400 conventional ASR scale and use word accuracy, so that higher numbers signify better performance on both scales.13 Figure 1 shows the results obtained in the 3 dialogue systems by the various ASR systems. The figures plot ASR performance against NLU performance; NLU results on manual transcriptions are included for comparison. There are too few data points for the correlations between ASR and NLU performance to be signific"
W13-4064,2007.sigdial-1.23,0,0.036582,"derstand or react well to, even when an alternative formulation is known to work. ASR performance as well. One important aspect is the broad physical differences among speakers, such as male vs female, adult vs child (e.g. Bell and Gustafson, 2003), or language proficiency/accent, that will have implications for the acoustics of what is said, and ASR results. Other aspects of users have implications for what will be said, and how successful the interface may be, overall. Many (e.g. Hassel and Hagen, 2006; Jokinen and Kanto, 2004) have looked at the differences between novice and expert users. Ai et al. (2007a) also points out a difference between real users and recruited subjects. Real users also come in many different flavors, depending on their purposes. E.g. are they interacting with the system for fun, to do a specific task that they need to get done, to learn something (specific or general), or with some other purpose in mind? We considered the following classes of users, ordered from easiest to hardest to get to acceptable performance and robustness levels: 3.2 Types of Dialogue System Genres Dialogue Genres can be distinguished along many lines, e.g. the number and relationship of particip"
W13-4064,N03-1001,0,0.0442742,"Missing"
W13-4064,robinson-etal-2008-ask,1,0.689243,"ring character, with no internal semantic representation and the primary NLU task merged with Dialogue management as selecting the best response. The original users were ICT demonstrators. However, there were also some experiments with recruited participants (Leuski et al., 2006a; Leuski et al., 2006b). Later SGT Blackwell became a part of the “best design in America” triennial at the Cooper-Hewitt Museum in New York City, and the data set here is from visitors to the museum, who are mostly casual users, but range from expert to red-team. Users spoke into a mounted directional microphone (see Robinson et al., 2008 for more details). Slot-filling Probably the most common type of dialogue system (at least in the research community) is slot-filling. Here the dialogue is fairly structured, with an initial greeting phase, then one or more tasks, which all start with the user selecting the task, and the system taking over initiative to “fill” and possibly confirm the needed slots, before retrieving some information from a database, or performing a simple service.11 This genre also requires a semantic representation, at least of the slots and acceptable values. Generally, the set of possible values is large e"
W13-4064,P04-1012,0,0.0133144,"ak” the system, or show it as not-competent, and may try to do things the system can’t understand or react well to, even when an alternative formulation is known to work. ASR performance as well. One important aspect is the broad physical differences among speakers, such as male vs female, adult vs child (e.g. Bell and Gustafson, 2003), or language proficiency/accent, that will have implications for the acoustics of what is said, and ASR results. Other aspects of users have implications for what will be said, and how successful the interface may be, overall. Many (e.g. Hassel and Hagen, 2006; Jokinen and Kanto, 2004) have looked at the differences between novice and expert users. Ai et al. (2007a) also points out a difference between real users and recruited subjects. Real users also come in many different flavors, depending on their purposes. E.g. are they interacting with the system for fun, to do a specific task that they need to get done, to learn something (specific or general), or with some other purpose in mind? We considered the following classes of users, ordered from easiest to hardest to get to acceptable performance and robustness levels: 3.2 Types of Dialogue System Genres Dialogue Genres can"
W13-4064,W06-1303,1,0.719162,"s on the size of the training and development sets may be found in Yao et al. (2010), here we report only the numbers relevant to the Twins domain and to the NLU analysis, which are not in Yao et al. (2010). SGT Blackwell was created as a virtual human technology demonstration for the 2004 Army Science Conference. This is a question-answering character, with no internal semantic representation and the primary NLU task merged with Dialogue management as selecting the best response. The original users were ICT demonstrators. However, there were also some experiments with recruited participants (Leuski et al., 2006a; Leuski et al., 2006b). Later SGT Blackwell became a part of the “best design in America” triennial at the Cooper-Hewitt Museum in New York City, and the data set here is from visitors to the museum, who are mostly casual users, but range from expert to red-team. Users spoke into a mounted directional microphone (see Robinson et al., 2008 for more details). Slot-filling Probably the most common type of dialogue system (at least in the research community) is slot-filling. Here the dialogue is fairly structured, with an initial greeting phase, then one or more tasks, which all start with the u"
W13-4064,yao-etal-2010-practical,1,0.963582,"du Abstract While this increased choice of quality recognizers is of great benefit to dialogue system developers, it also creates a dilemma – which recognizer to use? Unfortunately, the answer is not simple – it depends on a number of issues, including the type of dialogue domain, availability and amount of training data, availability of internet connectivity for the runtime system, and speed of response needed. In this paper we assess several freely available speech recognition engines, and examine their suitability and performance in several dialogue systems. Here we extend the work done in Yao et al. (2010) focusing in particular on cloud based freely available ASR systems. We include 2 local ASRs for reference, one of which was also used in the earlier work for easy comparison. We present an analysis of several publicly available automatic speech recognizers (ASRs) in terms of their suitability for use in different types of dialogue systems. We focus in particular on cloud based ASRs that recently have become available to the community. We include features of ASR systems and desiderata and requirements for different dialogue systems, taking into account the dialogue genre, type of user, and oth"
W14-4325,J96-2004,0,0.42436,"Missing"
W14-4325,P97-1034,0,0.128821,"omatically derivable, excluding features from (Nouri et al., 2013) such as the number of offers and the number of rejections or acceptances. 3 Initiative Labeling A common way of structuring dialogue is with Initiative-Response pairs, or IR units (Dahlb¨ack and J¨onsson, 1998), which are also similar to adjacency pairs (Levinson, 1983), or simple exchange units (Sinclair and Coulthard, 1975). Several researchers have also proposed multiple levels of initiative. For example, (Whittaker and Stenton, 1988) had levels based on the type of utterance (commands, questions, assertions, and prompts). (Chu-Carroll and Brown, 1997) posit two levels of initiative: discourse initiative, attained by providing reasons for responses, and critiques of proposed plans, and task initiative, obtained by suggesting new tasks or plans. Linell et al. examine several factors, such as initiative vs response, strength of initiative, adequacy of response, scope and focality of response (Linell et al., 1988). They end up with an ordered set of six possible strengths of initiative. Each of these schemes is somewhat complicated by the fact that turns can consist of multiple basic elements. Analyzing previous work, we can see that initiativ"
W14-4325,P94-1001,1,0.342439,"such as initiative vs response, strength of initiative, adequacy of response, scope and focality of response (Linell et al., 1988). They end up with an ordered set of six possible strengths of initiative. Each of these schemes is somewhat complicated by the fact that turns can consist of multiple basic elements. Analyzing previous work, we can see that initiative breaks down into two distinct concepts. First there is providing unsolicited, or optional, or extra material, that is not a required response to a previous initiative. Second, there is the sense of putting a new discourse obligation (Traum and Allen, 1994) on a dialogue partner to respond. These two concepts often come together, such as for new questions or proposals that require some sort of response: they are both unsolicited and impose an obligation, which is why (Whittaker and Stenton, 1988) indicate that control should belong to the speaker of these utterances. However, it is also possible to have each one without the other. Statements can include new unsolicited material, without imposing an obligation to respond (other than the weak obligation to ground understanding of any contribution). Likewise, clarification questions impose new obli"
W14-4325,P90-1010,0,0.53626,"s et al., 2008; Traum et al., 2008). In this paper we investigate the role that dialogue initiative plays in negotiation. Negotiations can be characterized by both the goals that each negotiator is trying to achieve, as well as the outcomes. Even for negotiations that attempt to partition a set of goods, the participants may have differences in their valuation of items, and the negotiations can be very different if people are trying to maximize the total gain or their individual gain, or gain a competitive advantage over the other. Negotiations between two people are usually mixed-initiative (Walker and Whittaker, 1990), with control of conversation being transferred from one person to another. To our knowledge, no previous studies have investigated the relationship between verbal initiative taking patterns and the goal or the outcome of the negotiation. We suspected that both of the mentioned characteristics of the negotiation (goal and outcome) might be correlated with different initiative-taking patterns. We used an existing negotiation dataset in order to study the mixed initiative patterns between the two parties in the negotiation. We describe this data set in Section 2, as well as previous work that a"
W14-4325,P88-1015,0,0.542859,"words, turns, words per turn and words related to the negotiation objects We used only features that were easily and automatically derivable, excluding features from (Nouri et al., 2013) such as the number of offers and the number of rejections or acceptances. 3 Initiative Labeling A common way of structuring dialogue is with Initiative-Response pairs, or IR units (Dahlb¨ack and J¨onsson, 1998), which are also similar to adjacency pairs (Levinson, 1983), or simple exchange units (Sinclair and Coulthard, 1975). Several researchers have also proposed multiple levels of initiative. For example, (Whittaker and Stenton, 1988) had levels based on the type of utterance (commands, questions, assertions, and prompts). (Chu-Carroll and Brown, 1997) posit two levels of initiative: discourse initiative, attained by providing reasons for responses, and critiques of proposed plans, and task initiative, obtained by suggesting new tasks or plans. Linell et al. examine several factors, such as initiative vs response, strength of initiative, adequacy of response, scope and focality of response (Linell et al., 1988). They end up with an ordered set of six possible strengths of initiative. Each of these schemes is somewhat compl"
W14-4333,W13-4039,1,0.86688,"e automatic dialogue models, specifically Nearest Context model (Gandhe and Traum, 2007) - this model orders candidate utterances from the corpus by the similarity of their previous two utterances to the current dialogue context. Score2 is surface text similarity, computed as the METEOR score (Lavie and Denkowski, 2009) between the candidate utterance and the actual response utterance present 3 Evaluation We evaluated the tool by having four human volunteers (wizards) use it in order to establish an upper baseline for human-level performance in the static context evaluation task described in (Gandhe and Traum, 2013). Wizards were instructed in how to use the search and relevance feedback features. In order to not bias the wizards, they were not told exactly what score1 and score2 indicate, but just that the scores can be useful in search. Each wizard is presented with a set of utterances (Utrain ) (|Utrain |≈ 500) and is asked to select a subset from these that will be appropriate as a response for the presented dialogue context. Each wizard was requested to select somewhere between 5 to 10 (at-least one) appropriate responses for each dialogue context extracted from 252 five different human-human dialog"
W14-4333,leuski-traum-2010-npceditor,1,0.823663,"iate responses (perhaps more than one) for a context which is extracted from a human-human dialogue. The context does not change based on the wizard’s choices (static context setting). A wizard tool should help with the challenges presented by these tasks. A challenge for both of these tasks is that if the number of utterances in the corpus is large (e.g., more than the number of buttons that can be placed on a computer screen), it may be very difficult for a wizard to locate appropriate utterances. For the second task of creating human-verified training/evaluation data, tools like NPCEditor (Leuski and Traum, 2010) have been developed which, allow the tagging of a many to many relationships between contexts (approximated simply as input utterance) and responses. In other cases, a corpus of dialogues is used to acquire the set of selectable utterances, in which each context is followed by a single next utterance, and many utterances appear only once. This sparsity of data makes the selection task hard. Moreover, it may be the case that there are many possible continuations of a context or contexts in which an utterance may be appropriate (DeVault et al., 2011). We address these needs with a semi-automate"
W14-4333,W06-1303,1,0.607605,"nt role as a resource for dialogue system creation. In addition to its traditional roles, such as training language models for speech recognition and natural language understanding, the dialogue corpora can be directly used for the selection approach to response formation (Gandhe and Traum, 2010). In the selection approach, the response is formulated by simply picking the appropriate utterance from a set of previously observed utterances. This approach is used in many wizard of oz systems, where the wizard presses a button to select an utterance, as well as in many automated dialogue systems (Leuski et al., 2006; Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) The resources required for the selection approach are a set of utterances to choose from and optionally, a set of pairs of hcontext, response utterancei to train automatic dialogue models. A wizard can generate such resources by performing two types of tasks. First is the traditional Wizardof-Oz dialogue collection, where a wizard interacts with a user of the dialogue system. Here the 251 Proceedings of the SIGDIAL 2014 Conference, pages 251–253, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics Figu"
W14-4333,sellberg-jonsson-2008-using,0,0.023138,"Missing"
W14-4333,W10-4345,1,\N,Missing
W14-4334,baccianella-etal-2010-sentiwordnet,0,0.00639605,"understanding of user speech. SimSensei Kiosk currently uses 4 statistically trained utterance classifiers to capture different aspects of user utterance meaning. The first NLU classifier identifies generic dialogue act types, including statements, yes-no questions, wh-questions, yes and no answers, and several others. This classifier is trained using the Switchboard DAMSL corpus (Jurafsky et al., 1997) using a maximum entropy model. The second NLU classifier assigns positive, negative, or neutral valence to utterances, in order to guide Ellie’s expression of empathy. We use SentiWordNet 3.0 (Baccianella et al., 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users (as recognized by the ASR); the valence assigned to an utterance is based primarily on the mean valence scores of Opening Rapport Building Phase What are some things you really like about LA? (top level question) I love the weather, I love the palm trees, I love the beaches, there’s a lot to do here. Ellie Diagnostic Phase Have you noticed any changes in your behavior or thoughts lately? (top level question) User Yes. Ellie Can you tell me about that? (continuation prompt) User I’m having a lot more nightma"
W14-4334,W13-4032,1,0.849274,"ss conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD) (DeVault et al., 2014). SimSensei Kiosk has two main functions – a virtual human called Ellie (pictured in Figure 1), who converses with a user in a spoken, semi-structured interview, and a multimodal perception system which analyzes the user’s behavior in real time to identify indicators of psychological distress. The system has been designed and developed over two years using a series of face-toface, Wizard-of-Oz, and automated system studies involving more than 350 human participants (Scherer et al., 2013; DeVault et al., 2013; DeVault et al., 2014). Agent design has been guided by two overarching goals: (1) the agent should make 254 Proceedings of the SIGDIAL 2014 Conference, pages 254–256, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics gine. The perception system analyzes audio and video in real time to identify features such as head position, gaze direction, smile intensity, and voice quality. DeVault et al. (2014) provides details on all the agent’s modules. 2 2.1 Ellie User Overview of Dialogue Processing ASR and NLU components Unlike many task-oriented dialogue domains"
W15-4605,W14-4308,0,0.0697525,"alists, collectivists, and altruists). The domain was negotiation between a florist and a grocer who had to agree on the temperature of a shared retail space. Georgila (2013) used RL to learn the dialog system policy in a two-issue negotiation domain where two participants (the user and the system) organize a party, and need to decide on both the day that the party will take place and the type of food that will be served. Also, Heeman (2009) modeled negotiation dialog for a furniture layout task, and Paruchuri et al. (2009) modeled negotiation dialog between a seller and buyer. More recently, Efstathiou and Lemon (2014) focused on non-cooperative aspects of trading dialog, and Georgila et al. (2014) used multi-agent RL to learn negotiation policies in a resource allocation scenario. Finally, Hiraoka et al. (2014) applied RL to the problem of learning cooperative persuasive policies using framing, and Nouri et al. (2012) learned models for cultural decision-making in a simple negotiation game (the Ultimatum Game). In contrast to typical 2 Reinforcement Learning Reinforcement learning (RL) is a machine learning technique for learning the policy of an agent 1 Note that there is some previous work on using RL to"
W15-4605,W10-4321,1,0.654188,"Missing"
W15-4605,P14-1047,1,0.927285,"Institute for Creative Technologies nouri@ict.usc.edu traum@ict.usc.edu Satoshi Nakamura Nara Institute of Science and Technology s-nakamura@is.naist.jp Abstract party. Trading dialogs can be considered as a kind of negotiation, in which participants use various tactics to try to reach an agreement. It is common to have dialogs that may involve multiple offers or even multiple trades. In this way, trading dialogs are different from other sorts of negotiation in which a single decision (possibly about multiple issues) is considered, for example partitioning a set of items (Nouri et al., 2013; Georgila et al., 2014). Another difference between trading dialogs and partitioning dialogs is what happens when a deal is not made. In partitioning dialogs, if an agreement is not reached, then participants get nothing, so there is a very strong incentive to reach a deal, which allows pressure and can result in a “chicken game”, where people give up value in order to avoid a total loss. By contrast, in trading dialogs, if no deal is made, participants stick with the status quo. Competitive two-party trading dialogs may result in a kind of stasis, where the wealthier party will pass up mutually beneficial deals, in"
W15-4605,W13-4016,1,0.842785,"mine a number of strategies for this game, including random, simple, and complex 32 Proceedings of the SIGDIAL 2015 Conference, pages 32–41, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics hand-crafted strategies, as well as several reinforcement learning (RL) (Sutton and Barto, 1998) algorithms, and examine performance with different numbers and kinds of opponents. slot-filling dialog systems, in these negotiation dialogs, the dialog system is rewarded based on the achievement of its own goals rather than those of its interlocutor. For example, in Georgila (2013), the dialog system gets a higher reward when its party plan is accepted by the other participant. Note that in all of the previous work mentioned above, the focus was on negotiation dialog between two participants only, ignoring cases where negotiation takes place between more than two interlocutors. However, in the real world, multiparty negotiation is quite common. In this paper, as a first study on multi-party negotiation, we apply RL to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents. We experiment with different RL algorithms a"
W15-4605,C14-1161,1,0.625159,"system policy in a two-issue negotiation domain where two participants (the user and the system) organize a party, and need to decide on both the day that the party will take place and the type of food that will be served. Also, Heeman (2009) modeled negotiation dialog for a furniture layout task, and Paruchuri et al. (2009) modeled negotiation dialog between a seller and buyer. More recently, Efstathiou and Lemon (2014) focused on non-cooperative aspects of trading dialog, and Georgila et al. (2014) used multi-agent RL to learn negotiation policies in a resource allocation scenario. Finally, Hiraoka et al. (2014) applied RL to the problem of learning cooperative persuasive policies using framing, and Nouri et al. (2012) learned models for cultural decision-making in a simple negotiation game (the Ultimatum Game). In contrast to typical 2 Reinforcement Learning Reinforcement learning (RL) is a machine learning technique for learning the policy of an agent 1 Note that there is some previous work on using RL to learn negotiation policies among more than two participants. For example, Mayya et al. (2011) and Zou et al. (2014) used multi-agent RL to learn the negotiation policies of sellers and buyers in a"
W15-4605,W11-2001,0,\N,Missing
W15-4613,georgila-etal-2012-practical,1,0.860385,"s. The evocative intention of an utterance is the behavior of the addressee that a speaker intends to evoke (Allwood, 1976; Allwood, 1995). In the case of the guessing game, a clue is given to evoke the expression of a target word. We ascertain a voice’s evocative function potential (EVP) by calculating the ratio of targets that a clue evokes from listeners. Each participant listens to many consecutive clues uttered with the same voice (extended continuous exposure). Our participants are recruited using the Amazon Mechanical Turk (AMT) service2 in the same fashion as in (Wolters et al., 2010; Georgila et al., 2012). To the best of our knowledge, our work is the first to systematically attempt to validate or disprove the hypotheses mentioned above, and compare the results of human transcriptions to ASR results in order to determine whether or not the latter can be used as an automatic intelligibility test for TTS system evaluations. This is also a first important step towards 2 speech synthesis evaluation in a full dialogue context. Finally, this is the first time that a systematic evaluation is conducted on a voice’s EVP. The rest of the paper is organized as follows. First, we discuss previous work in"
W15-4613,paetzel-etal-2014-multimodal,0,0.0291574,"under specific conditions” “a blank to talk too much” Example Usage “taxi” Word Relation “a mixture containing two or more blank elements or blank and nonblank elements Definition usually fused together or dissolving into each other when molten” “elephants may look alike to you and me, but the shapes of their blank flaps and their Example Usage tusks set them apart” “um not video but” Word Relation Target Word WordNet Bomb Dictionary.com Human Tendency Cab WordNet Metal Dictionary.com Ear Human Audio corpus which contains audio and video recordings of human pairs playing a word guessing game (Paetzel et al., 2014). We only used clues that were able to elicit at least one correct guess in a previous study designed to measure clue effectiveness (Pincus et al., 2014). Some example clues used in this experiment, their source, their type, and the target word they intend to evoke can be found in Table 1. Each of the 54 clues was synthesized in each of the voices. We categorized the 54 clues into 3 main clue types: a definition type which provided a definition of the target word, an example usage type which is generally a commonly used sentence that contains the word, and a word relation type which refers to"
W15-4629,W11-2037,1,0.936015,"Missing"
W15-4629,2005.sigdial-1.14,0,0.146605,"sed on the classifier’s confidence in the appropriateness of selected responses: this threshold finds an optimal balance between false positives (inappropriate responses above threshold) and false negatives (appropriate responses below threshold) in the training data. At runtime, if the confidence for a selected response falls below the predetermined threshold, that response is replaced with an “off-topic” utterance that asks the user to repeat the question or takes initiative and changes the topic (Leuski et al., 2006); such failure to return a direct response, also called non-understanding (Bohus and Rudnicky, 2005), is usually preferred over returning an inappropriate one (misunderstanding). is also above the threshold, Pinchas will respond with the lower ranked response. If the only responses above threshold are among the recently used then Pinchas will choose one of them, since repetition is considered preferable to responding with an off-topic or inappropriate statement. 3.5 Data collection The development process consisted of several stages: preliminary planning and question gathering, initial recording of survivor statements, Wizard of Oz studies using the recorded statements to identify gaps in th"
W15-4629,W06-1303,1,\N,Missing
W15-4629,W10-4345,1,\N,Missing
W15-4630,2007.sigdial-1.23,1,0.765828,"2013 to the REAL workshop on June 21, 2014, and beyond, this paper traces how REAL was managed, the proposals we received, what happened at the workshop, what follow up we have had and how we measure success. Introduction 2 Motivation Speech and spoken dialog researchers often note that whereas industry has access to a wealth of ecologically valid speech data, the academic community lags far behind. The lag in quantity of data can impede research on system evaluation and in training the machine learning (ML) system components. This chasm can be filled by using recruited subjects. But studies (Ai et al., 2007) have found that the resulting data does not resemble real user data. Paid users follow the rules, but are usuThis paper describes the REAL Challenge (REAL), including the motivations for the challenge and preliminary results from the first year and prospects for the near future. The ultimate goal of REAL is to bring about a steady stream of data from real users talking to spoken dialogue systems, that can be used for academic research. The immediate goal of the first year of REAL is to bring together high school and undergraduate students, who have fresh ideas of how people will 209 Proceedin"
W16-3640,J12-1001,0,0.058228,"Missing"
W16-3640,N13-2012,0,0.0206591,"prevalent for utterances of particular dialogue acts? Does the level of entrainment increase as dialogue progresses? 310 Proceedings of the SIGDIAL 2016 Conference, pages 310–318, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics 2 Related Works In detail, we can express this formula with word count CS1 (w) and CS2 (w), and all of words W as, 2.1 Varieties of entrainment En(V ) = ∑ CS2 (w) CS1 (w) − −∑ ∑ . wi ∈W CS1 (wi ) C (w ) i wi ∈W S2 As mentioned in the introduction, entrainment has been shown to occur at almost every level of human communication (Levitan, 2013), including both human-human and human-system conversation. In human-human conversation, Kawahara et al. (2015) showed the synchrony of backchannels to the preceding utterances in attentive listening, and they investigated the relationship between morphological patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue stra"
W16-3640,P08-2043,0,0.82587,"ical patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue strategy to increase the accuracy of speech recognition by using entrainment intentionally. Levitan (2013) unified these two works. One of the most important questions about entrainment with respect to dialogue systems is its association with dialogue quality. Nenkova et al. (2008) proposed a score to evaluate the lexical entrainment in highly frequent words, and found that the score has high correlation with task success and engagement. This indicates that lexical entrainment has an important role in dialogue. In addition, it suggests that entrainment of lexical choice is probably affected by more detailed dialogue information, such as dialogue act. w∈V (2) Nenkova et al. (2008) used following word classes as V . 25MFC: 25 Most frequent words in the corpus. The idea of using only frequent words is based on the fact that we would like to avoid the score being affected b"
W16-3640,P07-1102,0,0.0411561,"t, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also define a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner. 1 Introduction Entrainment is a conversational phenomenon in which dialogue participants synchronize to each other with regards to various factors: lexical choice (Brennan and Clark, 1996), syntax (Reitter and Moore, 2007; Ward and Litman, 2007), style (Niederhoffer and Pennebaker, 2002; DanescuNiculescu-Mizil et al., 2011), acoustic prosody (Natale, 1975; Coulston et al., 2002; Ward and Litman, 2007; Kawahara et al., 2015), pronunciation (Pardo, 2006) and turn taking (Campbell and Scherer, 2010; Beˇnuˇs et al., 2014). Previous works have reported that entrainment is correlated with dialogue success, naturalness and engagement. However, there is much that is still unclear with regards to how entrainment affects the overall flow of the dialogue. For example, can entrainment also be observed in choice of dialog"
W17-2808,Q13-1016,0,0.0662557,"the methodology of corpus-based robotics (Bugmann et al., 2004), where some natural language, primarily route instructions, is collected. Route instruction interpreters dating back to M ARCO (MacMahon, 2006), and more recently the robotic forklift (Tellex et al., 2011) and Tactical Behavior Specification grammar (Hemachandra et al., 2015; Boularias et al., 2016), rely on these initial route instructions to learn mappings to robot-executable procedures like path planning. Additionally, some use semantic parsers (e.g., (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Krishnamurthy and Kollar, 2013)) or translation (Matuszek et al., 2010) to map natural language to actions. A gap in these works is bi-directional dialogue interaction, specifically cases where initial instructions are not well-formed and need additional clarification, or when participants grow to better grasp the robot’s capabilities, varying instruction strategies over time. Our work collected instructions to a robot, but also included the dialogue and followon responses needed to establish or build common ground. This paper focused on analyzing initial robot-directed instructions, leaving analysis of responses during the"
W17-2808,Q13-1005,0,0.0154904,"to natural language interpretation for robots follow the methodology of corpus-based robotics (Bugmann et al., 2004), where some natural language, primarily route instructions, is collected. Route instruction interpreters dating back to M ARCO (MacMahon, 2006), and more recently the robotic forklift (Tellex et al., 2011) and Tactical Behavior Specification grammar (Hemachandra et al., 2015; Boularias et al., 2016), rely on these initial route instructions to learn mappings to robot-executable procedures like path planning. Additionally, some use semantic parsers (e.g., (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Krishnamurthy and Kollar, 2013)) or translation (Matuszek et al., 2010) to map natural language to actions. A gap in these works is bi-directional dialogue interaction, specifically cases where initial instructions are not well-formed and need additional clarification, or when participants grow to better grasp the robot’s capabilities, varying instruction strategies over time. Our work collected instructions to a robot, but also included the dialogue and followon responses needed to establish or build common ground. This paper focused on analyzing initial robot-directe"
W17-2808,W10-4328,1,0.77268,"s shown in Figure 5. command:send-image. Others require additional parameters to fully specify the complete action. Of particular interest to us is the information that participants chose to include in robot-directed motion requests. We focused on command:drive and command:rotate for variation in how participants communicated. We annotated motion-command parameters for their usage of metric (e.g., move forward 2 feet; turn left 90 degrees) and landmarkbased points of reference (e.g., move to the table; turn to face the doorway) similar to absolute and relative steps in route instructions from Marge and Rudnicky (2010). 3.2.3 Dialogue-Moves To analyze internal IU structure, we annotated Commander-issued lower-level dialogue-moves. This annotation scheme is inspired by a prior approach to military dialogue that identified dialogue-moves in calls for artillery fire (Roque et al., 2006). Examples of a command type request are command:drive or command:rotate, that instruct the robot to perform certain motions. A dialogue-move list is provided in Appendix B. Three annotators independently validated the dialogue-move set on 99 dialogue turns in our human-robot dialogue corpus. Annotators had high agreement (α = 0"
W17-2808,J97-1002,0,0.437754,"ial1 , Ashley Foots1 , Cory Hayes1 , Cassidy Henry1 , Kimberly A. Pollard1 , Ron Artstein2 , Clare R. Voss1 , and David Traum2 1 U.S. 2 USC Army Research Laboratory, Adelphi, MD 20783 Institute for Creative Technologies, Playa Vista, CA 90094 matthew.r.marge.civ@mail.mil Abstract ulate robot intelligence and actions without participant awareness. With ten participants, we collected ten hours of human-robot dialogue. We are currently undertaking corpus curation and plan to make the data freely available in the next year. In this experiment, a human and robot engage in a series of transactions (Carletta et al., 1997) where an instruction is issued, and wizards acting on behalf of the robot either perform a task or prompt for clarification until the requested task is completed or abandoned. We propose a new term, instruction unit (IU), to identify all commands within a transaction issued before the robot generates a response. IUs were analyzed both in structure and variation. Our findings suggest a general, initial preference for including metric information over landmarks in motion commands, but this decreased over time. Results will assist in future work adapting robot responses to varied instruction sty"
W17-2808,passonneau-2006-measuring,0,0.0754703,"e, we annotated Commander-issued lower-level dialogue-moves. This annotation scheme is inspired by a prior approach to military dialogue that identified dialogue-moves in calls for artillery fire (Roque et al., 2006). Examples of a command type request are command:drive or command:rotate, that instruct the robot to perform certain motions. A dialogue-move list is provided in Appendix B. Three annotators independently validated the dialogue-move set on 99 dialogue turns in our human-robot dialogue corpus. Annotators had high agreement (α = 0.92; Krippendorf’s α using the MASI distance measure (Passonneau, 2006)). 3.3 Participants This study recruited ten participants: two female, eight male. Ages ranged from 28 to 58 (mean = 44, s.d. = 10.6). Two participants reported one year or less of robotics research; others reported none.1 1 We collected measures that were included in our statistical analysis but not presented in this paper. The Spatial Orientation Survey, part of the Guilford-Zimmerman Aptitude Survey (Guilford and Zimmerman, 1948), assesses spatial orientation perception. The HRI Trust Survey (Schaefer, 2013) measures subjective trust of the robot, based upon personal belief of the robot’s c"
W17-2808,P08-1073,0,0.0405719,"angement), the environment is sparsely filled with objects and is not in a finished state. These were practical limitations of laboratory resources, but in future work we plan to explore the effects of the environment further by varying it in a fully simulated version of the experiment. 5.2 6.1 By far the WOz method’s most common use has been for handling natural language (Riek, 2012). Many studies use a wizard in automated dialogue system development (e.g., in virtual agent negotiation (Gandhe and Traum, 2007), time-offset storytelling (Artstein et al., 2015), and in-car personal assistants (Rieser and Lemon, 2008)). Some researchers have considered a multiwizard setup for multimodal interfaces. The SimSensei project (DeVault et al., 2014) used a twowizard setup during the development stage; one controlling the virtual agent’s verbal behaviors and another the non-verbal behaviors. Green et al. (2004) investigated using multiple wizards for dialogue processing and navigation capabilities for a robot in a home touring scenario, finding the multi-wizard approach effective when the robot and human were co-present. Dialogue-Move Types We found that most IUs contained command dialogue-moves, but with some exc"
W17-2808,eberhard-etal-2010-indiana,0,0.0441354,"Missing"
W17-2808,stoia-etal-2008-scare,0,0.324368,"Missing"
W17-2808,gargett-etal-2010-give,0,\N,Missing
W17-5521,L16-1435,1,0.845456,"f Engineering, University of Cambridge, Cambridge, UK {kyusongl,tianchez,yulund,edcai,arlu,max}@cs.cmu.edu 1 {pincus, traum}@ict.usc.edu 2 {su259,lmr46,mg436,sjy11}@eng.cam.ac.uk Abstract what users might expect, given their exposure to the Amazon ECHO1 and Google HOME2 , etc. In order to get a flow of users started, DialPort developers expanded the number of connected systems to make the portal offerings more attractive and relevant. They also made the interface easier to use. By the end of March 2017, in addition to the above systems, the portal also included Mr. Clue, a word game from USC (Pincus and Traum, 2016), a restaurant opinion bot (Let’s Discuss, CMU), and a bus information system derived from Let’s Go (Raux et al., 2005). The portal offers users the option of typing or talking and of seeing an agent or just hearing it. With few connected systems in previous versions it was difficult to assess the portal’s switching mechanisms. The increased number of systems challenges the portal to make better decisions and have better a switching strategy. It also demands changes in the frequency of recommendations to connected systems. And it challenged the nature of the agent: some users prefer no visual"
W17-5521,P94-1001,1,0.116125,"the use of a visual agent, the absence of both graphical and speech response, feedback and portal behavior. Some ES need graphics to supplement their verbal information. Since Mr Clue keeps score and timing of users’ answers, its instructions and scores are shown on a blackboard. Let’s Go shows a map with the bus trajectory from departure to arrival. Feedback and communication The portal gives users feedback for: available topics, system state, and present system state. Skylar doesn’t interrupt the dialog with a list of topics. Rather 171 to the way in which they made their earlier requests (Traum and Allen, 1994). For example, the weather system should produce the natural Yes it’s going to rain instead of a full weather report, for the third question above. We thus keep the user’s initial request intent in the global dialog context and share it with the relevant ESes. The recommendation policy has been improved in two ways: 1) All participating system developers agreed that Skylar should give ES recommendations on a rotating basis so that all systems are recommended equally. Skylar no longer makes a recommendation at the end of each system turn. Recommendations are made about every four turns and, as"
W17-5521,P17-4013,1,0.820805,"atabase of restaurant reviews obtained from Zomato and Yelp. We formed a list of general discussion points for restaurants (service, atmosphere, etc). For each discussion point, a list of relevant keywords was compiled using WordNet, thesaurus, and by categorizing the most frequently words found in reviews. Cambridge The Cambridge restaurant information system helps users find a restaurant in Cambridge, UK based on the area, the price range or the food type. The current database has just over 100 restaurants and is implemented using the multi-domain statistical dialogue system toolkit PyDial (Ultes et al., 2017). To connect PyDial to Dialport, PyDial’s dialogue server interface is used. It is implemented as an HTTP server expecting JSON messages from the Dialport client. The system runs a trained dialogue policy based on the GP-SARSA algorithm (Gaˇsi´c et al., 2010). Other Systems QuBot, a chatbot from Pohang University and CMU, is used for out-of-domain handling. Let’sForecast, from CMU, uses the NOAA website. Let’s Eat from CMU is based on Yelp, finding restaurants for cities that Cambridge does not cover and for Cambridge if that system is down. Let’s Go, derived from the Let’s Go system (Raux et"
W17-5521,W10-4334,1,\N,Missing
W18-5012,W15-4611,0,0.0604603,"Missing"
W18-5012,stoia-etal-2008-scare,0,0.18304,"contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s style to the expected input. The taxonomy is described in more detail in Section 3. We observe and analyze these stylistic differences in a corpus of human-robot direction-giving dialogue from Marge et al. (2017). These styles are not unique to this corpus; they emerge in other human-robot and human-human dialogue, such as TeamTalk (Marge and Rudnicky, 2011) and SCARE (Stoia et al., 2008). The corpus contains 60 dialogues from 20 participants (Section 4). The robot dialogue management in the corpus is controlled by a Wizard-of-Oz experimenter, allowing for the study of users’ style with a fluent and naturalistic partner (i.e., with an approximation of an idealized automated system). In Section 5, we investigate possible consequences and implications of these categorized styles in this corpus. We examine the relationship of style and miscommunication frequency, applying an existing taxonomy for miscommunication in human-agent conversational dialogue (Higashinaka et al., 2015a)"
W18-5012,D15-1268,0,0.319779,"z for dialogue management. This allowed us specifically to isolate the style usage and miscommunication errors of the human partner (because the Wizard makes very few errors on the robot’s end). Studies of human-robot automated systems tend to focus on the miscommunication errors of the dialogue system (i.e., the robot itself), rather than the miscommunication or style of the human partner. In conversational agents, the research focus is also primarily to categorize errors made by the agent, not the human, including errors in ASR, surface realization, or appropriateness of the response (e.g., Higashinaka et al. (2015b); Paek and Horvitz (2000)). The more generic task-oriented and agent-based response-level errors from Higashinaka et al. (2015a) map well to the user miscommunication in the corpus we examine, including excess/lack of information, nonunderstanding, unclear intention, and misunderstanding. Works that focus specifically on miscommunication from the user when interacting with a robot include those categorizing referential ambiguity and impossible-to-execute commands (Marge and Rudnicky, 2015). These categories are common in the data we examine as well. In this analysis, we predict that trust wi"
W18-5012,L16-1504,0,0.0301679,"ed dialogue management strategies and offer concluding remarks on future work (Sections 7 and 8). 2 3 Stylistic Differences We describe two classes of stylistic differences for instruction-giving: differences in the verbosity of an instruction, and in the structure of the instruction. These styles emerge when decomposing a high-level plan or intent (e.g., exploring a physical space) into (potentially, but not necessarily) lowlevel instructions (e.g., how to explore the space, where to move, how to turn). Related Work A number of human-human direction-giving corpora exist, among them, ArtWalk (Liu et al., 2016), CReST (Eberhard et al., 2010), SCARE (Stoia et al., 2008), and SaGA (L¨ucking et al., 2010). The majority of existing analyses on these 111 3.1 Verbosity while looking at a live 2D-map built from the robot’s LIDAR scanner. A low bandwidth environment was simulated by disabling video streaming; instead, photos could be captured on-demand from the robot’s front-facing camera. To allow full natural language use, users were not provided example commands to the robot, though they were provided with a list of the robot’s capabilities which they could reference throughout the trials. Well-formed in"
W18-5012,L18-1017,1,0.612951,"plan of the user. In Dialogue 3, the user issues a single instruction “go through the other door” and waits until the instruction has been completed. Upon receiving completion feedback from the robot (“executing” and “done” responses), the next instruction, “take a picture”, is issued. Compare this with Dialogue 4, where the intents “face your starting position” and “send a picture” are compounded together and issued at the same time. This is classified as an Extended intent structure: instructions that have more than one expressed intent. These structural definitions were first described in Traum et al. (2018) to classify the composition of an instruction. In this work, we use these definitions to classify the style of the user. 4 4.1 Corpus Statistics The corpus contains 3,573 utterances from 20 users, totaling 18,336 words. 1,981 instructions were issued. The least verbose instruction observed is 1 word (“stop”), and the most verbose is 59 words (mean 7.3, SD 5.8). Of the total instructions, 1,383 are of the Minimal style, and 598, Extended. A moderate, positive correlation exists between higher verbosity and the Extended style in this corpus (rs (1969) = .613, p < .001)), supporting an intuition"
W18-5012,W17-2808,1,0.679528,"uman-robot collaboration has been studied with respect to engagement with the robot, and memory of information from the robot (Powers et al., 2007). two Minimal) or Extended if it contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s style to the expected input. The taxonomy is described in more detail in Section 3. We observe and analyze these stylistic differences in a corpus of human-robot direction-giving dialogue from Marge et al. (2017). These styles are not unique to this corpus; they emerge in other human-robot and human-human dialogue, such as TeamTalk (Marge and Rudnicky, 2011) and SCARE (Stoia et al., 2008). The corpus contains 60 dialogues from 20 participants (Section 4). The robot dialogue management in the corpus is controlled by a Wizard-of-Oz experimenter, allowing for the study of users’ style with a fluent and naturalistic partner (i.e., with an approximation of an idealized automated system). In Section 5, we investigate possible consequences and implications of these categorized styles in this corpus. We exami"
W18-5012,W15-4604,1,0.853954,", not the human, including errors in ASR, surface realization, or appropriateness of the response (e.g., Higashinaka et al. (2015b); Paek and Horvitz (2000)). The more generic task-oriented and agent-based response-level errors from Higashinaka et al. (2015a) map well to the user miscommunication in the corpus we examine, including excess/lack of information, nonunderstanding, unclear intention, and misunderstanding. Works that focus specifically on miscommunication from the user when interacting with a robot include those categorizing referential ambiguity and impossible-to-execute commands (Marge and Rudnicky, 2015). These categories are common in the data we examine as well. In this analysis, we predict that trust will have an effect on stylistic variations. Factors of trust in co-present and remote human-robot collaboration has been studied with respect to engagement with the robot, and memory of information from the robot (Powers et al., 2007). two Minimal) or Extended if it contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s sty"
W19-1705,W09-0613,0,0.0312625,"Previous work has addressed the large number of symbols by dynamically changing the chart as symbols are input so that only valid options are presented to the user at each step (Netzer and Elhadad, 2006). 1 For example, in Figure 4, the official spelling of teacher is composed of the characters for person 2 www.blissonline.org 33 www.tobiidynavox.com, www.minspeak.com not in the Official Blissymbolics Dictionary. For now, we only applied morphological realization on recognized Bliss words, meaning only officially recognized words can be conjugated in new ways. We used the SimpleNLG realizer (Gatt and Reiter, 2009) to conjugate Bliss words. For example, if an input Bliss character sequence had a Bliss past tense indicator, we applied the past tense realization to it. So a user could input the spelling for the bliss word translating to cry, weep and append the past tense indicator to the end, and get the resulting words cried, wept. Currently the system is limited to morphological realization supported by SimpleNLG. There are over 40 morphological relationships included in Blissymbolics. Each relationship needs its own realizing mechanism, not all of which can be found in SimpleNLG. For example, there is"
W19-1705,P04-3031,0,0.185671,"ctionary, and the morphological realizer outputting a list of sets, where each set contains the possible English words that the given Bliss word may translate to. The system looks at each set to determine if it contains nouns using wordnet (Miller, 1995). If a noun is found, then a set of articles a, the, or a blank is inserted before the set of nouns. From here, the language model needs to decide the most probable gloss words from each set, and also which article, if any, is most probable. We created an N-gram model trained on the Gutenberg, brown, conll2000, and nps-chat corpora using NLTK (Bird and Loper, 2004). We used interpolation smoothing as in equation 1. Our Translation System We built a translation system in Python available on github3 . We made a few assumptions about how Blissymbolics would be used. First, we assumed that any input sequence would have a word separating token, as the Fundamental Rules of Blissymbolics dictate. Second, we used the encoding scheme found in The Official Blissymbolics Dictionary, where each Bliss symbol, word or character, is given a unique 4-5 digit numeric ID. Our translation system only accepts these IDs as input. We will need to create a graphical user inte"
W19-1705,N06-2027,0,0.062567,"atural language. Most online tools are focused on creating customized Bliss charts. For example, the chart in figure 3 about food was created using blissonline 1 . Figure 3 Bliss charts help users communicate with nonBlissymbolics users since the symbols are annotated with their translation. However, users are restricted to the number of symbols that fit on one chart and the expressiveness of Blissymbolics is reduced. Previous work has addressed the large number of symbols by dynamically changing the chart as symbols are input so that only valid options are presented to the user at each step (Netzer and Elhadad, 2006). 1 For example, in Figure 4, the official spelling of teacher is composed of the characters for person 2 www.blissonline.org 33 www.tobiidynavox.com, www.minspeak.com not in the Official Blissymbolics Dictionary. For now, we only applied morphological realization on recognized Bliss words, meaning only officially recognized words can be conjugated in new ways. We used the SimpleNLG realizer (Gatt and Reiter, 2009) to conjugate Bliss words. For example, if an input Bliss character sequence had a Bliss past tense indicator, we applied the past tense realization to it. So a user could input the"
W19-3322,W18-4912,1,0.829047,"e assertions. We found that more fine-grained speech act information is needed for human-robot dialogue. 5 Tense & Aspect AMR currently lacks information that specifies when an action occurs relative to speech time and whether or not this action is completed (if a past event) or able to be completed (if a future event). This information is essential for situated humanrobot dialogue, where successful collaboration depends on bridging the gap between differing perceptions of the shared environment and creating common ground (Chai et al., 2014). Our tense and aspect annotation scheme is based on Donatelli et al. (2018), who propose a four-way division of temporal annotation and three multi-valued categories for aspectual annotation that fits seamlessly into existing AMR annotation practice. We reduced the authors’ proposed temporal categories to three, to capture temporal relations before, during, and after the speech time. In addition to the aspectual categories proposed by Donatelli et al. (2018), we added the category :completable +/- to signal whether or not a hypothetical event has an end-goal that is executable for the robot (described further in Section 5.3). Our annotation categories for tense and a"
W19-3322,P13-1023,0,0.0277445,"systems for NLU. However, for many of these representations, there are no existing automatic parsers, limiting their feasibility for largerscale implementation. An exception is combinatory categorical grammar (CCG) (Steedman and Baldridge, 2011); CCG parsers have been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particular"
W19-3322,P14-1134,0,0.0835122,"ture, a declarative statement that the instruction is being carried out, and an acknowledgment that it has been carried out are critical for conveying the robot’s current status in a live system. (m / move-01 :ARG0 (i / i) :direction (f / forward) :extent (d / distance-quantity :quant 10 :unit (f2 / foot))) Figure 2: Identical AMR for I will move / I am moving / I moved forward...10 feet. Although the imperative Move forward 10 feet should receive an AMR marker :mode imperative, our evaluation of the existing 1 https://github.com/amrisi/amr-guidelines/blob/master/ amr.md 201 5.1 parsers JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015) showed that parser output does not include this marker as it is rare if not entirely missing from the AMR 1.0 or 2.0 training corpora (Section 6).2 As a result, the command to move forward also received the identical above AMR (Figure 2) in parser output. While this suggests that additional training data is needed that includes imperatives, this speaks to a larger issue of AMR: the existing representation is very limited with respect to speech act information. Current AMR includes :mode imperative and represents questions through the presence of amr-unknown standi"
W19-3322,W00-1015,0,0.0512794,"ch acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a kind of semantic frame is perhaps most similar to the dialogue primitives of Hagen and Popowich (2000). Combining these types with fully compositional AMRs will allow flexible expressiveness, inferential power and tractable connection to robot action. 8 Conclusions This paper has proposed refinements for AMR to encode information necessary for situated humanrobot dialogue. Specifically, we elaborate 36 templates specific to situated dialogue that capture i) tense and aspect information; ii) speech acts; and iii) spatial parameters for robot execution. These refinements come after evaluating the coverage of existing AMR for a corpus of human-robot dialogue elicited from tasks related to search-"
W19-3322,W13-2322,1,0.855315,"by the participant. The RN then tele-operates the robot to complete the participant’s instructions. Finally, the RN provides spoken feedback to the DM of completed actions or problems that arose, which are relayed by the DM to the participant. A sample interaction can be seen in Table 1. The corpus contains dialogues from a total of 82 participants across three separate phased data collections. The participants’ speech and the RN’s speech are transcribed and time-aligned with text messages generated by the DM and sent either to the participant or the RN. 2.2 3 Background: AMR The AMR project (Banarescu et al., 2013) has created a manually annotated semantics bank of text drawn from a variety of genres. Each sentence is represented by a rooted directed acyclic graph in which variables (or graph nodes) are introduced for entities, events, properties, and states; leaves are labeled with concepts (e.g., (d / dog)). For ease of creation and manipulation, annotators work with the PENMAN representation of the same information (Penman Natural Language Group, 1989), as in Figure 1. Dialogue Structure Annotations (w / want-01 :ARG0 (d / dog) :ARG1 (p / pet-01 :ARG0 (g / girl) :ARG1 d)) The corpus also includes ann"
W19-3322,P17-1104,0,0.0184341,"r feasibility for largerscale implementation. An exception is combinatory categorical grammar (CCG) (Steedman and Baldridge, 2011); CCG parsers have been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a"
W19-3322,T75-2014,0,0.588771,"Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a kind of semantic frame is perhaps most similar to the dialogue primitives of Hagen and Popowich (2000). Combining these types with fully compositional AMRs will allow flexible expressiveness, inferential po"
W19-3322,P13-2131,0,0.121004,"ection of communication. 204 Figure 7: Planned pipeline for implementing AMRs into our human-robot dialogue system: natural language instructions are parsed using AMR parsers into existing AMR, which is then converted via graph-to-graph transformation into one of our augmented AMR templates. If all required parameters in the template are complete and the instruction executable, it will be mapped onto one of the robot’s action specifications for execution. Clarifications and feedback from the robot are generated from the AMR templates. quate scores of .82, .82, and .91 using the Smatch metric (Cai and Knight, 2013). According to AMR development group communication, 2014, IAA Smatch scores on AMRs are generally between .7 and .8, depending on the complexity of the data. Having created a gold standard sample of our data, we ran both JAMR4 (Flanigan et al., 2014) and CAMR5 (Wang et al., 2015) on the same sample and obtained the Smatch scores when compared to the gold standard. We selected these two parsers to explore because JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in th"
W19-3322,N15-1114,0,0.046006,"ser-output AMRs to our set of in-domain AMRs. Rather than train parsers to parse directly into the augmented AMRs described here, a graph-to-graph transformation allows us to maintain the parser output as a representation of the sentence meanings themselves as input, while the output captures our contextual domain-specific layer and includes speaker intent on top of the sentence meaning. To create training data for graphto-graph transformation algorithms and to evaluate the coverage and quality of the set of in-domain 7 We plan to eventually model our graph-to-graph transformation on work by (Liu et al., 2015) for abstractive summarization with AMR, though in the opposite direction. 206 (Hakkani-T¨ur et al., 2016; Chen et al., 2016), the latter of which allows the system to take advantage of information from the discourse context to achieve improved NLU. Substantial challenges to these systems include working in domains with intents that have a large number of possible values for each slot and accommodation of out-ofvocabulary slot values (i.e. operating in a domain with a great deal of linguistic variability). able to perform slot-filling dialogue (Xu and Rudnicky, 2000) including clarification of"
W19-3322,P18-4016,1,0.891986,"Missing"
W19-3322,P18-1037,0,0.0227543,"Missing"
W19-3322,P14-5010,0,0.0028211,"of the data. Having created a gold standard sample of our data, we ran both JAMR4 (Flanigan et al., 2014) and CAMR5 (Wang et al., 2015) on the same sample and obtained the Smatch scores when compared to the gold standard. We selected these two parsers to explore because JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in the relations. CAMR, in contrast, starts by obtaining the dependency tree— in this case, using the Charniak parser6 and Stanford CoreNLP toolkit (Manning et al., 2014)—and then applies a series of transformations to the dependency tree, ultimately transforming it into an AMR graph. As seen in Table 3, CAMR performs better on both precision and recall when trained on AMR 1.0, thus obtaining the higher F-score. However, compared to their self-reported F-scores (0.58 for JAMR and 0.63 for CAMR) on other corpora, both under-perform on the human-robot dialogue data. Given the relatively poor performance of both parsers on the human-robot dialogue data and erParser Data Precision Recall F-score CAMR JAMR JAMR JAMR 1.0 1.0 2.0 2.0+D 0.33 0.27 0.46 0.56 0.51 0.44 0"
W19-3322,N15-1040,0,0.293404,"t the instruction is being carried out, and an acknowledgment that it has been carried out are critical for conveying the robot’s current status in a live system. (m / move-01 :ARG0 (i / i) :direction (f / forward) :extent (d / distance-quantity :quant 10 :unit (f2 / foot))) Figure 2: Identical AMR for I will move / I am moving / I moved forward...10 feet. Although the imperative Move forward 10 feet should receive an AMR marker :mode imperative, our evaluation of the existing 1 https://github.com/amrisi/amr-guidelines/blob/master/ amr.md 201 5.1 parsers JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015) showed that parser output does not include this marker as it is rare if not entirely missing from the AMR 1.0 or 2.0 training corpora (Section 6).2 As a result, the command to move forward also received the identical above AMR (Figure 2) in parser output. While this suggests that additional training data is needed that includes imperatives, this speaks to a larger issue of AMR: the existing representation is very limited with respect to speech act information. Current AMR includes :mode imperative and represents questions through the presence of amr-unknown standing in for the concept or pola"
W19-3322,W17-2808,1,0.930987,"(provided by one senior and two recently trained AMR annotators), based on existing guidelines.1 We then examined how effectively these gold, guideline-based AMRs can capture the distinctions of interest for humanrobot dialogue and how accurately two available AMR parsers generate those gold annotations. Common instructions in the corpus include Move forward 10 feet, Take a picture, and Turn right 45 degrees. People also used landmarkbased instructions such as Move to face the yellow cone, and Go to the doorway to your right, although these were less common than the metricbased instructions (Marge et al., 2017). In response to these instructions from the DM to the participant, common feedback would be indications that an instruction will be carried out (I will move forward 10 feet), is in progress (Moving. . . ), or completed (I moved forward 10 feet). Given that current AMR guidelines do not make tense/aspect distinctions, these three types of feedback from the robot are represented identically under the current guidelines (see Figure 2). The distinctions between a promise to carry out an instruction in the future, a declarative statement that the instruction is being carried out, and an acknowledg"
W19-3322,W00-0309,0,0.772256,"raph transformation on work by (Liu et al., 2015) for abstractive summarization with AMR, though in the opposite direction. 206 (Hakkani-T¨ur et al., 2016; Chen et al., 2016), the latter of which allows the system to take advantage of information from the discourse context to achieve improved NLU. Substantial challenges to these systems include working in domains with intents that have a large number of possible values for each slot and accommodation of out-ofvocabulary slot values (i.e. operating in a domain with a great deal of linguistic variability). able to perform slot-filling dialogue (Xu and Rudnicky, 2000) including clarification of missing or vague descriptions and, if all required parameters are present, will use the domain-specific AMR for robot execution. 7 7.1 Related Work Semantic Representation There is a long-standing tradition of research in semantic representation within NLP, AI, as well as theoretical linguistics and philosophy (see Schubert (2015) for an overview). Thus, there are a variety of options that could be used within dialogue systems for NLU. However, for many of these representations, there are no existing automatic parsers, limiting their feasibility for largerscale impl"
W19-3322,C18-1313,0,0.0293395,"Missing"
W19-3322,J05-1004,0,0.105239,"ch acts work such as Austin (1975) and Searle (1969). To capture the range of speech acts present in the corpus, we arrived at an inventory of 36 unique speech acts specific to human-robot dialogue, inspired loosely by the dialogue move annotation of Marge et al. (2017). These 36 speech acts are classified into 5 types. In Figure 4, these are listed with the number of their subtypes in parentheses, along with a list of example subtypes for the type command. A full listing of subtypes and can be found in the Appendix. To integrate speech acts into AMR design, we selected existing AMR/PropBank (Palmer et al., 2005) rolesets corresponding to each speech act (e.g., command-02, assert-02, request-01, etc.) 5.3 Spatial Information A key component of successful human-robot collaboration is whether or not robot-directed commands are executable. In the dialogues represented in the corpus, for a command to be effectively executable by the robot, it must have a clear beginning and endpoint and comprise a basic action. For example, Move forward is not executable, since it lacks a clear endpoint; Move forward two feet, which identifies an endpoint, is executable. Additionally, a command such as Explore this room i"
W93-0235,H93-1033,0,0.0598874,"Missing"
W93-0235,J86-3001,0,0.116762,"Missing"
W93-0235,W93-0232,0,0.086542,"Missing"
W99-0313,J97-1002,0,0.0558436,"Missing"
W99-0313,J97-1005,0,0.0514042,"ls, and our supposition that important relations may be found between micro-level schemes and the two-level scheme posited here, lay the groundwork for more focused investigations of coding schemes for discourse structure in dialogue than have previously existed within the DR/initiative. 1 Table 5: Palrwise kappa scores CGU, i.e. there are no CGUs on which the coders were split into two groups of four and five in their coding decision for IU-initial CGUs. A weaker reliability metric on the pooled data from nine coders, therefore, would provide a reliable majority coding on this dialogue (see (Passonneau and Litman, 1997) for discussion of how reliability is computed for pooled coding data). In fact, for the group of six coders who showed the most inter-coder agreement, the average palrwise kappa score is .80, which is highly reliable. 4 Summary and Future Work In addition to the quantitative analysis of eodings, the subgroup at the 1998 DRI meeting reiterated some goals for the scheme in general and made progress on several open theoretical issues3 First and foremost, it was agreed upon that CGU analysis at the meso-level allowed coders to abstract the ""messy"" bits of dialogue (e.g., local repair, turntaking,"
W99-0313,J86-3001,0,0.802885,"he same content at three different levels, or to code three types of content at the macro-level without making any attempt to relate that coding to other schemes in development within the DRI initiative. Thus, for our starting point we proposed two original coding schemes within this multi-dimensional space. One scheme which has as content Grounding (Clark and Schaefer, 1989; Traum, 1994), operated at a meso level of granularity, and used non-hierarchical (and possibly discontinuous ) utterance sets as its structuring principle. The second scheme concerned intentional/informational structure (Grosz and Sidner, 1986; Nakatani et al., 1995) as content, operated at a macro level of granularity, and was structured as hierarchical trees (with annotations for capturing discontinuities). In addition, these two schemes were linked by using the resulting structures from meso-level analysis as basic input for macro-level analysis. There were several factors motivating the decision to use these particular facets of discourse structure for initial analysis. First, considering intentions, it is clear that aspects of dialogue at all levels of granularity relate to the intentions of the participants. However, not all"
W99-0313,P96-1038,1,0.897746,"Missing"
W99-0313,P98-2129,0,0.0312677,"Missing"
yao-etal-2010-practical,W06-1303,1,\N,Missing
yao-etal-2010-practical,burger-etal-2006-competitive,0,\N,Missing
yao-etal-2010-practical,robinson-etal-2008-ask,1,\N,Missing
