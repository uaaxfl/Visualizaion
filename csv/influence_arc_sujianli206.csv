2020.acl-main.551,N18-1150,0,0.0306531,"Missing"
2020.acl-main.551,P18-1063,0,0.211362,"NN/Daily Mail have demonstrated the effectiveness of our model. 1 Introduction Abstractive summarization focuses on generating fluent and concise text from the original input document and has achieved considerable performance improvement with the rapid development of deep learning technology (See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Gehrmann et al., 2018). In abstractive summarization, the recently popular and practical paradigm usually generates summary sentences by independently compressing or rewriting each pre-extracted sentence, which is from the source documents (Chen and Bansal, 2018; Lebanoff et al., 2019). However, a single document sentence usually cannot provide enough information that a summary sentence expresses, which is supported by the recent study of Lebanoff et al. (2019). They show that a high percentage of summary sentences include information from more than one document sentences, and composing a summary through only compressing sentences can cause performance degradation. Simultaneously, in contrast to the brevity requirements of a summary, each document sentence usually offers trivial details and expresses a relatively independent meaning, posing difficult"
2020.acl-main.551,P16-1046,0,0.0308614,"se the ROUGE metrics (R-1, R-2 and R-L) (Lin, 2004). For our model, the dimensions of hidden states and word embeddings are set 256 and 128 respectively. The batch size of training is 32, and the discount factor for reward in RL training is set to 0.95. The optimizer is Adam (Kingma and Ba, 2015) with a 0.001 learning rate for pre-training and 0.0001 learning rate for RL training. 1 4.2 Results To evaluate model performance, we compare our model (named EDUSum) with the state-of-the-art extractive and abstractive summarization methods. Three extractive methods are a strong Lead-3 baseline, NN (Cheng and Lapata, 2016) which applies neural networks with attention to extract sentences directly, and REFRESH (Narayan et al., 2018) which uses reinforcement learning to rank sentences. Three abstractive methods for comparison include: Pointer Generator (See et al., 2017), a controllable text generation method (Fan et al., 2017), and Fast-Abs (Chen and Bansal, 2018) which uses 1 The source code is available at https://github.com/PKUTANGENT/EDUSum 6193 Model EDUSumSameSent EDUSumgroup−1 EDUSumgroup−2 EDUSumgroup−3 EDUSum R-1 41.17 40.02 41.09 40.20 41.40 R-2 17.84 17.21 17.59 17.06 18.03 R-L 38.62 37.76 38.54 37.53"
2020.acl-main.551,D18-1443,0,0.0400579,"Missing"
2020.acl-main.551,D14-1181,0,0.00534476,"a smart unified end-to-end method to implement both the extraction and grouping. Next, EDU Fusion takes the EDUs in a group to generate a fluent and informative sentence. To train our method, we adopt reinforcement learning to leverage both the two modules. Figure 1 shows the whole architecture of our method. 2.1 Eselect1 gt 1 EDU Selection The EDU selection model is mainly based on a sequence-to-sequence pointer network. In the encoding stage, we use a hierarchical encoder to get the contextual representation of each EDU, which consists of a word-level temporal convolutional neural network (Kim, 2014) and an EDU-level Bidirectional Long Short-Term Memory Network(BiLSTM) (Hochreiter and Schmidhuber, 1997). In the decoding stage, we design an LSTM decoder to identify the informative EDUs with their group information. To group the related EDUs, we design a particular label truncate whose representation is a trainable parameter htruncate . We also add another special label stop with its representation hstop to determine the end of the selection process. htruncate and hstop are first randomly initialized and then learned in the training process. In each decoding step, the decoder computes a sel"
2020.acl-main.551,P19-1209,0,0.0348861,"Missing"
2020.acl-main.551,W16-3617,0,0.0605977,"Missing"
2020.acl-main.551,W04-1013,0,0.0597754,"Missing"
2020.acl-main.551,D15-1166,0,0.038565,"rmation. To group the related EDUs, we design a particular label truncate whose representation is a trainable parameter htruncate . We also add another special label stop with its representation hstop to determine the end of the selection process. htruncate and hstop are first randomly initialized and then learned in the training process. In each decoding step, the decoder computes a selection probability distribution on EDUs, truncate and stop. Assuming at time step t, the indices of the EDUs which have been extracted are included in the set Selt , the decoder first uses the Luong attention (Luong et al., 2015) to get the context ct and then computes a score sti for each EDU or label by: ( vpT tanh(Wp [ct ; hi ]) i not in Selt sti = (1) −∞ otherwise where i represents the index of an EDU, truncate or stop, and hi denotes the corresponding representation. vp and Wp are the trainable parameters. In order to avoid repeated selection of the same EDUs, we assign the score of −∞ to the EDUs that have been extracted. It is noted that the label truncate can be generated multiple times since it E1 gt k E2 reward . . EDUs in document Policy Gradient Update summary sentences (ground truth) E3 En Eselect 2 . ."
2020.acl-main.551,N18-1158,0,0.0187918,"ddings are set 256 and 128 respectively. The batch size of training is 32, and the discount factor for reward in RL training is set to 0.95. The optimizer is Adam (Kingma and Ba, 2015) with a 0.001 learning rate for pre-training and 0.0001 learning rate for RL training. 1 4.2 Results To evaluate model performance, we compare our model (named EDUSum) with the state-of-the-art extractive and abstractive summarization methods. Three extractive methods are a strong Lead-3 baseline, NN (Cheng and Lapata, 2016) which applies neural networks with attention to extract sentences directly, and REFRESH (Narayan et al., 2018) which uses reinforcement learning to rank sentences. Three abstractive methods for comparison include: Pointer Generator (See et al., 2017), a controllable text generation method (Fan et al., 2017), and Fast-Abs (Chen and Bansal, 2018) which uses 1 The source code is available at https://github.com/PKUTANGENT/EDUSum 6193 Model EDUSumSameSent EDUSumgroup−1 EDUSumgroup−2 EDUSumgroup−3 EDUSum R-1 41.17 40.02 41.09 40.20 41.40 R-2 17.84 17.21 17.59 17.06 18.03 R-L 38.62 37.76 38.54 37.53 38.79 Model Fast-Abs EDUSumsel+RL EDUSum Read. 1.86 2.22 1.92 Non-redund. 2.1 1.94 1.96 Table 3: Human Evaluat"
2020.acl-main.551,P17-1099,0,0.290593,"ection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model. 1 Introduction Abstractive summarization focuses on generating fluent and concise text from the original input document and has achieved considerable performance improvement with the rapid development of deep learning technology (See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Gehrmann et al., 2018). In abstractive summarization, the recently popular and practical paradigm usually generates summary sentences by independently compressing or rewriting each pre-extracted sentence, which is from the source documents (Chen and Bansal, 2018; Lebanoff et al., 2019). However, a single document sentence usually cannot provide enough information that a summary sentence expresses, which is supported by the recent study of Lebanoff et al. (2019). They show that a high percentage of summary sentences include information from more"
2020.acl-main.551,D18-1116,1,0.904617,"Missing"
2020.ccl-1.82,N18-1016,0,0.0609271,"Missing"
2020.ccl-1.82,P15-1061,0,0.0270238,"t stands for no relationship and form the pseudo data. To be noticed, there might be multiple data records that match with the candidate pair, so the training data here is multi-labeled. The reason why we use an IE system instead of using the pseudo data straight away is because with the help of context information, the IE system can make better decisions and generalize better than the exact-match method. To train the IE system, we cast the relation extraction task into a classification problem by modeling whether an entity-value pair in the same sentence has relation or not (Zhang, 2004; dos Santos et al., 2015). We use neural network to train the relation extractor and ensemble various models to further improve the performance. Formally, given an input sentence x = {xt }nt=1 which contains an entity-value candidate pair (r.E, r.M), we first embed each word into a vector eW t . The embedding is then concatenated E V with two position embedding vectors et and et , which stands for the distance between  the word and E , eV the entity and the value. Then the final word embeddings et = concat eW , e are fed into a t t t bi-directional long short-term memory network (BiLSTM) or a convolutional neural net"
2020.ccl-1.82,P83-1022,0,0.671812,"n extractor. With the noise-reduced data as input, we implement a text generator which sequentially models the input data records and emits a summary. Experiments on the ROTOWIRE dataset verifies the effectiveness of our proposed method in both performance and efficiency. Introduction CC L2 Recently the task of generating text based on structured data has attracted a lot of interest from the natural language processing community. In its early stage, text generation (TG) is mainly accomplished with manually compiled rules or templates, which are inflexible and mainly based on expert knowledge (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). With the development of neural network techniques, especially sequence-to-sequence (seq2seq) models, generating short descriptive texts from structured data has achieved great successes, including generating wikipedia-style biographies (Lebret et al., 2016; Sha et al., 2017) and restaurant introductions (Novikova et al., 2017). However, the task of generating long text, such as generating sports news from data, still fails to achieve satisfactory results. The existing models often forge fake context, lose sight of key facts and display inter-sente"
2020.ccl-1.82,D16-1128,0,0.115187,"ntly the task of generating text based on structured data has attracted a lot of interest from the natural language processing community. In its early stage, text generation (TG) is mainly accomplished with manually compiled rules or templates, which are inflexible and mainly based on expert knowledge (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). With the development of neural network techniques, especially sequence-to-sequence (seq2seq) models, generating short descriptive texts from structured data has achieved great successes, including generating wikipedia-style biographies (Lebret et al., 2016; Sha et al., 2017) and restaurant introductions (Novikova et al., 2017). However, the task of generating long text, such as generating sports news from data, still fails to achieve satisfactory results. The existing models often forge fake context, lose sight of key facts and display inter-sentence incoherence (Wiseman et al., 2017). For the sports news generation task, one challenging problem is that the input records are both large and noisy. Specifically, the inputted box scores, which contains hundreds of data records, belong to 40 different categories, such as fouls, threepointer, starti"
2020.ccl-1.82,P09-1113,0,0.510787,": Statistics of data records mentioned in 3000 sports news. The horizontal axis stands for summary numbers and the vertical axis stands for data record numbers. game, and further to plan an appropriate order for the selected records. This is also similar to the action of human writers who usually plan the important information to include before they write their articles. Next, one key problem is to label the important records which would be time consuming and expensive. To solve this problem, inspired by Wiseman et al. (2017) which used an information extraction (IE) system for evaluation and Mintz et al. (2009) which used distance learning for relation extraction, we build an IE system based on distant supervision. The IE system extracts relations from gold text, matches them to the corresponding data records and its results can then be used to supervise the process of content selection and planning. Then, we design a ranking unit to learn which data records are selected and in what order they appear. Here we choose to use the learning-to-rank (L2R) method instead of a classifier, because there exists heavy imbalance between positive and negative instances. We also design a rulebased model to furthe"
2020.ccl-1.82,W17-5525,0,0.0650583,"Missing"
2020.ccl-1.82,N18-1137,0,0.0190719,", 2016) stated the task of generating natural language descriptive text of the restaurants from structured information of the restaurants. The Wikibio dataset (Novikova et al., 2017) gives the infobox of wikipedia as the input data and the first sentence of the corresponding biography as output text. Various approaches have achieved good results on these two datasets which considered content selection and planning. Sha et al. (Sha et al., 2017) proposed a method that models the order of information via link-based attention between different types of data records. Perez-Beltrachini and Lapata (Perez-Beltrachini and Lapata, 2018) introduce a content selection method based on multi-instance learning. Generating sport news summaries on the other hand,is more challenging because not only the output text is longer and more complex, but also the input data records are numerous and diversed. Wiseman et al. (Wiseman et al., 2017) proposed the ROTOWIRE data set and gave baselines model based on end-to-end neural networks with attention and copy mechanism, these models often overlook key facts, repeatedly output the same information and make up irrelevant content. Puduppully et al. (Puduppully et al., 2018) designed a system t"
2020.ccl-1.82,P17-1099,0,0.0501189,"lly model the input and maintain the encoder output vectors hidden states ht . ht = [hft ; hbt ] = BiLST M (et , hft−1 , hbt+1 ) (6) The decoder is built based on the Gated Recurrent Network (GRU). At each time step the decoder receives an input edt and calculates the output vector sdt . Meanwhile it updates its own hidden state hdt . sdt , hdt = GRU (edt , hdt−1 ) (7) Here we implement the attention mechanism, conditional copy mechanism and coverage mechanism to further improve the model’s performance. 02 0 Attention and Coverage The attention at each step is calculated similar to See et al.(See et al., 2017), which is called perception attention. To calculate the attention weight between the hidden state of the decoder hdt and one output of the encoder hi , we map the two vectors to fix size vectors seperately by two MLPs Wa and Ua with trainable bias ba as hai . Then we use a trainable vector va and dot multiply it with tanh(hai ) as the attention score sti . At last we calculate the softmax over attention scores {sti }ni=0 as the attention weights {ati }ni=0 . We finally dot-multiply the attention weights {ati }ni=0 with the encoder outputs {hi }ni=0 and sum them as the final attention vector h"
2020.coling-main.69,D17-1209,0,0.0276336,"results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of s"
2020.coling-main.69,D17-1047,0,0.0171665,"60 0.8023 0.8079 0.7780 0.8230 0.8357 0.8312 0.6583 0.7080 0.7084 0.7020 0.7402 0.7647 0.7376 0.6813 0.6870 0.7033 0.7210 0.7449 0.7654 0.7610 0.7719 0.8135 0.7993 0.6409 0.7135 0.7175 0.7250 0.7299 0.7834 0.7631 Our Model 0.7540 0.7417 0.8508 0.7794 0.8037 0.7694 Table 2: Main results. The results of baseline models are from published papers. “-” means not reported. • IAN: an RNN-based approach proposed by (Ma et al., 2017), which encodes contexts and aspects words by LSTM and interacts them with attention to generate the representations for aspects and contexts concerning each other. • RAM: Chen et al. (2017) proposed a method based on Memory Network and represents memory with LSTM. Then a gated recurrent unit network is applied to concatenate all the outputs for sentence representation from attention. • TNet: Li et al. (2018) uses BiLSTM embeddings as target-specific embeddings, and utilizes a CNN model to extract final embeddings. • HSCN: Li et al. (2019) selects target words and extracts target-specific contextual representation to measure the deviation between target-specific contextual representation and target representations by capturing interactions between the context and target. • CDT: a"
2020.coling-main.69,P14-2009,0,0.406341,"uct in a customer review into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al."
2020.coling-main.69,C18-1096,0,0.0212529,"evel sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sent"
2020.coling-main.69,D19-1549,0,0.094791,"atures to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeli"
2020.coling-main.69,S14-2076,0,0.022071,"processing task, has attracted lots of attentions recently. Its typical application is to classify the target aspects of a product in a customer review into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work usin"
2020.coling-main.69,P18-1087,0,0.0252476,".7540 0.7417 0.8508 0.7794 0.8037 0.7694 Table 2: Main results. The results of baseline models are from published papers. “-” means not reported. • IAN: an RNN-based approach proposed by (Ma et al., 2017), which encodes contexts and aspects words by LSTM and interacts them with attention to generate the representations for aspects and contexts concerning each other. • RAM: Chen et al. (2017) proposed a method based on Memory Network and represents memory with LSTM. Then a gated recurrent unit network is applied to concatenate all the outputs for sentence representation from attention. • TNet: Li et al. (2018) uses BiLSTM embeddings as target-specific embeddings, and utilizes a CNN model to extract final embeddings. • HSCN: Li et al. (2019) selects target words and extracts target-specific contextual representation to measure the deviation between target-specific contextual representation and target representations by capturing interactions between the context and target. • CDT: a method based on graph neural network proposed by (Sun et al., 2019). CDT builds graph by dependency parsing, which is similar but not the same to us and propagates graphs by graph convolution network. • SDGCN: also a GNN-"
2020.coling-main.69,S14-2004,0,0.0872265,"he previous outputs by max pooling, concatenate them, and use a fully connected layer to project the concatenated vector into the space of the targeted C classes. Pc = MaxPooling(Nc0 ) (17) MaxPooling(Na0 ) (18) Pa = logits = Wo [Pc ; Pa ] + bo (19) y = Softmax(logits) (20) where ; represents concatenation operation, Wo ∈ Rd×d , bo ∈ Rd . 4 Experiments In this section, we describe our experimental setup and report our experimental results. 4.1 Experimental Setup For experiments, we utilize three datasets, including SemEval 2014 Task 4 dataset composed of Restaurant reviews and Laptop reviews (Manandhar, 2014) and ACL 14 Twitter gathered by Dong et al. (2014). All the cases in these datasets are labeled with three sentiment polarities positive, negative and neutral. The details of these datasets are shown in Table 1. We fine-tune pre-trained BERT1 in our model. Embedding dim d is set to 768. We utilize Adam (Kingma and Ba, 2014) as optimizer with initial learning rate 2 × 10−5 . Dropout with a keep probability of 0.9 is applied after the dense layer. The batch size of our model is 32, and the max sequence length is set to 128. The number of training epochs is 100. We use Spacy (Honnibal and Montani"
2020.coling-main.69,D14-1162,0,0.0898333,"ust parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on their syntax graph, which can not tell the differences between nodes, and they simply using mean pooling to get final results after GCN with deeper integration between aspect and context. 2.3 BERT BERT is one of the key innovations in the recent progress of contextualized representation learning (Peters et al., 2018; Devlin et al., 2018). Traditional word embeddings like Glove (Mikolov et al., 2013; Pennington et al., 2014) are trained among large scale of corpora, while all these methods finally get the superposition of different meanings of words. BERT adopts a fine-tuning mechanism that almost needs no specific requirement architecture for each end task. Recently, some BERT-based models (Zhao et al., 2019; Song et al., 2019) have been adopted in sentiment analysis task. But almost all of these models utilize BERT as an embedding layer, which is better than Glove. 3 Methodology In this section, we will introduce the details of all the layers in our model separately. Figure 2 shows the overall architecture of t"
2020.coling-main.69,N18-1202,0,0.0402638,"raph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on their syntax graph, which can not tell the differences between nodes, and they simply using mean pooling to get final results after GCN with deeper integration between aspect and context. 2.3 BERT BERT is one of the key innovations in the recent progress of contextualized representation learning (Peters et al., 2018; Devlin et al., 2018). Traditional word embeddings like Glove (Mikolov et al., 2013; Pennington et al., 2014) are trained among large scale of corpora, while all these methods finally get the superposition of different meanings of words. BERT adopts a fine-tuning mechanism that almost needs no specific requirement architecture for each end task. Recently, some BERT-based models (Zhao et al., 2019; Song et al., 2019) have been adopted in sentiment analysis task. But almost all of these models utilize BERT as an embedding layer, which is better than Glove. 3 Methodology In this section, we will"
2020.coling-main.69,E09-1077,0,0.051317,"2014 datasets and experiments show that SAGAT achieves outstanding performance. 2 Related Work In this section, we will briefly review works on aspect-level sentiment analysis, graph neural network, and BERT language model. 2.1 Aspect-Level Sentiment Analysis Aspect-level sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduc"
2020.coling-main.69,D19-1569,0,0.329609,"evel sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al., 2018) based on Transformer can alleviate long-distance dependence of RNN, some studies have designed BERT-based models for the ABSA task and achieved outstanding results (Song et al., 2019; Rietzler et al., 2019; Zhao et al., 2019). To the best of our knowledge, most of these work just utilize BERT simply as an embedding layer, which sequentially models the position embedddings"
2020.coling-main.69,P15-1150,0,0.0556329,"between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on"
2020.coling-main.69,D15-1167,0,0.214089,"eview into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al., 2018) based on Tr"
2020.coling-main.69,D16-1021,0,0.0420437,"optimized in our model is the cross-entropy loss, which can be defined as: L=− C X yˆi log(y i ) (21) i=1 4.2 Baselines We compare our model with the following baseline models: • TD-LSTM: Tang et al. (2015a) utilizes two dependent LSTM network to model the left context with aspect and the right context, respectively. The left and right representations are concatenated for predicting the sentiment polarity. • ATAE-LSTM: Wang et al. (2016) appends the aspect words embeddings with each context word embeddings. They use LSTM and attention to get the final representation for prediction. • MemNet: Tang et al. (2016) utilizes multi-hops attention on the context word for sentence representation to tell the importance of each context word. 1 We use uncased BERT-base from https://github.com/google-research/bert 804 Models Twitter Restaurant Laptop Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 TD-LSTM ATAE-LSTM MemNet IAN RAM TNet HSCN CDT SDGCN-BERT AEN-BERT 0.7080 0.6850 0.6936 0.7312 0.6960 0.7466 0.7471 0.6900 0.6691 0.6730 0.7101 0.6610 0.7366 0.7313 0.7563 0.7720 0.7816 0.7860 0.8023 0.8079 0.7780 0.8230 0.8357 0.8312 0.6583 0.7080 0.7084 0.7020 0.7402 0.7647 0.7376 0.6813 0.6870 0.7033 0.7210 0"
2020.coling-main.69,D16-1058,0,0.154166,"ct-Level Sentiment Analysis Aspect-level sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods"
2020.coling-main.69,P18-1030,0,0.0130283,"ang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax informa"
2020.iwdp-1.1,P12-1007,0,0.0274855,"ics(MOE) Department of Computer Science, Peking University lisujian@pku.edu.cn Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed (Mann and Thompson, 1987; Lascarides and Asher, 2008; Prasad et al., 2008), and the correponding parsing methods are designed (Soricut and Marcu, 2003; Joty et al., 2012; Feng and Hirst, 2012; Hernault et al., 2010; Zhou et al., 2010; Wang et al., 2012; Lan et al., 2013; Liu and Li, 2016), promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view. First, as about discourse structure, we present why we choose to use the dependency strucutre. So far, there are two well known discourse representations which are widely researched in the field of natural language processing. One is PDTB and the other is RST. PDTB adopts the representation of one predicate and two arguments b"
2020.iwdp-1.1,prasad-etal-2008-penn,0,0.0326194,"ndency View Sujian li and Liang Wang and An Yang and Yi Cheng and Zhenwen Li Key Laboratory of Computational Linguistics(MOE) Department of Computer Science, Peking University lisujian@pku.edu.cn Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed (Mann and Thompson, 1987; Lascarides and Asher, 2008; Prasad et al., 2008), and the correponding parsing methods are designed (Soricut and Marcu, 2003; Joty et al., 2012; Feng and Hirst, 2012; Hernault et al., 2010; Zhou et al., 2010; Wang et al., 2012; Lan et al., 2013; Liu and Li, 2016), promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view. First, as about discourse structure, we present why we choose to use the dependency strucutre. So far, there are two well known discourse representations which are widely researched in the field of natural langu"
2020.iwdp-1.1,N03-1030,0,0.235659,"Li Key Laboratory of Computational Linguistics(MOE) Department of Computer Science, Peking University lisujian@pku.edu.cn Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed (Mann and Thompson, 1987; Lascarides and Asher, 2008; Prasad et al., 2008), and the correponding parsing methods are designed (Soricut and Marcu, 2003; Joty et al., 2012; Feng and Hirst, 2012; Hernault et al., 2010; Zhou et al., 2010; Wang et al., 2012; Lan et al., 2013; Liu and Li, 2016), promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view. First, as about discourse structure, we present why we choose to use the dependency strucutre. So far, there are two well known discourse representations which are widely researched in the field of natural language processing. One is PDTB and the other is RST. PDTB adopts the representa"
2020.iwdp-1.1,D12-1083,0,0.0399399,"Missing"
2020.iwdp-1.1,C12-1168,1,0.865804,"Missing"
2020.iwdp-1.1,P13-1047,0,0.016724,"ourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed (Mann and Thompson, 1987; Lascarides and Asher, 2008; Prasad et al., 2008), and the correponding parsing methods are designed (Soricut and Marcu, 2003; Joty et al., 2012; Feng and Hirst, 2012; Hernault et al., 2010; Zhou et al., 2010; Wang et al., 2012; Lan et al., 2013; Liu and Li, 2016), promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view. First, as about discourse structure, we present why we choose to use the dependency strucutre. So far, there are two well known discourse representations which are widely researched in the field of natural language processing. One is PDTB and the other is RST. PDTB adopts the representation of one predicate and two arguments by taking an implicit or explicit connective as a predicate of two sentences. In"
2020.iwdp-1.1,D18-1116,1,0.898421,"Missing"
2020.iwdp-1.1,P18-2071,1,0.834364,"Missing"
2020.iwdp-1.1,W16-3617,0,0.0583237,"Missing"
2020.iwdp-1.1,C10-2172,0,0.0213112,"ng University lisujian@pku.edu.cn Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed (Mann and Thompson, 1987; Lascarides and Asher, 2008; Prasad et al., 2008), and the correponding parsing methods are designed (Soricut and Marcu, 2003; Joty et al., 2012; Feng and Hirst, 2012; Hernault et al., 2010; Zhou et al., 2010; Wang et al., 2012; Lan et al., 2013; Liu and Li, 2016), promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view. First, as about discourse structure, we present why we choose to use the dependency strucutre. So far, there are two well known discourse representations which are widely researched in the field of natural language processing. One is PDTB and the other is RST. PDTB adopts the representation of one predicate and two arguments by taking an implicit or explicit connectiv"
2020.iwdp-1.1,P14-1003,1,0.800486,"ng. With such a generative tree structure for a text, we have two problems. On one hand, it is difficult to generalize the meaning of interior text spans and design a set of production rules as in syntactic parsing, as there are no determinate generative rules for the interior text spans. On the other hand, it is not easy to keep the consistency of relations at different levels. For example, the relation ”Expansion” may occur between two EDUs or between two paragraphs. To solve these problems, we propose to use the discourse dependency structure which only consider the relations between EDUs (Li et al., 2014a). Then we can analyze the relations between EDUs directly, without worrying about any interior text spans. Without interior nodes, Dependency trees contain much fewer nodes and on average their annotation is simpler than RST trees. In addition, dependency structures can deal with non-projective relations, while constituency-based models need the addition of complex mechanisms like transformations, movements and so on. For a discourse dependency tree, it consists of EDUs which are linked by the binary, asymmetrical relations called dependency relations. A dependency relation holds between a s"
2020.lrec-1.210,P05-1018,0,0.106522,"ng, coherence modeling, learnability verification, robustness assessment, human evaluation 1. Introduction Coherence modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et"
2020.lrec-1.210,J08-1001,0,0.0597846,"on 1. Introduction Coherence modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et al., 2016; Logeswaran et al., 2017; Cui et al., 2018; Yin et al., 2019), aiming at find"
2020.lrec-1.210,N04-1015,0,0.15467,"g. Keywords: text ordering, coherence modeling, learnability verification, robustness assessment, human evaluation 1. Introduction Coherence modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generativ"
2020.lrec-1.210,D18-1465,0,0.076732,"lly, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et al., 2016; Logeswaran et al., 2017; Cui et al., 2018; Yin et al., 2019), aiming at finding a framework to learn to generate an optimal order from a bunch of input sequences. However, most of these studies only focus on the sentence level task and experiment using the abstracts from academic papers, which is unrealistic in the real world where various lengths and text domains may be used. Moreover, none of them analyze the adaptability of models with respect to different qualities of language resource, which prevents further study to improve the models in a fine-grained aspect. In this paper, we firstly propose to study text ordering at the para"
2020.lrec-1.210,P11-2022,0,0.0243435,"rnability verification, robustness assessment, human evaluation 1. Introduction Coherence modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et al., 2016; Logeswaran et al"
2020.lrec-1.210,P13-1010,0,0.0184816,"e modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et al., 2016; Logeswaran et al., 2017; Cui et al., 2018; Yin et al., 2019), aiming at finding a framework to learn to ge"
2020.lrec-1.210,N16-1147,0,0.0374633,"e level and the rest are at the paragraph level. 2.1. Datasets for sentence ordering Four existing datasets are adopted to evaluate model performance at the sentence level. 1. NIPS Abstract. This dataset contains roughly 3K abstracts from NIPS papers from 2005 to 2015.1 . 2. ANN Abstract. It includes about 13K abstracts extracted from the papers in the ACL Anthology Network (AAN) corpus(Radev et al., 2016). 3. arXiv Abstract. We further consider another source of abstracts collected from arXiv. It consists of around 40k instances2 . 4. SIND. It has 50K stories for the visual storytelling task(Huang et al., 2016), which are from a different domain. Statistics of the datasets are listed in Table 1. 1 https://www.kaggle.com/benhamner/ nips-papers 2 https://www.kaggle.com/neelshah18/ arxivdataset 1695 Average sequence number Train Dev Test NIPS 6.40 6.66 6.48 AAN 5.08 5.31 5.51 arXiv 6.69 6.98 7.23 SIND 5.00 5.00 5.00 News 7.97(3.16) 7.96(3.16) 8.07(3.15) Statements 5.58(3.23) 5.47(3.54) 4.85(3.65) Economist 9.73(4.31) 10.04(4.69) 9.94(4.78) Lyrics 29.51(2.21) 29.42(2.23) 29.51(2.25) Dataset Average sequence length Train Dev Test 25.92 26.93 26.40 24.56 24.51 24.47 25.19 25.69 25.17 11.49 11.62 11.42 77."
2020.lrec-1.210,P03-1069,0,0.198818,"man evaluation on the rearranged passages from two competitive models and confirm that WLCS-l is a better metric performing significantly higher correlations with human rating than τ , the most prevalent metric used before. Results from these evaluations show that except for certain extreme conditions, the recurrent graph neural network-based model is an optimal choice for coherence modeling. Keywords: text ordering, coherence modeling, learnability verification, robustness assessment, human evaluation 1. Introduction Coherence modeling has been a topic for discourse analysis for a long time (Lapata, 2003). Text ordering is the standard task used to test a coherence model in NLP. While earlier work aims at distinguishing between a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities"
2020.lrec-1.210,D14-1218,0,0.0205307,"a coherently ordered list of sentences and a random permutation thereof, recent studies attempt to generate a consecutive text from a set of given sentences. Various frameworks exist, focusing on linguistic features via statistical models (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Elsner and Charniak, 2011). Especially, entity based models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) have shown the effectiveness of exploiting entities for this task. With the popularization of neural networks, studies formulate the problem as neural pair-wise discrimination (Li and Hovy, 2014; Chen et al., 2016), with a purpose on training a discriminator to separate coherent clique or pairs from noncoherent ones. Recently, thanks to the success of setto-sequence framework (Vinyals et al., 2015; Vinyals et al., 2016), the focus shifts to generative ordering problem (Gong et al., 2016; Logeswaran et al., 2017; Cui et al., 2018; Yin et al., 2019), aiming at finding a framework to learn to generate an optimal order from a bunch of input sequences. However, most of these studies only focus on the sentence level task and experiment using the abstracts from academic papers, which is unr"
2020.lrec-1.210,W04-1013,0,0.0453037,"ell the model performs on finding the absolute position. Kendall’s tau(τ ) is one of the most frequently used metrics for the automatic evaluation of document coherence. It could be formalized as: τ = 1 − 2 ×  (number of inversions)/ n2 , where n is the length of the sequence and the number of inversions denotes the number of pairs in the predicted sequence with incorrect relative order. This metric ranges from -1 (the worst) to 1 (the best). Length Adapted Weighted Longest Common Subsequence(WLCS-l) is an adapted version of original ROUGE-w, the metric of the extent of sequence overlapping (Lin, 2004). Compared with other LCS-based metrics in the ROUGE family, WLCS-l encourages the model to generate successive sequence when the overlapping proportion is same while automatically decreasing with the total sequence length. It could be formalized as: ! ˆ −1 W LCS(O, O) Pwlcsl = f (5) f (n) ! ˆ O) W LCS( O, Rwlcsl = f −1 (6) f 2 (n) 7 NLTK implementation: http://www.nltk.org/ As we fail to aquire the source code from the author, we replicate the model by ourselves. In practice we found the performance of our version approximates to the authentic release. 9 As we tried our best but still cannot"
2020.lrec-1.210,D15-1166,0,0.0323118,"eleased in an earlier time and tested on the ones released afterward. The statistics of the four are displayed in Table 1. 3. Method In this work, to evaluate the performance of text ordering, we choose four state-of-the-art generative models which outperform earlier discriminative models. These four models are: (1)LSTM+Ptr-Net (Gong et al., 2016), (2)Variant-LSTM+Ptr-Net (Logeswaran et al., 2017), (3)ATTOrderNet (Cui et al., 2018), (4) SE-Graph (Yin et al., 2019). Their decoders are all based on the Ptr-Net framework (Vinyals et al., 2015).Through the use of attention (Bahdanau et al., 2014; Luong et al., 2015), Ptr-Net works as a type of new decoder for sequence generation. While a normal attention network picks the weighted average of all vectors (e.g. word embeddings of a sentence) as the input for the current time step, Ptr-Net simply uses the attention value as the pointer and picks out one tensor from all input tensors. In this way, attention is used to retrieve the most likely vector as input for RNN and as a probability index when generating the output pointer. Formally, Ptr-Net can be described as:  t−1 t−1 htdec , ctdec = LST M ht−1 (1) dec , cdec , x  t,i edec = f si , htdec ; i ∈ {1, ."
2020.lrec-1.210,D14-1162,0,0.091912,"Missing"
2020.lrec-1.210,N16-1174,0,0.0242547,"ork picks the weighted average of all vectors (e.g. word embeddings of a sentence) as the input for the current time step, Ptr-Net simply uses the attention value as the pointer and picks out one tensor from all input tensors. In this way, attention is used to retrieve the most likely vector as input for RNN and as a probability index when generating the output pointer. Formally, Ptr-Net can be described as:  t−1 t−1 htdec , ctdec = LST M ht−1 (1) dec , cdec , x  t,i edec = f si , htdec ; i ∈ {1, ..., n} (2)  t t adec = Sof tmax edec (3) All the models based on a hierarchical architecture (Yang et al., 2016), in which LSTM (Hochreiter and Schmidhuber, 1997) works as the bottom level sequence encoder, encoding the sentence into a compressed representation in a way it can fit on the upper level structure for reasoning. The major differences of these models lie in their encoders. LSTM+PtrNet uses a conventional LSTM to read the paragraphs or sentences representation and learn the representations as a whole. Variant-LSTM+PtrNet is based on the set-to-sequence framework (Vinyals et al., 2016), which reads the discourse by a series of RNN units with weighted average of sequences vectors as input for ea"
2020.lrec-1.210,P18-1030,0,0.0129361,"tences representation and learn the representations as a whole. Variant-LSTM+PtrNet is based on the set-to-sequence framework (Vinyals et al., 2016), which reads the discourse by a series of RNN units with weighted average of sequences vectors as input for each time step and passes the last hidden layer to decoder as the final high level abstraction. ATTOrderNet adopts self-attention architecture (Vaswani et al., 2017),where the input is the concatenated sequence vectors for a self-attention encoder without 1696 the position encoding information. SE-Graph utilizes the Graph Recurrent Network (Zhang et al., 2018), that parallelly and iteratively updates its node states with a message passing framework (Gilmer et al., 2017). For every message passing step t, the state of each node update involves a massage calculating from its directly connected neighbors and applying the gated operations with the newly calculated message. 4. • For SE-Graph, the setting of the optimizer is the same as the setting of ATTOrderNet. We use the hidden size of 300 for both sequence encoder and GRN encoders for all sentence-level datasets except SIND. Besides these configurations, we keep all the rest hyperparameters as what"
2021.acl-long.472,2020.acl-main.175,0,0.0125372,"odels as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection"
2021.acl-long.472,D17-1209,0,0.0607661,"Missing"
2021.acl-long.472,2020.acl-main.461,0,0.0198281,"Missing"
2021.acl-long.472,N18-1150,0,0.0212707,"ent representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure. • Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 201"
2021.acl-long.472,P18-1063,0,0.0157292,"emantic relevance of the generated summaries and references, as the BERTScore improvements of BASS is obvious. Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter"
2021.acl-long.472,D19-5412,0,0.0207023,"(Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summariz"
2021.acl-long.472,2020.acl-main.703,0,0.0383412,"hysics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Ill"
2021.acl-long.472,2020.acl-main.555,1,0.901075,"representing them as nodes and their relations as edges. This greatly benefits global structure learning and longdistance relation modeling. Several previous works have attempted to leverage sentence-relation graph to improve long sequence summarization, where nodes are sentences and edges are similarity or dis6052 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6052–6067 August 1–6, 2021. ©2021 Association for Computational Linguistics course relations between sentences (Li et al., 2020). However, the sentence-relation graph is not flexible for fine-grained (such as entities) information aggregation and relation modeling. Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020). However, the OpenIE-based graph only contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence. For better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified se"
2021.acl-long.472,D18-1205,1,0.790714,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,D18-1441,1,0.763064,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,C18-1101,0,0.0214588,"sing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated summaries. Compared with"
2021.acl-long.472,N15-1114,0,0.0238886,"rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated s"
2021.acl-long.472,P19-1500,0,0.47225,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,D19-1387,0,0.342053,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,N19-1173,0,0.398581,"et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax s"
2021.acl-long.472,2021.ccl-1.108,0,0.0427439,"Missing"
2021.acl-long.472,P19-1212,0,0.133693,"12 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indic"
2021.acl-long.472,P14-5010,0,0.00266212,", two-hop meta-path represents more complex semantic relations in graph. For example, N-V-N like [Albert Einstein]-[won]-[the physics Nobel Prize] indicates SVO (subject–verb–object) relation. It is essential to effectively model the two-hop meta-path for complex semantic relation modeling. 3.2 Graph Construction In this section, we introduce the definition and construction of the unified semantic graph. To construct the semantic graph, we extract phrases and their relations from sentences by first merging tokens into phrases and then merging co-referent phrases into nodes. We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence. Based on the dependency parsing tree, we merge consecutive tokens that form a complete semantic unit into a phrase. Afterwards, we merge the same phrases from different positions and phrases in the same coreference chain to form the nodes in the semantic graph. The final statistics of the unified semantic graph on WikiSUM are illustrated in table 1, which indicates that the scale of the graph expands moderately with the inputs. This also demonstrates how the unified semantic graph compresses l"
2021.acl-long.472,2020.emnlp-main.748,0,0.0134901,"Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter size. This further demonstrates the effectiveness of our graph-augmented model on long-document summarization"
2021.acl-long.472,2020.findings-emnlp.217,0,0.0113767,"stractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.,"
2021.acl-long.472,W00-1009,0,0.480591,"or his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indicate different types, according to section 3.1. on the long source sequence (Shao et al., 2017). Thus"
2021.acl-long.472,2020.tacl-1.18,0,0.0241394,"cument summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several larg"
2021.acl-long.472,D19-1324,0,0.0156427,"deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propo"
2021.acl-long.472,2020.acl-main.451,0,0.191789,"Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficie"
2021.acl-long.472,2020.acl-main.640,0,0.0218044,"d by the graph. Node Initialization Similar to graph construction in section 3.2, we initialize graph representations following the two-level merging, token merging and phrase merging. The token merging compresses and abstracts local token features into higher-level phrase representations. The phrase merging aggregates co-referent phrases in a wide context, which captures long-distance and crossdocument relations. To be simple, these two merging steps are implemented by average pooling. Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask. Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are 6055 not enough for learning backward information. For better utilizing the properties of the united semantic graph, we further propose two novel graph augmentation methods. Supernode As the graph becomes larger, noises introduced by imperfect graph construction also increase, which may cause disconnec"
2021.acl-long.472,D15-1044,0,0.251484,"ion tasks. 1 relativity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was fo"
2021.acl-long.472,P17-1099,0,0.353644,"vity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectri"
2021.findings-acl.430,K18-1048,0,0.0146031,"is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawback is that options in th"
2021.findings-acl.430,2020.acl-main.130,0,0.12872,"ng Model ×N + dialogue option 1 (a) Independent Matching score 1 … score N 2.2 Matching Model … + dialogue option 1 option N (b) Joint Matching Figure 2: Overview of Independent Matching and Joint Matching. urally enables a simple yet effective data augmentation method. The basic idea is that since options are sequentially concatenated in JM, new training instances can be easily created by changing the permutation order of options. Therefore, a dialogue with M response options can create at most M ! (factorial M ) times as many training instances. We conduct experiments on the MuTual dataset (Cui et al., 2020), a publicly available English dataset for multi-turn dialogue response selection. Results show that JM advances IM on three matching models and can significantly reduce training time. Besides, the permutation-based data augmentation method gives further improvement. 2 Model The overview of IM and JM is shown in Figure 2. We describe the details in the following subsections1 . 2.1 Instead of conducting N times of independent matches, we make the first step outside the IM framework and explore a joint matching approach for this task. We first adds a special token [OP]j at the start of the j th"
2021.findings-acl.430,N19-1423,0,0.01245,"scores. Note that JM can complete a prediction with a single match, which means it only requires one gradient computation and context encoding. Besides, thanks to the self-attention mechanism (Vaswani et al., 2017) of BERT-based matching models, options can now directly attend to each other, rather than being agnostic. Another advantage of JM approach is that it nat4872 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4872–4877 August 1–6, 2021. ©2021 Association for Computational Linguistics on this task. Similar to using BERT for sentencepair classification (Devlin et al., 2019), they first concatenate the context (sentence A) and a candidate response (sentence B) as BERT input (i.e., “[CLS] Excuse me ... [SEP] Sorry ... [SEP]”). On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. In order to compete a prediction, M independent matchings have to be made, where M is the number of options. score 1 Matching Model ×N + dialogue option 1 (a) Independent Matching score 1 … score N 2.2 Matching Model … + dialogue option 1 option N (b) Joint Matching Figure 2: Overview of Independent Matching and Joint Ma"
2021.findings-acl.430,W15-4640,0,0.0559652,"Missing"
2021.findings-acl.430,P19-1001,0,0.0126482,"which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawback is that options in these models are ind"
2021.findings-acl.430,P19-1363,0,0.015173,"anging the concatenation order of the options. For example, from [OP ]1 O1 [OP ]2 O2 ...[OP ]N ON to [OP ]2 O2 [OP ]1 O1 ...[OP ]N ON , we create a new training example (see Figure 2). Correspondingly, the ground-truth label of the training instance may be changed. In this way, a single dialogue can create at most M ! times training instances. 3 3.1 Experiments Dataset We evaluate our model on the Mutual dataset (Cui et al., 2020), a human-labeled, open-domain and reasoning-based dataset for multi-turn response selection. Compared with previous datasets (Lowe et al., 2015; Zhang et al., 2018; Welleck et al., 2019), MuTual is more challenging since it requires some reasoning ability. Models that achieve closeto-human performance on previous datasets, still perform far behind human performance on MuTual. The statistics of MuTual are shown in Table 1. Note that since Mutual has 4 options for each dialogue, PBDA can thus creates at most 24 (4!) times as many training instances. 3.2 Settings We use PyTorch to implement JM on three matching models. We adopt AdamW (Loshchilov and Training set Validation set Test set # Avg. Turns / Dialogue # Avg. Words / Utterance # Options MuTual 7088 886 886 4.73 19.57 4 Ta"
2021.findings-acl.430,P17-1046,0,0.0201444,"e important task in dialogue systems is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficienc"
2021.findings-acl.430,D19-1011,0,0.0326048,"Missing"
2021.findings-acl.430,P18-1205,0,0.0131553,"stances by simply changing the concatenation order of the options. For example, from [OP ]1 O1 [OP ]2 O2 ...[OP ]N ON to [OP ]2 O2 [OP ]1 O1 ...[OP ]N ON , we create a new training example (see Figure 2). Correspondingly, the ground-truth label of the training instance may be changed. In this way, a single dialogue can create at most M ! times training instances. 3 3.1 Experiments Dataset We evaluate our model on the Mutual dataset (Cui et al., 2020), a human-labeled, open-domain and reasoning-based dataset for multi-turn response selection. Compared with previous datasets (Lowe et al., 2015; Zhang et al., 2018; Welleck et al., 2019), MuTual is more challenging since it requires some reasoning ability. Models that achieve closeto-human performance on previous datasets, still perform far behind human performance on MuTual. The statistics of MuTual are shown in Table 1. Note that since Mutual has 4 options for each dialogue, PBDA can thus creates at most 24 (4!) times as many training instances. 3.2 Settings We use PyTorch to implement JM on three matching models. We adopt AdamW (Loshchilov and Training set Validation set Test set # Avg. Turns / Dialogue # Avg. Words / Utterance # Options MuTual 7088"
2021.findings-acl.430,D16-1036,0,0.0290692,"Missing"
2021.findings-acl.430,P18-1103,0,0.0166709,"in dialogue systems is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawba"
2021.findings-emnlp.227,W10-1001,0,0.0563122,"Missing"
2021.findings-emnlp.227,P17-1042,0,0.0145203,"Missing"
2021.findings-emnlp.227,Q18-1039,0,0.0293507,"asets. 5 Experiments 5.1 Experimental Settings XLM: We use the pretrained XLM-RoBERTa (XLM-R) downloaded from Hugging Face 2 unmodified. We run 20 epochs with a batch size of 32 during zero-shot and few-shot training. We adopt Adam (Kingma and Ba, 2015) as the optimizer with a learning rate of 1e-5. Since the limited length of the pre-training model and all our data is long text, we divide each article in our datasets into one piece of data according to paragraphs. And we take only the first 512 tokens of each data to reduce the effects of the length limit in XLM. We extend the ADAN model in (Chen et al., 2018) to incorporate the language-invariant features, containing three main components in the network: a joint feature extractor F that maps the input to the shared feature space, a language discriminator D that predicts whether the input is from English or 2679 2 https://huggingface.com/ setting level data 3 3 5 5 r r+w w r+w Zero-shot textb extrab Few-shot textb extrab 50.82 43.55 36.30 51.61 69.84 68.28 60.27 65.07 51.67 51.62 N/A N/A 61.35 64.77 N/A N/A Table 3: XLM evaluation results. Bold is the best.3 represents training on 1-3 levels and 5 represents training on 1-5 levels. r represents RAZ"
2021.findings-emnlp.227,2020.bea-1.1,0,0.0227731,"Missing"
2021.findings-emnlp.227,D19-1279,0,0.0144236,"rossImagine searching the appropriate reading materi- lingual LR of low-resource languages such as Chinese. als for a 10-year-old child in the bookstore: the There has been a recent trend towards learnTale of Peter Rabbit is a bit outdated; Animal Farm, though sounds suitable, is too allegorical; ing language-invariant features to ease the crosslingual generalization from high-resource lanthe Harry Potter series may be just right for the guages to low-resource languages (Litschko et al., age. Leveled reading (LR) provides such selection guides by automatically classifying texts with re- 2018; Kondratyuk and Straka, 2019). We hypothegard to the reading level appropriate for readers, size that these language-invariant features also exist in LR, especially in the equivalent level of reading which has proven to be of importance in multiple fields, including education (Lennon and Burdick, among different languages, which may be automat2004), health (Petkovic et al., 2015) and adver- ically extracted through deep learning methods. tisement (Chebat et al., 2003). Different from the For example, the reading materials in different traditional readability assessment (Aluisio et al., languages in the equivalent level ma"
2021.findings-emnlp.227,W18-0535,0,0.0286995,"Missing"
2021.findings-emnlp.227,W12-2019,0,0.0307238,"rial training and multi-lingual pre-training, on our aligned LR datasets. 2 2.1 Related Works Leveled Reading Methods Early works on LR devised various readability formulas, such as the Gunning Fog Index (Gunning, 1952), Automated Readability Index (Senter and Smith, 1967) and Flesch Reading Ease (Kincaid et al., 1975), which mainly rely on shallow language features based on ratios of characters, phrases and sentences. Later work adopted statistical machine learning methods based on extensive feature engineering, which generally improved accuracy by capturing semantic and contextual features (Vajjala and Meurers, 2012; Xia et al., 2016; Vajjala and Luˇci´c, 2018b). Recently, Martinc et al. (2019) and Deutsch et al. (2020) used deep neural networks to enhance LR and achieved the state-ofthe-art performances. Due to resource limitations, only a few works study LR in Chinese (Liu et al., 2017; Sun et al., 2020), which does not have copious annotated data like English. 2.2 Cross-Lingual Methods et al., 2018) and heuristic initialization (Artetxe et al.). Madrazo Azpiazu and Pera (2020) first proposed to use cross-lingual strategy for enhancing readability assessment as a binary classification problem, which sh"
2021.findings-emnlp.227,W16-0502,0,0.0196274,"gual pre-training, on our aligned LR datasets. 2 2.1 Related Works Leveled Reading Methods Early works on LR devised various readability formulas, such as the Gunning Fog Index (Gunning, 1952), Automated Readability Index (Senter and Smith, 1967) and Flesch Reading Ease (Kincaid et al., 1975), which mainly rely on shallow language features based on ratios of characters, phrases and sentences. Later work adopted statistical machine learning methods based on extensive feature engineering, which generally improved accuracy by capturing semantic and contextual features (Vajjala and Meurers, 2012; Xia et al., 2016; Vajjala and Luˇci´c, 2018b). Recently, Martinc et al. (2019) and Deutsch et al. (2020) used deep neural networks to enhance LR and achieved the state-ofthe-art performances. Due to resource limitations, only a few works study LR in Chinese (Liu et al., 2017; Sun et al., 2020), which does not have copious annotated data like English. 2.2 Cross-Lingual Methods et al., 2018) and heuristic initialization (Artetxe et al.). Madrazo Azpiazu and Pera (2020) first proposed to use cross-lingual strategy for enhancing readability assessment as a binary classification problem, which shows improvement in"
2021.findings-emnlp.227,Q15-1021,0,0.0167914,"olds mature LR stancross-lingual pre-training method to transfer dards with abundant reading materials, such as Lexthe LR knowledge learned from annotated data in the rich-resource English language to Chiile (Lennon and Burdick, 2004) and Accelerated nese. For evaluation, we introduce the ageReader (Topping et al., 2008), and has recently based standard to align datasets with different developed a set of LR datasets for training autoleveling standards, and conduct experiments in matic methods, such as the WeeBit (Vajjala and both zero-shot and few-shot settings. ExperiMeurers, 2012), NewSela (Xu et al., 2015) and Onments show that the cross-lingual pre-training eStopEnglish (Vajjala and Luˇci´c, 2018a) corpus. method can capture language-invariant feaBy contrast, low-resource languages like Chinese tures more effectively than adversarial training. lack both established LR standards and training We also conduct analysis to propose further improvement in cross-lingual LR. data, which results in only a few LR research conducted in Chinese (Sun et al., 2020). Can we use 1 Introduction the existing resources of English to guide the crossImagine searching the appropriate reading materi- lingual LR of lo"
2021.iwpt-1.4,D13-1160,0,0.105633,"Missing"
2021.iwpt-1.4,P14-1133,0,0.0631626,"Missing"
2021.iwpt-1.4,P16-1004,0,0.0574668,"Missing"
2021.iwpt-1.4,D13-1161,0,0.0594703,"Missing"
2021.iwpt-1.4,2021.emnlp-main.707,0,0.197581,"tps: //github.com/Teddy-Li/SemiAuto_Data_ Text_SQL 39 trained on it still suffer a sharp performance drop when generalizing to unseen domains3 . Thus, additional resource for domain transfer is appealing. However, in this more complex multitable Text-to-SQL task, previous semi-automatic dataset construction methods face an even greater challenge. With multi-table SQL queries with more complex clauses, exhaustive enumeration is intractable in size and prone to mismatches. More recently, various methods of Text-to-SQL dataset construction have been proposed (Yu et al., 2020; Zhong et al., 2020; Zhang et al., 2021), further automating the SQL-to-NL step with neural question generation. However, for query construction, they either do vanilla grammar-based SQL query sampling (Zhang et al., 2021) or use template sketches from existing datasets (Zhong et al., 2020; Yu et al., 2020). On the other hand, we focus instead on the context-dependent construction of SQL queries that both generalize beyond existing datasets and remain realistic. 3 Figure 2: The grammar for generating SQL queries, listed iteratively. PREDEFINED TOPIC is the topic set with method in section 3.1; terminal nodes are in red, non-terminal"
2021.iwpt-1.4,J13-2005,0,0.025746,"sitionally, converted them to rigid pseudo natural language (Pseudo-NL) questions with rules, then crowd-sourced those Pseudo-NL questions into NL questions. Cheng et al. (2018) further broke down the pseudo-NL questions into question sequences to make them more digestible for crowd workers. While these approaches shed light on the methodology of semi-automatic construction of semantic parsing datasets, applying them to collect broad-coverage Text-to-SQL data for domain transfer is not trivial. Firstly, SQL language has a much larger variety of realistic queries than Lambda-DCS logical forms (Liang, 2013), which were the focus of earlier work. Blind enumeration-up-to-a-certaindepth from a CFG is therefore intractable in size. Secondly, Herzig and Berant (2019) have discovered a mismatch between semi-automatically constructed queries and real-world queries, in terms of the distribution of logical form and the style of natural language expressions. As achieving accuracy gains in domain transfer demands high quality for the in-domain data, narrowing these mismatches is crucial. In this paper, we propose a novel semi-automatic pipeline for robust construction of Text-SQL pairs as training data in"
2021.iwpt-1.4,Q14-1030,1,0.88278,"Missing"
2021.iwpt-1.4,2020.acl-main.677,0,0.245314,"transfer performance for SOTA semantic parsers. 1 Introduction Due to the broad use of SQL in real-world databases, the task of mapping natural language questions to SQL queries (Text-to-SQL) has drawn considerable attention. Several large-scale crossdomain Text-to-SQL datasets have been manually constructed and advanced the development of Textto-SQL semantic parsing (Zhong et al., 2017; Yu et al., 2018). While these datasets are built for domain-general semantic parsing, current state-of-the-art (SOTA) semantic parsers still suffer sharp performance drop when generalising to unseen domains (Wang et al., 2020; Guo et al., 2019; Zhang et al., 2019). This could be attributed to the observation that the mapping of Text-to-SQL vary vastly across different domains, particularly in terms of the expressions of predicates1 . It is very difficult for models to generalize to those variations in a zero-shot fashion. Thus, additional in-domain data is desirable when applying semantic parsers to novel domains. 1 An example illustrating such difference is presented in Appendix A 38 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 38–49 Bangkok, Thailand (online), Augus"
2021.iwpt-1.4,2020.emnlp-main.558,0,0.0419277,"ll be released at https: //github.com/Teddy-Li/SemiAuto_Data_ Text_SQL 39 trained on it still suffer a sharp performance drop when generalizing to unseen domains3 . Thus, additional resource for domain transfer is appealing. However, in this more complex multitable Text-to-SQL task, previous semi-automatic dataset construction methods face an even greater challenge. With multi-table SQL queries with more complex clauses, exhaustive enumeration is intractable in size and prone to mismatches. More recently, various methods of Text-to-SQL dataset construction have been proposed (Yu et al., 2020; Zhong et al., 2020; Zhang et al., 2021), further automating the SQL-to-NL step with neural question generation. However, for query construction, they either do vanilla grammar-based SQL query sampling (Zhang et al., 2021) or use template sketches from existing datasets (Zhong et al., 2020; Yu et al., 2020). On the other hand, we focus instead on the context-dependent construction of SQL queries that both generalize beyond existing datasets and remain realistic. 3 Figure 2: The grammar for generating SQL queries, listed iteratively. PREDEFINED TOPIC is the topic set with method in section 3.1; terminal nodes are"
2021.iwpt-1.4,P15-1129,0,0.047473,"Missing"
2021.iwpt-1.4,P14-1090,0,0.0733521,"Missing"
C12-1098,C08-2006,0,0.17358,"about the same topic. Recently, there have been many attempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty dete"
C12-1098,P11-1050,0,0.0162171,"cy (repeating less the same information). The score is an integer between 1 (very poor) and 5 (very good). We randomly select 28 topics from TAC 2011 data and assign each topic to three different assessors7. In Table 3, the left four columns report the average scores of each criterion for the three systems. The experimental results indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum (based on paired t-test with p-value &lt; 0.01). Simultaneously, a fairly standard approach for manual evaluation is conducted through pairwise comparison (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011). According to the rating scores, each pair of summaries is judged which one is better under each criterion. If two summaries have the same score, they are judged a tie (of the equal quality). We record the times of ‘winning’ (having a higher score) and tie for each system. In Table 3, the right six columns show the evaluation results in frequencies respectively for h-uHDPSum vs. Peer 43, and h-uHDPSum vs. 2LevLDASum. The experimental results also indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum. We also observe that the winning times of h-uHDPSum under the nove"
C12-1098,E12-1022,0,0.74095,"ome new may appear over time, causing the number of aspects and aspect structures to change at different epochs. 1 Aspect in this article is usually called cluster in evolutionary clustering. 1604 Under the framework of extractive summarization, it is important to acquire the relationship between sentences and aspects for sentence selection. However, in most existing HDP models, the sentence level is disregarded and we cannot directly get the aspect distribution of sentences. Inspired by the progress made in Latent Dirichlet Allocation (LDA) models (Chemudugunta et al., 2007; Li et al., 2010; Delort and Alfonseca, 2012), we newly add the sentence level between the word level and document level in the h-uHDP model. Since neighboring sentences in one document usually talk about one same aspect, we assume that the aspect assignment of each sentence is not conditionally independently. With such assumption, the aspect of each sentence is determined by the aspect distribution of both the document and its neighboring sentences. Our huHDP model is capable of mapping multiple levels of information into the latent aspect space. The rest of this paper is organized as follows. Section 2 discusses the related work on upd"
C12-1098,N09-1041,0,0.139189,"summarization aims to produce an update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the as"
C12-1098,P10-1066,0,0.12074,"ization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approaches are mainly based on the computation of lexical similarities between sentences and fail to consider higher level information to avoid semantic redundancy in update summarization. To solve these two problems, we borrow the techniques of evolutionary clustering which focuses on detecting the dynamics of a given topic. Normally, one topic is described from various specific aspects 1 , accompanied with the background information running the whole topic (Chemudugunta et al., 2007; Li et al., 2010). For example, the topic “Quebec independence” may involve the specific aspects including “leader in independence movement”, “referendum”, “related efforts in independence movement” and so on, while “Quebec” and “independence” are seen as the general background information. The evolving dynamics of a topic is mainly embodied in the birth, splitting, merging and death of the specific aspects (Ren et al., 2008). Then, the commonality and diversity between history documents and update documents can be easily summarized from the aspect level and update summarization is not limited to lexical redun"
C12-1098,C08-1062,0,0.370449,"tempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approac"
C12-1098,N03-1020,0,0.315773,"Missing"
C12-1098,W11-0507,0,0.0204407,"n update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the aspect distribution of SH an"
C12-1098,C10-1111,0,0.0419546,"Missing"
C12-1098,W04-3252,0,\N,Missing
C12-1168,P12-1007,0,0.0222664,"ed relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the su"
C12-1168,D09-1036,0,0.2519,"to each typical examp le in the set Ai . Each examp le in the union A0 =∪Bi (1≤i≤n) is labelled as R0 . Then the set of ordered pairs &lt;Ai , Ri > (0≤i≤n) can be used to train an implicit relat ion classifier for labelling Ri (1 ≤ i ≤ n). Both clustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inqui"
C12-1168,P02-1047,0,0.541136,"Missing"
C12-1168,C08-2022,0,0.703215,"Missing"
C12-1168,P09-1077,0,0.564315,"relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the supervised framework (Pitler, Louis and Nenkova, 2009; Lin, Kan and Ng, 2009; Wang, Su and Tan, 2010). L in, Kan and Ng (2009) conducted a thorough performance analysis for four classes of features including contextual relations, constituent parse features, dependency parse features and cross -argument lexical pairs, while Pit ler et al. (2009) applied several linguistically informed features, such as word polarity, verb classes, and word pairs. Wang, Su and Tan (2010) adopted the tree kernel approach to mine more structure informat ion and got better results. These efforts of feature selection have achieved better performance though not that satisfying. The quality of training data are partly responsible for the difficulty of improving the performance of implicit relation recognition . To better recognize the imp licit discourse relations, we propose to review the annotated discourse corpora available at hand, identify and choose t"
C12-1168,prasad-etal-2008-penn,0,0.784226,"gue that an effective train ing se t is co mposed of typical examples, wh ich have distinct characteristics to signify their discourse relations. These typical examples, however, can be either the natively implicit relations or the created imp licit relations with connectives removed fro m the exp licit relat ions. Using the typical examp les as training data, 2758 an implicit relation classifier with higher discrimination power can be built according to the linguistic features in the two arguments. We provide three Comparison relat ion examp les fro m the Penn Discourse TreeBank (PDTB) v2.0 (Prasad et al., 2008) wh ich is widely used in the research of relation recognition as follows to illustrate what the possible typical examples are like. (1) Arg 1: 44 North Koreans oppose the plan, Arg 2: (while) South Koreans, Japanese and Taiwanese accept it or are neutral. (2) Arg 1: In such situations, you cannot write rules in advance. Arg 2: you can only make sure the President takes the responsibility. (3) Arg 1: Columbia Savings is a major holder of so-called junk bonds. Arg 2: New federal leg islation requires that all thrifts divest themselves of such speculative securities over a period of years. Here,"
C12-1168,N06-2034,0,0.0224094,"Missing"
C12-1168,N03-1030,0,0.0878457,"ion. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al."
C12-1168,P10-1073,0,0.0778405,"Missing"
C12-1168,H05-1044,0,0.00261514,"lustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inquirer lexicon (Stone et al., 1966) divides each word into fine-g rained semantic categories described by the inquirer tags. Fro m all the categories, we select 21 pairs of complementary categories, such as: Rise versus Fall, or Pleasure versus Pain,"
C12-1168,W06-1317,0,0.26788,"Missing"
C12-1168,P95-1026,0,0.113132,"Missing"
C12-1168,W10-4326,0,0.0401837,"one 2759 presented by Marcu and Echihabi (2002) who applied massive amounts of unannotated explicit relations and lexical features to train the Naïve Bayes classifier for both exp licit and implicit discourse relation recognition. Following the same idea, Saito, Yamamoto and Sekine (2006) conducted the experiments with the co mb ination of cross -argument wo rd pairs and phrasal patterns as features on Japanese sentences. Blair-Go ldensohn (2007) further extended the work of Marcu and Echihabi (2002) by involving syntactic filtering and topic segmentation. Another interesting work is that of Zhou et al. (2010), which predicted discourse connectives between arguments via a language model. Then the generated connectives plus other linguistic features were combined in a supervised framework to determine the implicit discourse relation. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that u"
C12-1187,P08-2016,0,0.564263,"A."" To achieve this we need to have an abbreviation dictionary, which is laborious to manually maintain because the number of abbreviations increases rapidly (Chang and Schutze, 2006). Therefore, it is helpful to automatically generate abbreviation from full forms. This leads to the idea of ""abbreviation generation"", i.e., finding the correct abbreviation for a full form. The generation of abbreviations in Chinese differs from that for English. The reason is that Chinese itself lacks many commonly considered features in English abbreviation generation methods (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008; Ao and Takagi, 2005). Detailed differences between English abbreviation generation and Chinese abbreviation features are listed in TABLE 1. Due to these differences, specific attention should be paid to Chinese abbreviation generation. Feature Word boundary Case sensitivity English YES YES Chinese NO NO Table 1: Comparison between Chinese and English abbreviation generation with regards to features. Most of Chinese abbreviations are generated by selecting representative characters from the full forms1 . For example, the abbreviation of ""北京大学"" (Peking University) is ""北大"" which is generated by"
C12-1187,nenadic-etal-2002-automatic,0,0.586528,"Missing"
C12-1187,P02-1021,0,0.0350623,"y to include the abbreviation ""USA."" To achieve this we need to have an abbreviation dictionary, which is laborious to manually maintain because the number of abbreviations increases rapidly (Chang and Schutze, 2006). Therefore, it is helpful to automatically generate abbreviation from full forms. This leads to the idea of ""abbreviation generation"", i.e., finding the correct abbreviation for a full form. The generation of abbreviations in Chinese differs from that for English. The reason is that Chinese itself lacks many commonly considered features in English abbreviation generation methods (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008; Ao and Takagi, 2005). Detailed differences between English abbreviation generation and Chinese abbreviation features are listed in TABLE 1. Due to these differences, specific attention should be paid to Chinese abbreviation generation. Feature Word boundary Case sensitivity English YES YES Chinese NO NO Table 1: Comparison between Chinese and English abbreviation generation with regards to features. Most of Chinese abbreviations are generated by selecting representative characters from the full forms1 . For example, the abbreviation of ""北京大学"" (Pe"
C12-1187,W01-0516,0,0.748931,"l documents or other formal materials. Take ""丁型病毒性肝炎""(Viral Hepatitis D) as an example, our method generates ""丁肝"", while the reference is ""丁型肝炎"". Both of these results are acceptable, while the reference is more formal. Interestingly, we find that in this kind of errors, the ""false"" abbreviations are always shorter in length than the standard abbreviations, which is identical to the intuition that these abbreviations are more widely used orally. 6 Related work Previous research on abbreviations mainly focuses on ""abbreviation disambiguation"", and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly because the full form is not always ambiguous. In many cases the full form is definite while we don’t know the corresponding abbreviation. 3067 To solve this problem, some approaches maintain a database of abbreviations and their corresponding ""full form"" pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find t"
C12-1187,P09-1102,0,0.580149,"he query terms are returned, which provides a natural corpus for further analysis. In this paper, we propose a stacked approach to automatically generate Chinese abbreviations. This method consists of a candidate generation phase and a ranking phase. First, we generate a list of candidates for the given full form using sequence labeling method. Then a supervised re-ranking method based on Support Vector Machine (SVM) using web data is applied to find the exact abbreviation. We evaluate on a Chinese abbreviation corpus and compare it with previous methods. A pure sequence labeling approach by (Sun et al., 2009) and a state-of-art method to incorporate web data by (Jain et al., 2007) are chosen as baseline methods. The contribution of this paper is that we integrate sequence labeling and web data to create a robust and automatic abbreviation generator. Experiments show that this combination gets better result than existing methods. Using this method we build a Chinese abbreviation dictionary, which later can be used in other NLP applications to help improve performance. The paper is structured as follows. We first describe our approach. In section 2 we describe the sequence labeling procedure and in"
C12-1187,W05-1304,0,0.493702,"rocedure. Experiments are described in section 4. In section 5 we give a detailed analysis of the results. In section 6 related works are introduced, and the paper is concluded in the last section. 2 Candidate Generation 2.1 Sequence Labeling As mentioned in section 1, the generation of Chinese abbreviations can be formalized as a task of selecting characters from the full form, which can be solved by sequence labeling models. Previous works proved that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in the candidate generation stage. A CRFs model is a type of discriminative probabilistic model most often used for the labeling or parsing of sequential data. Detailed definition of CRF model can be found in (Lafferty et al., 2001; McCallum, 2002; Pinto et al., 2003). 2.2 Labeling strategy Considering both training efficiency and modeling ability, we use a labeling method which uses four tags, ""BIEP"". ""B"" stands for ""Beginning character of skipped characters"", ""I"" stands for ""Internal character of skipped characters,""E"" stands for ""End character of skipp"
C14-1113,P06-1039,0,0.0321269,"Missing"
C14-1113,P13-2099,1,0.87904,"Missing"
C14-1113,D11-1105,0,0.0515661,"struction of the two-layer graph and the semi-supervised learning and the experimental results are provided in Section 4. Then, Section 5 describes related work on query-focused multi-document summarization and topic modeling techniques and we conclude this paper in Section 6. 2 Topic Modeling 2.1 Model Description As discussed in Section 1, a collection of documents often involves different topics related to a specific event. The basic idea of our summarization approach is to discover the latent topics and cluster sentences according to the topics. Inspired by (Chemudugunta et al, 2006) and (Li et al, 2011), we find 4 types of words in the text: (1) Stop words that occur frequently in the text. (2) Background words that describe the general information about an event, such as ”Quebec” and ”independence”. (3) Aspect words talking about topics across the corpus. (4) Document-specific words that are local to a single document and do not appear across different corpus. Similar ideas can also be found in many LDA based summarization techniques (Haghighi and Vanderwende, 2009; Li et al, 2011; Delort and Alfonseca, 2012). Stop words can easily be filtered out by a standard list of stopwords. We use a b"
C14-1113,N06-1059,0,0.0261831,"t words. In addition, forcing the words in one sentence to share the same aspect topic can ensure semantic cohesion of the mined topics. Next, we compare our model with the following widely used summarization approaches. Manifold: One-layer graph-based semi-supervised approach developed by Wan et al.(2008). Sentence relations are calculated according to tf − idf and topic information is neglected. LexRank: An unsupervised graph-based summarization approach(Erkan and Radev, 2004), which is a revised version of the famous web ranking algorithm PageRank. KL-Divergence: The approach developed by (Lin et al, 2006) by using a KL-divergence based sentence selection strategy. ∑ P (w) (12) KL(Ps ||Qd ) = P (w)log Q(w) w where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection. Since this approach is designed for general summarization, query influence is not considered. 1203 Hiersum: A LDA based approach proposed by (Haghighi and Vanderwende, 2009), where unigram distribution is calculated from LDA topic model in Equation (12). MEAD: A centroid based summary algorithm by (Radev et al, 2004). Cluster centroids in MEAD consists of words which are"
C14-1113,W11-0507,0,0.080342,"lection, because sentences in an important topic would be more important than those talking about trivial topics (Hardy et al, 2002; Harabagiu and Lacatusu, 2005; Otterbacher et al, 2005; Wan and Yang, 2008). The topic models (Blei et al, 2003) offer a good opportunity for the topic-level information modeling by offering clear and rigorous probabilistic interpretations over other existing clustering techniques. So far, LDA has been widely used in summarization task by discovering topics latent in the document collections (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al, 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). However, as far as we know, how to combine topic information and semi-supervised learning into a unified framework has seldom been exploited. In this paper, inspired by the graph-based semi-supervised strategy and topic models, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach for ∗ correspondence author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http:// creativecommons.org/licenses/b"
C14-1113,H05-1115,0,0.0524094,"rning method is that sentences are ranked without considering topic level information. As we know, a collection of related documents usually covers a few different topics. For example, the specific event “Quebec independence” may involve the topics such as “leader in independence movement”, “referendum”, “related efforts in independence movement” and so on. It is important to discover the latent topics when summarizing a document collection, because sentences in an important topic would be more important than those talking about trivial topics (Hardy et al, 2002; Harabagiu and Lacatusu, 2005; Otterbacher et al, 2005; Wan and Yang, 2008). The topic models (Blei et al, 2003) offer a good opportunity for the topic-level information modeling by offering clear and rigorous probabilistic interpretations over other existing clustering techniques. So far, LDA has been widely used in summarization task by discovering topics latent in the document collections (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al, 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). However, as far as we know, how to combine topic information and semi-supervised learning into a unified framework has seldom been"
C14-1113,N13-1017,0,0.0123014,"06; Titov and McDonald, 2008; Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Li et al, 2013a; Li et al, 2013b). (Haghighi and Vanderwende, 2009) introduced a LDA based model called Hiersum to find the subtopics or aspects by combining KL-divergence criterion for selecting relevant sentences. AYESSUM (Daume and Marcu, 2006) and the Special Words and Background model (Chemudugunta et al, 2006) are very similar to Hiersum. In the same way, (Delort and Alfonseca, 2012) tried to use LDA to model different levels of information for novelty detection in update summarization. Furthermore, (Paul and Dredze, 2013) extends their f-LDA to jointly model combinations of drug, aspect and route of administration as an exploratory tool for extractive summarization. 6 Conclusions and Future Work In this paper, we propose a two-layer graph-based semi-supervised algorithm for query-focused MDS. Topic modeling techniques are used for sentence clustering and further graph construction. By considering different kinds of information such as background or document-specific information, our two LDA topic model extensions achieve better results than traditional clustering algorithms. One primary disadvantage of our mod"
C14-1113,radev-etal-2004-mead,0,0.0178827,"ithm PageRank. KL-Divergence: The approach developed by (Lin et al, 2006) by using a KL-divergence based sentence selection strategy. ∑ P (w) (12) KL(Ps ||Qd ) = P (w)log Q(w) w where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection. Since this approach is designed for general summarization, query influence is not considered. 1203 Hiersum: A LDA based approach proposed by (Haghighi and Vanderwende, 2009), where unigram distribution is calculated from LDA topic model in Equation (12). MEAD: A centroid based summary algorithm by (Radev et al, 2004). Cluster centroids in MEAD consists of words which are central not only to one article in a cluster, but to all the articles. Similarity is measured by using tf − idf . Approach W-LDA S-LDA Manifold LexRank KL-divergence Hiersum MEAD Rouge-1 0.3891 (0.3802-0.3980) 0.3902 (0.3821-0.3983) 0.3581 (0.3508-0.3656) 0.3442 (0.3381-0.3502) 0.3468 (0.3410-0.3526) 0.3599 (0.3526-0.3672) 0.3451 (0.3390-0.3512) Rouge-2 0.1192 (0.1147-0.1235) 0.1209 (0.1161-0.1257) 0.1007 (0.0952-0.1062) 0.0817 (0.0782-0.0852) 0.0820 (0.0782-0.0858) 0.1004 (0.0956-0.1052) 0.0862 (0.0817-0.0907) Rouge-SU4 0.1482 (0.1450-0."
C14-1113,E12-1022,0,\N,Missing
C14-1113,N09-1041,0,\N,Missing
C14-1113,H05-1033,0,\N,Missing
C14-1113,W03-1101,0,\N,Missing
C16-1053,P15-2136,1,0.147777,"y useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embeddin"
C16-1053,P16-1046,0,0.0295447,"th manual and system summaries for the task of summary evaluation. Their method, however, did not surpass ROUGE. Recently, some works (Cao et al., 2015a; Cao et al., 2015b) have tried to use neural networks to complement sentence ranking features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural"
C16-1053,D15-1042,0,0.00650885,"based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural attention model (Bahdanau et al., 2014) in the machine translation area to generate one-sentence summaries. We have described these methods in Section 2.2. 6 Conclusion and Future Work This paper proposes a novel query-focused summarization system called AttSum which jointly handles saliency ranking and relevance ranking. It automatically generates distributed representations for sentences as well as the document clu"
C16-1053,W06-1643,0,0.0186371,"o find the optimal solution (McDonald, 2007; Gillick and Favre, 2009). Graph-based models played a leading role in the extractive summarization area, due to its ability to reflect various sentence relationships. For example, (Wan and Xiao, 2009) adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships. In contrast to these unsupervised approaches, there are also various learning-based summarization systems. Different classifiers have been explored, e.g., conditional random field (CRF) (Galley, 2006), Support Vector Regression (SVR) (Ouyang et al., 2011), and Logistic Regression (Li et al., 2013), etc. Many query-focused summarizers are heuristic extensions of generic summarization methods by incorporating the information of the given query. A variety of query-dependent features were defined to measure the relevance, including TF-IDF cosine similarity (Wan and Xiao, 2009), WordNet similarity (Ouyang et al., 2011), and word co-occurrence (Prasad Pingali and Varma, 2007), etc. However, these features usually reward sentences similar to the query, which fail to meet the query need. 5.2 Deep"
C16-1053,W09-1802,0,0.148632,"document cluster embeddings. “⊕” stands for a pooling operation, while “⊗” represents a relevance measurement function. for reference. Apparently, even if a sentence is exactly the same as the query, it is still totally useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modaliti"
C16-1053,D15-1229,0,0.108359,"and, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automat"
C16-1053,W14-1504,0,0.158097,"Missing"
C16-1053,D15-1232,0,0.0981039,"rks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embedding and document embedding for saliency measurement, where the document embedding is derived from the sum pooling of sentence embeddings. In order to consider the relevance and saliency simultaneously, we introduce the weighted-sum pooling over sentence embeddings to represent the document, where the weight is the automatically learned query relevance of a sentence. In this way, the document representation will be biased to the sentence embeddings which match the meaning of both query and documents. The w"
C16-1053,P13-1099,0,0.185236,"Introduction Query-focused summarization (Dang, 2005) aims to create a brief, well-organized and fluent summary that answers the need of the query. It is useful in many scenarios like news services and search engines, etc. Nowadays, most summarization systems are under the extractive framework which directly selects existing sentences to form the summary. Basically, there are two major tasks in extractive query-focused summarization, i.e., to measure the saliency of a sentence and its relevance to a user’s query. After a long period of research, learning-based models like Logistic Regression (Li et al., 2013) etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in r"
C16-1053,W04-1013,0,0.0254933,"according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automatically scaled. According to (K˚ageb¨ack et al., 2014), cosine similarity is the best metrics to measure the embedding similarity for summarization. In the training process, we apply the pairwise ranking strategy (Collobert et al., 2011) to tune model parameters. Specifically, we calculate the ROUGE-2 scores (Lin, 2004) of all the sentences in the training dataset. Those sentences with high ROUGE-2 scores are regarded as positive samples, and the rest as negative samples. Afterwards, we randomly choose a pair of positive and negative sentences which are denoted as s+ and s− , respectively. Through the CNN Layer and Pooling Layer, we generate the embeddings of v(s+ ), v(s− ) and v(d|q). We can then obtain the ranking scores of s+ and s− according to Eq. 5. With the pairwise ranking criterion, AttSum should give a positive sample a higher score in comparison with a negative sample. The cost function is defined"
C16-1053,W12-2601,0,0.0619964,"= 50 for all the rest experiments. It is the same dimension as the word embeddings. During the training of pairwise ranking, we set the margin Ω = 0.5. The initial learning rate is 0.1 and batch size is 100. 4.3 Evaluation Metric For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004) 3 . It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems (Owczarzak et al., 2012). During the training data of pairwise ranking, we also rank the sentences according to ROUGE-2 scores. 4.4 Baselines To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused s"
C16-1053,D15-1044,0,0.141032,"rge. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is"
C16-1161,S07-1025,0,0.0217758,"a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally cons"
C16-1161,P14-2082,0,0.0362075,"15; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally consistent and accurate. Time-aware joint inference with"
C16-1161,P08-1090,0,0.0298628,"Missing"
C16-1161,P07-2044,0,0.0409635,"Missing"
C16-1161,D14-1165,0,0.0722366,"2013): the mean of correct entity ranks (Mean Rank) and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the metrics are desirable but flawed when a corrupted triple exists in the KG. As a countermeasure, we may filter out all these corrupted triples which have appeared in KG before ranking. We name the first evaluation set as Raw and the second as Filter. For each test quad (triple), we replace the head/tail entity ei by those entities with compatible types as removing triples with incompatible types during test time leads to better results (Chang et al., 2014; 1720 Wang et al., 2015). Entity type information is easy to obtain for YAGO and Freebase. Then we rank the generated corrupted triples in descending order, according to the plausibility (for baselines and TAE model) or the decision variables (for time-aware ILP model). Then we check whether the original correct triple ranks in top-10. To calculate Hit@10 for ILP model, for each test quad, we add additional P (r ) constraints that at most 10 corrupted are true: i,j xei e1j ≤ 10. Mean Rank is missing for ILP method as we could not rank the binary decision variables. Baseline methods. For compa"
C16-1161,P12-1012,0,0.0169546,"rnal information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Con"
C16-1161,D15-1038,0,0.0323252,"016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin,"
C16-1161,P15-1009,0,0.0161756,"esearch related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky an"
C16-1161,D15-1082,0,0.194831,"r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a temporal order amo"
C16-1161,W09-2418,0,0.0301052,"o et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure"
C16-1161,N13-1008,0,0.0132003,"08] and a person cannot marry two people at the same time. 5 Related Work There are two lines of research related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large"
C16-1161,N15-1118,0,0.0239417,"Missing"
C16-1161,D14-1167,0,0.148049,"Projection r1 r2 r2 r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a"
C16-1270,D15-1075,0,0.37321,"epresent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) c"
C16-1270,P16-1139,0,0.220145,"(Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. 2871 Premise representation ⨁ ~ ~ (a) LSTM Unit (b) rLSTM Unit Figure 1: The inner architecture of the traditional LSTM unit and the re-read LSTM unit. 3 3.1 Model Background The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this probl"
C16-1270,D16-1053,0,0.576145,"eep neural network for classification. However, in the sentence encoding process, the premise and the hypothesis cannot affect each other. It is well known that the encoding procedure is just automatically learning useful features. Without the impact between the two sentences, it is difficult for the encoder to extract the sentence-relationship-specific features. Other methods mainly make use of attention mechanism to capture the word-by word alignment information while training (Rockt¨aschel et al., 2015) or just integrate memory network into LSTM to make the model remember more information (Cheng et al., 2016). Among them, only the attention mechanism can make the two sentences contact with each other. However, the word-by-word attention does not represent a better understanding of the sentences. When deciding the entailment relationship between a pair of sentences, what is really matters? Unlike paraphrasing and machine translation, entailment relationship does not force the two sentences have the same meaning. Instead, as long as the premise can cover the meaning of the hypothesis, the entailment stands. Therefore, if the premise entails the hypothesis, that doesn’t really mean that the words in"
C16-1270,E09-1025,0,0.0238895,"ctional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the e"
C16-1270,C08-1043,0,0.0255133,"rage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the atten"
C16-1270,D16-1176,0,0.109637,"Missing"
C16-1270,W07-1407,0,0.0125968,"premise, and use a bidirectional rLSTM to read the hypothesis. The output of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check"
C16-1270,P16-2022,0,0.225039,"can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it tr"
C16-1270,D16-1244,0,0.295878,"Missing"
C16-1270,D14-1162,0,0.0887401,"premise and the hypothesis. 4.1 Datasets and Model Configuration We conduct experiments on the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015). The original data set contains 570,152 sentence pairs, each labeled with one of the following relationships: entailment, contradiction, neutral and −, where − indicates a lack of consensus from the 2874 human annotators. We discard the sentence pairs labeled with − and keep the remaining ones for our experiments. Table 2 summarizes the statistics of the three entailment classes in SNLI. We use 300 dimensional GloVe embeddings (Pennington et al., 2014) to represent words, which is trained on the Wikipedia+Gigaword dataset. The embeddings of unknown tokens are initialized by random vectors. 4.2 Methods for Comparison Although we list all of the approaches designed for recognizing textural entailment task on the SNLI dataset in Table 3, we mainly want to compare our model with the word-by-word attention model by Rockt¨aschel et al. (2015), long short term memory network model by Cheng et al. (2016) and the decomposable attention model by Parikh et al. (2016) since they are either related to our work or achieved the state-of-the-art performanc"
C16-1270,D15-1185,1,0.872603,"ariety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based techniq"
C16-1270,P11-2098,0,0.0237541,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,W11-2402,0,0.0166197,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,U06-1019,0,0.0101027,"utput of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer"
C16-1270,W07-1406,0,0.0319259,"eral input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used"
C16-1270,P06-1051,0,0.0406042,"ard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabi"
C18-1073,P04-3031,0,0.148766,"0.85 with passages in the CLOTH dataset. Furthermore, background text corpora also include training passages from the CLOTH dataset by filling the correct answer back into the corresponding blank. Accuracy is used as the evaluation metric. To make a fair comparison with Xie et al. (2017), we also report performance on CLOTH-M(middle school questions) and CLOTH-H(high school questions). Hyperparameters Our model is implemented with Tensorflow (Abadi et al., 2016). Hyperparameters are optimized with random search based on validation data. All our models are run on a single GPU(Tesla P40). NLTK (Bird and Loper, 2004) is used for tokenization. Word embeddings are initialized with 1 https://dumps.wikimedia.org/enwiki/ 861 300-dimensional GloVe (Pennington et al., 2014) vectors. Only vectors of top 1000 frequent words are fine-tuned during training. Our network is trained with Adam algorithm (Kingma and Ba, 2014). The initial learning rate is set to 10−3 . We decrease learning rate to 10−4 after 15k iterations and further decrease it to 10−5 after 50k iterations. Both forward and backward GRU have 128 hidden units. For input, we use a context window of 80 words. For 1D dilated convolution, we use 2 blocks, t"
C18-1073,P16-1223,0,0.177409,"actical requirements and is relatively easy to design. The research of cloze-style reading comprehension is first advanced by two large-scale corpora: the CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al., 2015) datasets, which are automatically constructed by randomly or periodically deleting a word from original passage. Though the automatically generated datasets usually consist of a large quantity of labeled data and make it possible to train large neural network models, they are in nature far away from real-world language understanding problems and have serious ambiguity issues (Chen et al., 2016). As a result, the state-of-the-art system of cloze test almost reaches the performance ceiling and loses its improvement direction due to the limitation of the corpus (Chen et al., 2016). In such situation, Xie et al. (2017) argues that it is a more reliable means to assess language proficiency with carefully designed questions by professional teachers, and releases a novel corpus CLOTH. The CLOTH dataset brings the new challenge of exploring a comprehensive evaluation of language proficiency and specifically divides the questions into several types including matching, reasoning and grammar e"
C18-1073,D17-1214,0,0.0216024,"(Paperno et al., 2016) and CLOTH (Xie et al., 2017) are all large-scale cloze-test datasets, with the difference that each question in CLOTH has four candidate options. Recently proposed Story Cloze (Mostafazadeh et al., 2017) is a cloze-test dataset that goes beyond words and phrases, and requires choosing a sentence as the appropriate story ending. Semi-supervised Methods for reading comprehension are widely studied due to the fact that labeled data is scarce. One major approach is pretraining a model for text representation and reusing the weights during supervised learning. Autoencoders (Hewlett et al., 2017), machine translation (McCann et al., 2017) and language model (Peters et al., 2018) can be used for representation learning. Another approach aims to directly construct training examples from unlabeled text corpora. Weighted loss function (Xie et al., 2017) and reinforcement learning (Yang et al., 2017) can be used to alleviate the discrepancy between human-labeled data and automatically-constructed data. 5 Conclusion In this paper, we propose a multi-perspective network MPNet for cloze-style reading comprehension. MPNet consists of several parallel context aggregation modules. Each module su"
C18-1073,D14-1181,0,0.00265399,"able 1. Attentive reader proposed by Chen et al. (2016) directly attends to the entire context and therefore avoids the difficulty of modeling long-range dependence. Original bilinear attention function (Chen et al., 2016) is slightly modified by introducing bar to model attention bias towards the ith word. u is the vector representation of a candidate, we omit its subscript for simplicity. αi = softmaxi (uT War hi + bTar hi ), i ∈ [1, n] n ∑ par = α i hi i=1 859 (2) • Iterative Dilated Convolution Convolutional neural networks have been a successful method for modeling both natural language (Kim, 2014) and images. Multiple layers of CNNs can extract features in a hierarchical way, which shares similarity with the compositional property of natural language. Dilated convolution is a variant of traditional convolution and is more efficient for multi-scale context aggregation (Yu and Koltun, 2015; Strubell et al., 2017). In this work, we use two blocks where each block consists of two dilated convolutions with dilation rate set to 1 and 3 respectively. Max pooling across filters is applied to get the final output pidc . • N-gram Statistics To explicitly incorporate collocation information, we u"
C18-1073,W17-0906,0,0.0174933,"res may have very different semantic meanings. Cloze Test is a particular form of reading comprehension task and has been widely adopted as a method for assessing students’ language proficiency. Zweig and Burges (2011) presented a challenging dataset for sentence completion but its scale is too small with only 1040 questions. CNN/Daily Mail (Hermann et al., 2015), CBT (Hill et al., 2015), LAMBADA (Paperno et al., 2016) and CLOTH (Xie et al., 2017) are all large-scale cloze-test datasets, with the difference that each question in CLOTH has four candidate options. Recently proposed Story Cloze (Mostafazadeh et al., 2017) is a cloze-test dataset that goes beyond words and phrases, and requires choosing a sentence as the appropriate story ending. Semi-supervised Methods for reading comprehension are widely studied due to the fact that labeled data is scarce. One major approach is pretraining a model for text representation and reusing the weights during supervised learning. Autoencoders (Hewlett et al., 2017), machine translation (McCann et al., 2017) and language model (Peters et al., 2018) can be used for representation learning. Another approach aims to directly construct training examples from unlabeled tex"
C18-1073,P16-1144,0,0.0695555,"Missing"
C18-1073,D14-1162,0,0.0810199,"rect answer back into the corresponding blank. Accuracy is used as the evaluation metric. To make a fair comparison with Xie et al. (2017), we also report performance on CLOTH-M(middle school questions) and CLOTH-H(high school questions). Hyperparameters Our model is implemented with Tensorflow (Abadi et al., 2016). Hyperparameters are optimized with random search based on validation data. All our models are run on a single GPU(Tesla P40). NLTK (Bird and Loper, 2004) is used for tokenization. Word embeddings are initialized with 1 https://dumps.wikimedia.org/enwiki/ 861 300-dimensional GloVe (Pennington et al., 2014) vectors. Only vectors of top 1000 frequent words are fine-tuned during training. Our network is trained with Adam algorithm (Kingma and Ba, 2014). The initial learning rate is set to 10−3 . We decrease learning rate to 10−4 after 15k iterations and further decrease it to 10−5 after 50k iterations. Both forward and backward GRU have 128 hidden units. For input, we use a context window of 80 words. For 1D dilated convolution, we use 2 blocks, the number of filters is 128 and the convolution width is 3 for all layers. Batch normalization and ReLU are applied on top of convolution. Gradients are"
C18-1073,N18-1202,0,0.441532,"er, a multi-perspective aggregation layer and an output layer. Input Layer Given the passage as a variable-length word sequence {wi }ni=1 , we embed each word into 300-dimensional word embeddings {ei }ni=1 using GloVe vectors. Then, we apply bidirectional GRU(BiGRU) on {ei }ni=1 to get contextualized word representations {hi }ni=1 (McCann et al., 2017) 858 Pointer Net P1 … P2 Pt-1 Pt Candidates M2 M1 … Mt-1 Mt BiGRU The news ____ him so much Figure 1: MPNet: Multi-Perspective context aggregation network. We only show part of the context “The news much” and “ ” is the blank to fill in. him so (Peters et al., 2018), since GRU is computationally more efficient and shows slightly better performance than LSTM. − → → −−−→ − h i = GRU ( h i−1 , ei ) ← − − ←−−− ← (1) h i = GRU ( h i+1 , ei ) − → ← − hi = [ h i ; h i ] |c| |c| We also use another GRU to encode candidates {ci }i=1 into fixed-length vectors {ui }i=1 , as candidates may be multi-word phrases. Multi-Perspective Aggregation Layer This layer consists of several independent aggregation modules {Mi }ti=1 . Computation can be easily parallelized since modules are independent. Each module Mi takes |c| contextualized word representations {hi }ni=1 and ca"
C18-1073,D16-1264,0,0.0404838,"ave a different word distribution. Thus, how to make use of huge unlabeled data to help training is a key for performance improvement, since training corpora of higher quality are generally smaller in scale. 4 Related Work Reading Comprension or machine reading is drawing more and more interests among NLP research communities. The CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al., 2015) are two automatically generated cloze-style datasets. Though they can be large in scale, the quality of automatically generated questions is generally lower than manually labeled ones. Instead, SQuAD (Rajpurkar et al., 2016) adopts a crowd-sourcing approach to ensure its quality. In SQuAD, each passage accompanies one or more questions and the answer is a text span of the given passage for the convenience of automatic evaluation. Rapid progress has been made with neural network based models (Wang et al., 2016). The performances of state-of-the-art models on SQuAD such as QANet (Yu et al., 2018) and ELMo (Peters et al., 2018) are already very close to human. There are also some datasets focusing on answering questions from real-world scenarios. MS MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) are two"
C18-1073,D17-1283,0,0.0220646,"u is the vector representation of a candidate, we omit its subscript for simplicity. αi = softmaxi (uT War hi + bTar hi ), i ∈ [1, n] n ∑ par = α i hi i=1 859 (2) • Iterative Dilated Convolution Convolutional neural networks have been a successful method for modeling both natural language (Kim, 2014) and images. Multiple layers of CNNs can extract features in a hierarchical way, which shares similarity with the compositional property of natural language. Dilated convolution is a variant of traditional convolution and is more efficient for multi-scale context aggregation (Yu and Koltun, 2015; Strubell et al., 2017). In this work, we use two blocks where each block consists of two dilated convolutions with dilation rate set to 1 and 3 respectively. Max pooling across filters is applied to get the final output pidc . • N-gram Statistics To explicitly incorporate collocation information, we use this module to output logarithmic n-gram counts png from English Wikipedia with n ∈ [1, 5]. Logarithmic function avoids the optimization difficulty with extremely large numbers. Note that the output psc from selective copying module and pidc from iterative dilated convolution module don’t depend on the candidates. W"
C18-1073,P17-1096,0,0.122291,"g a sentence as the appropriate story ending. Semi-supervised Methods for reading comprehension are widely studied due to the fact that labeled data is scarce. One major approach is pretraining a model for text representation and reusing the weights during supervised learning. Autoencoders (Hewlett et al., 2017), machine translation (McCann et al., 2017) and language model (Peters et al., 2018) can be used for representation learning. Another approach aims to directly construct training examples from unlabeled text corpora. Weighted loss function (Xie et al., 2017) and reinforcement learning (Yang et al., 2017) can be used to alleviate the discrepancy between human-labeled data and automatically-constructed data. 5 Conclusion In this paper, we propose a multi-perspective network MPNet for cloze-style reading comprehension. MPNet consists of several parallel context aggregation modules. Each module summarizes the variablelength context and candidates into a fixed-length vector from a unique perspective. We explore four effective implementations of aggregation modules in experiments. The architecture of MPNet is very flexible and can be easily extended by adding more task-specific modules. To overcome"
D12-1114,D08-1031,0,0.595918,"irwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logi"
D12-1114,W11-1904,0,0.0357831,".20 51.46 57.50 54.53 - MUC P 64.05 64.10 68.52 68.40 59.10 62.25 - F 60.89 60.90 59.26 58.73 58.30 58.13 55.8 R 67.11 67.12 60.85 59.79 71.00 63.72 - B-cube P 73.88 74.13 80.15 81.69 69.20 73.83 - F 70.33 70.45 69.18 69.04 70.10 68.40 69.29 R 47.6 47.70 51.6 53.03 48.10 47.20 - CEAF P 41.92 41.96 37.05 37.84 46.50 40.01 - F 44.58 44.65 43.13 44.17 47.30 43.31 43.96 Avg F 58.60 58.67 57.19 57.31 58.60 56.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner ("
D12-1114,C10-1019,0,0.0216608,"graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominative"
D12-1114,H05-1013,0,0.0639415,"Missing"
D12-1114,N07-1030,0,0.114892,"m joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as Search Optimization (LaSO) framework. Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. 6 Conclusion In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. Experimental results show that the joint learning method significantly outperforms baseline methods. Our method is also better than all the learning-based systems in CoNLL-2011 and reaches the same level of performance with the best system. In the future we will try to design more global constraints and"
D12-1114,D08-1069,0,0.0395601,"tems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and m"
D12-1114,P08-2012,0,0.439324,"s used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperforms a baseline that represents a typical system using the mention-pair model. Our method is also better than all learning systems from the Co"
D12-1114,W11-1902,0,0.149477,"Missing"
D12-1114,P04-1018,0,0.0629881,"6.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning"
D12-1114,H05-1004,0,0.235132,"rom the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is tagged in OntoNotes, but not appositives or predicate nomin"
D12-1114,P02-1014,0,0.826267,"ng methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phras"
D12-1114,P10-1142,0,0.0271294,"ms of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a bin"
D12-1114,D08-1068,0,0.455659,"how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weighted first-order clauses. In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good 1245 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity const"
D12-1114,D09-1001,0,0.030231,"se variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annot"
D12-1114,W11-1901,0,0.309737,"tion y ′ (i.e. the one with the highest score s(y ′ , xi ), equivalent to yˆ in Section 3.3)) is at least as big as the loss L(yi , y ′ ), while changing wt−1 as little as possible. The number of false ground atoms of coref predicate is selected as loss function in our experiments. Hard global constraints (i.e. best-first clustering or transitivity constraint) must be satisfied when inferring the best y ′ in each iteration, which can make learned weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different"
D12-1114,D09-1101,0,0.271277,"aset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in"
D12-1114,W11-1903,0,0.0338164,"Missing"
D12-1114,J01-4004,0,0.681245,"the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction"
D12-1114,W11-1908,0,0.0127817,"ion and mention clustering. 2.1 Mention Detection For mention detection, traditional methods include learning-based and rule-based methods. Which kind of method to choose depends on specific dataset. In 1246 this paper, we first consider all the noun phrases in the given text as candidate mentions. Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al. (2011), Uryupina et al. (2011)). Some examples of these erroneous candidates include stop words (e.g. uh, hmm), web addresses (e.g. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that uses maximum entropy mo"
D12-1114,M95-1005,0,0.527813,"weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is t"
D12-1114,P08-1096,0,0.0425499,": Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform j"
D12-1114,P09-1046,0,0.0740571,"y conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and"
D12-1114,D08-1067,0,\N,Missing
D13-1001,D08-1073,0,0.163414,"al analysis, document timestamps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). I"
D13-1001,P12-1011,0,0.0784756,"ocument retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discriminative classifiers – maximum entropy (MaxEnt) classifiers were used by incorporating linguistic features and temporal con"
D13-1001,de-marneffe-etal-2006-generating,0,0.00993176,"Missing"
D13-1001,D11-1142,0,0.0108213,"allenges and their corresponding solutions are presented. We first discuss the event extraction and processing involving relative temporal relation mining, event coreference resolution and distinguishing specific extractions from generic ones in Section 3.1. Then, we show the confidence boosting algorithm in detail in Section 3.2. 3.1 Event extraction and processing As mentioned in previous sections, events play a key role in the propagation models. We define an event as a Subject-Predicate-Object (SPO) triple. To extract events from raw text, an open information extraction software - ReVerb (Fader et al., 2011) is used. ReVerb is a program that automatically identifies and extracts relationships from English sentences. It takes raw text as input and outputs SPO triples which are called extractions. However, extractions extracted by ReVerb cannot be used directly for our propagation models for three main reasons. First, the relative temporal relations between documents and the extractions are unavailable. Second, the extractions extracted from different documents do not have any connection even if they refer to the same event. Third, propagations from generic events are very likely to lead to propaga"
D13-1001,P09-1046,0,0.0213263,"mps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discri"
D14-1186,C14-1035,0,0.0603901,"Missing"
D14-1186,P06-1085,0,0.305601,"ghbors. In a text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying. At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |"
D14-1186,C12-1098,1,0.839502,"llected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |V | Figure 1: DP-4 Model named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documen"
D14-1186,P09-1012,0,0.200969,"Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |V | Figure 1: DP-4 Model named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our model can be easily used as a semi-supervised framework, through exerting on the corpus level the word distributions learned from the available segmentation results. Referring to the work of Mochihashi et al. (2009), we conduct word segmentation using a sentence-wise Gibbs sampler, which combines the Gibbs sampling techniques with the dynamic programming strategy. During the sampling process, the importance values of segmented words are measured in domains and documents respectively, and words, terms and keywords are jointly learned. 2 DP-4 Model Goldwater et al. (2006) applied the HDP model on the word segmentation task. In essence, Goldwater’s model can be viewed as a bigram language model with a unigram back-off. With the language model, word segmentation is implemented by a character-based Gibbs samp"
D14-1186,W06-0127,0,0.0362503,"eness of our method. 1 •Ï (heparinoid) Ç(thrombocytopenia) (have) (with) sû(relation). This is a correctly segmented Chinese sentence. The document containing the example sentence mainly talks about the property of ”{ • (heparinoid)” which can be regarded as one keyword of the document. At the same time, the word @ •Ï Ç(thrombocytopenia) appears frequently in the disease domain and can be treated as a domain-specific term. However, for such a simple sentence, current segmentation tools perform poorly. The segmentation result with the state-of-the-art Conditional Random Fields (CRFs) approach (Zhao et al., 2006) is as follows: @ •(blood platelet) Ï (reduction) Ç(symptom) {(of same kind) •(liver) Introduction For Chinese language which does not contain explicitly marked word boundaries, word segmentation (WS) is usually the first important step for many Natural Language Processing (NLP) tasks including term extraction (TE) and keyword extraction (KE). Generally, Chinese terms and keywords can be regarded as words which are representative of one domain or one document respectively. Previous work of TE and KE normally used the pipelined approaches which first conducted WS and then extracted important wo"
D14-1198,P11-1056,0,0.114042,"Missing"
D14-1198,chang-manning-2012-sutime,0,0.0071302,"3.1 • An entity mention is correct if its entity type (7 in total) and head offsets are correct. • A relation is correct if its type (6 in total) and the head offsets of its two arguments are correct. • An event trigger is correct if its event subtype (33 in total) and offsets are correct. • An argument link is correct if its event subtype, offsets and role match those of any of the reference argument mentions. Rank 1 2 3 4 5 6 7 8 9 10 In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.2 Results Based on the results of our development set, we trained all models with 21 iterations and chose the beam size to be 8. For the k-best MIRA updates, we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods. Discussions Feature Study Feature Frame=Killing Die Frame=Travel Transport Physical(Artifact, Destination) w1 =“home” Transport Frame=Arriving Transport ORG-AFF(Person, Entity) Lemma=charge Charge-Indict Lemma=birth Be-Born Physical(Artifact,Origin) Frame=Cause harm Injure Weight 0.80 0.61 0.60 0.59 0.54 0.48 0.45 0.44 0.44 0.43 Table 4: To"
D14-1198,P04-1015,0,0.0284067,"b is the beam size, and m is the sentence length. The actual execution time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f (x, y 0 ) · w y 0 ∈Y(x) and then updates the feature weights by using: wnew = w + f (x, y) − f (x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min kwnew − wk s.t. wnew f (x, y) − wnew f (x, z) ≥ L(y, z) ∀z ∈ best"
D14-1198,W02-1001,0,0.0242356,"d joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relation"
D14-1198,N12-1015,0,0.0158919,"ion time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f (x, y 0 ) · w y 0 ∈Y(x) and then updates the feature weights by using: wnew = w + f (x, y) − f (x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min kwnew − wk s.t. wnew f (x, y) − wnew f (x, z) ≥ L(y, z) ∀z ∈ bestk (x, w) We employ the following three loss functions for comp"
D14-1198,P08-1030,1,0.435611,"Missing"
D14-1198,P14-1038,1,0.942874,"ty mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algor"
D14-1198,P13-1008,1,0.8719,"erdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based"
D14-1198,P05-1012,0,0.091829,"nts (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relations and events. For example, in Figure 1, if we know that the Person mention “Asif Mohammed Hanif ” is an Attacker of the Atta"
D14-1198,D11-1001,0,0.13364,"ons, relations and events from unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argumen"
D14-1198,W09-1406,0,0.170416,"and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perc"
D14-1198,P13-1161,0,0.0160849,"rom unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We ap"
D14-1198,P03-2030,0,\N,Missing
D15-1098,W13-3512,0,0.154906,"Missing"
D15-1098,P14-2131,0,0.0662531,"Missing"
D15-1098,D14-1082,0,0.0976709,"Missing"
D15-1098,cheng-etal-2014-parsing,0,0.0275981,"Missing"
D15-1098,C14-1015,0,0.0585293,"Missing"
D15-1098,P14-1146,0,0.0260875,"Missing"
D15-1098,P12-1092,0,0.014382,"work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line"
D15-1098,N15-1142,0,0.0132362,"Missing"
D15-1098,D14-1108,0,0.0310523,"Missing"
D15-1098,D13-1136,0,0.0522196,"Missing"
D15-1098,P14-2050,0,0.111025,"ly develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the o"
D15-1098,N15-1069,0,0.0148434,"enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the word"
D15-1098,Q15-1016,0,0.0671812,"Missing"
D15-1098,P14-2089,0,0.0295662,"Missing"
D15-1098,P14-1140,0,0.035612,"Missing"
D15-1098,P13-1013,0,0.0493386,"Missing"
D15-1185,D11-1142,0,0.0773194,"married to a doctor who lives in Austin, the capital of Texas. T1. bemarriedTo(Ayrton Senna,Doctor) T1. Ayrton Senna was married to a doctor T2. livein(Doctor, Austin) T2. [The] doctor lives in Austin T3. beCapitalof(Austin, Texas) T3. Austin [is] the capital of Texas H1. livein(Ayrton Senna, Texas) Hypothesis: Ayrton Senna lives in Texas. Hypothesis: Ayrton Senna lives in Texas. Figure 2: Text Commitments Example Figure 3: Text Predicates Example ments to predicates. For example, the commitments in Figure 2 can be transformed to the predicates (or triples) shown in Figure 3. We use R E VERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the inference process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, replace it with that NP, and the predicates are successfully extracted. facts. We use AMIE to mine inference rules from YAGO. AMIE1 (Gal´arraga et al., 2013) is a state-ofthe-art inference rule mining system. The motivation of AMIE is that KBs themselves often already contain enough information to derive and add new facts. If, for ex"
D15-1185,H05-1049,0,0.143446,"n et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likel"
D15-1185,C08-1043,0,0.722221,"atural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely to support the"
D15-1185,N06-1006,0,0.0918988,"Missing"
D15-1185,W07-1407,0,0.79166,"(entailed) from the other one (T )(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments fro"
D15-1185,P09-1113,0,0.0204461,"in Austin, the capital of Texas, in 1998. H: Ayrton Senna lives in Texas. R T: R1(e11,e12) R2(e21,e22) R3(e31,e32) H: RH(eH1,eH2) Discourse commitment extract YAGO R .. R YAGO H Inference rules R R T R AIME .. R R R1(e11,e12)^R2(e21,e22)=&gt;R3(e31,e32) R4(e41,e42)^R5(e51,e52)=&gt;R6(e61,e62) R7(e71,e72)^R8(e81,e82)=&gt;R9(e91,e92) MLN Construct Markov Logic Network .. Rp(ep1,ep2)^Rq(eq1,eq2)=&gt;Rr(er1,er2) facts P(RH) H is True or false? train Figure 1: The Framework of our RTE system pose to use the predicate-argument structure to represent the extracted discourse commitments. Inspired by the work of (Mintz et al., 2009), we make use of the external knowledge YAGO and borrow the distant supervision technique to mine implicit facts for the extracted predicates. For example, Ayrton Senna was married to a doctor who lives in Austin, the capital of Texas, in 1998. We translate this example into the predicateargument structures such as bemarried(Senna, doctor), livein(doctor, Austin), captial(Austin, Texas). Then through distant supervision, we can get some new facts livein(Senna, Austin), livein(Senna, Texas). To judge the confidence of the new facts, we construct a probabilistic network with all the facts and ad"
D15-1185,P06-2105,0,0.216099,"of the commitments from H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Align the predicates to the YAGO database T: Ayrton Senna was married to a doctor"
D15-1185,U06-1019,0,0.10598,"ng can be corrected by our framework. For instance, T is “Hughes loved his wife, Gracia, and was absolutely obsessed with his little daughter Elicia.” and H is “Gracia’s daughter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T 0 , H 0 ) and (T 00 , H 00 ). Secondly, some approaches extract the knowledge in T -H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T -H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the t"
D15-1185,P06-1051,0,0.467153,"ing predicates and mining inference rules. YAGO2 contains more than 940K facts and about 470K entities. We run the AMIE system on YAGO2 for only one time to get all inference rules (about more than 1.8K in total). For each T -H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T -H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T -H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which"
D16-1075,P15-1056,1,0.182234,"challenges for summarizing a text stream. First, a stream summarization model should be able to be aware of redundant information in the stream for avoiding generating redundant content in the summary; second, a stream summarization algorithm should be capable of analyzing text content on the stream level for identifying the most important information in the stream; third, a stream summarization model should be efficient, scalable and able to run in an online fashion because data size of a text stream is usually huge, and it is dynamic and updated every second. The previous approaches (e.g., (Ge et al., 2015b)) tend to cluster similar documents as event detection to avoid redundancy, rank the clusters based on their sizes and topical relevance to the reference summaries, and select one document from each cluster as representative documents. Due to the high time complexity of clustering models, their approaches usually run slowly and are not scalable. 785 To overcome the limitations, we propose Burst Information Networks (BINet) as a novel representation of a text stream. In a BINet (Figure 2), a node is a burst word (including entities) with the time span of one of its burst periods, and an edge"
D16-1075,P15-1155,0,0.0215125,"xt stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem in the big data age a"
D16-1075,P13-2099,1,0.841386,"on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem"
D16-1075,W04-1013,0,0.022973,"Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated by various approaches, annotated each entry based on the reference summary and the manually edited event chronicles on the web, and used precision@K to evaluate the quality of top K event entries in a stream summary instead of using ROUGE (Lin, 2004) because news stream summaries are eventcentric. In this paper, we adopt the same evaluation setting and use the same reference summaries and the annotations with our previous work (Ge et al., 2015b) to evaluate our summaries’ quality. For the event entries that are not in Ge et al. (2015b)’s annotations, we have 3 human judges annotate them according to the previous annotation guideline and consider an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseli"
D16-1075,P14-5010,0,0.00387579,"APW and XIN news stories in English Gigaword (Graff et al., 2003)) as a news stream. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which models word burst detection as a burst state decoding problem. In total, there are 140,557 documents in the dataset. Topic Disaster Sports Politics Military Comprehensive #Entry 35 19 8 14 85 #Entry in corpus 28 12 5 13 64 Table 2: The number of event entries in the reference summaries. The third column is the number of event entries excluding those events that do not appear in the corpus. We removed stopwords and used Stanford CoreNLP (Manning et al., 2014) to do lemmatization and named tagging, and built BINets on the news stream during 2009 and 2010 separately. On the 2009 news stream, there are 31,888 nodes and 833,313 edges while there are 32,997 nodes and 825,976 edges on the 2010 stream. Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated"
D16-1075,Q14-1015,0,0.0274957,"marization challenge. However, they studied the problem on a static timestamped corpus instead of on a dynamic text stream and their proposed pipeline-style approach cannot be applied on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select importan"
D16-1075,P12-2009,0,0.164471,"r an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseline randomly selects documents in the dataset as event entries. • N B: this baseline uses Naive Bayes to cluster documents for event detection and ranks the clusters based on the combination score of topical relevance and the event impact (i.e., event cluster size). The earliest documents in the topranked clusters are selected as entries. • B-H AC: similar to N B except that BurstVSM representation (Zhao et al., 2012) is used for event detection using Hierarchical Agglomerative Clustering algorithm. • TA HBM: similar to N B except that the stateof-the-art event detection model (TaHBM) proposed by Ge et al. (2015b) is used for event detection. • Ge et al. (2015b): the state-of-the-art stream summarization approach which used TaHBM to detect events and L2R model to rank events. Note that we did not compare with previous multidocument summarization models because the goal and setting of stream summarization are different from multi-document summarization, as Section 1 https://en.wikipedia.org/wiki/2009 Random"
D16-1075,N16-3015,0,\N,Missing
D16-1130,W05-0613,0,0.03059,"ces to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the proc"
D16-1130,D15-1262,0,0.470245,"ic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy classifier. It is noted that P&C2012 and J&E2015 merged the “EntRel” relation into the “Expansion” relation1 . For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. Our NNMA model with two attention levels exhibits obvious advantages over the six baseline methods on the whole. It is worth noting that NNMA is even better than the R&X2015 approach which employs extra data. As for the performance on each discourse relation, with respect to the F1 meas"
D16-1130,Q15-1024,0,0.622009,"2), we use the following formulae for Arg-1. where Wp ∈ Rn×6d and bp ∈ Rn are the transformation weights. 2.3 Model Training To train our model, the training objective is defined as the cross-entropy loss between the outputs of the softmax layer and the ground-truth class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks. To avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). T"
D16-1130,N16-1037,0,0.416089,"than the R&X2015 approach which employs extra data. As for the performance on each discourse relation, with respect to the F1 measure, we can see that our NNMA model can achieve the best results on the “Expansion”, “Expansion+EntRel” and “Temporal” relations and competitive results on the “Contingency” relation . The performance of recognizing the “Comparison” relation is only worse than R&X2014 and R&X2015. As (Rutherford and Xue, 2014) stated, the “Comparison” relation is closely related to the constituent parse feature of the text, like production rules. How to represent and 1229 • Ji2016: Ji et al. (2016) proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences. 1 EntRel is the entity-based coherence relation which is independent of implicit and explicit relations in PDTB. However some research merges it into the implicit Expansion relation. exploit these information in our model will be our next research focus. 3.3 Analysis of Attention Levels The multiple attention levels in our model greatly boost the performance of classifying implicit discourse relations. In this subsection, we perform both qualitative"
D16-1130,D12-1083,0,0.0207705,"s such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has t"
D16-1130,D14-1181,0,0.00743079,"th Expansion relation Figure 2: Visualization Examples: Illustrating Attentions Learned by NNMA. (The blue grid means the the attention on this word is lower than the value of a uniform distribution and the red red grid means the attention is higher than that.) Arg-1 Arg-2 0.6 0.5 0.4 0.3 0.2 0.1 0 l Figure 3: KL-divergences between attention levels latent variables connecting two token sequences and trained a discourse informed language model. 4.2 Neural Networks and Attention Mechanism Recently, neural network-based methods have gained prominence in the field of natural language processing (Kim, 2014). Such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Collobert et al., 2011). Attention mechanism was first introduced into neural models to solve the alignment problem 1231 between different modalities. Graves (2013) designed a neural network to generate handwriting based on a text. It assigned a window on the input text at each step and generate characters based on the content within the window. Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribut"
D16-1130,D09-1036,0,0.666984,"ided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012). Implicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is (Pitler et al., 2009). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans. Very recently, Liu et al. (2016) proposed a twodimensional convolutional neural network (CNN) to model the argument pairs and employed a multitask learning framework to boost the performance by learnin"
D16-1130,N13-1090,0,0.0262584,"attention level, we adopt the attention mechanism to determine which words should be focused on. An external short-term memory is designed to remember what has seen in the prior levels and guide the attention tuning process in current level. Specifically, in the first attention level, we concatenate R01 , R02 and R01 −R02 and apply a non-linear transformation over the concatenation to catch the general understanding of the argument pair. The use of R01 −R02 takes a cue from the difference between two vector representations which has been found explainable and meaningful in many applications (Mikolov et al., 2013). Then, we get the memory vector M1 ∈ Rdm of the first attention level as M1 = tanh(Wm,1 [R01 , R02 , R01 −R02 ]) (9) where Wm,1 ∈ Rdm ×6d is the weight matrix. With M1 recording the general meaning of the argument pair, our model re-calculates the importance of each word. We assign each word a weight measuring to what degree our model should pay attention to it. The weights are so-called “attention” in our paper. This process is designed to simulate the process that we re-read the arguments and pay more attention to some specific words with an overall understanding derived from the first-pass"
D16-1130,W12-1614,0,0.16192,"he State-of-the-art Approaches. that the “Comparison” relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the “Comparison” depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010). Next, we compare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed th"
D16-1130,D14-1162,0,0.0873507,"Missing"
D16-1130,P09-1077,0,0.565611,"many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific lear"
D16-1130,prasad-etal-2008-penn,0,0.897266,"h class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks. To avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). Table 1 summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal. Relation Comparison Contingency Expansion Temporal Total Train 1855 3235 6673 582 12345 Dev 189 281 638 48 1156 Test 145 273 538 55 1011"
D16-1130,E14-1068,0,0.574712,"ning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy classifier. It is noted that P&C2012 and J&E2015 merged the “EntRel” relation into the “Expansion” relation1 . For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. O"
D16-1130,N15-1081,0,0.513125,"fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). Table 1 summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal. Relation Comparison Contingency Expansion Temporal Total Train 1855 3235 6673 582 12345 Dev 189 281 638 48 1156 Test 145 273 538 55 1011 Table 1: Statistics of Implicit Discourse Relations in PDTB. We first convert the tokens in PDTB to lowercase. The word"
D16-1130,N03-1030,0,0.0845232,") support a set of sentences to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, re"
D16-1130,N09-1064,0,0.0940206,"Missing"
D16-1130,P15-2116,0,0.0604773,"Missing"
D16-1130,C12-1168,1,0.854064,"159 Wall Street Journal articles. Each document is annotated with the predicate-argument structure, where the predicate is the discourse connective (e.g. while) and the arguments are two text spans around the connective. The discourse connective can be either explicit or implicit. In PDTB, a hierarchy of relation tags is provided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012). Implicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is (Pitler et al., 2009). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem."
D16-1130,D15-1266,0,0.443472,"pare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy c"
D16-1130,C10-2172,0,0.533579,".29 57.17 44.95 57.57 Comp. 31.32 35.93 32.03 39.70 41.00 36.36 37.91 36.70 39.86 Cont. 49.82 52.78 47.08 54.40 53.80 55.76 55.88 54.48 53.69 Binary Expa. Expa.+EntRel 79.22 80.02 68.96 80.22 70.20 80.44 69.40 61.76 69.97 70.43 80.73 69.71 80.86 Temp. 26.57 27.63 20.29 28.70 33.30 27.30 37.17 38.84 37.61 Table 4: Comparison with the State-of-the-art Approaches. that the “Comparison” relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the “Comparison” depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010). Next, we compare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans."
D16-1212,P06-2013,0,0.553957,") task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible ar"
D16-1212,D08-1034,1,0.920352,"Missing"
D16-1212,J02-3001,0,0.822677,"revious works did not explicitly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predica"
D16-1212,P16-1116,1,0.675445,"ial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the two argument relationships explicitly to achieve a better performance on Chinese SRL. 3 Capturing the Relationship Between Arguments We found that there are two typical relationships between candidate arguments: (1) Compatible arguments: if one candidate argument belongs to one 2012 event, then the other is more likely to belong to the same event; (2) incompatible arguments: if one candidate argument belongs to one event, then the other is less likely to belong to the same event. We trained a maximum entropy classifier to predict the relationship between two candidate"
D16-1212,N04-1032,0,0.812915,"citly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is mor"
D16-1212,D09-1153,1,0.914347,"elated Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned a"
D16-1212,P10-2031,0,0.246301,"Relationships for Chinese Semantic Role Labeling Lei Sha, Tingsong Jiang, Sujian Li, Baobao Chang, Zhifang Sui Key Laboratory of Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same pr"
D16-1212,D15-1186,1,0.468647,"neering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument"
D16-1212,W03-1707,0,0.841245,"ure the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candida"
D16-1212,J08-2004,0,0.877101,"when investing entrepreneurs” 2 Related Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese"
D16-1212,D14-1041,0,0.334716,"assified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the"
D16-1260,S13-2002,0,0.0328848,"Missing"
D16-1260,P14-2082,0,0.0158495,"Missing"
D16-1260,P08-1090,0,0.0168089,"Missing"
D16-1260,P07-2044,0,0.0828666,"Missing"
D16-1260,Q14-1022,0,0.0213226,"Missing"
D16-1260,D14-1165,0,0.0225719,"Missing"
D16-1260,D15-1038,0,0.0453683,"Missing"
D16-1260,P15-1009,0,0.0235187,"Missing"
D16-1260,D15-1082,0,0.00895825,"Missing"
D16-1260,N13-1008,0,0.0236909,"Missing"
D16-1260,D14-1167,0,0.0386669,"Missing"
D17-1139,A00-2004,0,0.70769,"l information for tasks such as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of"
D17-1139,N13-1019,0,0.463244,"unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First, it is not precise to measure coherence with text similarity, since text similarity is just one aspect to influence coherence. Second, many assumptions and manually set parameters are usually involved in the complex modeling techniques, due to the abs"
D17-1139,D08-1035,0,0.938593,"rence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First, it is not precise to measure coherence with text similarity, since text similarity is just one aspect to influence coherence. Second, many assumptions and manually set parameters are usually involved in the"
D17-1139,J97-1003,0,0.968799,"h easier to navigate. It also provides helpful information for tasks such as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling tex"
D17-1139,D14-1162,0,0.0809434,"Neural Network News 184 Lecture 120 Report 160 Biography 400 Table 1: Overview of four datasets. score Figure 1: Semantic Coherence Neural Network Baselines To compare with our method, TextTiling (Hearst, 1997), TopicTiling (Riedl and Biemann, 2012b) and BayesSeg (Eisenstein and Barzilay, 2008) are adopted as three baselines. We use open source implementations of TextTiling2 and TopicTiling3 , and results of BayesSeg are from (Jeong and Titov, 2010). Hyperparameters Our neural network implementation is based on Tensorflow (Abadi et al., 2015). We use pre-trained 50 dimensional Glove vectors (Pennington et al., 2014)4 for word embeddings initialization. Each text pair consists of 2 text segments, and each text segment consists of To model the text pair instances, we develop a symmetric convolutional neural network (CNN) architecture, as shown in Figure 1. Our model consists of two symmetric CNN models, and the two CNNs share their network configuration and 1 We do not compare with MultiSeg model proposed by (Jeong and Titov, 2010), since our model is for singledocument topic segmentation while MultiSeg is for multidocument topic segmentation. 2 https://github.com/nltk/nltk/tree/develop/nltk/tokenize 3 htt"
D17-1139,J02-1002,0,0.424949,"fine-tuning, etc. no more than 3 sentences. Stop words and digits are removed from input text, and all words are converted to lowercase. We pad input sequence to 40 tokens. In order to capture information of different granularity, convolution window size of both 2 and 3 are used, with 64 filters for each window size. L2 regularization coefficient is set to 0.001. Adam algorithm (Kingma and Ba, 2014) is used for loss function minimization. We set α to 0.7 for pointwise ranking. Evaluation System performance is evaluated according to three metrics: Pk (Beeferman et al., 1999), WindowDiff(W D) (Pevzner and Hearst, 2002) and F 1 score. Pk and W D are calculated based on sliding windows, and can assign partial score to incorrect segmentation. Note that Pk and W D are penalty metrics, smaller value means better performance. 3.2 Results and Analysis Experimental results are shown in Table 2. Our proposed model is examined in 4 different settings, including whether to use pointwise ranking or pairwise ranking algorithm, and whether to fine-tune word embeddings or not. The best model Ours-pointwise-static is able to achieve better or competitive performance compared to BayesSeg and TopicTiling according to all thr"
D17-1139,W12-3307,0,0.696022,"ch as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First,"
D17-1139,Q16-1019,0,\N,Missing
D18-1072,D14-1162,0,0.0800625,"Missing"
D18-1072,P15-1152,0,0.0422368,"Missing"
D18-1072,N15-1020,0,0.0556934,"Missing"
D18-1072,D14-1179,0,0.00964984,"Missing"
D18-1072,W17-5526,0,0.0489816,"Missing"
D18-1072,E17-1042,0,0.0666184,"Missing"
D18-1116,J15-3002,0,0.379485,"Missing"
D18-1116,P03-1054,0,0.0614017,"egmenter using CRF model; (4) CODRA (Joty et al., 2015) uses fewer features and a simple logistic regression model to achieve impressive results; (5) Reranking (Bach et al., 2012) reranks the N-best outputs of a base CRF segmenter; (6) Two-Pass (Feng and Hirst, 2014) conducts a second segmentation after extracting global features from the first segmentation result. All these methods rely on tree features and we list their performance given different parse trees, where Gold are the trees extracted from the Penn Treebank (Prasad et al., 2005), Stanford represents trees from the Stanford parser (Klein and Manning, 2003) and BLLIP represents those from the BLLIP parser (Charniak and Johnson, 2005). It should be noted that the results of SPADE and CRFSeg are taken from Bach et al. (2012) since the original papers adopt different evaluation metrics. All the other results are taken from the corresponding original papers. From Table 2, we can see that our model achieves state-of-the-art performance without extra parse trees. Especially, if no gold parse trees are provided, our system outperforms other methods by more than 1.7 points in F1 score. Since the gold parse trees are not available when processing new sen"
D18-1116,W12-1623,0,0.452279,"Missing"
D18-1116,W01-1605,0,0.938661,"le achieving new stateof-the-art performance. 1 1 Table 1: A sentence that is segmented into five EDUs But extracting such features usually takes a long time, which contradicts the fundamental role of discourse segmentation and hinders its actual use. Considering the great success of deep learning on many NLP tasks (Lu and Li, 2016), it’s a natural idea for us to design an end-to-end neural model that can segment texts fast and accurately. The first challenge of applying neural methods to discourse segmentation is data insufficiency. Due to the limited size of labeled data in existing corpus (Carlson et al., 2001), it’s quite hard to train a data-hungry neural model without any prior knowledge. In fact, some traditional features, such as the POS tags or parse trees, naturally provide strong signals for identifying EDUs. Removing them definitely increases the difficulty of learning an accurate model. Secondly, many EDU boundaries are actually not determined locally. For example, to recognize the boundary between e3 and e4 in Table 1, our model has to be aware that e3 is an embedded clauses starting from “overlooking”, otherwise it could regard “San Fernando Valley” as the subject of e4 . Such kind of lo"
D18-1116,P05-1022,0,0.0201878,"and a simple logistic regression model to achieve impressive results; (5) Reranking (Bach et al., 2012) reranks the N-best outputs of a base CRF segmenter; (6) Two-Pass (Feng and Hirst, 2014) conducts a second segmentation after extracting global features from the first segmentation result. All these methods rely on tree features and we list their performance given different parse trees, where Gold are the trees extracted from the Penn Treebank (Prasad et al., 2005), Stanford represents trees from the Stanford parser (Klein and Manning, 2003) and BLLIP represents those from the BLLIP parser (Charniak and Johnson, 2005). It should be noted that the results of SPADE and CRFSeg are taken from Bach et al. (2012) since the original papers adopt different evaluation metrics. All the other results are taken from the corresponding original papers. From Table 2, we can see that our model achieves state-of-the-art performance without extra parse trees. Especially, if no gold parse trees are provided, our system outperforms other methods by more than 1.7 points in F1 score. Since the gold parse trees are not available when processing new sentences, this improvement becomes more valuable when the system is put into use"
D18-1116,W16-3617,0,0.0790941,"d on a large corpus into our task, and show that this transIntroduction Discourse segmentation, which divides text into proper discourse units, is one of the fundamental tasks in natural language processing. According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a complex text is composed of non-overlapping Elementary Discourse Units (EDUs), as shown in Table 1. Segmenting text into such discourse units is a key step in discourse analysis (Marcu, 2000) and can benefit many downstream tasks, such as sentence compression (Sporleder and Lapata, 2005) or document summarization (Li et al., 2016). Since EDUs are initially designed to be determined with lexical and syntactic clues (Carlson et al., 2001), existing methods for discourse segmentation usually design hand-crafted features to capture these clues (Feng and Hirst, 2014). Especially, nearly all previous methods rely on syntactic parse trees to achieve good performance. 1 Our code is available at https://github.com/ PKU-TANGENT/NeuralEDUSeg 962 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 962–967 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computationa"
D18-1116,D14-1162,0,0.0949254,"nced by nearby EDUs, thereby forcing the model to attend to the whole sequence will bring in unnecessary noises. Therefore, we propose a restricted self-attention mechanism, which only collects information from a fixed neighborhood. To do this, we first compute the similarity between current word xi and each nearby word xj within a window: T si,j = wattn [hi , hj , hi hj ] 3.2 αi,j = PK k=−K ai = XK j=−K esi,i+k αi,i+k hi+k Dataset and Metrics Implementation Details We tune all the hyper-parameters according to the model performance on the separated validation set. The 300-D Glove embeddings (Pennington et al., 2014) are employed and kept fixed during training. We use the AllenNLP toolkit (Gardner et al., 2018) to compute the ELMo word representations. The hidden size of our model is set to be 200 and the batch size is 32. L2 regularization is applied to trainable variables with its weight as 0.0001 and we use dropout between every two layers, where the dropout rate is 0.1. For model training, we employ the Adam algorithm (Kingma and Ba, 2014) with its initial learning rate as 0.0001 and we clip the gradients to a maximal norm 5.0. Exponential moving average is applied to all trainable variables with a de"
D18-1116,N18-1202,0,0.0299846,"oblem and has been widely used in many NLP tasks (Sutton and McCallum, 2012). To approach our discourse segmentation task in a neural way, we adopt the BiLSTM-CRF model (Huang et al., 2015) as the framework of our system. Formally, given an input sentence x = {xt }nt=1 , we first embed each word into a vector et . Then these word embeddings are fed into a bi-directional LSTM 963 BiLSTM layer together with hi in order to fuse the information: to encode text and capture useful signals. Instead of training the transferred model by ourselves, in this paper, we adopt the ELMo word representations (Peters et al., 2018), which are derived from a bidirectional language model (BiLM) trained on one billion word benchmark corpus (Chelba et al., 2014). Specifically, this BiLM has one character convolution layer and two biLSTM layers, and correspondingly there are three internal representations for each word xt , which are denoted as 3 {hLM t,l }l=1 . Following (Peters et al., 2018), we compute the ELMo representation rt for word xt as follows: rt = γ LM X3 sLM hLM t,l l=0 l ˜ t = BiLSTM(h ˜ t−1 , [ht , at ]) h ˜ t as the new input to the CRF layer. We use h 3 3.1 (3) Restricted Self-Attention As we have introduce"
D18-1116,N03-1030,0,0.874151,"Missing"
D18-1116,H05-1033,0,0.645118,"tations, we transfer a word representation model trained on a large corpus into our task, and show that this transIntroduction Discourse segmentation, which divides text into proper discourse units, is one of the fundamental tasks in natural language processing. According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a complex text is composed of non-overlapping Elementary Discourse Units (EDUs), as shown in Table 1. Segmenting text into such discourse units is a key step in discourse analysis (Marcu, 2000) and can benefit many downstream tasks, such as sentence compression (Sporleder and Lapata, 2005) or document summarization (Li et al., 2016). Since EDUs are initially designed to be determined with lexical and syntactic clues (Carlson et al., 2001), existing methods for discourse segmentation usually design hand-crafted features to capture these clues (Feng and Hirst, 2014). Especially, nearly all previous methods rely on syntactic parse trees to achieve good performance. 1 Our code is available at https://github.com/ PKU-TANGENT/NeuralEDUSeg 962 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 962–967 c Brussels, Belgium, October 31 - Novembe"
D18-1504,D15-1141,0,0.0214216,"res, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long distance dependencies and character-level features. To make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirectional gated recurrent units networks (HMBiGRU) which uses a multi-layer Bi-GRU to automatically learn character features (e.g. capitalization, noun suffix,"
D18-1504,W17-1106,0,0.0787827,"formance, we adopt Precision, Recall and F-measure. In our experiments, we evaluate the performance of detecting targets (DT) and targeted sentiment analysis (TSA) which a target is taken as correct only when the boundary and the sentiment are both correctly recognized. We also adopt Precision, Recall and F-measure used in Zhang et al. (2015) to evaluate our model. The reason why we don’t compare with Mitchell et al. (2013) is that they only evaluate the beginning of targets along with the sentiment expressed towards it. In our experiments, we use embeddings from Pennington et al. (2014)2 and Cieliebak et al. (2017)3 for English words and Spanish words respectively. The character embeddings are initialized by Xavier (Glorot and Bengio, 2010) and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier Glorot and Bengio (2010). The dimensions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respectively. The layer number of multi-layer bidirectional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sfi and tfi , and the dropout rate is set to 0.5. The word embeddings and character embeddings will"
D18-1504,P13-2147,0,0.0298715,"al and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA rega"
D18-1504,C10-1074,0,0.200297,"the hierarchical multi-layer bidirectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features. 1 Introduction Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to"
D18-1504,D14-1162,0,0.0862383,"et. To evaluate the system performance, we adopt Precision, Recall and F-measure. In our experiments, we evaluate the performance of detecting targets (DT) and targeted sentiment analysis (TSA) which a target is taken as correct only when the boundary and the sentiment are both correctly recognized. We also adopt Precision, Recall and F-measure used in Zhang et al. (2015) to evaluate our model. The reason why we don’t compare with Mitchell et al. (2013) is that they only evaluate the beginning of targets along with the sentiment expressed towards it. In our experiments, we use embeddings from Pennington et al. (2014)2 and Cieliebak et al. (2017)3 for English words and Spanish words respectively. The character embeddings are initialized by Xavier (Glorot and Bengio, 2010) and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier Glorot and Bengio (2010). The dimensions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respectively. The layer number of multi-layer bidirectional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sfi and tfi , and the dropout rate is set to 0.5. The word embeddings a"
D18-1504,D16-1103,0,0.0402932,"ment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two task"
D18-1504,C16-1311,0,0.0600708,"and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to dif"
D18-1504,D16-1059,0,0.0333403,"Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and p"
D18-1504,D16-1058,0,0.0713024,"Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and p"
D18-1504,P13-1161,0,0.212768,"multi-layer bidirectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features. 1 Introduction Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jo"
D18-1504,D15-1073,0,0.431616,"them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long dis"
D18-1504,D13-1171,0,0.375352,"Missing"
D18-1504,P14-1028,0,0.0187621,"ual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long distance dependencies and character-level features. To make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirectional gated recurrent units networks (HMBiGRU) which uses a multi-layer Bi-GRU to automatically learn character features (e.g. capitali"
D19-1241,P17-1105,0,0.0149618,"sed seq2tree for semantic parsing and out performed the seq2seq model. One drawback is that their generation is at token level so it cannot guarantee the result is syntactically correct. Grammar rules were used to solve this problem. Another drawback is that they needed special tokens for predicting branches, which are not necessary for MWPs because all operators are binary operators. The similar framework is also used in code generation (Zhang et al., 2016; Yin and Neubig, 2017). Alvarez-Melis and Jaakkola (2017) presented doubly recurrent neural networks to predict tree topology explicitly. Rabinovich et al. (2017) presented a abstract syntax network that combines edge information for code generation. Convolution neural networks (CNNs) were used for code generation decoding because the output program is much longer than semantic parsing and MWPs, and RNNs suffer from the long dependency problem (Sun et al., 2018). 3 Model Our model consists of two stages as shown in Figure 2: the encoder stage that encodes the input natural language into a sequence of representation vectors and the decoder stage that receives these vectors and decodes the AST of the equation with Equation Quantities Template Prefix temp"
D19-1241,D15-1202,0,0.295332,"single model performance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They broug"
D19-1241,N19-1272,0,0.525246,"Missing"
D19-1241,P16-1004,0,0.173413,"ict the operators. However, the topology of the AST is determined in the first step without tree structure information. To our best knowledge, we are the first to explicitly give the model guidance of parent and sibling nodes. 2.2 Seq2Tree Architectures Seq2Tree-style encoder-decoder is mainly used in two fields both of which try to bridge natural language and a tree structured output. Semantic parsing is the task that translates natural language text to formal meaning logical forms or structured queries. Code generation maps a piece of program description to programming language source code. Dong and Lapata (2016) first used recurrent neural networks (RNNs) based seq2tree for semantic parsing and out performed the seq2seq model. One drawback is that their generation is at token level so it cannot guarantee the result is syntactically correct. Grammar rules were used to solve this problem. Another drawback is that they needed special tokens for predicting branches, which are not necessary for MWPs because all operators are binary operators. The similar framework is also used in code generation (Zhang et al., 2016; Yin and Neubig, 2017). Alvarez-Melis and Jaakkola (2017) presented doubly recurrent neural"
D19-1241,Q18-1012,0,0.293648,"Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which fi"
D19-1241,D14-1058,0,0.0698436,"k, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al. (2018b) first extended the seq2seq model by decoding the suffix order sequence of the equations. Wang et al. (2018a) introduced equation normalization techniques that leverage the duplicated template problem"
D19-1241,D17-1084,0,0.15949,"ance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pip"
D19-1241,P16-1084,0,0.0485036,"ntroduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al. (2018b) first extended the seq2seq model b"
D19-1241,Q15-1042,0,0.0466133,"s to map the problems into several predefined templates in classification style or retrieval style. The major drawback of these approaches is that they are inflexible for new templates and require extra effort to design rules, features and templates. Modeling the tree structure of math equations has been considered as an important factor when building models for MWP. As shown in Figure 1, each equation could be transformed into an abstract syntax tree (AST). Roy and Roth (2017) built an expression tree and combined two classifiers for quantity relevance prediction and operator classification. Koncel-Kedziorski et al. (2015) designed a template ranking function based on the ∗ This denotes equal contribution. + 660 34 Figure 1: One example of MWP. Problem refers to the natural language descriptions. Equation refers to the formal math equation. Prefix refers to the prefix notation of the equation. Answer refers to the final quantity solution. AST refers to the AST of the equation. AST of the equations. However, these approaches are based on traditional methods and require feature engineering. Recently, the appearance of large-scale datasets and the development of neural generative models have opened a new research"
D19-1241,P14-1026,0,0.567564,"real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth,"
D19-1241,D15-1135,0,0.106382,"state-of-the-art single model performance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514"
D19-1241,E17-1047,0,0.0517011,"with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al."
D19-1241,D18-1132,0,0.479935,"he equations. However, these approaches are based on traditional methods and require feature engineering. Recently, the appearance of large-scale datasets and the development of neural generative models have opened a new research line for MWP. Wang et al. (2017) cast this task as a sequence generation problem and used a sequence-to-sequence (seq2seq) model to learn the mapping from natural language text to a math equation. Recent approaches use the Reverse Polish notation, also called suffix notation of equations in which operators follow their operands to implicitly model the tree structure (Wang et al., 2018a; Chiang and Chen, 2018). However, these studies lost sight of important information of the math equation ASTs 2370 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2370–2379, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (e.g., parent and siblings of each node), despite of their promising results. Thus, their model has to use extra effort to memorize various pieces of auxiliary information such as the sibling node of the current step"
D19-1241,D17-1088,0,0.577866,"Missing"
D19-1241,P17-1041,0,0.0284643,"Missing"
D19-1345,D17-1209,0,0.0299535,"is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural Network (GCN) in text classification task and outperformed the traditional CNN models. Further, Yao et al. (2019) improved Defferrard et al. (2016)’s work by applying article nodes and weighted edges in the graph, and their model outperformed the state-of-the-art text classification methods. However, these GNN-based models usually adopt the way of building one graph for the whole corpus, which causes the following problems in practice. First, high memory consumption is requir"
D19-1345,E17-2068,0,0.0537432,"ion as other graph-based models do. Finally, the representations of all nodes in the text are used to predict the label of the text: X r0n + b)) (5) yi = softmax(Relu(W Datasets # Train # Test Categories Avg. Length R8 R52 Ohsumed 5485 6532 3357 2189 2568 4043 8 52 23 65.72 69.82 135.82 Table 1: Datasets overview. • CNN Proposed by (Kim, 2014), perform convolution and max pooling operation on word embeddings to get representation of text. • LSTM Defined in (Liu et al., 2016), use the last hidden state as the representation of the text. Bi-LSTM is a bi-directional LSTM. • fastText Proposed by (Joulin et al., 2017), average word or n-gram embeddings as documents embeddings. • Graph-CNN Operate convolution over word embedding similarity graphs by fourier filter, proposed by (Defferrard et al., 2016). n∈Ni where W ∈ Rd×c is a matrix mapping the vector into an output space, Ni is the node set of text i and b ∈ Rc is bias. The goal of training is to minimize the crossentropy loss between ground truth label and predicted label: loss = −gi log yi , (6) where gi is the “one-hot vector” of ground truth label. 3 Experiments In this section, we describe our experimental setup and report our experimental results."
D19-1345,D14-1181,0,0.19485,"significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory. 1 Introduction Text classification is a fundamental problem of natural language processing (NLP), which has lots of applications like SPAM detection, news filtering, and so on (Jindal and Liu, 2007; Aggarwal and Zhai, 2012). The essential step for text classification is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural"
D19-1345,D14-1162,0,0.0962989,"from the training set to build validation set. The overview of datasets is listed in Table 1. We compare our method with the following baseline models. It is noted that the results of some models are directly taken from (Yao et al., 2019). 1 2 https://www.cs.umb.edu/˜smimarog/textmining/datasets/ http://disi.unitn.it/moschitti/corpora.htm • Text-GCN A graph based text classification model proposed by (Yao et al., 2019), which builds a single large graph for whole corpus. 3.2 Implementation Details We set the dimension of node representation as 300 and initialize with random vectors or Glove (Pennington et al., 2014). k discussed in Section 2.1 is set to 2. We use the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 10−3 , and L2 weight decay is set to 10−4 . Dropout with a keep probability of 0.5 is applied after the dense layer. The batch size of our model is 32. We stop training if the validation loss does not decrease for 10 consecutive epochs. For baseline models, we use default parameter settings as in their original papers or implementations. For models using pre-trained word embeddings, we used 300-dimensional GloVe word embeddings. 3.3 Experimental Results Table 2 reports the"
D19-1345,P12-2018,0,0.0318507,"r results. In (3), we remove the pre-trained word embeddings from nodes and initialize all the nodes with random vectors. Compared with the original model, the performances are slightly decreased without pre-trained word embeddings. Therefore, we believe that the pre-trained word embeddings have a particular effect on improving the performance of our model. 4 Related Work 4.2 Text Classification Text classification is a classic problem of natural language processing and has a wide range of applications in reality. Traditional text classification like bag-of-words (Zhang et al., 2010), n-gram (Wang and Manning, 2012) and Topic Model (Wallach, 2006) mainly focus on feature engineering and algorithms. With the development of deep learning techniques, more and more deep learning models are applied for text classification. Kim (2014); Liu et al. (2016) applied CNN and RNN into text classification and achieved results which are much better than traditional models. With the development of GNN, some graphbased classification models are gradually emerging (Hamilton et al., 2017; Veliˇckovi´c et al., 2017; Peng et al., 2018). Yao et al. (2019) proposed Text-GCN and achieved state-of-the-art results on several main"
D19-1345,P18-1030,0,0.0141604,"2012). The essential step for text classification is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural Network (GCN) in text classification task and outperformed the traditional CNN models. Further, Yao et al. (2019) improved Defferrard et al. (2016)’s work by applying article nodes and weighted edges in the graph, and their model outperformed the state-of-the-art text classification methods. However, these GNN-based models usually adopt the way of building one graph for the whole corpus, which causes the following problems"
D19-1412,D15-1075,0,0.0422682,"., 2018) and CoVe (McCann et al., 2017) directly use the learned representations as additional features for downstream tasks, while BERT (Devlin et al., 2018), ULMFiT (Howard and Ruder, 2018), XLM (Lample and Conneau, 2019), and OpenAI GPT (Radford et al., 2018, 2019) require fine-tuning both pre-trained parameters and task-specific parameters on labeled data. The state-of-the-art performances have been significantly advanced for classification and sequence 1 The code and pre-trained models are available at https: //github.com/yuantiku/PoDA. labeling tasks, such as natural language inference (Bowman et al., 2015), named-entity recognition, SQuAD question answering (Rajpurkar et al., 2016) etc. However, little attention has been paid to pretraining for seq2seq text generation (Sutskever et al., 2014). A typical seq2seq network consists of a bidirectional encoder, a unidirectional decoder and attention between the encoder and decoder. Previous work mainly focuses on encoderonly or decoder-only pre-training. For example, BERT pre-trains a bidirectional encoder, and OpenAI GPT pre-trains a language model which is essentially a unidirectional decoder. Ramachandran et al. (2016) propose to train two indepen"
D19-1412,P18-1015,1,0.85152,"headline with an average length of 8.3 words. The Gigaword dataset provided by Rush et al. (2015) is already tokenized and lower-cased. Since our vocabulary is case-sensitive, such inconsistency is expected to hurt our system’s performance. Evaluation We report evaluation results in terms of of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) using the pyrouge7 package. For the CNN/Daily Mail dataset, PGNet (See et al., 2017), Lead3 (See et al., 2017), rnn-ext + RL (?), NeuSum (Zhou et al., 2018) are used as baselines. For the Gigaword dataset, ABS+ (Rush et al., 2015), CGU (Lin et al., 2018), FTSum (Cao et al., 2018b), and Re3 Sum (Cao et al., 2018a) are used as baselines. Results for CNN/Daily Mail Considering the characteristics of news articles, baselines such as Lead3 (simply choose the first 3 sentences) can achieve strong performance in terms of ROUGE 5 We use the non-anonymized version, which is considered to be more realistic. 6 https://github.com/abisee/cnn-dailymail 7 https://github.com/andersjo/pyrouge 4006 System Lead3 PGNet rnn-ext + RL NeuSum PoDA w/o pre-training PoDA 1 40.34 36.44 41.47 41.59 40.82 41.87 ROUGE 2 17.70 15.66 18.72 19.01 18.46 19.27 L 36.57 33.42 37.76 37.98 37.61 38.54 Tab"
D19-1412,P16-1014,0,0.0197525,"periment with three types of noises: randomly shuffle, delete or replace the words in a given sequence. It is noted PoDA is simple, easy-to-implement and applicable to virtually all seq2seq architectures, including ConvS2S (Gehring et al., 2017) and Transformer (Vaswani et al., 2017). Here, we adopt the hybrid architecture of Transformer and pointer-generator networks (See et al., 2017). Transformer is effective at modeling long-distance dependencies, highly parallelizable and demonstrates good performance empirically. Pointer-generator network incorporates copying mechanism (Gu et al., 2016; Gulcehre et al., 2016) which is helpful for most text 4003 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4003–4015, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics masked loss The fox jumps over the lazy dog . copy attention Pointer-Generator Layer …… Transformer Encoder fox Transformer Decoder &lt;bos&gt; The fly over the dog lazy . fox jumps over the lazy dog Figure 1: PoDA model architecture. The masked loss is calculated only for the blue underlined words. “&lt;"
D19-1412,N16-1162,0,0.065552,"Missing"
D19-1412,N19-1409,0,0.0585406,"Missing"
D19-1412,P18-1031,0,0.0245196,"performance over strong baselines without using any task-specific techniques and significantly speed up convergence. 1 1 Introduction Methods based on unsupervised pre-training and supervised fine-tuning for NLP have achieved phenomenal successes in the last two years. Most of the proposed methods in the literature choose language modeling or its variant as the pre-training task. After the pre-training stage, ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017) directly use the learned representations as additional features for downstream tasks, while BERT (Devlin et al., 2018), ULMFiT (Howard and Ruder, 2018), XLM (Lample and Conneau, 2019), and OpenAI GPT (Radford et al., 2018, 2019) require fine-tuning both pre-trained parameters and task-specific parameters on labeled data. The state-of-the-art performances have been significantly advanced for classification and sequence 1 The code and pre-trained models are available at https: //github.com/yuantiku/PoDA. labeling tasks, such as natural language inference (Bowman et al., 2015), named-entity recognition, SQuAD question answering (Rajpurkar et al., 2016) etc. However, little attention has been paid to pretraining for seq2seq text generation (Suts"
D19-1412,N18-1055,0,0.0306114,"aining, PoDA greatly boosts the F0.5 score from 54.01 to 59.40(+5.39) for CoNLL-2014 dataset, 8 4007 https://www.nltk.org/ P 49.24 60.90 62.70 19.22 65.63 70.10 R 23.77 23.74 27.69 31.73 31.62 36.88 F0.5 40.56 46.38 50.04 20.86 54.01 59.40 65.49 66.77 71.01 33.14 34.49 37.68 54.79 56.25 56.52 60.34 as a simple data augmentation method for GEC. Instead, PoDA learns generic text representations and requires task-specific fine-tuning. Techniques from previous work for GEC such as language model based rerank (Chollampatt and Ng, 2018a), data augmentation (Ge et al., 2018a), and domain adaptation (Junczys-Dowmunt et al., 2018) can be easily incorporated. A parallel work (Zhao et al., 2019) observes similar gain by combining simpler pre-training strategy and various GEC-specific techniques. Table 6: Precision (P ), recall (R) and F0.5 scores for CoNLL-2014 test set. We only list systems trained on public data. Ge et al. (2018b) reported better performance with additional 4 million non-public sentence pairs. System (single) MLConv NRL dual-boost PoDA w/o fine-tuning PoDA w/o pre-training PoDA Ensemble MLConv(+rerank) SMT-NMT(+rerank) PoDA Human valid 47.71 49.82 51.35 34.43 51.57 53.16 test 51.34 53.98 56.33 36.83 56"
D19-1412,W14-1702,0,0.0145031,"ive methods and achieves performance improvements (?). PoDA is a purely abstractive summarization system and performs stably better than all the methods. “PoDA w/o pre-training” only has moderate success with ROUGE-1 40.82, ROUGE-2 18.46 and ROUGE-L 37.61. When combined with pretraining, PoDA establishes new state-of-the-art on CNN/Daily Mail dataset. System ABS+ CGU FTSum Re3 Sum PoDA w/o pre-training PoDA 1 29.76 36.3 37.27 37.04 37.24 38.29 ROUGE 2 11.88 18.0 17.65 19.03 18.28 19.06 lic datasets for training: Lang-8 NAIST (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013) and CLC FCE (Felice et al., 2014). The test set of CoNLL-2013 shared task is used as validation set for the CoNLL-2014 task. JFLEG has its own validation set. For preprocessing, we use NLTK8 to tokenize sentences, and remove all sentence pairs without any edits in Lang-8 NAIST. Simple spelling errors are corrected based on edit distance. The dataset statistics are shown in Table 5. Corpus Lang-8 NAIST NUCLE CLC FCE CoNLL-2013 test set JFLEG valid set CoNLL-2014 test set JFLEG test set #Sent Pairs 1, 097, 274 57, 113 32, 073 1, 381 754 1, 312 747 Split train train train valid valid test test Table 5: Dataset statistics for gra"
D19-1412,D18-1426,0,0.0214902,"gnificant improvements on NLP benchmarks (Wang et al., 2018). Autoencoders have long been used for representation learning of images (Vincent et al., 2010) and text (Li et al., 2015). However, precisely reconstructing the clean input is probably too easy for high-capacity models. Sparse autoencoders (Deng et al., 2013), contractive autoencoders (Rifai et al., 2011), and denoising autoencoders (Vincent et al., 2010) are several popular variants. Denoising autoencoders (DA) are shown to be able to learn better representations for downstream tasks (Vincent et al., 2010, 2008; Hill et al., 2016). Freitag and Roy (2018) use seq2seq DAs for unsupervised natural language generation in dialogue, and (Kim et al., 2018) propose to improve the quality of machine translation with DAs. Text Generation covers a wide spectrum of NLP tasks, including machine translation (Wu et al., 2016), summarization (See et al., 2017), response generation (Vinyals and Le, 2015), paraphrase generation, grammatical error correction etc. Early studies on text generation mainly adopt template-based (Reiter and Dale, 2000) or example-based (Watanabe and Takeda, 1998) methods. With the emergence of deep learning for NLP, seq2seq models (S"
D19-1412,P18-1097,0,0.14963,"where the input sequence is a sentence possibly containing some grammatical errors, and the output is a clean and grammatical sentence. We experiment PoDA on two GEC datasets: CoNLL-2014 (Ng et al., 2014) and JFLEG (Napoles et al., 2017). We use three pubEvaluation To compare with previous work, we use the official evaluation metrics: MaxMatch (M 2 ) F0.5 (Dahlmeier and Ng, 2012) for CoNLL2014 and GLEU (Napoles et al., 2015) for JFLEG dataset. Both metrics are shown to correlate well with human evaluation scores. MLConv (Chollampatt and Ng, 2018a), char-seq2seq (Xie et al., 2016), dual-boost (Ge et al., 2018a), Hybrid SMT-NMT (Grundkiewicz and JunczysDowmunt, 2018), NQE (Chollampatt and Ng, 2018b), and NRL (Sakaguchi et al., 2017) are used as baselines. Results As shown in Table 6 and Table 7, when trained only on the supervised data, “PoDA w/o pre-training” can still achieve an impressive performance with F0.5 score 54.01 on CoNLL-2014 test set and GLEU score 56.52 on JFLEG test set, surpassing previous state-of-the-art single model results. This once again shows the effectiveness of the Transformer architecture. For GEC task, most words in the output sequence also appear in the input sequence,"
D19-1412,D18-1101,0,0.0272846,"resentation learning of images (Vincent et al., 2010) and text (Li et al., 2015). However, precisely reconstructing the clean input is probably too easy for high-capacity models. Sparse autoencoders (Deng et al., 2013), contractive autoencoders (Rifai et al., 2011), and denoising autoencoders (Vincent et al., 2010) are several popular variants. Denoising autoencoders (DA) are shown to be able to learn better representations for downstream tasks (Vincent et al., 2010, 2008; Hill et al., 2016). Freitag and Roy (2018) use seq2seq DAs for unsupervised natural language generation in dialogue, and (Kim et al., 2018) propose to improve the quality of machine translation with DAs. Text Generation covers a wide spectrum of NLP tasks, including machine translation (Wu et al., 2016), summarization (See et al., 2017), response generation (Vinyals and Le, 2015), paraphrase generation, grammatical error correction etc. Early studies on text generation mainly adopt template-based (Reiter and Dale, 2000) or example-based (Watanabe and Takeda, 1998) methods. With the emergence of deep learning for NLP, seq2seq models (Sutskever et al., 2014) become a popular choice for text generation tasks and show better performa"
D19-1412,D18-2012,0,0.0226685,"elf-attention layer followed by one layer of positionwise feedforward network. For the output layer, we use a pointer-generator layer to allow both copying from the input sequence and generation from a fixed vocabulary. The implementation is detailed in Appendix. As a side note, we want to point out that the seq2seq architecture is not limited to the one we propose and other networks such as ConvS2S, RNN-based seq2seq models are also applicable. Pointer-generator networks are also not the only solution for handling out-of-vocabulary(OOV) words, and subword-based methods such as sentencepiece (Kudo and Richardson, 2018) can be used at the cost of making the input and output sequences longer. 2.2 Noising and Denoising Similar to denoising autoencoders, PoDA involves two parts: noising and denoising. The noising part corrupts a given word sequence x = {xi }ni=1 and ′ gets a noisy word sequence x′ = {x′ i }ni=1 . The denoising part tries to recover x given x′ using a seq2seq model. We use three noising functions: randomly shuffle, delete or replace the words in x. The details are shown in Algorithm 1, where N (0, σ) is a gaussian distribution with mean 0 and variance σ. B(p) is a Bernoulli distribution, and Bet"
D19-1412,P15-1107,0,0.0267596,"d sequence learning (Dai and Le, 2015; Peters et al., 2017) attempting to incorporate language modeling as an auxiliary task. Recently, several pre-training methods based on language models are presented, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019) etc. The combination of more compute, larger model capacity and large-scale text corpora lead to significant improvements on NLP benchmarks (Wang et al., 2018). Autoencoders have long been used for representation learning of images (Vincent et al., 2010) and text (Li et al., 2015). However, precisely reconstructing the clean input is probably too easy for high-capacity models. Sparse autoencoders (Deng et al., 2013), contractive autoencoders (Rifai et al., 2011), and denoising autoencoders (Vincent et al., 2010) are several popular variants. Denoising autoencoders (DA) are shown to be able to learn better representations for downstream tasks (Vincent et al., 2010, 2008; Hill et al., 2016). Freitag and Roy (2018) use seq2seq DAs for unsupervised natural language generation in dialogue, and (Kim et al., 2018) propose to improve the quality of machine translation with DAs"
D19-1412,N18-2046,0,0.0411183,"Missing"
D19-1412,P16-1154,0,0.0305014,"an be used. We experiment with three types of noises: randomly shuffle, delete or replace the words in a given sequence. It is noted PoDA is simple, easy-to-implement and applicable to virtually all seq2seq architectures, including ConvS2S (Gehring et al., 2017) and Transformer (Vaswani et al., 2017). Here, we adopt the hybrid architecture of Transformer and pointer-generator networks (See et al., 2017). Transformer is effective at modeling long-distance dependencies, highly parallelizable and demonstrates good performance empirically. Pointer-generator network incorporates copying mechanism (Gu et al., 2016; Gulcehre et al., 2016) which is helpful for most text 4003 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4003–4015, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics masked loss The fox jumps over the lazy dog . copy attention Pointer-Generator Layer …… Transformer Encoder fox Transformer Decoder &lt;bos&gt; The fly over the dog lazy . fox jumps over the lazy dog Figure 1: PoDA model architecture. The masked loss is calculated only for the b"
D19-1412,P18-2027,0,0.0222436,"Daily Mail, and one short headline with an average length of 8.3 words. The Gigaword dataset provided by Rush et al. (2015) is already tokenized and lower-cased. Since our vocabulary is case-sensitive, such inconsistency is expected to hurt our system’s performance. Evaluation We report evaluation results in terms of of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) using the pyrouge7 package. For the CNN/Daily Mail dataset, PGNet (See et al., 2017), Lead3 (See et al., 2017), rnn-ext + RL (?), NeuSum (Zhou et al., 2018) are used as baselines. For the Gigaword dataset, ABS+ (Rush et al., 2015), CGU (Lin et al., 2018), FTSum (Cao et al., 2018b), and Re3 Sum (Cao et al., 2018a) are used as baselines. Results for CNN/Daily Mail Considering the characteristics of news articles, baselines such as Lead3 (simply choose the first 3 sentences) can achieve strong performance in terms of ROUGE 5 We use the non-anonymized version, which is considered to be more realistic. 6 https://github.com/abisee/cnn-dailymail 7 https://github.com/andersjo/pyrouge 4006 System Lead3 PGNet rnn-ext + RL NeuSum PoDA w/o pre-training PoDA 1 40.34 36.44 41.47 41.59 40.82 41.87 ROUGE 2 17.70 15.66 18.72 19.01 18.46 19.27 L 36.57 33.42 37"
D19-1412,I11-1017,0,0.104012,"shown in Table 3. “rnn-ext+RL” combines both extractive and abstractive methods and achieves performance improvements (?). PoDA is a purely abstractive summarization system and performs stably better than all the methods. “PoDA w/o pre-training” only has moderate success with ROUGE-1 40.82, ROUGE-2 18.46 and ROUGE-L 37.61. When combined with pretraining, PoDA establishes new state-of-the-art on CNN/Daily Mail dataset. System ABS+ CGU FTSum Re3 Sum PoDA w/o pre-training PoDA 1 29.76 36.3 37.27 37.04 37.24 38.29 ROUGE 2 11.88 18.0 17.65 19.03 18.28 19.06 lic datasets for training: Lang-8 NAIST (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013) and CLC FCE (Felice et al., 2014). The test set of CoNLL-2013 shared task is used as validation set for the CoNLL-2014 task. JFLEG has its own validation set. For preprocessing, we use NLTK8 to tokenize sentences, and remove all sentence pairs without any edits in Lang-8 NAIST. Simple spelling errors are corrected based on edit distance. The dataset statistics are shown in Table 5. Corpus Lang-8 NAIST NUCLE CLC FCE CoNLL-2013 test set JFLEG valid set CoNLL-2014 test set JFLEG test set #Sent Pairs 1, 097, 274 57, 113 32, 073 1, 381 754 1, 312 747 Split train tra"
D19-1412,P15-2097,0,0.0447333,"which utilizes unlabeled text corpora by first retrieving and then rewriting relevant snippets. 3.3 Grammatical Error Correction (GEC) Datasets GEC can also be seen as a text generation task, where the input sequence is a sentence possibly containing some grammatical errors, and the output is a clean and grammatical sentence. We experiment PoDA on two GEC datasets: CoNLL-2014 (Ng et al., 2014) and JFLEG (Napoles et al., 2017). We use three pubEvaluation To compare with previous work, we use the official evaluation metrics: MaxMatch (M 2 ) F0.5 (Dahlmeier and Ng, 2012) for CoNLL2014 and GLEU (Napoles et al., 2015) for JFLEG dataset. Both metrics are shown to correlate well with human evaluation scores. MLConv (Chollampatt and Ng, 2018a), char-seq2seq (Xie et al., 2016), dual-boost (Ge et al., 2018a), Hybrid SMT-NMT (Grundkiewicz and JunczysDowmunt, 2018), NQE (Chollampatt and Ng, 2018b), and NRL (Sakaguchi et al., 2017) are used as baselines. Results As shown in Table 6 and Table 7, when trained only on the supervised data, “PoDA w/o pre-training” can still achieve an impressive performance with F0.5 score 54.01 on CoNLL-2014 test set and GLEU score 56.52 on JFLEG test set, surpassing previous state-of"
D19-1412,E17-2037,0,0.160977,"lated only for the blue underlined words. “&lt;bos&gt;” is a special begin-of-sequence padding symbol. The example input-output pair is explained in Section 2.2. generation tasks. The text corpora used for pre-training are the Billion Word Benchmark (Chelba et al., 2013) and English Wikipedia, both of which are publicly available and consists of nearly 2.99 billion words in total. We conduct experiments on two abstractive summarization datasets (CNN/Daily Mail (See et al., 2017) and Gigaword (Rush et al., 2015)), and two grammatical error correction datasets (CoNLL-2014 (Ng et al., 2014) and JFLEG (Napoles et al., 2017)). With simple maximum likelihood training and no task-specific techniques, PoDA achieves superior or comparable performance against state-of-the-art systems and speeds up convergence for all four datasets. 2 Method 2.1 Model Architecture First, we design a seq2seq model as the backbone architecture of our proposed pre-training method, which is a combination of Transformer and pointer-generator networks, as shown in Figure 1. The input representations are the sum of word embeddings and sinusoidal positional encodings. Both the Transformer encoder and the decoder consist of 6 layers of transfor"
D19-1412,W14-1701,0,0.303352,"re. The masked loss is calculated only for the blue underlined words. “&lt;bos&gt;” is a special begin-of-sequence padding symbol. The example input-output pair is explained in Section 2.2. generation tasks. The text corpora used for pre-training are the Billion Word Benchmark (Chelba et al., 2013) and English Wikipedia, both of which are publicly available and consists of nearly 2.99 billion words in total. We conduct experiments on two abstractive summarization datasets (CNN/Daily Mail (See et al., 2017) and Gigaword (Rush et al., 2015)), and two grammatical error correction datasets (CoNLL-2014 (Ng et al., 2014) and JFLEG (Napoles et al., 2017)). With simple maximum likelihood training and no task-specific techniques, PoDA achieves superior or comparable performance against state-of-the-art systems and speeds up convergence for all four datasets. 2 Method 2.1 Model Architecture First, we design a seq2seq model as the backbone architecture of our proposed pre-training method, which is a combination of Transformer and pointer-generator networks, as shown in Figure 1. The input representations are the sum of word embeddings and sinusoidal positional encodings. Both the Transformer encoder and the decode"
D19-1412,P02-1040,0,0.104221,"is dataset-specific, but the learning rate scheduling, dropout, early stopping, and gradient clipping are exactly the same as pre-training. The objective function for fine-tuning is the word-level negative log-likelihood. Here we do not use reinforcement learning to tune towards the automatic evaluation metrics such as ROUGE 2 3 i=1 #words 2.22B 0.76B 2.99B Empirically, we set σ = 0.5 for Gaussian distri4005 https://dumps.wikimedia.org/ http://www.statmt.org/lm-benchmark/, we do not use BooksCorpus (Zhu et al., 2015) used by BERT, because it is not publicly available now. (Lin, 2004) or BLEU (Papineni et al., 2002), because it may overfit evaluation metrics and barely show improvements in human evaluations (Wu et al., 2016). 3 3.2 Abstractive Summarization Datasets We use two summarization datasets: CNN/Daily Mail5 (See et al., 2017) and Gigaword (Rush et al., 2015) dataset. The official split for training, validation, and test is shown in Table 2. Experiments Corpus 3.1 Setup CNN/Daily Mail The network architecture used by our experiments has 97 million parameters. It consists of 6 layers of encoder blocks, 6 layers of decoder blocks, and 1 pointer-generator layer. The hidden size of each positionwise"
D19-1412,P17-1161,0,0.0298483,"Bengio et al. (2007) proposed layer-wise pre-training for deep belief networks (DBN) to tackle the difficulty of training deep neural networks based on a reconstruction objective. (Erhan et al., 2010; Dahl et al., 2012) showed the effectiveness of pre-training for tasks such as speech recognition. In the area of computer vision, using ImageNet pre-trained models have become a standard practice. In NLP community, using pre-trained word embeddings is the most popular way to transfer knowledge from the unlabeled corpus. There are also work on semi-supervised sequence learning (Dai and Le, 2015; Peters et al., 2017) attempting to incorporate language modeling as an auxiliary task. Recently, several pre-training methods based on language models are presented, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019) etc. The combination of more compute, larger model capacity and large-scale text corpora lead to significant improvements on NLP benchmarks (Wang et al., 2018). Autoencoders have long been used for representation learning of images (Vincent et al., 2010) and text (Li et al., 2015). However, precisely reconstructing the cle"
D19-1412,N18-1202,0,0.297993,"or PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence. 1 1 Introduction Methods based on unsupervised pre-training and supervised fine-tuning for NLP have achieved phenomenal successes in the last two years. Most of the proposed methods in the literature choose language modeling or its variant as the pre-training task. After the pre-training stage, ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017) directly use the learned representations as additional features for downstream tasks, while BERT (Devlin et al., 2018), ULMFiT (Howard and Ruder, 2018), XLM (Lample and Conneau, 2019), and OpenAI GPT (Radford et al., 2018, 2019) require fine-tuning both pre-trained parameters and task-specific parameters on labeled data. The state-of-the-art performances have been significantly advanced for classification and sequence 1 The code and pre-trained models are available at https: //github.com/yuantiku/PoDA. labeling tasks, such as natural language inference (Bowman e"
D19-1412,D16-1264,0,0.0448384,"ations as additional features for downstream tasks, while BERT (Devlin et al., 2018), ULMFiT (Howard and Ruder, 2018), XLM (Lample and Conneau, 2019), and OpenAI GPT (Radford et al., 2018, 2019) require fine-tuning both pre-trained parameters and task-specific parameters on labeled data. The state-of-the-art performances have been significantly advanced for classification and sequence 1 The code and pre-trained models are available at https: //github.com/yuantiku/PoDA. labeling tasks, such as natural language inference (Bowman et al., 2015), named-entity recognition, SQuAD question answering (Rajpurkar et al., 2016) etc. However, little attention has been paid to pretraining for seq2seq text generation (Sutskever et al., 2014). A typical seq2seq network consists of a bidirectional encoder, a unidirectional decoder and attention between the encoder and decoder. Previous work mainly focuses on encoderonly or decoder-only pre-training. For example, BERT pre-trains a bidirectional encoder, and OpenAI GPT pre-trains a language model which is essentially a unidirectional decoder. Ramachandran et al. (2016) propose to train two independent language models for the encoder and decoder respectively. All of the afo"
D19-1412,D15-1044,0,0.697775,"over the dog lazy . fox jumps over the lazy dog Figure 1: PoDA model architecture. The masked loss is calculated only for the blue underlined words. “&lt;bos&gt;” is a special begin-of-sequence padding symbol. The example input-output pair is explained in Section 2.2. generation tasks. The text corpora used for pre-training are the Billion Word Benchmark (Chelba et al., 2013) and English Wikipedia, both of which are publicly available and consists of nearly 2.99 billion words in total. We conduct experiments on two abstractive summarization datasets (CNN/Daily Mail (See et al., 2017) and Gigaword (Rush et al., 2015)), and two grammatical error correction datasets (CoNLL-2014 (Ng et al., 2014) and JFLEG (Napoles et al., 2017)). With simple maximum likelihood training and no task-specific techniques, PoDA achieves superior or comparable performance against state-of-the-art systems and speeds up convergence for all four datasets. 2 Method 2.1 Model Architecture First, we design a seq2seq model as the backbone architecture of our proposed pre-training method, which is a combination of Transformer and pointer-generator networks, as shown in Figure 1. The input representations are the sum of word embeddings an"
D19-1412,I17-2062,0,0.0397541,"Missing"
D19-1412,P17-1099,0,0.442979,"is able to jointly pre-train all components of seq2seq networks. Like denoising autoencoders, PoDA works by denoising the noise-corrupted text sequences. Any noising function that fits in the seq2seq framework can be used. We experiment with three types of noises: randomly shuffle, delete or replace the words in a given sequence. It is noted PoDA is simple, easy-to-implement and applicable to virtually all seq2seq architectures, including ConvS2S (Gehring et al., 2017) and Transformer (Vaswani et al., 2017). Here, we adopt the hybrid architecture of Transformer and pointer-generator networks (See et al., 2017). Transformer is effective at modeling long-distance dependencies, highly parallelizable and demonstrates good performance empirically. Pointer-generator network incorporates copying mechanism (Gu et al., 2016; Gulcehre et al., 2016) which is helpful for most text 4003 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4003–4015, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics masked loss The fox jumps over the lazy dog . copy attention Poi"
D19-1412,W18-5446,0,0.0503668,"Missing"
D19-1412,1983.tc-1.13,0,0.226111,"Missing"
D19-1412,D18-1112,0,0.0215391,"al., 2017), response generation (Vinyals and Le, 2015), paraphrase generation, grammatical error correction etc. Early studies on text generation mainly adopt template-based (Reiter and Dale, 2000) or example-based (Watanabe and Takeda, 1998) methods. With the emergence of deep learning for NLP, seq2seq models (Sutskever et al., 2014) become a popular choice for text generation tasks and show better performance in terms of both 4010 automatic evaluation metrics and human evaluations (Wu et al., 2016). There are also studies focusing on text generation from structured data such as SQL-to-text (Xu et al., 2018). Previous pre-training for text generation is usually done by independently pre-training encoder-side or decoder-side language models (Ramachandran et al., 2016). Concurrent to our work, Edunov et al. augment encoder representation with ELMostyle models, MASS (Song et al., 2019) masks continuous text fragments for pre-training, and UNILM (Dong et al., 2019) proposes to pre-train for both language understanding and generation tasks. 6 Conclusion This paper presents a new transfer learning approach for seq2seq text generation named PoDA. It involves two steps: first, pre-train a customized seq2"
D19-1412,N19-1014,1,0.854097,"oNLL-2014 dataset, 8 4007 https://www.nltk.org/ P 49.24 60.90 62.70 19.22 65.63 70.10 R 23.77 23.74 27.69 31.73 31.62 36.88 F0.5 40.56 46.38 50.04 20.86 54.01 59.40 65.49 66.77 71.01 33.14 34.49 37.68 54.79 56.25 56.52 60.34 as a simple data augmentation method for GEC. Instead, PoDA learns generic text representations and requires task-specific fine-tuning. Techniques from previous work for GEC such as language model based rerank (Chollampatt and Ng, 2018a), data augmentation (Ge et al., 2018a), and domain adaptation (Junczys-Dowmunt et al., 2018) can be easily incorporated. A parallel work (Zhao et al., 2019) observes similar gain by combining simpler pre-training strategy and various GEC-specific techniques. Table 6: Precision (P ), recall (R) and F0.5 scores for CoNLL-2014 test set. We only list systems trained on public data. Ge et al. (2018b) reported better performance with additional 4 million non-public sentence pairs. System (single) MLConv NRL dual-boost PoDA w/o fine-tuning PoDA w/o pre-training PoDA Ensemble MLConv(+rerank) SMT-NMT(+rerank) PoDA Human valid 47.71 49.82 51.35 34.43 51.57 53.16 test 51.34 53.98 56.33 36.83 56.52 59.02 52.48 53.29 - 57.47 61.50 59.48 62.38 4 Analysis In th"
D19-1412,P18-1061,0,0.0276722,"word consists of one sentence with an average length of 31.3 words, which is much shorter than CNN/Daily Mail, and one short headline with an average length of 8.3 words. The Gigaword dataset provided by Rush et al. (2015) is already tokenized and lower-cased. Since our vocabulary is case-sensitive, such inconsistency is expected to hurt our system’s performance. Evaluation We report evaluation results in terms of of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) using the pyrouge7 package. For the CNN/Daily Mail dataset, PGNet (See et al., 2017), Lead3 (See et al., 2017), rnn-ext + RL (?), NeuSum (Zhou et al., 2018) are used as baselines. For the Gigaword dataset, ABS+ (Rush et al., 2015), CGU (Lin et al., 2018), FTSum (Cao et al., 2018b), and Re3 Sum (Cao et al., 2018a) are used as baselines. Results for CNN/Daily Mail Considering the characteristics of news articles, baselines such as Lead3 (simply choose the first 3 sentences) can achieve strong performance in terms of ROUGE 5 We use the non-anonymized version, which is considered to be more realistic. 6 https://github.com/abisee/cnn-dailymail 7 https://github.com/andersjo/pyrouge 4006 System Lead3 PGNet rnn-ext + RL NeuSum PoDA w/o pre-training PoDA"
D19-1534,P18-1198,0,0.0212208,"y; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrap"
D19-1534,P19-1329,0,0.0385549,"training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al."
D19-1534,D14-1162,0,0.100642,"to numbers outside its training range. We are especially intrigued by the model’s ability to learn numeracy, i.e., how does the model know the value of a number given its embedding? The model uses standard embeddings (GloVe and a Char-CNN) and receives no direct supervision for number magnitude/ordering. To understand how numeracy emerges, we probe token embedding methods (e.g., BERT, GloVe) using synthetic list maximum, number decoding, and addition tasks (Section 3). We find that all widely-used pre-trained embeddings, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GloVe (Pennington et al., 2014), capture numeracy: number magnitude is present in the embeddings, even for numbers in the thousands. Among all embeddings, characterlevel methods exhibit stronger numeracy than word- and sub-word-level methods (e.g., ELMo excels while BERT struggles), and character-level models learned directly on the synthetic tasks are the strongest overall. Finally, we investigate why NAQANet had trouble extrapolating—was it a failure in the model or the embeddings? We repeat our probing tasks and test for model extrapolation, finding that neural models struggle to predict numbers outside the training rang"
D19-1534,D16-1264,0,0.0650128,", or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing number magnitude or performing explicit comparisons. We refer readers to Yu et al. (2018) and Dua et al. (2019) for further details. 2.1 2.3 2 Numeracy Case Study: DROP QA DROP Dataset DROP is a reading comprehension dataset that tests numerical reasoning operations such as counting, sorting, and addition (Dua et al., 2019). The dataset’s input-output format is a superset of SQuAD (Rajpurkar et al., 2016): the answers are paragraph spans, as well as question Comparative and Superlative Questions We focus on questions that NAQANet requires numeracy to answer, namely Comparative and Superlative questions.2 Comparative questions 1 Result as of May 21st, 2019. DROP addition, subtraction, and count questions do not require numeracy for NAQANet, see Appendix A. 5308 2 Question Type Example Reasoning Required Comparative (Binary) Comparative (Non-binary) Superlative (Number) Superlative (Span) Which country is a bigger exporter, Brazil or Uruguay? Which player had a touchdown longer than 20 yards? Ho"
D19-1534,N19-1423,0,0.51046,", BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e)"
D19-1534,K19-1033,0,0.0951641,"Missing"
D19-1534,N19-1246,1,0.888358,"ccurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e) Char-CNN (f) Char-LSTM 0 −500 500 0 −500 500 0 −500 −2000 −1000 0 1000 I put Number 2000−2000 −1000 0 1000 I put Number 2000 Figure 1: We train a probing model to decode a number from it"
D19-1534,P17-1025,0,0.0372483,"ger range [0,150] and evaluated on integers from the Test Range. The probing model struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more"
D19-1534,P18-1027,0,0.050128,"Missing"
D19-1534,D17-1160,1,0.827689,"ate-of-the-art NAQANet answers every question correct. Plausible answer candidates to the questions are underlined and the model’s predictions are shown in bold. spans, number answers (e.g., 35), and dates (e.g., 03/01/2014). The only supervision provided is the question-answer pairs, i.e., a model must learn to reason numerically while simultaneously learning to read and comprehend. 2.2 NAQANet Model This section examines the state-of-the-art model for DROP by investigating its accuracy on questions that require numerical reasoning. Modeling approaches for DROP include both semantic parsing (Krishnamurthy et al., 2017) and reading comprehension (Yu et al., 2018) models. We focus on the latter, specifically on Numerically-augmented QANet (NAQANet), the current state-of-the-art model (Dua et al., 2019).1 The model’s core structure closely follows QANet (Yu et al., 2018) except that it contains four output branches, one for each of the four answer types (passage span, question span, count answer, or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing numb"
D19-1534,Q16-1037,0,0.10323,"Missing"
D19-1534,N19-1112,1,0.797676,"t al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrapolation: neural mod"
D19-1534,L18-1008,0,0.0281129,"Missing"
D19-1534,P18-1196,0,0.124058,"Missing"
D19-1534,P18-2117,0,0.163586,"extrapolate to other values. 2.6 Whence this behavior? NAQANet exhibits numerical reasoning capabilities that exceed our expectations. What enables this behavior? Aside from reading and comprehending the passage/question, this kind of numerical reasoning requires two components: numeracy (i.e., representing numbers) and comparison algorithms (i.e., computing the maximum of a list). Although the natural emergence of comparison algorithms is surprising, previous results show neural models are capable of learning to count and sort synthetic lists of scalar values when given explicit supervision (Weiss et al., 2018; Vinyals et al., 2016). NAQANet demonstrates that a model can learn comparison algorithms while simultane3 Probing Numeracy of Embeddings We use synthetic numerical tasks to probe the numeracy of token embeddings. 3.1 Probing Tasks We consider three synthetic tasks to evaluate numeracy (Figure 3). Appendix C provides further details on training and evaluation. List Maximum Given a list of the embeddings for five numbers, the task is to predict the index of the maximum number. Each list consists of values of similar magnitude in order to evaluate fine-grained comparisons (see Appendix C). As i"
D19-1534,P18-2102,0,0.027081,"struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. P"
I17-1050,P14-5010,0,0.00631574,"Missing"
I17-1050,P97-1013,0,0.167118,"is ADJP JJ likely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in"
I17-1050,P17-1152,0,0.134763,"orporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. An"
I17-1050,W12-1614,0,0.0606847,"b-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Rec"
I17-1050,D14-1162,0,0.0804132,"s are extremely imbalanced in PDTB. However, recent work put more emphasis on the multi-class classification, where the goal is to identify a discourse relation from all possible choices. According to Rutherford and Xue (2014), the multi-class classification setting is more natural and realistic. Moreover, the multi-class classifier can directly serve as one building block of a complete discourse parser (Qin et al., 2017). Therefore, in this work, we will focus on the multiω 50 τ 50 d 250 η 0.01 λ 0.0001 b 10 Table 2: Hyper-parameters of our model The Pre-trained 50-dimentional Glove Vectors (Pennington et al., 2014), which is caseinsensitive, are used for initializing the word embeddings and they are tuned together with other parameters in the same learning rate during training. 501 Systems Zhang et al. (2015) Rutherford and Xue (2014) Rutherford and Xue (2015) Liu et al. (2016) Liu and Li (2016) Ji et al. (2016) Tag-Enhanced Tree-LSTM Tag-Enhanced Tree-GRU We adopt the AdaGrad optimizer (Duchi et al., 2011) for training our model and we validate the performance every epoch. It takes around 5 hours (5 epochs) for the Tag-Enhanced Tree-LSTM and 4 hours (6 epochs) for the Tag-Enhanced TreeGRU model to conv"
I17-1050,P16-1078,0,0.332691,"oposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful p"
I17-1050,P09-1077,0,0.196333,"urse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether su"
I17-1050,N16-1037,0,0.0247637,"al neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5 CD 0.4 0.3 And in Table 4, we present the following systems, which focus on Level-2 classification: VBG 0.1 0.1 PP WP$ PRP$ WHADJP NX VP WHADVP WHNP NNP S PRT NN ADJP TO RP PDT VBZ RBR -LRB- EX NP-TMP VB FRAG INTJ JJR ADVP LS RRC SBAR MD NNPS RB FW VBP RBS NP SQ LST JJS QP PRN CC WP UCP 0.2 • Lin et al. (2009) uses traditional featurebased model to classify relations. Especially, constituent and dependen"
I17-1050,E17-2093,0,0.24905,"d on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encodi"
I17-1050,prasad-etal-2008-penn,0,0.123225,"or identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic modeling for implicit discourse relation classification. 2.1 Implicit Discourse Relation Classication Discourse relation identification is an important but difficult sub-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discour"
I17-1050,D09-1036,0,0.0910292,"Missing"
I17-1050,P15-1132,0,0.0242798,"k models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient exploding or vanishing problem due to th"
I17-1050,D16-1130,1,0.705053,"“but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve"
I17-1050,D16-1246,0,0.0767037,"tification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity of deep learning methods, many cutting-edge models are also applied to our task of implicit discourse relation classification. Qin et al. (2016) tries to model the sentences with Convolutional Neural Networks. Liu and Li (2016) encodes the text with Long Short Term Memory model and employ multi-level attention mechanism to capture important signals. Qin et al. (2017) proposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the represen"
I17-1050,P17-1093,0,0.248193,"plicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic model"
I17-1050,E14-1068,0,0.216059,"e analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity"
I17-1050,N15-1081,0,0.0166996,"ompared with other state-of-the-art systems. model have less parameters to train which could alleviate the problem of overfitting and also cost less training time. 4.4 Comparison with Other Systems For a comprehensive study, we compare our models with other state-of-the-art systems. The systems that conduct Level-1 classification are reported in Table 3, including: • Zhang et al. (2015) proposes to use convolutional neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5"
I17-1050,D13-1170,0,0.00737155,"f text, tree-structured neural network models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient explo"
I17-1050,N03-1030,0,0.25336,"kely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have be"
I17-1050,N09-1064,0,0.0735037,"Missing"
I17-1050,P15-1150,0,0.628509,"parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. Another important syntactic signal comes from the constituent tags on the tree nodes (e.g., NP, VP, ADJP). Those tags, derived from the production rules, describe the generative process of text and therefore could indicate which part is more important in each constituent. For example, considering a node tagged with NP, its child node tagged with DT is usually neglectable. Thus we propos"
I17-1050,D15-1266,0,0.0357118,"Missing"
I17-1064,D16-1171,0,0.35288,"Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat limited to either of the"
I17-1064,P14-5010,0,0.00535743,"ch sentence plays a different role, and from a specific view all the sentences should be paid different attention, in document-level sentiment classification. Here, we still consider the influence from user, product and their combination, and get the corresponding document representation du , dp , dup , where d∗ (∗ ∈ {u, p, up}) is computed as follows: d∗ = m X t=1 βt∗ s∗t Datasets To validate the effectiveness of our model, we use three real-world datasets: IMDB, Yelp 2013 and Yelp 2014 collected by Tang et al. (2015b). For these data, we preprocess the text including using Stanford CoreNLP (Manning et al., 2014) to split the review documents into sentences and tokenizing all words. Table 1 shows the details of the three datasets including number of documents (#docs), average number of documents per user posts(#docs/user) etc. It is also noted that IMDB is rated with 10 sentiment labels (i.e., 1-10 stars) while Yelp has 5 labels (i.e., 1-5 stars). We also adopt the same data partition used in (Tang et al., 2015b) and (Chen et al., 2016) for training, developing and test. (9) exp(γ(s∗t , δ ∗ )) βt∗ = Pm ∗ ∗ t=1 exp(γ(st , δ )) (10) Evaluation Metrics where s∗t is the sentence representation of sentence"
I17-1064,I13-1156,0,0.309598,"set to 10−5 . 3.1 Method Comparison To comprehensively evaluate the performance of CMA, we list some baseline methods for comparison. The baselines are introduced as follows. • Majority assigns the largest sentiment polarity occurred in the training set to each sample in the test set. • Trigram uses the unigrams, bigrams and trigrams features to train a SVM classifier for sentiment classification. • TextFeature extracts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196"
I17-1064,W02-1011,0,0.0225933,"ultiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway"
I17-1064,W06-3808,0,0.0534092,"ct (δ up ), with blue, red and green color series respectively. The deeper color means higher weight. Figure 2(a)∼(f) display the three ways of attention to words for sentence1 ∼ sentence6 respectively. We can see that different words are as4 Related work Document-level sentiment classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro"
I17-1064,C10-1103,0,0.0311466,"ent classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro t no , ilk rm tte bu l rea th es wi ak nc de pa ma me ho . zen fro t no , h atc scr m fro de ma s it cu bis (1 2 ) 1 2 ) 131 2 ) 1 Figure 2: Case Study: Illustration of Attention Weights. ument. Chen et al. (2016) employ two layers of long-short term memory (LSTM) with"
I17-1064,P12-1092,0,0.0564451,"composed of a sequence of words wt1 , wt2 , · · · , wtnt where wtj denotes a specific word. To represent a word, we embed each word into a low dimensional real-value vector, called word embedding (Bengio et al., 2003). Then, we can get wtj ∈ Rd from M v×d , where t is the sentence index in a document, j denotes the word index in sentence t, d means the embedding dimension and v gives the vocabulary size. Word embeddings can be regarded as parameters of neural networks or pre-trained from proper corpus via language model (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010; Huang et al., 2012). In our model, we choose the second strategy. Next, deeper word semantics representations can be learned by using the neural network models, such as convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, the LSTM model is employed to obtain the word representation, since it has the good performance of learning the long-term dependencies and can well model the dependence between words. Formally, for sentence St , we input its word embeddings wt1 , wt2 , ..., wtnt to the LSTM networks and get the final word representations rt1 , rt2 , ..., rtnt . Cascading Multiw"
I17-1064,D12-1110,0,0.15676,"Missing"
I17-1064,P14-1062,0,0.103355,"Missing"
I17-1064,D11-1014,0,0.0955439,"the document level, CMA keeps using the multiway attention mechanism to generate attention to sentences. Experimental results on IMDB, Yelp 2013 and Yelp 2014 verify that CMA can learn efficient representations for sentences and documents and provide rich information for judging the document-level sentiment polarity. Recently, neural network approaches have achieved a comparable performance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neura"
I17-1064,D14-1181,0,0.039791,"Missing"
I17-1064,D13-1170,0,0.0229744,"has a high accuracy and a low RMSE. J =− C X i=1 gi log(yi ) + λr ( X θ2 ) θ∈Θ 637 Dataset IMDB Yelp2013 Yelp2014 #docs 84919 78966 231163 #users 1310 1631 4818 #products 1635 1633 4194 #docs/user 64.82 48.42 47.97 #docs/product 51.94 48.36 55.11 #sents/doc 16.08 10.89 11.41 #words/doc 24.54 17.38 17.26 #labels 10 5 5 Table 1: Data Statistics of IMDB, Yelp2013 and Yelp 2014. et al., 2015a) implements the distributed memory model of paragraph vectors (PV-DM) to get document representations for sentiment classification. • RNTN+RNN models sentences using recursive neural tensor networks (RNTN) (Socher et al., 2013). Then sentence representations are fed into the recurrent neural networks (RNN) and their hidden states are averaged to get the document representation. • UPNN designs preference matrices for each user and product to modify word representations (Tang et al., 2015b). Word representations are then fed into the convolution neural networks (CNNs) and concatenated with the user/product representation to generate document representation before a softmax layer. Without considering user and product information, the UPNN(noUP) method just uses CNN to model the documents. • NSC+UPA proposes the hierarc"
I17-1064,J11-2001,0,0.0482152,"provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway attention networks to generate attention"
I17-1064,P15-1150,0,0.0170995,"erformance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neural network methods (Tang et al., 2015b; Chen et al., 2016). Tang et al. (2015a) design preference matrices for each user and each product to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole docAcknowledgments We would like to thank the anonymous reviwers for thier insightful suggestions. Our work is supported by National Hi"
I17-1064,D15-1167,0,0.456706,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P15-1098,0,0.849911,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P14-1146,0,0.105949,"ts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196 2.495 Trigram 0.399 1.783 TextFeature 0.402 1.793 AvgWordvec 0.304 1.985 SSWE 0.312 1.973 PV 0.341 1.814 RNTN+RNN 0.400 1.764 UPNN(noUP) 0.405 1.629 0.487 1.381 NSC+LA CA-null 0.491 1.408 Trigram+UPF 0.404 1.764 TextFeature 0.402 1.774 +UPF 0.435 1.602 UPNN NSC+UPA 0.533 1.281 CMA 0.540 1.191 Model Yelp 2013 Acc. RMSE 0.411 1.060 0.569 0.841 0.556 0.814 0.526 0.898 0.549 0.849 0.554 0.832 0.574 0.804 0.577 0.812 0.631"
I17-1064,N16-1174,0,0.21451,"Missing"
N15-1126,P11-1051,0,0.0155573,"Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Acknowledgement This work was supported by the U.S. DARPA Award No. FA8750-13-2-0045 in the Deep Exploration a"
N15-1126,N12-1009,0,0.0328479,"Missing"
N15-1126,W14-2907,0,0.0253086,"Missing"
N15-1126,P14-5010,0,0.00539561,"types is regarded as the right boundary. The rule-based scoping result of the walk-through example is as follows: Paul Francis Conrad and his twin [<brother>, James, were] [<born> in Cedar Rapids, Iowa, on June 27, 1924,] [<sons> of Robert H. Conrad and Florence Lawler Conrad.] 2.2.2 Supervised Classification Alternatively we regard scope identification as a classification task. For each detected trigger, scope identification is performed as a binary classification of each token in the sentence as to whether it is within or outside of a trigger’s scope. We apply the Stanford CoreNLP toolkit (Manning et al., 2014) to annotate part-of-speech tags and names in each document. We design the following features to train a classifier. • Position: The feature takes value 1 if the word appears before the trigger, and 0 otherwise. • Distance: The distance (in words) between the word and the trigger. • POS: POS tags of the word and the trigger. • Name Entity: The name entity type of the word. • Interrupt: The feature takes value 1 if there is a verb or a trigger with other fact type between the trigger and the word, and 0 otherwise. Verbs and triggers with other fact types can effectively change the current topic"
N15-1126,W09-1304,0,0.0189502,"upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Ackn"
N15-1126,W08-0606,0,0.0120013,"ly expensive: Distant Supervision (Surdeanu et al., 2010) relies upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate"
N16-1049,P98-1013,0,0.153293,"es has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atom"
N16-1049,D13-1178,0,0.129215,"t al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint entity-driven model to induct e"
N16-1049,P04-1056,0,0.16463,"rates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jen"
N16-1049,P08-1090,0,0.311784,"traction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion"
N16-1049,P09-1068,0,0.11024,"rlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint"
N16-1049,P11-1098,0,0.513984,"vent. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To better model the inner connect"
N16-1049,D13-1185,0,0.722137,"of bombing event in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events,"
N16-1049,P11-1054,0,0.0179381,"ates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chamber"
N16-1049,N13-1104,0,0.728221,"in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learn"
N16-1049,P03-1028,0,0.0639924,"ts. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can"
N16-1049,J93-3001,0,0.333736,"t templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information"
N16-1049,D11-1142,0,0.015167,"and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary"
N16-1049,P06-2027,0,0.810867,"the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as"
N16-1049,C92-2082,0,0.0452342,"chmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of s"
N16-1049,P10-1029,0,0.0133174,"ent-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captur"
N16-1049,P08-1030,0,0.0579146,"strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal"
N16-1049,W10-0905,0,0.0276645,"animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and"
N16-1049,P15-1019,0,0.389029,"Missing"
N16-1049,D07-1075,0,0.0822568,"and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates w"
N16-1049,D09-1016,0,0.0220445,"nstraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with un"
N16-1049,M92-1008,0,0.221381,"assigned different templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other"
N16-1049,W98-1106,0,0.159823,"t filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432"
N16-1049,P06-2094,0,0.3539,"ng a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To"
N16-1049,N06-1039,0,0.06114,"Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. Ho"
N16-1049,P03-1029,0,0.0731297,"e full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekin"
N16-1049,H91-1059,0,0.102863,"slot constraint. Generally, we have the following optimization objective: ( ) tr XTT JJ T XT ) ε3 (XT , XS ) = ( T T tr XS JJ XS (8) The whole joint model is shown in Eq 9. The detailed derivation1 is shown in the supplement file. Bombing XT , XS = argmax ε1 (XT ) + ε2 (XS ) + ε3 (XT , XS ) Perpetrator XT ,XS s.t. XT ∈ {0, 1}|E|×|T |XT 1|T |= 1|E| XS ∈ {0, 1}|E|×|S |XS 1|S |= 1|E| Victim The police chief El salvador Students The guerrillas The Peruvian embassy The drag mafia The diplomat Drug traffickers soldiers The Atlacatl battalion (9) 4 Experiment 4.1 Dataset In this paper, we use MUC-4(Sundheim, 1991) as our dataset, which is the same as previous works (Chambers and Jurafsky, 2011; Chambers, 2013). MUC-4 corpus contains 1300 documents in the training set, 200 in development set (TS1, TS2) and 200 in testing set (TS3, TS4) about Latin American news of terrorism events. We ran several times on the 1500 documents (training/dev set) and choose the best |T |and |S |as |T |= 6, |S |= 4. Then we report the performance of test set. For each document, it provides a series of hand-constructed event schemas, which are called gold schemas. With these gold schemas we can evaluate our results. The MUC-4"
N16-1049,C00-2136,0,0.180339,"ore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007;"
N16-1049,N07-4013,0,0.0431629,"garber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011"
N18-1018,P16-1046,0,0.0723454,"Missing"
N18-1018,N16-1012,0,0.0446173,"lt to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machi"
N18-1018,P16-1154,0,0.23065,"-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART"
N18-1018,D15-1229,0,0.48478,"contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to train the model, with the default hyper-parameters: the learning rate α = 0.001, and β1 = 0.9, β2 = 0.999,  = 1e − 8. 3 Experiments Following the previous work (Cao et al., 2017), we test our model on the following two paraphrase orientated tasks: text simplification and short text abstractive summarization. 3.1 Text Simplification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine trans"
N18-1018,P17-2100,1,0.925978,"f PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, l"
N18-1018,N15-1022,0,0.189994,"ioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from 1 to 5 (1 is very bad and 5 is very good). Simplicity shows how simpler the model outputs are than the source text, which ranges from 1 to 5. • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2se"
N18-1018,P15-1001,0,0.0312775,"erate a fluent sentence, but the meaning is different from the source text, and even more difficult to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et"
N18-1018,K16-1028,0,0.124083,"SARI. 3.1.5 3.1.4 Baselines We compare our model with several neural text simplification systems. Results We compare WEAN with state-of-the-art models for text simplification. Table 1 and Table 2 summarize the results of the automatic evaluation. On PWKP dataset, we compare WEAN with PBMT, Hybrid, EncDecA, DRESS and DRESSLS. WEAN achieves a BLEU score of 54.54, outperforming all of the previous systems. On EWSEW dataset, we compare WEAN with PBMT-R, Hybrid, SBMT-SARI, and the neural models described above. We do not find any public release code of PBMT-R and SBMT-SARI. Fortunately, Xu et al. (2016) provides the predictions of PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN"
N18-1018,P13-1151,0,0.0185862,"Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces a parallel simplification corpus by evaluating the similarity between the source text and the simplified ˇ text based on WordNet. Glavaˇs and Stajner (2015) propose an unsupervised approach to lexical simplification that makes use of word vectors and require only regular corpora. Xu et al. (2016) desi"
N18-1018,P14-1041,0,0.190927,". • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2seq (our implementation) WEAN (our proposal) BLEU 46.31 53.94 47.93 34.53 36.32 48.26 54.54 PWKP NTS-w2v DRESS-LS WEAN Reference EW-SEW Fluency Adequacy Simplicity All PBMT-R 3.36 2.92 3.37 3.22 SBMT-SARI 3.41 3.63 3.25 3.43 NTS-w2v 3.56 3.52 3.42 3.50 DRESS-LS 3.59 3.43 3.65 3.56 WEAN 3.61 3.56 3.65 3.61 Reference 3.71 3.64 3.45 3.60 Table 1: Automatic evaluation of our model and other related systems on PWKP datasets. The results are reported on the test sets. EW-SEW PBMT-R (Wubben et al.,"
N18-1018,P17-2014,0,0.141044,"e decoder input and the predicted output share the same vocabulary and word embeddings. Besides, we do not use any pretrained word embeddings in our model, so that all of the parameters are learned from scratch. 2.5 Training Although our generator is a retrieval style, WEAN is as differentiable as the sequence-to-sequence model. The objective of training is to minimize the cross entropy between the predicted word probability distribution and the golden one-hot distribution: X L=− yˆi log p(yi ) (7) each sentence pair as a good, good partial, partial, or bad match. Following the previous work (Nisioi et al., 2017), we discard the unclassified matches, and use the good matches and partial matches with a scaled threshold greater than 0.45. The corpus contains about 150K good matches and 130K good partial matches. We use this corpus as the training set, and the dataset provided by Xu et al. (Xu et al., 2016) as the validation set and the test set. The validation set consists of 2,000 sentence pairs, and the test set contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to tra"
N18-1018,D17-1222,0,0.497871,"eq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015)"
N18-1018,P02-1040,0,0.104711,"implification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug"
N18-1018,N03-1020,0,0.390902,"pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and oth"
N18-1018,C16-1275,0,0.226628,"Missing"
N18-1018,D15-1044,0,0.294351,"idation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and other related systems, and the results are shown in Table 3. DRESS-LS is based on the reinforcement learning, and it encourages the fluency, simplicity and relevance of the outputs. Therefore, it achieves a high score in our human evaluation. WEAN gains a even better score than DRESS-LS. Beside"
N18-1018,D17-1062,0,0.19844,"he gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to evaluate the quality of the model outputs. Following Nisioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from"
N18-1018,C10-1152,0,0.182865,", and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to ev"
N18-1018,D16-1112,0,0.160819,"Missing"
N18-1018,D17-1020,0,0.0247035,"model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs."
N18-1018,D11-1038,0,0.0306716,"(Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces"
N18-1018,P12-1107,0,0.0647051,"Missing"
O02-2003,P93-1022,0,0.0432139,"Missing"
O05-5011,J94-4001,0,0.090344,"Missing"
O05-5011,A94-1007,0,0.0862698,"Missing"
P10-2055,H05-1115,0,0.0842511,"Missing"
P13-2039,P11-1032,1,0.75723,"alities of reviewers could predict spammers, without using any textual features. Li et al. (2011) carefully explored review-related features based on content and sentiment, training a semi-supervised classifier for opinion spam detection. However, the disadvantages of standard supervised learning methods are obvious. First, they do not generally provide readers with a clear probabilistic preIntroduction Consumers rely increasingly on user-generated online reviews to make purchase decisions. Positive opinions can result in significant financial gains. This gives rise to deceptive opinion spam (Ott et al., 2011; Jindal et al., 2008), fake reviews written to sound authentic and deliberately mislead readers. Previous research has shown that humans have difficulty distinguishing fake from truthful reviews, operating for the most part at chance (Ott et al., 2011). Consider, for example, the following two hotel reviews. One is truthful and the other is deceptive1 : 1. My husband and I stayed for two nights at the Hilton Chicago. We were very pleased with the accommodations and enjoyed the service every minute of it! The bedrooms are immaculate, and the linens are very soft. We also appreciated the free w"
P13-2039,D09-1026,0,0.19765,"Missing"
P13-2039,P10-1066,0,0.0510505,"Missing"
P13-2099,D11-1040,0,0.363441,"Missing"
P13-2099,N03-1020,0,\N,Missing
P14-1003,W05-0613,0,0.2812,"otated as either nucleus or satellite depending on how salient they are for interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the"
P14-1003,D09-1036,0,0.124101,"Missing"
P14-1003,W01-1605,0,0.196768,"ency parsing is to parse an optimal spanning tree from VRV-0. Here we follow the arc factored method and define the score of a dependency tree as the sum of the scores of all the arcs in the tree. Thus, the optimal dependency tree for T is a spanning tree with the highest score and obtained through the function DT(T,w): DT (T , w )  argmaxGT V  RV0 score(T , GT ) To automatically conduct discourse dependency parsing, constructing a discourse dependency treebank is fundamental. It is costly to manually construct such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT) (Carlson et al., 2001) is an available resource to help with. A RST tree constitutes a hierarchical structure for one document through rhetorical relations. A total of 110 fine-grained relations (e.g. Elaboration-part-whole and List) were used for tagging RST-DT. They can be categorized into 18 classes (e.g. Elaboration and Joint). All these relations can be hypotactic (“mononuclear”) or paratactic (“multi-nuclear”). A hypotactic relation holds between a nucleus span and an adjacent satellite span, while a paratactic relation connects two or more equally important adjacent nucleus spans. For convenience of computat"
P14-1003,C96-1058,0,0.471564,"out worrying about any interior text spans. Since dependency trees contain much fewer nodes and on average they are simpler than constituency based trees, the current dependency parsers can have a relatively low computational complexity. Moreover, concerning linearization, it is well known that dependency structures can deal with non-projective relations, while constituency-based models need the addition of complex mechanisms like transformations, movements and so on. In our work, we adopt the graph based dependency parsing techniques learned from large sets of annotated dependency trees. The Eisner (1996) algorithm e3 e2 1 e3 e2 e1 3 e1*-e2-e3 *-e e1 e2 e1 5 e1 e3 e1 6 e3 e2 4 2-e3 e1-e2*-e3 e2-e3* e2*-e3 e3 e1*-e2 2 e2-e3* e2 e3 e2 e1 e1-e2-e3* e1-e2-e3* e1*-e2 e1-e2* e1-e2* e1 e1*-e2-e3 e1-e2-e3* e1-e2*-e3 e1 and maximum spanning tree (MST) algorithm are used respectively to parse the optimal projective and non-projective dependency trees with the large-margin learning technique (Crammer and Singer, 2003). To the best of our knowledge, we are the first to apply the dependency structure and introduce the dependency parsing techniques into discourse analysis. The rest of this paper is organize"
P14-1003,H05-1066,0,0.128654,"1 to n 3 For i := 1 to n 4 j=i+m 5 if j> n then break; 6 # Create subgraphs with c=0 by adding arcs 7 E[i, j, 0, 0]=maxiqj (E[i,q,1,1]+E[q+1,j,0,1]+(ej,ei)) 8 E[i, j, 1, 0]=maxiqj (E[i,q,1,1]+E[q+1,j,0,1]+(ei,ej)) 9 # Add corresponding left/right subgraphs 10 E[i, j, 0, 1]=maxiqj (E[i,q,0,1]+E[q,j,0,0] 11 E[i, j, 1, 1]=maxiqj (E[i,q,1,0]+E[q,j,1,1]) A ... ... B A B Figure 4: Pictorial Diagram of Non-projective Trees Chu and Liu (1965) and Edmonds (1967) independently proposed the virtually identical algorithm named the Chu-Liu/Edmonds algorithm, for finding MSTs on directed graphs (McDonald et al. 2005b). Figure 5 shows the details of the Chu-Liu/Edmonds algorithm for discourse parsing. Each node in the graph greedily selects the incoming arc with the highest score. If one tree results, the algorithm ends. Otherwise, there must exist a cycle. The algorithm contracts the identified cycle into a single node and recalculates the scores of the arcs which go in and out of the cycle. Next, the algorithm recursively call itself on the contracted graph. Finally, those arcs which go in or out of one cycle will recover themselves to connect with the original nodes in V. Like McDonald et al. (2005b),"
P14-1003,P12-1007,0,0.54857,"attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art constituency based parsing techniques, the appro"
P14-1003,P09-1077,0,0.0249236,"hniques are mainly based on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (Soricut and Marcu, 200"
P14-1003,D10-1039,0,0.106064,"or interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art constituency based parsi"
P14-1003,W09-3813,0,0.332613,"nt they are for interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art c"
P14-1003,N09-1064,0,0.0711676,"Missing"
P14-1003,P10-1073,0,0.0122735,"sed on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (Soricut and Marcu, 2003; Baldridge and Las"
P14-1003,W06-1317,0,0.0276017,"discourse parsing techniques are mainly based on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (S"
P14-1003,C04-1048,0,\N,Missing
P14-1003,P05-1012,0,\N,Missing
P14-1003,N03-1030,0,\N,Missing
P14-1003,prasad-etal-2008-penn,0,\N,Missing
P15-1056,W06-0901,0,0.0467242,"relevant event chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is re"
P15-1056,S13-2002,0,0.0329879,"Missing"
P15-1056,P08-1030,1,0.800799,"vent chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most"
P15-1056,R09-1032,1,0.842039,"er from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the per"
P15-1056,D08-1073,0,0.0878802,"Missing"
P15-1056,Q14-1022,0,0.0297756,"Missing"
P15-1056,S13-2012,0,0.0607878,"Missing"
P15-1056,P13-2099,1,0.719355,"n aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the performance of our approach in labeling event time. For disaster, sports and war, the accurac"
P15-1056,D12-1092,0,0.0143416,"times called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymou"
P15-1056,N09-2053,1,0.822412,"on work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focu"
P15-1056,P13-1008,1,0.798212,"2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for their thought-provoki"
P15-1056,C12-1033,0,0.0200147,"c detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for the"
P15-1056,P10-1081,0,0.0232743,"re some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeA"
P15-1056,D12-1062,0,0.0241202,"blems. Disaster event chronicles suffer from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time"
P15-1056,D13-1001,1,0.844429,"in sports chronicles but it is not a good entry in comprehensive chronicles. Compared with comprehensive event chronicles, events in other chronicles tend to describe more details. For example, a sports chronicle may regard each match in the World Cup as an event while comprehensive chronicles consider the World Cup as one event, which requires us to adapt event granularity for different chronicles. Also, we evaluate the time of event entries in these five event chronicles because event’s happening time is not always equal to the timestamp of the document creation time (UzZaman et al., 2012; Ge et al., 2013). We collect existing manually edited 2010 chronicles on the web and use their event time as gold standard. We define a metric to evaluate if the event entry’s time in our chronicle is accurate: P diff = e∈E∩E∗ |(te − t∗e )|/|E ∩ E∗ | not show significant improvement. A possible reason is that a comprehensive event chronicle does not care the topical relevance of a event. In other words, its ranking problem is simpler so that the learning-to-rank does not improve the basic ranking criterion much. Moreover, we analyze the incorrect entries in the chronicles generated by our approaches. In gener"
P15-1056,P14-5010,0,0.00567471,"ifically, we collected disaster, sports, war, politics and comprehensive chronicles during 2009 from mapreport7 , infoplease and Wikipedia8 . To generate chronicles during 2010, we use 2009-2010 APW and Xinhua news in English Gigaword (Graff et al., 2003) and remove documents whose titles and first paragraphs do not include any burst words. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which is a 2-state finite automaton model and widely used to detect bursts. In total, there are 140,557 documents in the corpus. Preprocessing: We remove stopwords and use Stanford CoreNLP (Manning et al., 2014) to do lemmatization. Parameter setting: For TaHBM, we empirically set α = 0.05, βz = 0.005, βe = 0.0001, γs = 0.05, γx = 0.5, ε = 0.01, the number of topics K = 50, and the number of events E = 5000. We run Gibbs sampler for 2000 iterations with burn-in period of 500 for inference. For event ranking, we set regularization parameter of SVMRank c = 0.1. Chronicle display: We use a heuristic way to generate the description of each event. Since the first paragraph of a news article is usually a good summary of the article and the earliest document in a cluster usually explicitly describes the eve"
P15-1056,P11-1113,0,0.0152544,"ent detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We t"
P15-1056,P05-3021,0,0.114846,"Missing"
P15-1056,P12-2009,0,0.252735,"to the event chronicle) in Section 3.3 are labeled as high rank priority while those without positive documents are labeled as low priority. 4.2 |De | σe : Features We use the following features to train the ranking model, all of which can be provided by TaHBM. • P (s = 1|e): the probability that an event e is topically relevant to the reference chronicle. • P (e|z): the probability reflects an event’s impact given its topic. • σe : the parameter of an event e’s Gaussian distribution. It determines the ‘bandwidth’ 7 8 580 http://www.mapreport.com http://en.wikipedia.org/wiki/2009 5.2 schema (Zhao et al., 2012) to detect events, which is the state-of-the-art event detection method for general domains. Evaluation Methods and Baselines Since there is no existing evaluation metric for the new task, we design a method for evaluation. Although there are manually edited event chronicles on the web, which may serve as references for evaluation, they are often incomplete. For example, the 2010 politics event chronicle on Wikipedia has only two event entries. Hence, we first pool all event entries of existing chronicles on the web and chronicles generated by approaches evaluated in this paper and then have 3"
P15-1057,W08-0336,0,0.0275193,"riginal meanings. 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers t"
P15-1057,P06-1017,0,0.020688,"Missing"
P15-1057,P13-2006,0,0.0430093,"Missing"
P15-1057,W13-0908,0,0.0277964,"d malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect"
P15-1057,P13-1107,1,0.928563,"ecoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practical morph decoder, as depicted in Figure 1, consists of two problems: (1) Morph Extraction: given a corpus, extract morph mentions; and (2). Morph Resolution: For each morph mention, figure out the entity that it refers to. In this paper, we aim to solve the fundamental research problem of end-to-end morph deco"
P15-1057,P14-1036,1,0.900239,"Missing"
P15-1057,D14-1067,0,0.0149744,"(e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. A"
P15-1057,W13-0906,0,0.0253293,"avior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm"
P15-1057,P14-1038,1,0.800559,"2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca"
P15-1057,D14-1071,0,0.0150722,"s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in secti"
P15-1057,P95-1026,0,0.257659,"s in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genre"
P15-1057,P14-2105,0,0.0142968,"nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to t"
P15-1057,N07-1025,0,0.0387675,"ntext. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genres consistent bet"
P15-1057,W03-1730,0,0.00977908,"4), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candid"
P15-1057,P14-2115,1,0.48139,"{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2 lisujian@pku.edu.cn, 3 cyl@microsoft.com 4 hanj@illinois.edu, 5 zhenwen@us.ibm.com, 6 yzsun@ccs.neu.edu, 7 hanj@illinois.edu Abstract a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practic"
P15-1057,P05-1049,0,0.0177967,"Missing"
P15-1057,D08-1063,0,0.0286454,"2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide"
P15-1057,W12-4304,0,0.0148793,"; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic"
P15-1057,N03-1033,0,0.00839243,"e recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. F"
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2102,P12-1092,0,0.0590827,"Missing"
P15-2102,P14-2131,0,0.0541362,"Missing"
P15-2102,D14-1108,0,0.0262901,"Missing"
P15-2102,D14-1082,0,0.0385828,"Missing"
P15-2102,P14-2050,0,0.0356419,"Missing"
P15-2102,cheng-etal-2014-parsing,0,0.025317,"Missing"
P15-2102,Q15-1016,0,0.0389344,"Missing"
P15-2102,P14-1140,0,0.0298355,"Missing"
P15-2102,W13-3512,0,0.0456091,"Missing"
P15-2102,P14-1146,0,0.0385086,"Missing"
P15-2102,N15-1142,0,0.0259363,"Missing"
P15-2102,C14-1015,0,0.025463,"Missing"
P15-2102,D13-1136,0,0.052079,"Missing"
P15-2102,N15-1069,0,0.0263371,"Missing"
P15-2102,D09-1026,0,0.0521129,"xpertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc. Despite the demand to access expertise, the challenges of identifying domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; 616 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 616–622, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics mains’ knowledge. Using such trees allows us to flexibly incorporate more information into the data repres"
P15-2102,P14-2089,0,0.0368303,"Missing"
P15-2102,P13-1013,0,0.0606307,"Missing"
P15-2102,D14-1162,0,\N,Missing
P15-2136,D14-1181,0,0.00276888,"Missing"
P15-2136,W04-1013,0,0.0525643,"ver, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe , shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp , xe ] (3) wrT (4) yˆ = ×φ AVG-CF Description The position of the sentence. The averaged term frequency values of words in the sentence. The averaged cluster frequency values of words in the sentence. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowl"
P15-2136,W02-0401,0,0.431087,"res beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). HowIntroduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September"
P15-2136,P14-2105,0,0.013617,"The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar"
P15-2136,C14-1220,0,0.00457144,"the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu"
P15-2136,W06-1643,0,\N,Missing
P15-2136,E14-1075,0,\N,Missing
P16-1116,P11-1098,0,0.0369488,"from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one eve"
P16-1116,P15-1017,0,0.13171,"o and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-only or features-only. Moreover, all of these methods consider arguments sepa1225 rately while ignoring the relationship between arguments, which is also important for argument identification. Even the joint method (Li et al., 2013) does not model argument relations directly. We use trigger embedding, sentencelevel embedding, and pattern features tog"
P16-1116,P98-1067,0,0.0819726,", sentence-level embedding, and pattern features together as the our features for balancing. • We proposed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and N"
P16-1116,P06-1061,0,0.041959,"ootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et"
P16-1116,P11-1113,0,0.84565,"aining (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate mod"
P16-1116,P11-1114,0,0.19134,"ed and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type,"
P16-1116,E12-1029,0,0.213239,"how that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et"
P16-1116,P08-1030,0,0.738218,"ents. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explore"
P16-1116,W05-0610,0,0.0178924,"f and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into conside"
P16-1116,P13-1008,0,0.390565,"2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works ar"
P16-1116,P10-1081,0,0.591789,"n the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff e"
P16-1116,E12-1030,0,0.0160451,"tern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other met"
P16-1116,P12-1088,0,0.0652043,"and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-onl"
P16-1116,P07-1075,0,0.0284876,"). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based me"
P16-1116,D07-1075,0,0.0301605,"ethods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for exam"
P16-1116,C00-2136,0,0.154785,"ed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly super"
P16-1116,P05-1062,0,0.0399324,"3; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (M"
P16-1116,D09-1016,0,0.293776,"re patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012)"
P16-1116,P06-2094,0,0.0388605,"improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,N06-1039,0,0.0146725,"n method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,P05-1047,0,0.0304587,"o make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of e"
P16-1116,P03-1029,0,0.0553078,"d method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pat"
P16-1116,C98-1064,0,\N,Missing
P17-2029,J92-4003,0,0.0952582,", we use 18 coarse-grained relations and binarize non-binary relations with right-branching (Sagae and Lavie, 2005). For preprocessing, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to lemmatize words, get POS tags, segment sentences and syntactically parse them. To directly compare with other discourse parsing systems, we employ the same evaluation met• N-gram features: the first and the last n words and their POS tags in the text of S1 , S2 , Q1 , where n ∈ {1, 2}. • Nucleus features: the dependency heads of the nucleus EDUs2 for S1 , S2 , Q1 and their POS tags; brown clusters (Brown et al., 1992; 2 Nucleus EDU is defined by recursively selecting the Nucleus in the binary tree until an EDU (leaf node) is reached. 186 rics, i.e. the precision, recall and F-score 3 with respect to span (S), nuclearity (N) and relation (R), as defined by Marcu (2000). 4.2 egy is adopted, Level denotes whether three kinds of relations (i.e., within-sentence, across-sentence, and across-paragraph) are differently classified, and Tree represents whether relation labeling uses tree features generated in the first stage. The simplest model Simp-1 is almost the same as (Heilman and Sagae, 2015) except that we"
P17-2029,P14-1048,0,0.536975,"o that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very few labelled text-level discourse corpus available for training. The limited training data will cause unbalanced actions and lead to the problems of data sparsity and overfitting. Thus, we propose to use the transition-based model to parse a naked disPrevious work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and rel"
P17-2029,P14-1002,0,0.71867,"el, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very few labelled text-level discourse corpus available for train"
P17-2029,W06-2933,0,0.0128128,"@pku.edu.cn Abstract tions to form larger text spans until the final tree is built. RST also depicts which part is more important in a relation by tagging Nucleus or Satellite. Generally, each relation at least includes a Nucleus and there are three nuclearity types: NucleusSatellite (NS), Satellite-Nucleus (SN) and NucleusNucleus (NN). Therefore, the performance of RST discourse parsing can be evaluated from three aspects: span, nuclearity and relation. To parse discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in"
P17-2029,W09-3813,0,0.186245,"e discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very fe"
P17-2029,W05-1513,0,0.0515507,"sing task. • Structural features: nuclearity type (NN, NS or SN) of S1 , S2 ; number of EDUs and sentences in S1 , S2 ; length comparison of S1 , S2 with respect to EDUs and sentences. • Dependency features: whether dependency relations exist between S1 , S2 or between S1 , Q1 ; the dependency direction and relation type. 4.1 Setup RST-DT annotates 385 documents (347 for training and 38 for testing) from the Wall Street Journal using Rhetorical Structure Theory (Mann and Thompson, 1988). Conventionally, we use 18 coarse-grained relations and binarize non-binary relations with right-branching (Sagae and Lavie, 2005). For preprocessing, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to lemmatize words, get POS tags, segment sentences and syntactically parse them. To directly compare with other discourse parsing systems, we employ the same evaluation met• N-gram features: the first and the last n words and their POS tags in the text of S1 , S2 , Q1 , where n ∈ {1, 2}. • Nucleus features: the dependency heads of the nucleus EDUs2 for S1 , S2 , Q1 and their POS tags; brown clusters (Brown et al., 1992; 2 Nucleus EDU is defined by recursively selecting the Nucleus in the binary tree until an EDU ("
P17-2029,P10-1040,0,0.0081553,"e stack. A parse tree can be finally constructed until the queue is empty and the stack only contains the complete tree. Only one classifier is learned to judge the actions at each step. To derive a discourse tree in a unified framework, prior systems design multiple reduce actions 1 Relation label is actually assigned to the satellite subtree and a “Span” label is assigned to the nucleus substree. 185 whether its left and right subtrees are in different paragraphs, or the same paragraph, or the same sentence. For each level, we predict a relation label using the corresponding classifier. 2.3 Turian et al., 2010) of all the words in the nucleus EDUs of S1 , S2 , Q1 . Next, we list all the features used for the three relation classifiers. Given an internal node P in the naked tree, we aim to predict the relation between its left child Clef t and right child Cright . Dependency features, N-gram features and nucleus features discussed above are also needed, the only difference is that these features are applied to the left and right children. Other features include: • Refined Structural features: nuclearity type of node P ; distance from P , Clef t , Cright to the start and end of the document / paragrap"
P17-2029,P13-1048,0,0.392385,"rs), pages 184–188 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2029 course tree (i.e., identifying span and nuclearity) in the first stage. The benefits are three-fold. First, we can still use the transition based model which is a good tree construction tool. Second, much fewer actions need to be identified in the tree construction process. Third, we could separately label relations, which needs careful consideration. In the second stage, relation labels for each span are determined independently. Prior studies (Joty et al., 2013; Feng and Hirst, 2014) have found that rhetorical relations distribute differently intra-sententially vs. multi-sententially. They discriminate the two levels by training two models with different feature sets. We take a further step and argue that relations between paragraphs are usually more loosely connected than those between sentences within the same paragraph. Therefore we train three separate classifiers for labeling relations at three levels: withinsentence, across-sentence and across-paragraph. Different features are used for each classifier and the naked tree structure generated in"
P17-2029,D14-1220,0,0.152824,"Missing"
P17-2029,W03-3023,0,0.0291753,"{yizhong, lisujian, wanghf}@pku.edu.cn Abstract tions to form larger text spans until the final tree is built. RST also depicts which part is more important in a relation by tagging Nucleus or Satellite. Generally, each relation at least includes a Nucleus and there are three nuclearity types: NucleusSatellite (NS), Satellite-Nucleus (SN) and NucleusNucleus (NN). Therefore, the performance of RST discourse parsing can be evaluated from three aspects: span, nuclearity and relation. To parse discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based mode"
P17-2029,P99-1047,0,\N,Missing
P18-1015,P16-1154,0,0.0665386,"(Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future"
P18-1015,W17-3204,0,0.0762296,"Missing"
P18-1015,W04-1013,0,0.062659,"seq model to generate more faithful and informative summaries. Specifically, since the input of our system consists of both the sentence and soft template, we use the concatenation function3 to combine the hidden states of the sentence and template: 2.2.1 Rerank In Retrieve, the template candidates are ranked according to the text similarity between the corresponding indexed sentences and the input sentence. However, for the summarization task, we expect the soft template r resembles the actual summary y∗ as much as possible. Here we use the widely-used summarization evaluation metrics ROUGE (Lin, 2004) to measure the actual saliency s∗ (r, y∗ ) (see Section 3.2). We utilize the hidden states of x and r to predict the saliency s of the template. Specifically, we regard the output of the BiRNN as the representation of the sentence or template: ← − → − hx = [ h x1 ; h x−1 ] ← − → − hr = [ h r1 ; h r−1 ] Hc = [hx1 ; · · · ; hx−1 ; hr1 ; · · · ; hr−1 ] The combined hidden states are fed into the prevailing attentional RNN decoder (Bahdanau et al., 2014) to generate the decoding hidden state at the position t: st = Att-RNN(st−1 , yt−1 , Hc ), s(r, x) = (2) ot = sof tmax(st Wo ), (3) + bs ), (6) w"
P18-1015,D15-1166,0,0.0773642,"eNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for summarization. This model contained two-layer LSTMs with 500 hidden units in each layer. OpenNMT We also implement the standard attentional seq2seq model with OpenNMT. All the settings are the same as our system. It is noted that OpenNMT officially examined the Gigaword dataset. We distinguish the official result6 and our experimental result with suffixes “O” and “I” respectively. FTSum Cao et al. (2017b) encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries. In addition, to evaluate the effectiveness of our joint"
P18-1015,P00-1041,0,0.523097,"of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) fou"
P18-1015,P14-5010,0,0.00357672,"LESS 3 The number of the generated summaries, which contains less than three tokens. These extremely short summaries are usually unreadable. COPY The proportion of the summary words (without stopwords) copied from the source sentence. A seriously large copy ratio indicates that the summarization system pays more attention to compression rather than required abstraction. NEW NE The number of the named entities that do not appear in the source sentence or actual summary. Intuitively, the appearance of new named entities in the summary is likely to bring unfaithfulness. We use Stanford CoreNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for sum"
P18-1015,K16-1028,0,0.407324,"ng online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C, which speeds up converge"
P18-1015,P16-1223,0,0.0259771,"Missing"
P18-1015,D15-1044,0,0.0589636,"and templateaware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries. 1 Introduction The exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1015,N16-1012,0,0.53363,"e exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C,"
P18-1015,P17-1099,0,0.146846,"16; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future Work This paper proposes to introduce soft templates as additional input to guide the seq2seq summarization. We use the popular IR platform Lucene to retrieve proper existing summaries as candidate soft template"
P18-1015,W04-1000,0,0.59971,"ctive function of likelihood and ROUGE scores. Guu et al. (2017) also proposed to encode human-written sentences to improvement the performance of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as na"
P18-1178,P16-1223,0,0.0449865,"answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings. 1 Introduction Machine reading comprehension (MRC), empowering computers with the ability to acquire knowledge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent (Chen et al., 2016). Recent years have seen rapid growth in the MRC community. With the release of various datasets, the MRC task has evolved from the early cloze-style test (Hermann et al., 2015; Hill et al., 2015) to answer extraction from a single passage (Rajpurkar et al., 2016) and to the latest more complex question answering on web data (Nguyen et al., 2016; Dunn et al., 2017; He et al., 2017). Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage (Wang and Jiang, 2016; Seo et al., 2016; Pan et al., 2017). A significant milestone i"
P18-1178,W18-2605,1,0.891557,"Missing"
P18-1178,D17-1215,0,0.0195692,"h question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer. One of the intrinsic challenges for such multipassage MRC is that since all the passages are question-related but usually independently written, it’s probable that multiple confusing answer candidates (correct or incorrect) exist. Table 1 shows an example from MS-MARCO. We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect. As is shown by Jia and Liang (2017), these confusing answer candidates could be quite difficult for MRC models to distinguish. Therefore, special consideration is required for such multi-passage MRC problem. In this paper, we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers. Our hypothesis is that the cor* This work was done while the first author was doing internship at Baidu Inc. 1 https://rajpurkar.github.io/SQuAD-explorer/ 1918 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pag"
P18-1178,P17-1147,0,0.0608865,"ages. For the model training, Xiong et al. (2017) argues that the boundary loss encourages exact answers at the cost of penalizing overlapping answers. Therefore they propose a mixed objective that incorporates rewards derived from word overlap. Our joint training approach has a similar function. By taking the content and verification loss into consideration, our model will give less loss for overlapping answers than those unmatched answers, and our loss function is totally differentiable. Recently, we also see emerging interests in multi-passage MRC from both the academic (Dunn et al., 2017; Joshi et al., 2017) and industrial community (Nguyen et al., 2016; He et al., 2017). Early studies (Shen et al., 2017; Wang et al., 2017c) usually concat those passages and employ the same models designed for singlepassage MRC. However, more and more latest studies start to design specific methods that can read multiple passages more effectively. In the aspect of passage selection, Wang et al. (2017a) introduced a pipelined approach that rank the passages first and then read the selected passages for answering questions. Tan et al. (2017) treats the passage ranking as an auxiliary task that can be trained jointl"
P18-1178,P14-5010,0,0.00415096,"s one single answer here. Therefore, we also report the proportion of questions that have multiple answer spans to match with the human-generated answers. A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model"
P18-1178,P02-1040,0,0.101008,"or “No”. For DuReader, the retrieved document usually contains a large number of paragraphs that cannot be fed into MRC models directly (He et al., 2017). The original paper employs a simple a simple heuristic strategy to select a representative paragraph for each document, while we train a paragraph ranking model for this. We will demonstrate the effects of these two technologies later. 3.3 Results on MS-MARCO Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set. We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002). As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human perModel Match-LSTM BiDAF PR + BiDAF Our Model Human BLEU-4 31.8 31.9 37.55 40.97 56.1 ROUGE-L 39.0 39.2 41.81 44.18 57.4 Table 4: Performance on the DuReader test set Model Complete Model Answer Verification Content Modeling Joint Training YesNo Classification Boundary Baseline ROUGE-L 45.65 44.38 44.27 44.12 41.87 38.95 ∆ -1.27 -1.38 -1.53 -3.78 -6.70 Table 5: Ablation study on MS-MARCO development set formance. If we ensemb"
P18-1178,D14-1162,0,0.0827217,"ieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model S-Net (Ensemble) Our Model (Ensemble) Human ROUGE-L 33.67 37.33 38.81 42.89 45.23 46.15 46.65 46.66 47 BLEU-1 33.93 40.72 39.86 42.22 43.78 44.47 44.78 45.41 46 Table 3: Performance of our method"
P18-1178,C02-1169,0,0.176529,"Missing"
P18-1178,P17-1018,0,0.637994,"we formally present the details of modeling the question and passages. Encoding We first map each word into the vector space by concatenating its word embedding and sum of its character embeddings. Then we employ bi-directional LSTMs (BiLSTM) to encode the question Q and passages {Pi } as follows: Q Q Q uQ t = BiLSTMQ (ut−1 , [et , ct ]) (1) i uPt i = BiLSTMP (uPt−1 , [ePt i , cPt i ]) (2) Q Pi Pi where eQ t , ct , et , ct are the word-level and character-level embeddings of the tth word. uQ t and uPt i are the encoding vectors of the tth words in Q and Pi respectively. Unlike previous work (Wang et al., 2017c) that simply concatenates all the passages, we process the passages independently at the encoding and matching steps. Q-P Matching One essential step in MRC is to match the question with passages so that important information can be highlighted. We use the Attention Flow Layer (Seo et al., 2016) to conduct the Q-P matching in two directions. The similarity matrix S ∈ R|Q|×|Pi |between the question and passage i is changed to a simpler version, where the similarity between the tth word in the question and the k th word in passage i is computed as: | Pi St,k = uQ t · uk (3) Then the context-to"
P18-1178,K17-1028,0,0.0888184,"Missing"
P18-1178,D16-1264,0,\N,Missing
P18-2071,P04-3031,0,0.245735,"Missing"
P18-2071,D17-1136,0,0.0305757,"(Li et al., 2014) and treat the preceding EDU as the head. Another issue on coherence relations is about polynary relations which involve more than two EDUs. The first scenario is that one EDU dominates a set of posterior EDUs as its member. In this case, we annotate binary relations from head EDU to each member EDU with the same relation. The second scenario is that several EDUs are of equal importance in a polynary relation. For this case, we link each former EDU to its neighboring EDU with the same relation, forming a relation chain similar to “right-heavy” binarization transformation in (Morey et al., 2017). By assuring that each EDU has one and only one head EDU, we can obtain a dependency tree for each scientific abstract. An example of dependency annotation is shown in Figure 1. 3 Annotator Recruitment To select annotators, we put forward two requirements to ensure the annotation quality. First, we required the candidates to have linguistic knowledge. Second, each candidate was asked to join a test annotation of 20 abstracts, whose quality was evaluated by experts. After the judgement, 5 annotators were qualified to participate in our work. EDU Segmentation We performed EDU segmentation in a"
P18-2071,W03-3017,0,0.148335,"l features in relation type prediction, the two-stage dependency parser gets better performance on LAS than vanilla system on both development and test set. Compared with graph-based model, the two transition-based baselines achieve higher accuracy with regard to UAS and LAS. Using more effective training strategies like MIRA may improve graph-based models. We can also see that human performance is still much higher than the three parsers, meaning there is large space for improvement in future work. Vanilla Transition-based Parser We adopt the transition-based method for dependency parsing by Nivre (2003). The action set of arc-standard system (Nivre et al., 2004) is employed. We build an SVM classifier to predict most possible transition action for given configuration. We adopt the N-gram features, positional features, length features and dependency features for top-2 EDUs in the stack and top EDU in the buffer, which can be referred from (Li et al., 2014; Wang et al., 2017) Two-stage Transition-based Parser We implement a two-stage transition-based dependency parser following (Wang et al., 2017). First, an unlabeled tree is produced by vanilla transition-based approach. Then we train a separ"
P18-2071,W04-2407,0,0.254265,"Missing"
P18-2071,A00-2018,0,0.0195318,"s, we put forward two requirements to ensure the annotation quality. First, we required the candidates to have linguistic knowledge. Second, each candidate was asked to join a test annotation of 20 abstracts, whose quality was evaluated by experts. After the judgement, 5 annotators were qualified to participate in our work. EDU Segmentation We performed EDU segmentation in a semi-automatic way. First, we did sentence tokenization on raw texts using NLTK 3.2 (Bird and Loper, 2004). Then we used SPADE (Soricut and Marcu, 2003), a pre-trained EDU segmenter relying on Charniak’s syntactic parser (Charniak, 2000), to automatically cut sentences into EDUs. Then, we manually checked each segmented abstract to ensure the segmentation quality. Two annotators conducted the checking task, with one proofreading the output of SPADE, and the other reviewing the proofreading. The checking process was recorded for statistical analysis. Tree Annotation Labeling dependency trees was the most labor-intensive work in the corpus construction. 798 segmented abstracts were labeled by 5 annotators in 6 months. 506 abstracts were annotated more than twice separately by different annotators, with the purpose of analysing"
P18-2071,D14-1168,0,0.0317059,"framework, annotation workflow and some statistics about SciDTB. Furthermore, our treebank is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work. 1 Introduction Discourse relation depicts how the text spans in a text relate to each other. These relations can be categorized into different types according to semantics, logic or writer’s intention. Annotations of such discourse relations can benefit many down-stream NLP tasks including machine translation (Guzm´an et al., 2014; Joty et al., 2014) and automatic summarization (Gerani et al., 2014). Several discourse corpora have been proposed in previous work, grounded with various discourse theories. Among them Rhetorical Structure Theory TreeBank (RST-DT) (Carlson et al., 2003) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) are the most widely-used resources. PDTB focuses on shallow discourse relations between two arguments and ignores the whole organization. RST-DT based on Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents a text into a hierarchical discourse tree. Though 1 The treebank is available at https://github.com/PKUTANGENT/SciDTB 444 Proceedin"
P18-2071,N03-1030,0,0.335999,"ple of dependency annotation is shown in Figure 1. 3 Annotator Recruitment To select annotators, we put forward two requirements to ensure the annotation quality. First, we required the candidates to have linguistic knowledge. Second, each candidate was asked to join a test annotation of 20 abstracts, whose quality was evaluated by experts. After the judgement, 5 annotators were qualified to participate in our work. EDU Segmentation We performed EDU segmentation in a semi-automatic way. First, we did sentence tokenization on raw texts using NLTK 3.2 (Bird and Loper, 2004). Then we used SPADE (Soricut and Marcu, 2003), a pre-trained EDU segmenter relying on Charniak’s syntactic parser (Charniak, 2000), to automatically cut sentences into EDUs. Then, we manually checked each segmented abstract to ensure the segmentation quality. Two annotators conducted the checking task, with one proofreading the output of SPADE, and the other reviewing the proofreading. The checking process was recorded for statistical analysis. Tree Annotation Labeling dependency trees was the most labor-intensive work in the corpus construction. 798 segmented abstracts were labeled by 5 annotators in 6 months. 506 abstracts were annotat"
P18-2071,P14-1065,0,0.111038,"Missing"
P18-2071,L16-1167,0,0.0725486,"Missing"
P18-2071,P17-2029,1,0.801523,"see that human performance is still much higher than the three parsers, meaning there is large space for improvement in future work. Vanilla Transition-based Parser We adopt the transition-based method for dependency parsing by Nivre (2003). The action set of arc-standard system (Nivre et al., 2004) is employed. We build an SVM classifier to predict most possible transition action for given configuration. We adopt the N-gram features, positional features, length features and dependency features for top-2 EDUs in the stack and top EDU in the buffer, which can be referred from (Li et al., 2014; Wang et al., 2017) Two-stage Transition-based Parser We implement a two-stage transition-based dependency parser following (Wang et al., 2017). First, an unlabeled tree is produced by vanilla transition-based approach. Then we train a separate SVM classifier to predict relation types on the tree in pre-order. For the 2nd-stage, apart from features in the 1ststage, two kinds of features are added, including depth of head and dependent in the tree and the predicted relation between the head and its head. 6 Conclusions In this paper, we propose to construct a discourse dependency treebank SciDTB for scientific abs"
P18-2071,W14-3352,0,0.138734,"Missing"
P18-2071,J05-2005,0,0.138917,"“dependent”), which makes non-projective structure possible. However, Li et al. (2014) and Yoshida et al. (2014) mainly focused on the definition of discourse dependency structure and directly transformed constituency trees in RST-DT into dependency trees. On the one hand, they only simply treated the transformation ambiguity, while constituency structures and dependency structures did not correspond one-toone. On the other hand, the transformed corpus still did not contain non-projective dependency trees, though “crossed dependencies” actually exist in the real flexible discourse structures (Wolf and Gibson, 2005). In such case, it is essential to construct a discourse dependency treebank from scratch instead of through automatically converting from the constituency structures. In this paper, we construct the discourse dependency corpus SciDTB1 . based on scientific abstracts, with the reference to the discourse deAnnotation corpus for discourse relations benefits NLP tasks such as machine translation and question answering. In this paper, we present SciDTB, a domainspecific discourse treebank annotated on scientific articles. Different from widelyused RST-DT and PDTB, SciDTB uses dependency trees to r"
P18-2071,D14-1196,0,0.0951194,"discourse representation. Stede et al. (2016) adopted dependency tree format to compare RST structure and Segmented Discourse Representation Theory(SDRT) (Lascarides and Asher, 2008) structure for a corpus of short texts. Their discourse dependency framework is adapted from syntactic dependency structure (Hudson, 1984; B¨ohmov´a et al., 2003), with words replaced by elementary discourse units (EDUs). Binary discourse relations are represented from dominant EDU (called “head”) to subordinate EDU (called “dependent”), which makes non-projective structure possible. However, Li et al. (2014) and Yoshida et al. (2014) mainly focused on the definition of discourse dependency structure and directly transformed constituency trees in RST-DT into dependency trees. On the one hand, they only simply treated the transformation ambiguity, while constituency structures and dependency structures did not correspond one-toone. On the other hand, the transformed corpus still did not contain non-projective dependency trees, though “crossed dependencies” actually exist in the real flexible discourse structures (Wolf and Gibson, 2005). In such case, it is essential to construct a discourse dependency treebank from scratch"
P18-2071,P14-1003,1,0.7186,"dency structures into discourse representation. Stede et al. (2016) adopted dependency tree format to compare RST structure and Segmented Discourse Representation Theory(SDRT) (Lascarides and Asher, 2008) structure for a corpus of short texts. Their discourse dependency framework is adapted from syntactic dependency structure (Hudson, 1984; B¨ohmov´a et al., 2003), with words replaced by elementary discourse units (EDUs). Binary discourse relations are represented from dominant EDU (called “head”) to subordinate EDU (called “dependent”), which makes non-projective structure possible. However, Li et al. (2014) and Yoshida et al. (2014) mainly focused on the definition of discourse dependency structure and directly transformed constituency trees in RST-DT into dependency trees. On the one hand, they only simply treated the transformation ambiguity, while constituency structures and dependency structures did not correspond one-toone. On the other hand, the transformed corpus still did not contain non-projective dependency trees, though “crossed dependencies” actually exist in the real flexible discourse structures (Wolf and Gibson, 2005). In such case, it is essential to construct a discourse depende"
P19-1226,D18-1454,0,0.469513,"ar, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding language better than previously appreciated (Peters et al., 2018b; Goldberg, 2019). However, as widely recognized, genuine reading comprehension requires not only language understanding, but also knowledge that supports sophisticated reasoning (Chen et al., 2016; Mihaylov and Frank, 2018; Bauer et al., 2018; Zhong 2346 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2346–2357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018). Thereby, we argue that pre-trained LMs, despite their powerfulness, could be further improved for MRC by integrating background knowledge. Fig. 1 gives a motivating example from ReCoRD (Zhang et al., 2018). In this example, the passage describes that Sudan faces trade sanctions from US due to its past support for North Korea. The cloze-style question states that Sudan is s"
P19-1226,P04-3031,0,0.16502,"2017), where the WordNet embeddings were pre-trained on a subset consisting of 151,442 triples with 40,943 synsets and 18 relations, and the NELL embeddings pre-trained on a subset containing 180,107 entities and 258 3 In this paper, we restrict ourselves to improvements involving a single model, and hence do not consider ensembles. 2350 concepts. Both groups of embeddings are 100-D. Refer to (Yang and Mitchell, 2017) for details. Then we retrieve knowledge from the two KBs. For WordNet, we employ the BasicTokenizer built in BERT to tokenize text, and look up synsets for each word using NLTK (Bird and Loper, 2004). Synsets within the 40,943 subset are returned as candidate KB concepts for the word. For NELL, we link entity mentions to the whole KB, and return associated concepts within the 258 subset as candidate KB concepts. Entity mentions are given as answer candidates on ReCoRD, and recognized by Stanford CoreNLP (Manning et al., 2014) on SQuAD1.1. Finally, we follow Devlin et al. (2018) and use the FullTokenizer built in BERT to segment words into wordpieces. The maximum question length is set to 64. Questions longer than that are truncated. The maximum input length (|S|) is set to 384. Input sequ"
P19-1226,P16-1223,0,0.0693758,"Missing"
P19-1226,P18-1224,0,0.0556635,"., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independently to predict the"
P19-1226,P18-1078,0,0.012744,"UN 1. nongovorganization: 0.986 2. sentinel: 0.012 3. terroristorganization: 0.001 Concepts from WordNet: ban 1. forbidding_NN_1: 0.861 2. proscription_NN_1: 0.135 3. ban_VB_2: 0.002 sanctions 1. sanction_VB_1: 0.336 2. sanction_NN_3: 0.310 3. sanction_NN_4: 0.282 (a) (a) KT-NET (b) BERT Figure 3: Case study. Heat maps present similarities between question (row) and passage (column) words. Line charts show probabilities of answer boundaries. In KT-NET, top 3 most relevant KB concepts are further given. with 12 self-attention heads and 768 hidden units); (iii) DocQA (Liu et al., 2018) and SAN (Clark and Gardner, 2018) are two previous state-of-the-art MRC models; (iv) the pre-trained LM ELMo (Peters et al., 2018a) is further used in DocQA. All these models, except for DCReader+BERT, were re-implemented by the creators of the dataset and provided as official baselines (Zhang et al., 2018). On SQuAD6 (Table 3): (i) BERT+TriviaQA is the former best model officially submitted by Google. It is an uncased, large model, and further uses data augmentation with TriviaQA (Joshi et al., 2017); (ii) WD, nlnet, and MARS are three competitive models that have not been published; (iii) QANet is a well performing MRC mode"
P19-1226,P17-1055,0,0.0245036,". We observed similar phenomena on SQuAD1.1 and report the results in Appendix B. 5 Related Work Machine Reading Comprehension In the last few years, a number of datasets have been created for MRC, e.g., CNN/DM (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016, 2018), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and MS-MARCO (Nguyen et al., 2016). These 7 During visualization, we use a row-wise softmax operation to normalize similarity scores over all passage tokens. datasets have led to advances like Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2017), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017), R-Net (Wang et al., 2017), and QANet (Yu et al., 2018). These end-to-end neural models have similar architectures, starting off with an encoding layer to encode every question/passage word as a vector, passing through various attention-based interaction layers and finally a prediction layer. More recently, LMs such as ELMo (Peters et al., 2018b), GPT (Radford et al., 2018), and BERT (Devlin et al., 2018) have been devised. They pre-train deep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MR"
P19-1226,P17-1147,0,0.402474,"n) WordNet: (sanctions, common-hypernym-with, ban) Figure 1: An example from ReCoRD, with answer candidates marked (underlined) in the passage. The vanilla BERT model fails to predict the correct answer. But it succeeds after integrating background knowledge collected from WordNet and NELL. Introduction Machine reading comprehension (MRC), which requires machines to comprehend text and answer questions about it, is a crucial task in natural language processing. With the development of deep learning and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts."
P19-1226,P18-1157,0,0.0363982,"122 3. organization: 0.003 UN 1. nongovorganization: 0.986 2. sentinel: 0.012 3. terroristorganization: 0.001 Concepts from WordNet: ban 1. forbidding_NN_1: 0.861 2. proscription_NN_1: 0.135 3. ban_VB_2: 0.002 sanctions 1. sanction_VB_1: 0.336 2. sanction_NN_3: 0.310 3. sanction_NN_4: 0.282 (a) (a) KT-NET (b) BERT Figure 3: Case study. Heat maps present similarities between question (row) and passage (column) words. Line charts show probabilities of answer boundaries. In KT-NET, top 3 most relevant KB concepts are further given. with 12 self-attention heads and 768 hidden units); (iii) DocQA (Liu et al., 2018) and SAN (Clark and Gardner, 2018) are two previous state-of-the-art MRC models; (iv) the pre-trained LM ELMo (Peters et al., 2018a) is further used in DocQA. All these models, except for DCReader+BERT, were re-implemented by the creators of the dataset and provided as official baselines (Zhang et al., 2018). On SQuAD6 (Table 3): (i) BERT+TriviaQA is the former best model officially submitted by Google. It is an uncased, large model, and further uses data augmentation with TriviaQA (Joshi et al., 2017); (ii) WD, nlnet, and MARS are three competitive models that have not been published; (iii) Q"
P19-1226,P14-5010,0,0.00277388,"s. 2350 concepts. Both groups of embeddings are 100-D. Refer to (Yang and Mitchell, 2017) for details. Then we retrieve knowledge from the two KBs. For WordNet, we employ the BasicTokenizer built in BERT to tokenize text, and look up synsets for each word using NLTK (Bird and Loper, 2004). Synsets within the 40,943 subset are returned as candidate KB concepts for the word. For NELL, we link entity mentions to the whole KB, and return associated concepts within the 258 subset as candidate KB concepts. Entity mentions are given as answer candidates on ReCoRD, and recognized by Stanford CoreNLP (Manning et al., 2014) on SQuAD1.1. Finally, we follow Devlin et al. (2018) and use the FullTokenizer built in BERT to segment words into wordpieces. The maximum question length is set to 64. Questions longer than that are truncated. The maximum input length (|S|) is set to 384. Input sequences longer than that are segmented into chunks with a stride of 128. The maximum answer length at inference time is set to 30. Comparison Setting We evaluate our approach in three settings: KT-NETWordNet, KT-NETNELL, and KT-NETBOTH, to incorporate knowledge from WordNet, NELL, and both of the two KBs, respectively. We take BERT"
P19-1226,D18-1260,0,0.0467552,"ra to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly improve the performance in either a fine-tuning or feature-based way. Built upon pre-trained LMs, our work further explores the potential of incorporating structured knowledge from KBs, combining the strengths of both text and knowledge representations. Incorporating KBs Several MRC datasets that require external knowledge have been proposed, such as ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve"
P19-1226,P18-1076,0,0.485861,"y the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding language better than previously appreciated (Peters et al., 2018b; Goldberg, 2019). However, as widely recognized, genuine reading comprehension requires not only language understanding, but also knowledge that supports sophisticated reasoning (Chen et al., 2016; Mihaylov and Frank, 2018; Bauer et al., 2018; Zhong 2346 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2346–2357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018). Thereby, we argue that pre-trained LMs, despite their powerfulness, could be further improved for MRC by integrating background knowledge. Fig. 1 gives a motivating example from ReCoRD (Zhang et al., 2018). In this example, the passage describes that Sudan faces trade sanctions from US due to its past support for North Korea. The cloze-style question st"
P19-1226,P18-2124,0,0.023982,"tes questions that require external knowledge and reasoning. It also filters out questions that can be answered simply by pattern matching, posing further challenges to current MRC systems. We take it as the major testbed for evaluating our approach. SQuAD1.1 (Rajpurkar et al., 2016) is a wellknown extractive MRC dataset that consists of questions created by crowdworkers for Wikipedia articles. The golden answer to each question is a span from the corresponding passage. In this paper, we focus more on answerable questions than unanswerable ones. Hence, we choose SQuAD1.1 rather than SQuAD2.0 (Rajpurkar et al., 2018). Table 1 provides the statistics of ReCoRD and SQuAD1.1. On both datasets, the training and development (dev) sets are publicly available, but the test set is hidden. One has to submit the code to retrieve the final test score. As frequent submissions to probe the unseen test set are not encouraged, we only submit our best single model for testing,3 and conduct further analysis on the dev set. Both datasets use Exact Match (EM) and (macro-averaged) F1 as the evaluation metrics (Zhang et al., 2018). 3.2 Experimental Setups Data Preprocessing We first prepare pre-trained KB embeddings. We use t"
P19-1226,D16-1264,0,0.525999,"rdNet: (government, same-synset-with, administration) WordNet: (sanctions, common-hypernym-with, ban) Figure 1: An example from ReCoRD, with answer candidates marked (underlined) in the passage. The vanilla BERT model fails to predict the correct answer. But it succeeds after integrating background knowledge collected from WordNet and NELL. Introduction Machine reading comprehension (MRC), which requires machines to comprehend text and answer questions about it, is a crucial task in natural language processing. With the development of deep learning and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner"
P19-1226,S18-1119,0,0.0163245,"ep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly improve the performance in either a fine-tuning or feature-based way. Built upon pre-trained LMs, our work further explores the potential of incorporating structured knowledge from KBs, combining the strengths of both text and knowledge representations. Incorporating KBs Several MRC datasets that require external knowledge have been proposed, such as ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode"
P19-1226,S18-1120,0,0.0120194,"et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independently to predict the answer. And the mod"
P19-1226,D19-5804,0,0.116397,"ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independ"
P19-1226,N18-1202,0,0.720563,"and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts. Among different pre-training mechanisms, BERT (Devlin et al., 2018), which uses Transformer encoder (Vaswani et al., 2017) and trains a bidirectional LM, is undoubtedly the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding"
P19-1226,D18-1179,0,0.320665,"and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts. Among different pre-training mechanisms, BERT (Devlin et al., 2018), which uses Transformer encoder (Vaswani et al., 2017) and trains a bidirectional LM, is undoubtedly the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding"
P19-1226,P17-1018,0,0.0227181,"rt the results in Appendix B. 5 Related Work Machine Reading Comprehension In the last few years, a number of datasets have been created for MRC, e.g., CNN/DM (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016, 2018), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and MS-MARCO (Nguyen et al., 2016). These 7 During visualization, we use a row-wise softmax operation to normalize similarity scores over all passage tokens. datasets have led to advances like Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2017), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017), R-Net (Wang et al., 2017), and QANet (Yu et al., 2018). These end-to-end neural models have similar architectures, starting off with an encoding layer to encode every question/passage word as a vector, passing through various attention-based interaction layers and finally a prediction layer. More recently, LMs such as ELMo (Peters et al., 2018b), GPT (Radford et al., 2018), and BERT (Devlin et al., 2018) have been devised. They pre-train deep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly"
P19-1226,P17-1132,0,0.462652,"and Text fusion NET), a new approach to MRC which improves pre-trained LMs with additional knowledge from knowledge bases (KBs). The aim here is to take full advantage of both linguistic regularities covered by deep LMs and high-quality knowledge derived from curated KBs, towards better MRC. We leverage two KBs: WordNet (Miller, 1995) that records lexical relations between words and NELL (Carlson et al., 2010) that stores beliefs about entities. Both are useful for the task (see Fig. 1). Instead of introducing symbolic facts, we resort to distributed representations (i.e., embeddings) of KBs (Yang and Mitchell, 2017). With such KB embeddings, we could (i) integrate knowledge relevant not only locally to the reading text but also globally about the whole KBs; and (ii) easily incorporate multiple KBs at the same time, with minimal task-specific engineering (see § 2.2 for detailed explanation). As depicted in Fig. 2, given a question and passage, KT-NET first retrieves potentially relevant KB embeddings and encodes them in a knowledge memory. Then, it employs, in turn, (i) a BERT encoding layer to compute deep, context-aware representations for the reading text; (ii) a knowledge integration layer to select d"
P19-1344,S14-2051,0,0.710072,"on term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extracted using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the opt"
P19-1344,W14-4012,0,0.0400851,"Missing"
P19-1344,D14-1179,0,0.0449907,"Missing"
P19-1344,P17-1036,0,0.0610237,"ntiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016;"
P19-1344,C10-1074,0,0.235204,"and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other r"
P19-1344,D17-1310,0,0.817565,"zza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advan"
P19-1344,D15-1168,0,0.765517,"nd their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole"
P19-1344,D15-1166,0,0.0448049,"2014b; Sutskever et al., 2014), and first used in the field of machine translation. In addition, Cho et al. (2014a) improves the decoding by beam-search. However, vanilla Seq2Seq model performs worse in generating long sentences. The reason is that the encoder needs to compress the whole sentence into a fix length representation. To address this problem, Bahdanau et al. (2014) introduce an attention mechanism which selects important parts of the source sentence with respect to the previous hidden state in decoding the next state. Afterward, some studies focus on improving attention mechanism (Luong et al., 2015). So far, Seq2Seq models and attention mechanism have been applied to many fields such as dialog (Serban et al., 2016) generation, text summarization (Nallapati et al., 2016) and etc. In this paper, we first attempt to formalize the ATE as a sequence-to-sequence learning task because it can make full use of both the meaning of the sentence and label dependencies compared with existing methods. Furthermore, we design a position-aware attention model and gated unit networks to make Seq2Seq model better suit to this task. Generally, Seq2Seq model is timeconsuming in many fields because the target"
P19-1344,C10-2090,0,0.119147,"timent analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is"
P19-1344,D10-1101,0,0.337486,"nd aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the"
P19-1344,K16-1028,0,0.199335,"ng label dependencies because they only use transition matrix to encourage valid label paths and discourage other paths (Collobert et al., 2011). As we know, the label of each word is conditioned on its previous label. For example, “O” is followed by “B/O” but not “I” in the B-I-O tagging schema. To the best of our knowledge, no neural networks based method utilizes the previous label to improve their performances directly. Recently, sequence to sequence (Seq2Seq) learning has been successfully applied to many generation tasks (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014; Nallapati et al., 2016). Seq2Seq learning encodes a source sequence into a fixed-length vector based on which a decoder generates a target sequence. It just has the benefits of first collecting comprehensive information from the source text and then paying more attention to the generation of the target sequence. Thus, we propose to formalize the ATE task as a sequence-to-sequence learning problem, where 3538 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–3547 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the source and"
P19-1344,D14-1162,0,0.0893689,"N) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extra"
P19-1344,P08-1036,0,0.0797458,"(ABSA) is a subfield of sentiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014;"
P19-1344,S16-1002,0,0.319126,"Missing"
P19-1344,S16-1045,0,0.584081,"using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the optimization objective formalized as w1 + r ≈ w2 , where w1 , w2 are words, r is the corresponding dependency path. Then, the learned embeddings of words and dependency paths are utilized as features in CRF for ATE (Yin et al., 2016). 5 https://github.com/facebookresearch/ fastText 6 https://sklearn-crfsuite.readthedocs. io/en/latest/ • RNCRF 8 learns structure features for each word from parse tree by Recursive Neural Networks, and the learned features are fed to CRF to decode the label for each word (Wang et al., 20"
P19-1344,S15-2082,0,0.487521,"Missing"
P19-1344,S14-2004,0,0.503276,"here labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism. 1 Introduction Aspect term extraction (ATE) is a fundamental task in aspect-level sentiment analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development"
P19-1344,I08-1038,1,0.716497,"evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention mode"
P19-1344,D16-1059,0,0.763714,"taff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they can"
P19-1344,D09-1159,0,0.177878,"s appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequence-to-sequence model is a generative mo"
P19-1344,P18-2094,0,0.849769,"the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advance. In addition, n"
P19-1344,J11-1002,0,0.267231,"s in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved goo"
P19-1344,P13-1161,0,0.0264517,"; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 20"
P19-1344,D17-1035,0,0.0195966,"he second group of baselines employs neural networks methods to address the ATE problem: • Bi-LSTM applies different kinds of BiRNN (Elman/Jordan-type RNN) with different kinds of embeddings in the ATE task (Liu et al., 2015). • GloVe-CNN7 uses multi-layer Convolution Neural networks (CNN) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classificati"
P19-1344,P17-2023,0,0.0286843,"ned aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequenc"
Q13-1008,P10-1084,0,0.607486,"s that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA model to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both useful text features and the latent semantic structures from by LDA topic model. One related work is from Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for sentence evaluation. Then the scores are used for tuning the weights of various features that helpful for summary generation. Their work made a good step of combining topic model with feature based supervised learning. However, what their approach confuses us is that whether a topic model only based on word frequency is good enough to generate an appropriate sentence score for regression. Actually, how to incorporate features into LDA topic model has"
Q13-1008,P06-1039,0,0.0725469,"Missing"
Q13-1008,N06-1059,0,0.311964,"Missing"
Q13-1008,W10-4327,0,0.0500763,"ncy. E-step initialize φ0sk := 1/K for all i and s. initialize γmi := αmi + N )m/K for all i. initialize ηkt = 0 for all k and t. while not convergence for m = 1 : M t+1 according to Eqn.(6) update γm for s = 1 : Nm for k = 1 : K update φt+1 sk according to Eqn.(7) normalize the sum of φt+1 sk to 1. Minimize L(η) according to Eqn.(11)-(15). M-step: update β according to Eqn.(8)  K T X X    Ysy exp[Ψ(γms i ) − Ψ( γ ) + ηkt Ysy ]  m k s    t=1 k=1 Y ∝ × βkw if k = x     w∈s    4.3 0 if k 6= x (15) 4.2 Feature Space Lots of features have been proven to be useful for summarization (Louis et al., 2010). Here we discuss several types of features which are adopted in S-sLDA model. The feature values are either binary or normalized to the interval [0,1]. The following features are used in S-sLDA: Cosine Similarity with query: Cosine similarity is based on the tf-idf value of terms. 2 This is reasonable because the influence of γ and β have been embodied in φ during each iteration. 93 Figure 5: Learning process of η in S-sLDA Sentence Selection Strategy Next we explain our sentence selection strategy. According to our intuition that the desired summary should have a small KL divergence with que"
Q13-1008,W11-0507,0,0.0145846,"blem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA mode"
Q13-1008,W02-0401,0,0.425884,"-supervised) approaches, supervised approaches, and Bayesian approaches. Unsupervised (semi-supervised) approaches such as Lexrank (Erkan and Radex, 2004), manifold (Wan et al., 2007) treat summarization as a graphbased ranking problem. The relatedness between the query and each sentence is achieved by imposing querys influence on each sentence along with the propagation of graph. Most supervised approaches regard summarization task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on 90 query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and con"
Q13-1008,H05-1115,0,0.0759342,"Missing"
Q13-1008,D09-1026,0,0.151958,"Missing"
Q13-1008,W03-0502,0,0.102642,"Missing"
Q13-1008,N09-1041,0,\N,Missing
S17-2161,P06-4018,0,0.0222574,"henomenon is the poor numbers for Task keyphrases. Most of Material and Process keyphrases are noun phrases or have capital letters, they are relatively easy to discriminate by part-of-speech pattern. However, Task keyphrases cover a wide range of part-ofspeech patterns, and some of them have verb or conjunction. It remains a challenge for our system to achieve satisfying performance for Task keyphrases. Experiments For details about this shared task and dataset, please refer to SemEval 2017 Task 10 description paper (Augenstein et al., 2017). 3.1 Experimental Setup Preprocessing We use nltk (Bird, 2006) to segment each paragraph into a list of sentences, tokenize every sentence and then get part-of-speech tag for every token. Snowball Stemmer is used for stemming. Stop words, punctuations and digits are removed for feature engineering, but not for keyphrase candidate generation. We use simple heuristics to parse the IEEE taxonomy pdf file and get 6978 phrases in total. recall Material 0.715 Process 0.608 Task 0.334 Micro-average 0.606 Table 3: Recall for keyphrases. An important metric for our pipeline system is recall for keyphrases in candidate generation step. Table 3 shows that our heuri"
S17-2161,S15-1013,0,0.0855581,"which also explore unsupervised techniques as auxiliaries. It involves three steps: candidate generation, keyphrase ranking and keyphrase type classification. For candidate generation, we use chunking-based approach to discover phrases that match a predefined part-of-speech pattern. Heuristic rules are manually designed by experience and applied to filter out those phrases which are unlikely to be keyphrases. For keyphrase ranking in subtask A, we use a straightforward regression-based pointwise ranking method. Here, two unsupervised algorithms TextRank (Mihalcea and Tarau, 2004) and SGRank (Danesh et al., 2015) are incorporated into random forest by providing their output as complementary features. In our experiments, we find that stacking linear model upon random forest can provide extra performance gain. For keyphrase type classification in subtask B, we model it as a three-way classification problem, with the same feature set and classifiers used in subtask A. Feature engineering is a critical part for supervised model. The task of keyphrase extraction heavily relies on statistical features(such as TFIDF) and semantic features. However, due to the limited size of labeled dataset, it is hard to ge"
S17-2161,P14-1119,0,0.33887,"Missing"
S17-2161,S10-1004,0,0.365406,"etrieval, topic modeling and text classification. However, manually labeling keyphrase would be far too time-consuming, and unrealistic especially when dealing with web-scale collection of documents. Therefore, automatic keyphrase extraction has drawn growing interests among NLP research communities for years. For state-of-the-art system on keyphrase extraction, Hasan and Ng(2014) presents a comprehensive survey. Their experiments demonstrate that unsupervised approaches including graph-based ranking and topic modeling techniques perform best on News and Blogs dataset. In SemEval 2010 Task 5 (Kim et al., 2010) (Kim et al., 2013), which also aims to tackle the challenge of keyphrase extraction in scientific area, a majority of the participants adopt supervised approaches, and especially the top 2 systems are both supervised. Thus, in our work, we argue that super934 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 934–937, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 Methodology keyphrases related to technical areas. Articles in this shared task come from three domains: computer science, material science and phy"
S17-2161,D14-1162,0,0.0816612,"the top 2 systems are both supervised. Thus, in our work, we argue that super934 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 934–937, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 Methodology keyphrases related to technical areas. Articles in this shared task come from three domains: computer science, material science and physics. All three domains are covered by IEEE. We add a boolean feature which indicates whether the given candidate keyphrase appears in this list. • Pre-trained Glove embeddings.3 (Pennington et al., 2014) Word embeddings trained on billions of tokens provide a simple way to incorporate semantic knowledge. They prove to be helpful in many NLP tasks especially when labeled data is limited. In our system, we use IDF-weighted word embeddings for phrase representation. Given a phrase consisting of n words w1 , w2 ...wn , its representation is calculated as follows. ∑n IDF (wi ) · Ewi i=1 ∑n (7) i=1 IDF (wi ) Our system works in a pipeline fashion. It involves candidate generation, keyphrase ranking for subtask A and keyphrase type classification for subtask B. As the third step use the same feature"
U19-1024,P18-1068,0,0.153232,"rse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the final result so that the high-level model can p"
U19-1024,C18-1018,0,0.021539,"017). Some cases of math word problems, math equations, templates and sketches are shown in Table 2 These methods are intuitive, but it is difficult to obtain high-quality templates due to data sparsity and transfer them to other datasets. The second category of methods mainly exploits the seq2seq framework to generate the solution equations (Wang et al., 2018a). Recently this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mech"
U19-1024,D17-1084,0,0.0302057,"rees (ASTs) (Rabinovich et al., 2017). Its decoder uses a dynamically-determined modular structure paralleling the structure of the output tree. 2.2 Math Word Problem Math word problem (MWP) aims to teach computers to read the questions in natural language and generate the corresponding math equations. The methods of solving math word problems can be mainly classified into two categories. The first category is the template-based models which summarize some templates through locating similar questions from a given dataset and then fill the concrete numbers into the templates to solve problems (Huang et al., 2017; Wang et al., 2017). Some cases of math word problems, math equations, templates and sketches are shown in Table 2 These methods are intuitive, but it is difficult to obtain high-quality templates due to data sparsity and transfer them to other datasets. The second category of methods mainly exploits the seq2seq framework to generate the solution equations (Wang et al., 2018a). Recently this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforc"
U19-1024,P13-2009,0,0.0149845,"de], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 2017). Its decoder uses a"
U19-1024,N16-1086,0,0.0342508,"this kind of methods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mechanism with two levels of decoders. The first decoder generates a raw sequence and the second decoder polishes and refines the raw sentence with deliberation. Their model performs excellently on machine translation and text summarization, which is also the first time to use this kind of coarse-to-fine model. Wang et al. (2018b) used a similar framework to write paper ab"
U19-1024,N19-1272,0,0.0324643,"Missing"
U19-1024,P17-1020,0,0.030628,"ods have shown outstanding performance without manual feature engineering, but they are prone to generate wrong numbers due to its generation flexibility. Some researches have applied reinforcement learning (Huang et al., 2018) or a stack (Chiang and Chen, 2018) to improve the decoding process. 2.3 Coarse-to-fine method Generalized coarse-to-fine method divides problems into different stages and solves them from coarse to fine. This method is widely applied in computer vision (Gangaputra and Geman, 2006; Pedersoli et al., 2011; Wen et al., 2019) and natural language process (Mei et al., 2016; Choi et al., 2017). The special coarse-to-fine method in this paper is based on end-to-end framework. It has two seq2seq models, generating target data from a coarse stage to a fine stage. Xia et al. (2017) proposed polish mechanism with two levels of decoders. The first decoder generates a raw sequence and the second decoder polishes and refines the raw sentence with deliberation. Their model performs excellently on machine translation and text summarization, which is also the first time to use this kind of coarse-to-fine model. Wang et al. (2018b) used a similar framework to write paper abstracts. In the fiel"
U19-1024,P17-1105,0,0.0327583,"Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 2017). Its decoder uses a dynamically-determined modular structure paralleling the structure of the output tree. 2.2 Math Word Problem Math word problem (MWP) aims to teach computers to read the questions in natural language and generate the corresponding math equations. The methods of solving math word problems can be mainly classified into two categories. The first category is the template-based models which summarize some templates through locating similar questions from a given dataset and then fill the concrete numbers into the templates to solve problems (Huang et al., 2017; Wang et al., 2017"
U19-1024,W00-1317,0,0.11608,"method and method set to bytes.decode[bytes.decode], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs)"
U19-1024,D18-1132,0,0.0462238,"in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the"
U19-1024,P18-2042,0,0.120966,"in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model. 1 Introduction The coarse-to-fine (coarse2fine) methods have been applied in many generation tasks such as machine translation (Xia et al., 2017) , abstract writing (Wang et al., 2018b) and semantic parsing (Dong and Lapata, 2018). They have shown excellent performances but still have many disadvantages. Traditional coarse2fine models usually tackle one task in two stages. In the first stage (coarse stage), a low-level seq2seq model is used to generate a rough sketch, which makes the data more compact and alleviates the problem of data sparsity. Some examples of sketches are shown in Table 1. Besides, Sketches in this stage are also easier to generate. Then, in the fine stage, both text and previous sketches will be input to another high-level seq2seq model to predict the"
U19-1024,D17-1088,0,0.0506169,"Missing"
U19-1024,P07-1121,0,0.0455695,"bytes.decode[bytes.decode], substitute the result for decode. x = 150 + 2 − 50 x = hnumi + hnumi − hnumi There are 150 science books, and the storybooks are 50 books less than the science books. How many books are there in the storybooks? Table 1: Examples of text, sketches and generating goals in different datasets. method we applied. 2.1 Semantic Parsing Semantic parsing is a task of translating natural language into computer executable language such as logic form, code in computer language and SQL query. Traditional semantic parsing usually adopts rule based method Tang and Mooney (2000); Wong and Mooney (2007); Andreas et al. (2013). Recently, with the development of neural network techniques, there are many new semantic parsing models with neural methods. Of them, Seq2seq models have been widely applied in semantic parsing tasks. The encoder encodes the text and the decoder predicts the logic symbols (Dong and Lapata, 2016). The seq2tree model encodes inputs by LSTM and generates the logic form by conditioning the output sequences or trees on the encoding vectors.(Dong and Lapata, 2016). Abstract syntax networks (ASN) represent the output as the abstract syntax trees (ASTs) (Rabinovich et al., 201"
U19-1024,P17-1041,0,0.0304312,"in23K. Examples of original data and sketches of these three datasets are shown in Table 1. Task Text2logic Text2code MWP Emb 150 200 128 Hidden 250 300 512 Epoch 50 150 150 LR 0.005 0.005 0.01 Table 4: Model parameters and training settings 4.2 Preprocess To compare the result equally, we made our preprocessing in accord with Dong and Lapata (2018)’s experiment as much as possible. For GEO, we followed Dong and Lapata’s work, transforming all words into lower type and replacing the entity mentions with a sign and a counting number. And for Django, we chosed to use the processed data given by Yin and Neubig (2017). They tokenized and POS tagged sentences using NLTK. In MWP, we followed Wang et al.’s work. To reduce the influence of OOV, we normalized numbers as the order of their appearance. Examples of some processed cases of Math23k are shown in Table 2, who have same sketches. 4.3 Results We has compared our improved coarse2fine model with different published models. The optimizer is Adam and many details of training and testing are shown in Table 3 and Table 4. To compare the result equally, we chose the same model parameters as Dong and Lapata (2018) in semantic parsing tasks. Accuracy in this pap"
W12-6321,P11-1115,0,0.0536292,"Missing"
W12-6321,W12-6326,0,0.0311846,"iverse kinds of representative features, the NERD system has to determine which feature is more important. One team uses supervised method to tune the weight of different features (Tian et al., 2012), while another team uses the information gain criterion (Wei et al., 2012). Besides a good representation of both source text and knowledge base entities, there are other aspects that may benefit a NERD system. One team use model combination method: there are several rank score and each with different feature input; a classification model finally determine the relative importance of each scoring (Liu et al., 2012). Training set can be used to decide the threshold in NIL linking and tune the weight of different features and models. One team also uses the extended version of KB from Baidu Baike to enrich the feature set (Liu et al., 2012), and constructs a one-to-one mapping from Baike to KB, because most of the entities is constructed from Baike. 4.2 Analysis of difficult queries Table 4 shows detailed top/median precision/recall/f-score across all teams, for each query name. The result shows that the performance is good for most of the queries, except for a few, like “田野” “黄河” “黄莺” “黄龙”. As we did not"
W12-6321,W12-6322,0,0.0509936,"Missing"
W12-6321,W12-6328,0,0.0260328,"NIL entities (Peng et al., 2012; Zhang et al., 2012). • a separate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” an"
W12-6321,W12-6325,0,0.116886,"is determined by the task requirements: • preprocessing: the KB and Source text are segmented into Chinese words, and other processing like POS-tagging and named entity recognition are alternatively used; • information extraction: keywords, entities and relevant attributes are extracted, to construct a vector representation of KB and Source text; • similarity calculation: the similarity is computed with feature vector, and entities in KB is generated by the rank score. Most teams use simply the unsupervised method to rank candidates, and some teams use semantic resources like Tongyici Cilin (Tian et al., 2012) or the Web for a better scoring; • “NIL” entity clustering: maximum similarity score below a threshold is a good sign of determining if the entity is in the KB. Hierarchical clustering method is used by many teams to group NIL entities (Peng et al., 2012; Zhang et al., 2012). • a separate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to"
W12-6321,W12-6327,0,0.0295257,"eparate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” and “Out n” ): one team benefits from a explicit representat"
W12-6321,W12-6324,0,0.015289,"tracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” and “Out n” ): one team benefits from a explicit representation of common words and outof-KB entities (Peng et al., 2012), rather than using same set of feature for classification and clustering. They leverage the Web to discover keywords frequently occurring with common names. They further make the assumption that if all the"
W12-6321,W12-6323,0,0.059223,"Missing"
W12-6321,S07-1012,0,\N,Missing
W18-2611,D08-1064,0,0.0300847,"elation to human overall judgment. In this task, we notice that ROUGE is much more effective than BLEU, which may reflect the importance of recall in MRC evaluation. For the comparison between adapted and vanilla metrics, adapted ROUGE-L performs better than vanilla version on every question type. However, our adapted BLEU4 only works better on evaluating entity answers, which is different from the result on single question level. We think it may be due to the peculiar way BLEU employs to get overall score for multiple questions, which was discussed as the “decomposability” problem of BLEU in Chiang et al. (2008). This issue will be explored in our future work. We can see the annotators achieve high agreement on candidate judgment, which indicates the practicability of our scoring criterion and the reliability of the human annotation. Effectiveness of Adaptations The correlation between automatic and manual evaluation metrics is calculated on both single question and overall score levels. On single question level, each candidate answer is taken as a sample to be scored by the two metrics and score pairs are collected across all the samples to compute PCC. On the overall level, predicted answers to 30"
W18-2611,D13-1020,0,0.0367604,"ems. 1 Introduction The goal of current MRC tasks is to develop agents which are able to comprehend passages automatically and answer open-domain questions correctly. With the release of several large-scale datasets like SQuAD (Rajpurkar et al., 2016), MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017), many MRC models have been proposed in previous works (Wang and Jiang, 2016; Seo et al., 2016; Wang et al., 2017). Although MRC model architectures have been intensively studied, the evaluation metrics for them are rarely discussed. For early cloze-style and multiple choice datasets (Richardson et al., 2013; Hermann et al., 2015), this may not be problematic. However, considering the trend that the model is required to generate answers and question type is becoming more variable and closer to real cases, we believe the design * This work was done while the first author was doing internship at Baidu Inc. 98 Proceedings of the Workshop on Machine Reading for Question Answering, pages 98–104 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics ciprocal rank (MRR). With the addition of complex non-factoid questions such as definition questions in TREC 2003 (Voorhees,"
W18-2611,voorhees-tice-2000-trec,0,0.186445,"-L and BLEU are employed as metrics at the same time, with the former as the primary criterion for ranking participating systems. Their modifications will be elaborated separately. 3.1 Adaptations on BLEU For one question sample with single candidate and several gold answers, Papineni et al. (2002) define cumulative BLEU-n with uniform n-gram weight as follows: QA Evaluation Metrics In the past competitions on question answering, various evaluation metrics were proposed to make comparisons between participating systems. Early tasks including TREC-8 and TREC-9 QA tracks (Voorhees et al., 1999; Voorhees and Tice, 2000) only consist of factoid questions. The ordered candidate answers are evaluated manually to give binary correctness judgment and summarized by mean reBLEUcum = BP · n Y !1 n Pi (1) i=1 In the equation, Pi is the precision of i-gram in the candidate answer P Pi = Countclip (i–gram) i–gram∈C P i–gram0 ∈C 99 Count (i–gram0 ) (2) where C is i-gram set of the candidate answer, Count(x) calculates the number of times that igram x appear in candidate and Countclip (x) clips Count(x) to the maximum times that x appears in references. BP stands for brevity penalty item, given reference length r and can"
W18-2611,W04-3250,0,0.234148,"as nuggets and put more weight on them for scoring. To ensure the quality and credibility of the human judgment, we measure the argeement between the 2 annotators. Table 1 shows the Pearson correlation coefficients for each question type and on overall. PCC4 Yes-No 0.878 Entity 0.906 Description 0.870 Adapted ROUGE-L ROUGE-L Adapted BLEU-4 BLEU-4 Overall 0.891 Overall 0.570 0.504 0.481 0.450 Our adaptations bring substantial gain on PCCs for both ROUGE-L and BLEU-4 on single question level. To check the significance of these results, we follow the paired bootstrap resampling test mentioned in Koehn (2004). For a pair of metrics, samples are bootstrapped 100 times and in each time the PCCs are recomputed and compared. For both ROUGE-L and BLEU-4, the paired test between original and adapted versions are performed on yes-no, entity and overall sets. In all the 6 tests, the adapted metric shows significant better performance than the original one. We also calculate PCCs between automatic and human metrics on overall score level. The results are shown in Table 3. Similar to single question level, adapted ROUGE-L still gains the highest correlation to human overall judgment. In this task, we notice"
W18-2611,P17-1018,0,0.125382,"or answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of realscene MRC systems. 1 Introduction The goal of current MRC tasks is to develop agents which are able to comprehend passages automatically and answer open-domain questions correctly. With the release of several large-scale datasets like SQuAD (Rajpurkar et al., 2016), MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017), many MRC models have been proposed in previous works (Wang and Jiang, 2016; Seo et al., 2016; Wang et al., 2017). Although MRC model architectures have been intensively studied, the evaluation metrics for them are rarely discussed. For early cloze-style and multiple choice datasets (Richardson et al., 2013; Hermann et al., 2015), this may not be problematic. However, considering the trend that the model is required to generate answers and question type is becoming more variable and closer to real cases, we believe the design * This work was done while the first author was doing internship at Baidu Inc. 98 Proceedings of the Workshop on Machine Reading for Question Answering, pages 98–104 c Melbourne, Au"
W18-2611,P08-2051,0,0.0431013,"annotated and weighted by human assessors, which is laborintensive. Breck et al. (2000) proposed to use word recall against the stemmed gold answer as an automatic evaluation metric. Following this idea, metrics evaluating n-gram overlap and LCS length between candidate and gold answers are designed and become prevalent, among which BLEU (Papineni et al., 2002) and ROUGE (Dang et al., 2007) are most widely-used. In general, BLEU focuses more on n-gram precision and ROUGE is recalloriented. Later work has made adaptations on these metrics from different perspectives (Banerjee and Lavie, 2005; Liu and Liu, 2008). In this paper, our adaptations are aimed at increasing their correlation to real human judgment on yes-no and entity question answering, which are proved to be practical. to some extent. However, from the perspective of simplicity and scalability to growing question type category, we hope to design a unified and end-toend evaluation metric which is calculated automatically. We propose some adaptations for ROUGE and BLEU which provide them awareness to yesno opinion and entity agreement. Compared with original metrics, our modified ROUGE and BLEU achieve higher correlation to human judgment o"
W18-2611,P02-1040,0,0.109312,"EU to Better Evaluate Machine Reading Comprehension Task An Yang1 2* , Kai Liu2 , Jing Liu2 , Yajuan Lyu2 and Sujian Li1 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Baidu Inc., Beijing, China {yangan, lisujian}@pku.edu.cn {liukai20, liujing46, lvyajuan}@baidu.com Abstract of evaluation metric is indeed an issue to be focused on. Currently, the criterion for comparing generated and gold answers is mostly based on lexical overlap. For example, SQuAD uses exact-match ratio and word-level F1-score, while MS-MARCO and DuReader employ ROUGE-L (Lin, 2004) and BLEU (Papineni et al., 2002) which measure ngram consistency or longest common sequence (LCS) length. For some question types, we notice these metrics may not correlate with semantic correspondence well in some cases. In this paper, we mainly tackle the issue of yes-no and entity questions. For yes-no questions, overlapbased metrics may ignore the yes-or-no opinion which is more crucial in determining agreement between answers. Answers with contrary opinions may have high lexical overlap, such as “The radiation of wireless routers has an impact on people” and “The radiation of wireless routers has no impact on people”. S"
W18-2611,W05-0909,0,\N,Missing
W18-2611,D16-1264,0,\N,Missing
W19-8102,D18-1013,0,0.0306583,"refore, we propose to employ a two-step filterand-sort method to pick out the most similar stories. 2.2.1 ply sort the roughly similar sequences with cosine similarity for downstream models. 3 3.1 In the filter step, we use object co-occurrence to discriminate ’roughly similar’ image sequences from ’dissimilar’ ones. Here we filter by image object information because it conforms with the intuition that images with similar objects describe relevant events. It is also because object information has been widely used in image captioning as helpful information on images. (Mishra and Liwicki, 2019; Liu et al., 2018; Jiang et al., 2018; Anderson et al., 2017; Yin and Ordonez, 2017; Wang et al., 2018a). We first get the types and numbers of objects in each image using an object recognition model, and then we measure image similarity with a categorical criterion and a numerical criterion. Formally, Oa and Ob are the set of objects present in image a and b respectively, ckx is the count of occurrence for object k in image x. The categorical criterion concerns the types of common objects, namely scorecat = √|Oa ∩Ob |; the numerical cri|Oa ||Ob | terion concerns the differences in times of occur|Oa ||Ob | ren"
W19-8102,D15-1166,0,0.0568811,"al Storytelling Framework Most previous works on visual storytelling followed the Seq2Seq framework, taking image recognition models such as ResNet (He et al., 2015) or Inception (Szegedy et al., 2016) to extract image features, feeding them into a storylevel RNN encoder, bringing encoder output to the sentence-level decoder throughout the generation of the corresponding sentence. We base our model on this framework with two key modifications: first, we design a text encoder to model the most similar stories which may provide evidence for story generation; second, we adopt the Luong attention Luong et al. (2015) mechanism on the textual side of encoded input to better utilize its information. 2.2 Text Encoder We use an RNN encoder to model the textual inputs. For each story, we feed its 5 sentences into the RNN one by one, retaining the hidden state across sentences. We take the RNN output of every step through the fully connected layers as encoder output. Textual Evidence Selection To provide strong textual evidence for story generation, we aim to select stories which are most similar to the expected story for the given sequence of images. With the assumption that similar images usually have similar"
W19-8102,P18-1015,1,0.844164,"in and validation set as reference corpus, and a Fast RCNN (He et al., 2017; Abdulla, 2017) model pre-trained on COCO dataset (Lin et al., 2014) to detect objects from each image. Roughly similar stories are filtered with numerical criterion at 500 candidate size as it shows the best performance. Filter k∈(Oa ∪Ob ) Experiments Sort After obtaining a small set of roughly similar image sequences, we use feature vectors to rank similarity more precisely. Here we experiment on two approaches: a simple cosine similarity measure and a Bi-Linear model with Meteor score as gold annotation inspired by Cao et al. (2018). Empirically we find that Bi-Linear model shows no advantage against cosine similarity. Thus, we simResults Methods R/C/M Huang et al. (2016) Yu et al. (2017) Gonzalez-Rico and Pineda (2018) Huang et al. (2018) 31.4 29.5 7.5 34.1 29.2 5.1 34.4 30.8 10.7 35.2 GLACNet(2018) (re-trained) GLAC-TG-top1(ours) 26.3 2.2 33.0 26.5 2.0 33.4 XE-ss(2018b) AREL(2018b) XE-TG-top1(ours) XE-TG-top3(ours) XE-TG-top1-attn(ours) XE-TG-top3-attn(ours) XE-TG-only 29.7 29.9 30.0 29.6 29.9 29.4 29.1 8.7 8.4 8.7 8.3 9.2 9.2 7.7 34.8 35.2 35.5 35.4 35.2 35.0 34.8 Table 1: Performance of our method compared to existin"
W19-8102,N18-1198,0,0.0668364,"Missing"
W19-8102,P18-1083,0,0.332961,"oder and attention. Experiments on the VIST dataset show that our method outperforms state-of-the-art baseline models without heavy engineering. 1 Introduction Multi-image visual storytelling is extended from a long trend of research in image captioning and has attracted considerable attention in recent years. To generate the stories, previous work employed a Seq2Seq framework, using image encoder to encode the image sequences and sentence decoder to generate stories from encoded image sequences. Most of the researches (Smilevski et al., 2018; Kim et al., 2018; Gonzalez-Rico and Pineda, 2018; Wang et al., 2018b; Huang et al., 2018; Yu et al., 2017) focused on improving the decoder, and took simple concatenation or an LSTM as encoder. With such design, only images are utilized as input in generating the stories. However, through our observations, the images alone are inadequate for visual storytelling. Storytelling is creative and diversified, so background knowledge is often required to convert a few images to a complete story. However, extracting such background knowledge is very difficult, especially with limited data. 2 Method Our method is based on the Seq2Seq framework, composed of a two-chann"
W19-8102,D17-1017,0,0.0206542,"to pick out the most similar stories. 2.2.1 ply sort the roughly similar sequences with cosine similarity for downstream models. 3 3.1 In the filter step, we use object co-occurrence to discriminate ’roughly similar’ image sequences from ’dissimilar’ ones. Here we filter by image object information because it conforms with the intuition that images with similar objects describe relevant events. It is also because object information has been widely used in image captioning as helpful information on images. (Mishra and Liwicki, 2019; Liu et al., 2018; Jiang et al., 2018; Anderson et al., 2017; Yin and Ordonez, 2017; Wang et al., 2018a). We first get the types and numbers of objects in each image using an object recognition model, and then we measure image similarity with a categorical criterion and a numerical criterion. Formally, Oa and Ob are the set of objects present in image a and b respectively, ckx is the count of occurrence for object k in image x. The categorical criterion concerns the types of common objects, namely scorecat = √|Oa ∩Ob |; the numerical cri|Oa ||Ob | terion concerns the differences in times of occur|Oa ||Ob | rence, namely scorenum = |Σ . (ck −ck )2 | a 3.2 b Additionally, we s"
W19-8102,D17-1101,0,0.0676264,"Missing"
W19-8104,D14-1224,0,0.201782,"iscourse parsing, with no need of a large scale of Chinese labeled data. 1 Introduction Discourse parsing aims to analyze the inner structure of texts, which is fundamental to many natural language processing applications, such as question answering and summarization. The construction of discourse corpora has promoted the development of discourse parsing techniques. In English, the widely-used discourse corpora include the Rhetorical Structure Theory Treebank (RSTDT) (Carlson et al., 2001) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). Recently, Li et al. (2014a) and Yoshida et al. (2014) proposed the discourse dependency structure (DDS). DDS directly links the EDUs, so it has fewer nodes and simpler structures compared to RST and PDTB. In addition, it can easily represent non-projective structures, while hierarchical structures need other complex mechanisms to do so. DDS is especially important for Chinese. Kang et al. (2019) analyzes almost all the existing Chinese discourse treebanks and concludes that DDS is the future direction due to its right balance between expressiveness and practicality. However, little research has been done on Chinese 24 Proceedings of the 1st Work"
W19-8104,W17-3610,0,0.0305352,"ources, because they are short texts with obvious logic and within the same domain as the English treebank (SciDTB) (Yang and Li, 2018a). Specifically, 108 abstracts are selected from a Chinese NLP journal JCIP 1 . Second, we manually separate these abstracts into elementary discourse units (EDUs), the basic units of a parsing tree. Each segmented abstract is checked at least twice to ensure segmentation quality. Our EDU segmentation mainly refer to the criteria of RST-DT (Carlson and Marcu, 2001) and make some modifications to the guideline based on the linguistic characteristics of Chinese (Cao et al., 2017; Yang and Li, 2018b). Due 1 3 Zero-shot Chinese Dependency Parsing As stated above, our method aims to generate a dependency parsing tree with relation types between EDUs identified for a Chinese text. It is assumed that golden EDU segmentation has alhttp://jcip.cipsc.org.cn/CN/volumn/home.shtml 25 sources.]¯ u07 . Our modification is to move “that” 0 from u ¯6 to u ¯07 , because a relative pronoun should be with the clause it introduces, according to the EDU segmentation criteria of RST-DT. ready been conducted for the text. Formally, given a Chinese text tC = (u1 , u2 , ..., uk ) composed o"
W19-8104,W18-6302,0,0.0190149,"cy parsing trees are the same. It can be seen from the figure that the logical organization of a text is similar at the macro discourse level regardless of languages, in spite of lexical or grammatical differences. Based on this observation, we employ machine translation (MT) and English discourse parsing techniques to parse a Chinese text. Our proposed method is simple but feasible, because English discourse dependency parsing has made progress, especially in parsing discourse tree structures (Liu and Lapata, 2017; Kim et al., 2017), and Chineseto-English MT techniques are relatively mature (Nikolov et al., 2018; Hadiwinoto and Ng, 2018). Specifically, we first make use of MT techniques to translate a Chinese text into English and then adopt a transition-based English parser to analyze the translated text. Finally, we map this English parsing result to the Chinese text. During this process, some modifications are made to MT and the parsing result for performance improvement . Due to the absence of labeled data, discourse parsing still remains challenging in some languages. In this paper, we present a simple and efficient method to conduct zero-shot Chinese text-level dependency parsing by leveraging"
W19-8104,W01-1605,0,0.646848,"onding English translations to obtain the discourse trees for the Chinese text. This method can automatically conduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data. 1 Introduction Discourse parsing aims to analyze the inner structure of texts, which is fundamental to many natural language processing applications, such as question answering and summarization. The construction of discourse corpora has promoted the development of discourse parsing techniques. In English, the widely-used discourse corpora include the Rhetorical Structure Theory Treebank (RSTDT) (Carlson et al., 2001) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). Recently, Li et al. (2014a) and Yoshida et al. (2014) proposed the discourse dependency structure (DDS). DDS directly links the EDUs, so it has fewer nodes and simpler structures compared to RST and PDTB. In addition, it can easily represent non-projective structures, while hierarchical structures need other complex mechanisms to do so. DDS is especially important for Chinese. Kang et al. (2019) analyzes almost all the existing Chinese discourse treebanks and concludes that DDS is the future direction due to its right balance between e"
W19-8104,W03-3017,0,0.586389,"Missing"
W19-8104,W04-2407,0,0.456737,"Missing"
W19-8104,L18-1003,0,0.0212607,"he same. It can be seen from the figure that the logical organization of a text is similar at the macro discourse level regardless of languages, in spite of lexical or grammatical differences. Based on this observation, we employ machine translation (MT) and English discourse parsing techniques to parse a Chinese text. Our proposed method is simple but feasible, because English discourse dependency parsing has made progress, especially in parsing discourse tree structures (Liu and Lapata, 2017; Kim et al., 2017), and Chineseto-English MT techniques are relatively mature (Nikolov et al., 2018; Hadiwinoto and Ng, 2018). Specifically, we first make use of MT techniques to translate a Chinese text into English and then adopt a transition-based English parser to analyze the translated text. Finally, we map this English parsing result to the Chinese text. During this process, some modifications are made to MT and the parsing result for performance improvement . Due to the absence of labeled data, discourse parsing still remains challenging in some languages. In this paper, we present a simple and efficient method to conduct zero-shot Chinese text-level dependency parsing by leveraging English discourse labeled"
W19-8104,prasad-etal-2008-penn,0,0.0240366,"for the Chinese text. This method can automatically conduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data. 1 Introduction Discourse parsing aims to analyze the inner structure of texts, which is fundamental to many natural language processing applications, such as question answering and summarization. The construction of discourse corpora has promoted the development of discourse parsing techniques. In English, the widely-used discourse corpora include the Rhetorical Structure Theory Treebank (RSTDT) (Carlson et al., 2001) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). Recently, Li et al. (2014a) and Yoshida et al. (2014) proposed the discourse dependency structure (DDS). DDS directly links the EDUs, so it has fewer nodes and simpler structures compared to RST and PDTB. In addition, it can easily represent non-projective structures, while hierarchical structures need other complex mechanisms to do so. DDS is especially important for Chinese. Kang et al. (2019) analyzes almost all the existing Chinese discourse treebanks and concludes that DDS is the future direction due to its right balance between expressiveness and practicality. However, little research"
W19-8104,K16-2003,0,0.0478724,"Missing"
W19-8104,P17-2029,1,0.882056,"Missing"
W19-8104,P18-2071,1,0.924363,"our method exhibits promising performance. This corpus will be released soon. The experiment results demonstrates that our method is potentially helpful in building large-scale data for Chinese neural NLG systems that make use of discourse structure. To the best of our knowledge, we are the first to conduct discourse dependency parsing in Chinese. 2 Chinese Discourse Dependency Corpus Construction In this work, a small-scale Chinese discourse dependency treebank is constructed for evaluation. Here, we primarily follow the guideline of building the English discourse dependency treebank SciDTB (Yang and Li, 2018a) to explore the specifics of labeling DDS in Chinese. First, scientific abstracts are chosen as corpus sources, because they are short texts with obvious logic and within the same domain as the English treebank (SciDTB) (Yang and Li, 2018a). Specifically, 108 abstracts are selected from a Chinese NLP journal JCIP 1 . Second, we manually separate these abstracts into elementary discourse units (EDUs), the basic units of a parsing tree. Each segmented abstract is checked at least twice to ensure segmentation quality. Our EDU segmentation mainly refer to the criteria of RST-DT (Carlson and Marc"
W19-8104,D14-1196,0,0.0228954,"nduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data. 1 Introduction Discourse parsing aims to analyze the inner structure of texts, which is fundamental to many natural language processing applications, such as question answering and summarization. The construction of discourse corpora has promoted the development of discourse parsing techniques. In English, the widely-used discourse corpora include the Rhetorical Structure Theory Treebank (RSTDT) (Carlson et al., 2001) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). Recently, Li et al. (2014a) and Yoshida et al. (2014) proposed the discourse dependency structure (DDS). DDS directly links the EDUs, so it has fewer nodes and simpler structures compared to RST and PDTB. In addition, it can easily represent non-projective structures, while hierarchical structures need other complex mechanisms to do so. DDS is especially important for Chinese. Kang et al. (2019) analyzes almost all the existing Chinese discourse treebanks and concludes that DDS is the future direction due to its right balance between expressiveness and practicality. However, little research has been done on Chinese 24 Proceedings of the 1st Work"
W19-8104,P12-1008,0,0.0572234,"Missing"
W19-8104,miltsakaki-etal-2004-penn,0,\N,Missing
W19-8104,P14-1003,1,\N,Missing
W19-8104,Q18-1005,0,\N,Missing
Y03-1031,W96-0213,0,0.0501755,"Missing"
Y03-1031,W03-1713,1,\N,Missing
