2001.jeptalnrecital-poster.13,P98-1082,0,\N,Missing
2001.jeptalnrecital-poster.13,C98-1079,0,\N,Missing
2001.jeptalnrecital-poster.13,W99-0904,0,\N,Missing
2002.jeptalnrecital-long.3,J95-4004,0,0.0382364,"Missing"
2002.jeptalnrecital-long.3,W98-1504,0,0.0709291,"Missing"
2002.jeptalnrecital-long.3,A97-1016,0,0.0206001,"Missing"
2002.jeptalnrecital-long.3,J93-2006,0,0.0490421,"Missing"
2002.jeptalnrecital-long.3,W02-0304,1,0.778864,"Missing"
2003.jeptalnrecital-long.27,W99-0904,0,0.265217,"Missing"
2003.jeptalnrecital-long.27,2002.jeptalnrecital-long.22,0,0.0998581,"Missing"
2004.jeptalnrecital-long.16,C92-2082,0,0.0359804,"Missing"
2004.jeptalnrecital-long.16,muresan-klavans-2002-method,0,0.0453398,"Missing"
2005.jeptalnrecital-long.26,H91-1026,0,0.243455,"Missing"
2005.jeptalnrecital-long.26,knight-al-onaizan-1998-translation,0,0.0869807,"Missing"
2005.jeptalnrecital-long.26,J98-4003,0,0.102612,"Missing"
2005.jeptalnrecital-long.26,C04-1117,0,0.0297801,"Missing"
2005.jeptalnrecital-long.26,tsuji-etal-2002-extracting,0,0.0595132,"Missing"
2005.jeptalnrecital-long.5,N04-1007,0,0.0606433,"Missing"
2005.jeptalnrecital-long.5,voorhees-tice-2000-trec,0,0.0603205,"Missing"
2005.jeptalnrecital-long.9,P98-1082,0,0.078689,"Missing"
2005.jeptalnrecital-long.9,P99-1044,0,0.0496055,"Missing"
2005.jeptalnrecital-long.9,2005.jeptalnrecital-long.30,0,0.0564338,"Missing"
2005.jeptalnrecital-long.9,2003.jeptalnrecital-long.27,1,0.725279,"Missing"
2007.jeptalnrecital-long.7,2005.jeptalnrecital-long.7,1,0.717877,"Missing"
2010.jeptalnrecital-demonstration.12,C92-2082,0,0.111774,"Missing"
2011.jeptalnrecital-long.6,2010.jeptalnrecital-long.39,0,0.0910184,"Missing"
2011.jeptalnrecital-long.6,C10-2030,0,0.0255174,"Missing"
2011.jeptalnrecital-long.6,W08-0615,0,0.0669835,"Missing"
2011.jeptalnrecital-long.6,P09-3003,0,0.0664102,"Missing"
2013.mtsummit-papers.18,C02-2020,1,0.872177,"article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. proach and previous works addressing the task of bilingual lexicon extraction from comparable corpora. In Section 3, we present our context vector disambiguation process. Before concluding in section 5, we describe the experimental protocol we followed and discuss the obtained results in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors ar"
2013.mtsummit-papers.18,P04-1067,0,0.0791038,"Missing"
2013.mtsummit-papers.18,P95-1050,0,0.0670506,"esults on four specialized French-English comparable corpora show that our method outperforms two state-of-the-art approaches. Introduction Bilingual lexicons play an important role in many natural language processing applications such as machine translation or cross-language information retrieval (Shi, 2009). Research on lexical extraction from multilingual corpora have largely focused on parallel corpora. The scarcity of such corpora in particular for specialized domains and for language pairs not involving English pushed researchers to investigate the use of comparable corpora (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003), in which texts are not exact translation of each other but share common features. The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based xcas pz@limsi.fr nasredine.semmar@cea.fr Abstract 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France on the characterization and comparison of lexical environments represented by context"
2013.mtsummit-papers.18,hazem-morin-2012-adaptive,0,0.0600701,"ng to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. Hazem and Morin (2012) filtered the entries of the bilingual dictionary on the basis of partof-speech tags and of domain relevance criteria but no improvement was demonstrated. Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. However, only small improvements are reported. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we con"
2013.mtsummit-papers.18,P99-1067,0,0.0539965,"of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dic"
2013.mtsummit-papers.18,C10-1070,0,0.0814639,"Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. proach and previous works addressing the task of bilingual lexicon extraction from comparable corpora. In Section 3, we present our context vector disambiguation process. Before concluding in section 5, we describe the experimental protocol we followed and discuss the obtained results in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target la"
2013.mtsummit-papers.18,W11-1205,0,0.1434,"lts in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is t"
2013.mtsummit-papers.18,2009.mtsummit-posters.14,0,0.0130285,"nslation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity pr"
2015.jeptalnrecital-court.40,moriceau-tannier-2014-french,1,0.880988,"Missing"
2015.jeptalnrecital-long.3,P10-1052,0,0.066151,"Missing"
2015.jeptalnrecital-long.3,moriceau-tannier-2014-french,1,0.877846,"Missing"
2015.jeptalnrecital-long.3,S10-1071,0,0.0564766,"Missing"
2015.jeptalnrecital-long.3,P05-3021,0,0.110123,"Missing"
2016.jeptalnrecital-poster.20,boulaknadel-etal-2008-multi,0,0.045874,"Missing"
2016.jeptalnrecital-poster.20,E93-1011,0,0.603963,"Missing"
2016.jeptalnrecital-poster.20,W03-1802,0,0.0639815,"Missing"
2016.jeptalnrecital-poster.20,C00-1077,0,0.201914,"Missing"
2016.jeptalnrecital-poster.20,W14-4807,0,0.0541764,"Missing"
2016.jeptalnrecital-poster.7,D14-1187,0,0.0468592,"Missing"
2017.jeptalnrecital-court.28,W16-5107,1,0.88332,"Missing"
2017.jeptalnrecital-court.28,W16-6113,1,0.885124,"Missing"
2017.jeptalnrecital-court.29,E12-1058,0,0.0609025,"Missing"
2018.jeptalnrecital-court.27,P02-1051,0,0.139816,"Missing"
2018.jeptalnrecital-court.27,D11-1006,0,0.03472,"ogies établies en anglais ou en français, alors considérées comme des terminologies de référence (Lelubre, 2008). Ce constat n’est toutefois pas valable pour tous les domaines. Ainsi, certains termes médicaux anglais sont issus de l’arabe (Wulff, 2004). Partant de ce constat mais aussi d’observations réalisées en corpus, nous avons choisi d’exploiter la présence de termes anglais dans les terminologies et les textes de spécialité rédigés en arabe. La méthode que nous proposons vise à exploiter la translittération des termes anglais en caractères arabes et le principe du transfert translingue (McDonald et al., 2011). À notre connaissance, il n’existe pas de systèmes de translittération de l’anglais vers l’arabe. L’application de la méthode sur un corpus parallèle anglais-arabe aligné au niveau des mots permet d’extraire des termes arabes après reconnaissance ou extraction des termes anglais. Afin d’évaluer l’apport de la translittération de termes anglais en caractères arabes sur la construction d’une terminologie bilingue à partir d’un corpus parallèle anglais-arabe de textes médicaux, nous présentons dans cet article un système de translittération qui permet d’extraire des couples de termes médicaux tr"
2018.jeptalnrecital-court.27,L16-1054,0,0.0227641,"Missing"
2018.jeptalnrecital-court.27,J03-1002,0,0.0113783,"aitements automatiques, pose des nombreux problèmes (Habash, 2010) que nous avons tenté de résoudre mais qui rendent la tâche de nettoyage très coûteuse en temps : erreur de forme des caractères, utilisation d’un caractère persan ressemblant graphiquement à un caractère arabe, etc. L’ensemble des documents du corpus est ensuite segmenté par MADAMIRA (Pasha et al., 2014) et aligné au niveau des paragraphes et au niveau des phrases. La qualité de l’alignement a été vérifiée manuellement. 3.2 Alignement au niveau des mots Un alignement au niveau des mots a été obtenu en utilisant l’outil GIZA++ (Och & Ney, 2003). Nous avons défini trois alignements entre le corpus anglais et le corpus arabe qui prennent en compte diffé1 http://www.nlm.nih.gov/medlineplus/languages/all_healthtopics.html rentes informations morphologiques fournies par MADAMIRA sur le corpus arabe : (Alignement1) l’alignement est réalisé sans traitement morphologique particulier ; il s’agit de notre base de comparaison ; (Alignement2) avant l’alignement, les enclitiques et proclitiques sont désagglutinés du mot arabe auquel ils se rapportent ; (Alignement3) avant alignement, les enclitiques, les proclitiques et les articles sont désaggl"
2018.jeptalnrecital-court.27,pasha-etal-2014-madamira,0,0.0171699,"on, nous avons ajouté 62 documents, soit 25 379 mots pour le corpus anglais et 21 950 mots pour le corpus arabe. Contrairement aux documents en anglais et en français, la conversion au format texte des documents en arabe, en vue de réaliser des traitements automatiques, pose des nombreux problèmes (Habash, 2010) que nous avons tenté de résoudre mais qui rendent la tâche de nettoyage très coûteuse en temps : erreur de forme des caractères, utilisation d’un caractère persan ressemblant graphiquement à un caractère arabe, etc. L’ensemble des documents du corpus est ensuite segmenté par MADAMIRA (Pasha et al., 2014) et aligné au niveau des paragraphes et au niveau des phrases. La qualité de l’alignement a été vérifiée manuellement. 3.2 Alignement au niveau des mots Un alignement au niveau des mots a été obtenu en utilisant l’outil GIZA++ (Och & Ney, 2003). Nous avons défini trois alignements entre le corpus anglais et le corpus arabe qui prennent en compte diffé1 http://www.nlm.nih.gov/medlineplus/languages/all_healthtopics.html rentes informations morphologiques fournies par MADAMIRA sur le corpus arabe : (Alignement1) l’alignement est réalisé sans traitement morphologique particulier ; il s’agit de not"
2018.jeptalnrecital-court.27,samy-etal-2012-medical,0,0.0236681,"Missing"
2018.jeptalnrecital-court.27,P07-1109,0,0.0453716,"Missing"
2020.bucc-1.2,2020.bucc-1.11,0,0.0293513,"Missing"
2020.bucc-1.2,W15-3411,1,0.812659,"r of 2 billion, Russian about 3 billion running words (Sharoff et al., 2017). The compressed sizes of the Wikipedia corpora are: English: 3.6 GB, Spanish: 0.9 GB, Chinese: 0.4 GB. They are in a one-line per document format. The first tabseparated field in each line contains metadata, the second field contains the text. Paragraph boundaries are marked with HTML tags. As cleaning up the original Wikipedia dump files is not trivial, occasionally there can be some noise in the form of not fully cleaned HTML and Javascript fragments. Details of the cleanup and preparation procedure can be found in Sharoff et al. (2015). Expressions of interest to participate in the shared task Release of shared task training sets Release of shared task test sets Submission of shared task results 3.2 Table 3: Time schedule. 2 ukWaC deWaC esWiki deWiki frWaC deWaC en deWaC ukWaC Table 4: Language pairs supported and corpora (WaCky or Wikipedia) to be used in the closed track. Table 2: Checklist for participants (abbreviated). Any time Corpora Table 4 lists the corpora to be used for the language pairs supported in the closed track. Due to their free availability for several languages and their size, for the shared task we use"
2020.bucc-1.2,Q17-1010,0,0.151641,"Missing"
2020.bucc-1.2,J17-2001,0,0.0752755,"ir competition between systems. This was accomplished by providing corpora and bilingual datasets for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, and by comparing the results using a common evaluation framework. For the shared task we provided corpora as well as training and test data. However, as we anticipated that these corpora and datasets may not suit all needs, we divided the shared task into two tracks: Quite a few research groups have been working on this problem using a wide variety of approaches. There are comprehensive studies such as Irvine & Callison-Burch (2017) and also overview papers at least in part discussing the topic like Jakubina & Langlais (2016), Rapp et al. (2016), Sharoff et al. (2013). 1 Target (French) bébé poupon bain lit plumard commodité médecin docteur aigle montagne nerveux travail  https://comparable.limsi.fr/bucc2020/bucc2020-task.html 6 In the closed track, participants were required to only use the data provided by the organizers. In this way equal conditions were ensured and, as the outcome of 3. this track, the systems could be compared and ranked according to the quality of their results. In the open track, participants wer"
2020.bucc-1.2,2020.bucc-1.9,0,0.0902575,"Missing"
2020.bucc-1.2,W05-0809,0,0.0808028,"llel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are available in much larger quantities than parallel corpora, this approach might help in relieving the data acquisition bottleneck which tends to be especially severe when dealing with language pairs involving low resource languages (see e.g. Martin et al., 2005). Table 1: Sample word translations from English to French. In the shared task a similar tab-separated format was used. However, as up to now there was no standard way to measure the performance of the systems, the published results are not comparable and the pros and cons of the various approaches are not clear. A well-established practical task to approach this topic is bilingual lexicon extraction from comparable corpora, which is in the focus of this shared task. Typically, its aim is to extract word translations such as exemplified in Table 1 from comparable corpora, where a given source"
2020.bucc-1.2,W03-0301,0,0.111131,"r ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition is the results of a number of systems which provide surprisingly good solutions to an ambitious problem. Keywords: bilingual dictionary, lexicon induction, comparable corpora 1. Introduction Source (English) baby baby bath bed bed convenience doctor doctor eagle mountain nervous work In the framework of machine translation, the extraction of bilingual dictionaries from parallel corpora has been conducted very successfully (see e.g. Mihalcea & Pedersen, 2003). But on the other hand, human second language acquisition appears not to be based on parallel data. This means that there must be a way of acquiring and relating lexical knowledge across two or more languages without the use of parallel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are availabl"
2020.coling-main.609,N19-1423,0,0.04791,"ractice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growi"
2020.coling-main.609,P19-1266,0,0.031135,"Missing"
2020.coling-main.609,P19-2041,1,0.894488,"Missing"
2020.coling-main.609,P16-1100,0,0.0205295,"eptually simpler word-level models. This new variant does not rely on wordpieces but instead consults the characters of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly"
2020.coling-main.609,W19-5006,0,0.0200639,"between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the growing complexity of recent representation models, the default trend seems to favor re-training general-domain models on specialized corpora rather than building models from scratch with a specialized vocabulary (e.g., BlueBERT (Peng et al., 2019) and BioBERT (Lee et al., 2020)). While these methods undeniably produce good models 2 , a few questions remain: How suitable are the predefined general-domain vocabularies when used in the context of specialized domains (e.g., the medical domain)? Is it better to train specialized models with specialized subword units? Do we induce any biases by training specialized models with general-domain wordpieces? In this paper, we propose CharacterBERT, a possible solution for avoiding any biases that may come from the use of a predefined wordpiece vocabulary, and an effort to revert back to conceptua"
2020.coling-main.609,N18-1202,0,0.048799,"ed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly more robust to noise and misspellings. To the best of our knowledge, this is the first work that replaces BERT’s wordpiece system with"
2020.coling-main.609,P19-1561,0,0.0612103,"Missing"
2020.coling-main.609,D18-1187,0,0.0534015,"Missing"
2020.coling-main.609,P19-1139,0,0.0221757,"present entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the"
2020.eamt-1.57,N19-1423,0,0.00800429,"of the project, a connection to eTranslation,8 an online machine translation service provided by the European Commission, will be established to foster the provision of multilingual datasets by public administrations that may in turn improve the coverage and quality of machine translation systems. 2 Approach At its core, the MAPA anonymisation toolkit will rely on Named Entity Recognition and Classification (NERC) techniques using neural networks and deep learning techniques. The latest deep learning architectures and the availability of pre-trained multilingual language models, such as BERT (Devlin et al., 2019) have pushed the state of the art in NERC to new levels of performance. In addition, thanks to the transfer learning capabilities shown by this type of deep learning models, new systems can be trained using smaller datasets of manually labelled data, and the knowledge acquired for a given domain or language can be reused in a cross-domain or cross-language setting (Garc´ıa-Pablos et al., 2020). MAPA will leverage the most innovative technology to provide robust models for the 24 official European languages, trained to detect named entities that involve sensitive information, depending on the a"
2020.eamt-1.57,2020.lrec-1.552,1,0.874506,"Missing"
2020.latechclfl-1.20,J93-1004,0,0.7754,"n Dashboard is designed as an adjunctive tool for researchers in translation studies grounded in the comparative literature tradition. It provides a reading environment that could display the visualizations and text in parallel in order to allow scholars to easily see patterns of structural divergence between the source text and translations at different levels of granularity. Text is aligned at the paragraph, sentence, and world level using Natural Language Processing algorithms, including the IBM Models 1 and 2 for Statistical Machine Translation(Collins, 2011) and the Gale-Church Algorithm(Gale and Church, 1993). 3.2.1 Paragraph Count Analysis After selecting a specific node on the World Map, the default view of the Translation Dashboard displays a per-chapter paragraph count, based on newlines and white space in that source text. The deviation in paragraph count between a source text and its translation is reflected in the color variation in the Heat Map within the table (see Figure 2, center image). An exceedingly high divergence from the source paragraph count alerts the scholar that there may be data cleaning issues (e.g. one instance where each line in a poem embedded in a narrative was treated"
2020.latechclfl-1.20,gilmanov-etal-2014-swift,0,0.0409937,"Missing"
2020.latechclfl-1.20,smith-jahr-2000-cairo,0,0.303029,"PENCIL (Kakoyianni-Doa et al., 2013) and The Sketch Engine (Kilgarriff et al., 2014), but there remain very little designed specifically for translated literature. While the interpretive nature of literary translations has caused a lag in their adoption as a source for NLP development, multiple recent projects have developed parallel corpora based on well-known texts including the Harry Potter series and Le Petit Prince. 2.3 Alignment Visualization Tools While there already exist alignment visualization tools such as ANNIS (Druskat et al., 2016), SWIFT Aligner (Gilmanov et al., 2014), Cario (Smith and Jahr, 2000), VisualTCA (Gomes et al., 2007) and MkAlign (Fleury and Zimina, 2007), most of them focus on word alignment. Further, even though some of these tools provide sentence alignment visualization, they are meant to be an intermediate step before the lexicon level. There are currently no other tools that allow users to explore data in a chapterparagraph-sentence/word, coarse-to-fine fashion. Moreover, these tools are not oriented towards literary texts, which is more challenging for alignment approaches. Though alignment should be as confident as possible (Xu et al., 2015), this is complicated by t"
2020.lrec-1.241,W16-2922,0,0.106392,"n of rule-based and entity-form-based methods is that they require entity mentions to present some similarity with concept labels to be efficient. For instance, rule-based methods cannot assign the mention “T-cell” to a concept labelled “lymphocyte”, unless a similar form of the term “T-cell” is added to the concept 1959 no manually annotated corpus, based on a weak supervision strategy. between distant TF-IDF bag-of-words representations of mentions and associated concept labels. Moreover, embedding based methods vary depending on the representation of examples and algorithm hyperparameters (Chiu et al., 2016). We propose an experimental setting with the aim of highlighting the influence of a wide range of different factors (corpus selection, pre-processing, word embedding hyperparameters) and finding the optimal configuration for adapting a generic normalization method to a specific task. Pre-processing still receives too little focus when evaluating embedding-based systems (CamachoCollados and Pilehvar, 2018), in favour of hyperparameter study or the use of general precomputed embeddings. Some advances in NLP have been made through word embeddings as built by Word2Vec (Mikolov et al., 2013), GloV"
2020.lrec-1.241,W13-2027,0,0.0277062,"concepts. The learning optimization goal is to minimize globally the Euclidean distance between each projected mention vector and its concept vector(s) (see Figure 1). To handle domain-specific ambiguities, which notably increase with strategies such as third-party resources and inflection generation, and simultaneously to specialize for the task domain, some methods use a hand-crafted blacklist: for instance, the ToMap method (Golik et al., 2011) has a version adapted to the BB3 task, and the Peregrine method (Schuemie et al., 2007) has a version adapted to BioCreative II. Another method by Claveau (2013) weights tokens with an Information Retrieval measure, which aims to automatically mitigate the weight of ambiguous words. CONTES then uses the learned parameters to project any mention vector onto the ontological space. It computes a cosine similarity between the projected vector and every concept vector. CONTES finally selects the concept with the most similar vector as the prediction for normalization (see Figure 2). Other methods use vector representations to compute a similarity measure between text mentions and concept labels. Notably, Tiftikci et al. (2016) and Mehryary et al. (2017) es"
2020.lrec-1.241,L18-1543,1,0.859935,"in these domains (Wei et al., 2015; Roberts et al., 2017; Deléger et al., 2016). Entity Normalization methods handle the problem as a classification problem. They are based either on pattern-matching rules (Aronson, 2001) or on Machine Learning (ML) algorithms (Leaman et al., 2013). Both are able to operate on the surface form of entities (i.e. their sequence of characters), on NLP analyses (lemmatization, POStagging, syntactic parsing) (Aronson, 2001), or on distributional semantic representations such as word embeddings (Limsopatham and Collier, 2016). CONTES (Ferré et al., 2017) and HONOR (Ferré et al., 2018) are two recent methods that address training data paucity by exploiting ontological subsumption information (is_a relation between concepts or categories). CONTES uses the subsumption graph of the ontology together with word embeddings. HONOR combines CONTES and the rule-based method ToMap (Golik et al., 2011). HONOR achieves state of the art performance on the Bacteria Biotope normalization task of BioNLP Shared Task 2016 (BB3) (Deléger et al., 2016). Nevertheless, both of these methods still need an annotated corpus which provides ground truth training examples. In this paper we present nov"
2020.lrec-1.241,W17-2312,1,0.907499,"rk datasets have been proposed in these domains (Wei et al., 2015; Roberts et al., 2017; Deléger et al., 2016). Entity Normalization methods handle the problem as a classification problem. They are based either on pattern-matching rules (Aronson, 2001) or on Machine Learning (ML) algorithms (Leaman et al., 2013). Both are able to operate on the surface form of entities (i.e. their sequence of characters), on NLP analyses (lemmatization, POStagging, syntactic parsing) (Aronson, 2001), or on distributional semantic representations such as word embeddings (Limsopatham and Collier, 2016). CONTES (Ferré et al., 2017) and HONOR (Ferré et al., 2018) are two recent methods that address training data paucity by exploiting ontological subsumption information (is_a relation between concepts or categories). CONTES uses the subsumption graph of the ontology together with word embeddings. HONOR combines CONTES and the rule-based method ToMap (Golik et al., 2011). HONOR achieves state of the art performance on the Bacteria Biotope normalization task of BioNLP Shared Task 2016 (BB3) (Deléger et al., 2016). Nevertheless, both of these methods still need an annotated corpus which provides ground truth training example"
2020.lrec-1.241,W16-3008,0,0.0290658,"es to a taxonomy, and bacterial habitats to one or more of the 2,320 concepts of the dedicated ontology OntoBiotope 1. Each concept can also contain some synonyms of the label (0.2 synonyms on average), which gives a total of 2,739 terms associated to concepts of the ontology. The normalization of taxonomic mentions of bacteria is not much of a challenge for the BioNLP community because the nomenclature is complete, variations are relatively standardized, and synonymy is rare (except in some special cases such as strain names). Thus string matching with basic variations yields decent results (Grouin, 2016). Habitat mentions are subject to much more variations, and, due to the microscopic nature of bacteria, any object or place can be construed as a habitat. The normalization of habitat mentions is thus a challenging task that has generated many studies. For stemming, we tested an implementation of the Snowball algorithm (Porter, 1980), and for lemmatization we used GeniaTagger2, a state-of-the-art lemmatizer and POS-tagger for the biomedical domain. For stopword filtering, we removed grammatical words (determiners, prepositions, conjunctions, “to”), and punctuations. For masking, we replaced ea"
2020.lrec-1.241,W17-2310,0,0.100172,"method by Claveau (2013) weights tokens with an Information Retrieval measure, which aims to automatically mitigate the weight of ambiguous words. CONTES then uses the learned parameters to project any mention vector onto the ontological space. It computes a cosine similarity between the projected vector and every concept vector. CONTES finally selects the concept with the most similar vector as the prediction for normalization (see Figure 2). Other methods use vector representations to compute a similarity measure between text mentions and concept labels. Notably, Tiftikci et al. (2016) and Mehryary et al. (2017) estimate the semantic similarity between two expressions by computing a cosine similarity between TF-IDF bag-of-words representations (Manning et al., 2009). These representations are based on word forms, and fail to link mentions that do not share any common token with the correct concept label. To address this limitation, the ML-based DNorm method (Leaman et al., 2013) learns a function that estimates high similarities The vector space of concepts includes vectors computed with ontological information rather than one-hot vectors (where all weights are set to zero except the weight associate"
2020.lrec-1.241,D14-1162,0,0.0929695,"e propose an experimental setting with the aim of highlighting the influence of a wide range of different factors (corpus selection, pre-processing, word embedding hyperparameters) and finding the optimal configuration for adapting a generic normalization method to a specific task. Pre-processing still receives too little focus when evaluating embedding-based systems (CamachoCollados and Pilehvar, 2018), in favour of hyperparameter study or the use of general precomputed embeddings. Some advances in NLP have been made through word embeddings as built by Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2016) or more recently ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018), as a way to compute and represent the meaning of words from the contexts in which they are observed. Word embeddings are vectors with the advantage of a smaller number of dimensions. However, their acquisition requires large amounts of untagged corpora. To favour their mapping, text mentions and labels can be represented by embeddings in the same space. So, a first approach to normalize a mention embedding is to find the nearest concept label embedding. For instance, the BOUNEL metho"
2020.lrec-1.241,N18-1202,0,0.0117092,"de range of different factors (corpus selection, pre-processing, word embedding hyperparameters) and finding the optimal configuration for adapting a generic normalization method to a specific task. Pre-processing still receives too little focus when evaluating embedding-based systems (CamachoCollados and Pilehvar, 2018), in favour of hyperparameter study or the use of general precomputed embeddings. Some advances in NLP have been made through word embeddings as built by Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2016) or more recently ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018), as a way to compute and represent the meaning of words from the contexts in which they are observed. Word embeddings are vectors with the advantage of a smaller number of dimensions. However, their acquisition requires large amounts of untagged corpora. To favour their mapping, text mentions and labels can be represented by embeddings in the same space. So, a first approach to normalize a mention embedding is to find the nearest concept label embedding. For instance, the BOUNEL method (Karadeniz and Özgür, 2019) computes a cosine similarity between these embeddi"
2020.lrec-1.241,W16-4208,0,0.0493268,"BioNLP Shared Task 2016 (BB3). This task illustrates the challenge that we aim to address: it is a domain-specific normalization task, wellrecognized in the BioNLP community, and with a small amount of training data available compared to the number of concepts of the task. 2. 2.1 Related Work The results of word embedding-based methods significantly depend on the choice of a large unannotated corpus, on the chosen hyper-parameters (Chiu et al., 2016), and on parameter initialization. Moreover, through specialized corpora, domain specialized embeddings can increase the performance of methods (Roberts, 2016). This specialization can be emphasized by exploiting external knowledge (Faruqui et al., 2014; De Vine et al., 2014; Celikyilmaz, 2015), such as that contained in ontologies (Ferré et al., 2017; Yen et al., 2018). A Brief History of Entity Normalization Methods in Scientific Domains Most normalization methods in technical and scientific domains rely on the similarity between entity forms and concept labels. Due to frequent linguistic variations (e.g. noun-phrase inversion, typographic variations, synonymy), these methods are dependent on comprehensive lexicons. Several strategies are used to"
2020.lrec-1.241,W16-3007,0,0.124996,"to BioCreative II. Another method by Claveau (2013) weights tokens with an Information Retrieval measure, which aims to automatically mitigate the weight of ambiguous words. CONTES then uses the learned parameters to project any mention vector onto the ontological space. It computes a cosine similarity between the projected vector and every concept vector. CONTES finally selects the concept with the most similar vector as the prediction for normalization (see Figure 2). Other methods use vector representations to compute a similarity measure between text mentions and concept labels. Notably, Tiftikci et al. (2016) and Mehryary et al. (2017) estimate the semantic similarity between two expressions by computing a cosine similarity between TF-IDF bag-of-words representations (Manning et al., 2009). These representations are based on word forms, and fail to link mentions that do not share any common token with the correct concept label. To address this limitation, the ML-based DNorm method (Leaman et al., 2013) learns a function that estimates high similarities The vector space of concepts includes vectors computed with ontological information rather than one-hot vectors (where all weights are set to zero"
2021.eval4nlp-1.1,Q17-1010,0,0.00626055,"token is associated to one true label and named entities are encoded according to the BIO (begin, inside, outside) scheme. In the present work we deal with tokens rather than entities, so that we can apply the presented method directly. We consider that ‘O’ labels are negatives and that all other labels are positives. A true positive system prediction is an association between an input token and a non-‘O’ label that is the gold-standard label for this token. We are comparing entity detection systems that rely on word embeddings based upon CharacterBert (El Boukkouri et al., 2020) or fastText (Bojanowski et al., 2017), pre-trained on different corpora, either as-is or concatenated with knowledge embeddings learned using node2vec (Grover and Leskovec, 2016) on two biomedical vocabularies (the Medical Suject Headings (MeSH), and SNOMED CT). Moreover, we also consider a variant of CharacterBert where the node2vec embeddings are injected within the model architecture. The fastText embeddings are either randomly initialized, which we note “fastTextRandom”; pre-trained on a newswire corpus (Gigaword (Graff et al., 2007)), which we note “fastTextGigaword”; or on medical corpora (PubMed Central3 and MIMIC-III (Joh"
2021.eval4nlp-1.1,2020.coling-main.609,1,0.733787,"Missing"
2021.eval4nlp-1.1,2020.emnlp-main.393,0,0.03534,"sign choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to provide a finer understanding of the issues raised by the input text and of the limitations of the evaluated systems, we propose a new qualitative analysis method that takes into account the observed relative difficulty of predicting gold labels for each input. This difficulty is assessed pragmatically bas"
2021.eval4nlp-1.1,C96-1079,0,0.860475,"ipating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset). 1 Introduction The analysis of NLP system results has mainly focused on evaluation scores meant to rank systems and feed leaderboards. In tasks such as information extraction, text classification, etc., evaluation generally relies on the comparison of a hypothesis (typically a system output) with a gold standard, generally produced through manual annotation. Since the MUC-6 conference (Grishman and Sundheim, 1996), the metrics used were created for information retrieval (Cleverdon, 1960): recall (true positive rate), precision (positive predictive value) and their harmonic (possibly weighted) mean, the F1score. Evaluation scripts are widely available nowadays, for instance those of the CoNLL shared tasks (Tjong Kim Sang and De Meulder, 2003). These scripts rely on an annotation scheme based on the 1 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 1–10 c November 10, 2021. 2021 Association for Computational Linguistics Figure 1: Example input file for a"
2021.eval4nlp-1.1,W16-2703,0,0.017816,"r outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as pr"
2021.eval4nlp-1.1,W04-1013,0,0.0384163,"o Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scor"
2021.eval4nlp-1.1,P02-1040,0,0.112402,"on: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrate"
2021.eval4nlp-1.1,2021.acl-long.179,0,0.0278283,"machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to"
bouamor-etal-2012-identifying,W10-4006,0,\N,Missing
bouamor-etal-2012-identifying,A94-1006,0,\N,Missing
bouamor-etal-2012-identifying,P02-1040,0,\N,Missing
bouamor-etal-2012-identifying,P93-1003,0,\N,Missing
bouamor-etal-2012-identifying,W09-2907,0,\N,Missing
bouamor-etal-2012-identifying,W11-0805,0,\N,Missing
bouamor-etal-2012-identifying,N03-1017,0,\N,Missing
bouamor-etal-2012-identifying,2005.mtsummit-papers.11,0,\N,Missing
bouamor-etal-2012-identifying,2005.mtsummit-posters.11,0,\N,Missing
bouamor-etal-2012-identifying,besancon-etal-2010-lima,1,\N,Missing
bouamor-etal-2012-identifying,vintar-fiser-2008-harvesting,0,\N,Missing
bouamor-etal-2012-identifying,2007.jeptalnrecital-long.37,0,\N,Missing
bouamor-etal-2012-identifying,boulaknadel-etal-2008-multi,0,\N,Missing
bouamor-etal-2012-identifying,2006.amta-papers.11,0,\N,Missing
bouamor-etal-2012-identifying,W11-2107,0,\N,Missing
C02-2020,P98-1069,0,0.70485,"1998). A limiting factor in these experiments was an expensive investment of human effort for collecting large-size parallel corpora, although Chen and Nie (2000)’s experiments show a potential solution by automatically collecting parallel Web pages. Comparable corpora are “texts which, though composed independently in the respective language communities, have the same communicative function” (Laffling, 1992). Such non-parallel texts can become prevalent in the development of bilingual lexicons and in cross-language information research as they may be easier to collect than parallel corpora (Fung and Yee, 1998; Rapp, 1999; Picchi and Peters, 1998). Among these, Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Fung and Yee (1998) demonstrated that the associations between a word and its context seed words are preserved in comparable texts of different languages. By designing procedures to retrieve crosslingual lexical equivalents together, Picchi and Peters (1998) proposed that their system could have applications such as retrieving documents containing terms or contexts which are semantically equivalent in more"
C02-2020,1998.amta-tutorials.5,0,0.0380923,"Missing"
C02-2020,P99-1067,0,0.762345,"actor in these experiments was an expensive investment of human effort for collecting large-size parallel corpora, although Chen and Nie (2000)’s experiments show a potential solution by automatically collecting parallel Web pages. Comparable corpora are “texts which, though composed independently in the respective language communities, have the same communicative function” (Laffling, 1992). Such non-parallel texts can become prevalent in the development of bilingual lexicons and in cross-language information research as they may be easier to collect than parallel corpora (Fung and Yee, 1998; Rapp, 1999; Picchi and Peters, 1998). Among these, Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Fung and Yee (1998) demonstrated that the associations between a word and its context seed words are preserved in comparable texts of different languages. By designing procedures to retrieve crosslingual lexical equivalents together, Picchi and Peters (1998) proposed that their system could have applications such as retrieving documents containing terms or contexts which are semantically equivalent in more than one la"
C02-2020,C69-0401,0,0.610892,"he present work addresses this issue in a specialized domain: medicine. We aim at identifying French-English translation candidates from comparable medical corpora, extending an existing specialized bilingual lexicon. These translational equivalents may then be used, e.g., for query expansion and translation. We first recall previous work on this topic, then present the corpora and initial bilingual lexicon we start with, and the method we use to build, transfer and compare context vectors. We finally provide and discuss experimental results on a test set of French medical words. 2 Background Salton (1970) first demonstrated that with carefully constructed thesauri, cross-language retrieval can perform as well as monolingual retrieval. In many experiments, parallel corpora have been used for training statistical models for bilingual lexicon compilation and disambiguation of query translation (Hiemstra et al., 1997; Littman et al., 1998). A limiting factor in these experiments was an expensive investment of human effort for collecting large-size parallel corpora, although Chen and Nie (2000)’s experiments show a potential solution by automatically collecting parallel Web pages. Comparable corpor"
C02-2020,C98-1066,0,\N,Missing
C12-2079,J08-4004,0,0.650981,"us (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state"
C12-2079,J11-4004,0,0.0379275,"e obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number of categories, the lower the inter-annotator agreement. However, categories prone to confusion are in limited number. The meta-analysis presented by Bayerl and Paul (2011) extends this research on the factors influencing agreement results, identifying 8 such factors and proposing useful recommendations to improve manual annotation reliability. However, neither of these studies provides a clear picture of the behavior of the agreement coefficients 0 This work has been partially financed by OSEO, the French State Agency for Innovation, under the Quaero program. 810 or of their meanings. The experiments detailed in (Reidsma and Carletta, 2008) constitute an interesting step in this direction, focusing on the effect of annotation errors on machine learning systems"
C12-2079,fort-etal-2012-analyzing,1,0.700025,"present the pros and cons of these methods, from the statistical and mathematical points of view, with some hints about specific issues raised in some annotation campaigns, like the prevalence of one category. A section of their article is dedicated to various attempts at providing an interpretation scale for the Kappa family coefficients and how they failed to converge. Works such as (Gwet, 2012) are also to be mentioned. They present various inter-rater reliability coefficients and insist on benchmarking issues related to their interpretation. Many authors, among whom (Grouin et al., 2011; Fort et al., 2012), tried to obtain a more precise assessment of the quality of the annotation in their campaigns by computing different coefficients and analyzing the obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number o"
C12-2079,W11-0411,1,0.934731,"nterannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state of the art (Section 2), then detail the principles of our method to benchmark measures (Section 3) and show on some examples how different coefficients can be compared (Section 4). We finally discuss current limitations and point out future developments. 2 State of the art A quite detailed analysis of the most c"
C12-2079,J02-1002,0,0.272818,"on the contrary, a reference element is missing (false negative). All of these error paradigms tend to damage the annotations, so each of them should be taken into account by agreement measures. We propose here to apply each measure to a set of corpora, each of which embeds errors from one or more paradigms, and with a certain magnitude (the higher the magnitude, the higher the number of errors). This experiment should allow us to observe how the measures behave w.r.t. the different paradigms, and with a full range of magnitudes. The idea of creating artificial damaged corpora is inspired by Pevzner and Hearst (2002), then Bestgen (2009) in thematic segmentation, but our goal (giving meaning to measures) and our method (e.g. applying progressive magnitudes) are very different. 3.2 Protocol Reference. A reference annotation set (called reference) is provided to the system: a true Gold Standard or an automatically generated set based on a statistical model. It is assumed to correspond exactly to what annotations should be, with respect to the annotation guidelines. Shuffling. A shuffling process is an algorithm that automatically generates a multi-annotated corpus given three parameters: a reference annotat"
C12-2079,J08-3001,0,0.379365,", and the obtained results are used to model the behavior of these measures and understand their actual meaning. KEYWORDS: inter-annotator agreement, manual corpus annotation, evaluation. Proceedings of COLING 2012: Posters, pages 809–818, COLING 2012, Mumbai, December 2012. 809 1 Introduction The quality of manual annotations has a direct impact on the applications using them. For example, it was demonstrated that machine learning tools learn to make the same mistakes as the human annotators, if these mistakes follow a certain regular pattern and do not correspond to simple annotation noise (Reidsma and Carletta, 2008; Schluter, 2011). Furthermore, errors in a manually annotated reference corpus (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can"
C96-1025,E93-1021,0,0.320433,"Missing"
C96-1025,1995.mtsummit-1.1,0,0.0252783,"Missing"
C96-1025,C94-2168,0,0.0663985,"Missing"
C96-1025,C88-1036,0,\N,Missing
chatzimina-etal-2014-use,N04-1043,0,\N,Missing
chatzimina-etal-2014-use,J92-4003,0,\N,Missing
chatzimina-etal-2014-use,P08-2026,0,\N,Missing
chatzimina-etal-2014-use,P06-1043,0,\N,Missing
chatzimina-etal-2014-use,P06-1055,0,\N,Missing
chatzimina-etal-2014-use,N13-1090,0,\N,Missing
chatzimina-etal-2014-use,P10-1040,0,\N,Missing
chatzimina-etal-2014-use,P10-1052,0,\N,Missing
D13-1046,C02-2020,1,0.915827,"Missing"
D13-1046,hazem-morin-2012-adaptive,0,0.0216078,"Missing"
D13-1046,C10-2055,0,0.354564,"Missing"
D13-1046,C10-1070,0,0.153346,"Missing"
D13-1046,W11-1205,0,0.0354002,"Missing"
D13-1046,J03-1002,0,0.00653281,"Missing"
D13-1046,2009.mtsummit-posters.14,0,0.445836,"Missing"
D13-1046,P95-1050,0,0.318963,"Missing"
D13-1046,P99-1067,0,0.574046,"Missing"
deleger-etal-2014-annotation,W12-4304,0,\N,Missing
deleger-etal-2014-annotation,W12-2411,0,\N,Missing
deleger-etal-2014-annotation,E12-2021,0,\N,Missing
deleger-zweigenbaum-2010-identifying,I05-1011,0,\N,Missing
deleger-zweigenbaum-2010-identifying,W03-1609,0,\N,Missing
deleger-zweigenbaum-2010-identifying,W03-1608,0,\N,Missing
deleger-zweigenbaum-2010-identifying,W09-3102,1,\N,Missing
deleger-zweigenbaum-2010-identifying,W07-1007,0,\N,Missing
deleger-zweigenbaum-2010-identifying,P01-1008,0,\N,Missing
deleger-zweigenbaum-2010-identifying,N03-1003,0,\N,Missing
deleger-zweigenbaum-2010-identifying,P05-1074,0,\N,Missing
deleger-zweigenbaum-2010-identifying,P99-1044,0,\N,Missing
E09-1056,J93-2003,0,0.00944004,"If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487–495, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 487 2.2 In the remainder of this paper, we first present in Section 2 the principle of analogical learning. Practical issues in analogical learning are discussed in Section 3 al"
E09-1056,W05-0616,1,0.948584,"ector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recogn"
E09-1056,W02-1001,0,0.0246147,"Missing"
E09-1056,P08-1059,0,0.0441301,"Missing"
E09-1056,2007.mtsummit-papers.19,0,0.512696,"such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms of the medical domain; a domain well-known for its tendency to create new words, many of which being complex lexical constructions."
E09-1056,W07-0705,0,0.0544343,"Missing"
E09-1056,W97-0119,0,0.0759318,"Missing"
E09-1056,2007.mtsummit-papers.36,0,0.0122855,"nguage pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements. 1 Introduction If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487–495, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 487 2.2 In the remainder of this pap"
E09-1056,koen-2004-pharaoh,0,0.0664297,"Missing"
E09-1056,D07-1092,1,0.880221,"gree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms o"
E09-1056,C08-2013,1,0.8318,"f I(u), denoted N (t). Those solutions that belong to the input space are the z-forms retained; EI (u) = { hx, y, zi : EI (u) = { hx, y, zi : x ∈ I, hy, zi ∈ C(hx, ti), [x : y = z : t] } where C(hx, ti) denotes the set of pairs hy, zi which satisfy the count property. This strategy will only work if (i) the number of quadruplets to check is much smaller than the number of triplets we can form in the input space (which happens to be the case in practice), and if (ii) we can efficiently identify the pairs hy, zi that satisfy a set of constraints on character counts. To this end, we proposed in (Langlais and Yvon, 2008) to organize the input space into a data structure which supports efficient runtime retrieval. x ∈ N (t) , y ∈ N (x), z ∈ [y : x = t : ? ] ∩ I } This strategy (hereafter named LP) directly follows from a symmetrical property of an analogy ([x : y = z : t] ⇔ [y : x = t : z]), and reduces the search procedure to the resolution of a number of analogical equations which is quadratic with the number of pairs hx, yi sampled. We found this strategy to be of little use for input spaces larger than a few tens of thousands forms. To solve this problem, we exploit a property on symbol counts that an anal"
E09-1056,2005.iwslt-1.4,0,0.0842875,"another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an"
E09-1056,P98-1120,0,0.343527,"set. Our solver, depicted in Algorithm 2, is thus controlled by a sampling size s, the impact of which is illustrated in Table 1. By increasing s, the solver generates more (mostly spurious) solutions, but also increases the relative frequency with which the expected output is generated. In practice, provided a large enough sampling size,2 the expected form very often appears among the most frequent ones. selecting good candidates involves some practical issues. Since searching for input triplets might involve the need for solving (input) equations, we discuss the solver first. 3.1 The solver Lepage (1998) proposed an algorithm for solving an analogical equation [x : y = z : ? ]. An alignment between x and y and between x and z is first computed (by edit-distance) as illustrated in Figure 1. Then, the three strings are synchronized using x as a backbone of the synchronization. The algorithm can be seen as a deterministic finite-state machine where a state is defined by the two edit-operations being visited in the two tables. This is schematized by the two cursors in the figure. Two actions are allowed: copy one symbol from y or z into the solution and move one or both cursors. x: r e a d er y:"
E09-1056,P06-1096,0,0.0437426,"Missing"
E09-1056,P07-1084,0,0.0158288,"ing: Application to Translating multi-Terms of the Medical Domain Philippe Langlais DIRO Univ. of Montreal, Canada felipe@iro.umontreal.ca Franc¸ois Yvon and Pierre Zweigenbaum LIMSI-CNRS Univ. Paris-Sud XI, France {yvon,pz}@limsi.fr Abstract with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several"
E09-1056,C98-1116,0,\N,Missing
E09-1056,2007.iwslt-1.7,0,\N,Missing
F12-2002,W11-0207,1,0.881677,"Missing"
F12-2002,P07-2045,0,0.0132777,"Missing"
F12-2002,C02-1003,0,0.0575184,"Missing"
F12-2002,J04-4002,0,0.0172252,"Missing"
F12-2002,W09-1906,0,0.022249,"Missing"
F12-2002,N01-1026,0,0.137191,"Missing"
F13-1024,C02-1166,0,0.0918764,"Missing"
F13-1024,P95-1032,0,0.357061,"Missing"
F13-1024,2007.mtsummit-papers.26,0,0.108017,"Missing"
F13-1024,P04-1067,0,0.0620838,"Missing"
F13-1024,hazem-morin-2012-adaptive,0,0.0346641,"Missing"
F13-1024,2009.mtsummit-posters.26,0,0.107232,"Missing"
F13-1024,C10-1070,0,0.0410905,"Missing"
F13-1024,C10-1073,0,0.0437096,"Missing"
F13-1024,W11-1205,0,0.0411106,"Missing"
F13-1024,2009.mtsummit-posters.14,0,0.0667486,"Missing"
F13-1024,P95-1050,0,0.288959,"Missing"
F13-1024,2010.jeptalnrecital-demonstration.6,0,0.11305,"Missing"
F13-1024,E12-1046,0,0.0262621,"Missing"
F13-2018,W13-0106,0,0.045046,"Missing"
F13-2018,P06-1095,0,0.0447524,"Missing"
F13-2018,C08-3012,0,0.047038,"Missing"
galibert-etal-2010-named,W04-1213,0,\N,Missing
galibert-etal-2010-named,galibert-etal-2010-hybrid,1,\N,Missing
galibert-etal-2010-named,C96-1079,0,\N,Missing
galibert-etal-2010-named,P05-1045,0,\N,Missing
galibert-etal-2012-extended,grover-etal-2008-named,0,\N,Missing
galibert-etal-2012-extended,A00-1044,0,\N,Missing
galibert-etal-2012-extended,C02-1130,0,\N,Missing
galibert-etal-2012-extended,W11-0411,1,\N,Missing
galibert-etal-2012-extended,I11-1058,1,\N,Missing
galibert-etal-2012-extended,doddington-etal-2004-automatic,0,\N,Missing
galibert-etal-2012-extended,bick-2004-named,0,\N,Missing
galibert-etal-2012-extended,galibert-etal-2010-named,1,\N,Missing
galibert-etal-2012-extended,C96-1079,0,\N,Missing
I11-1058,P98-1031,0,0.164757,"(2009) and many others have focused on speech data. Named Entity detection evaluation over French spoken data has been proposed within the Ester II project, as described by Galliano et al. (2009). Within the framework of the Quaero project,we proposed an extended named entity definition with compositional and hierarchical structure. This extension raises new issues and challenges in NER evaluation. First, as we shall explain below in more detail, the usual evaluation methods cannot compute the Slot Error Rate (SER) metric when named entities are compositional and recursive. Second, following Burger et al. (1998) and Hirschman et al. (1999), we consider that the evaluation of named entity recognition on noisy text output by automatic speech recognition (ASR) systems should take as reference the named entities found in the human annotation of a humantranscribed text: what should have been there in the ASR output. This requires to project the clean reference to the noisy text, which is made all the more difficult because of the compositional and hierarchical structure of the named entities. Introduction Named Entity Detection has been studied since the MUC conferences in 1987. The notion has been extend"
I11-1058,C02-1130,0,0.144389,"are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as g"
I11-1058,galibert-etal-2010-named,1,0.873075,"Missing"
I11-1058,moreau-etal-2010-evaluation,1,0.810616,"ed to project the clean reference on the noisy text in order to build a new reference. That new reference then allows us to apply the clean text methodology. This projection method consists in finding new positions for the frontiers through either a dynamic programming alignment (standard sclitetype ASR evaluation alignment) or a phone-level dynamic programming alignment using canonical phonetizations. They noticed the result was too strict frontier-wise and required reducing the weight of frontier errors to obtain significant results. In Question Answering from speech transcripts evaluation, Moreau et al. (2010) required that QA systems extract answers to natural language questions from ASR outputs of broadcast news shows. The inherent application was to replay the sound segment containing the answer, with a time interval as an answer; it tolerated a time interval around the boundaries. The results were satisfactory. We thus decided to project the clean reference on the noisy text following five steps: pers.ind name.first recevrons Benoît name.last Majimel 167.5s recevrons l' acteur 168s Benoît magie name.first 168.5s mais l' acteur name.last pers.ind Figure 4: Example of a fuzzy reference built by t"
I11-1058,C96-1079,0,0.792768,"al nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date"
I11-1058,W11-0411,1,0.350134,"om news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Co"
I11-1058,W03-0419,0,0.0376895,"Missing"
I11-1058,sekine-nobata-2004-definition,0,0.68687,"of proper names (e.g., phrases built around substantives). In this work, we decided to extend the coverage of the named entities rather than sub-dividing the existing classes as it has been done in previous work. As we aimed to build a fact database from news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and c"
I11-1058,I05-1058,0,0.0287484,"ussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as gene, protein, DNA etc. are also addressed (Ohta, 2002) and campaigns are organized f"
I11-1058,W04-1213,0,\N,Missing
I11-1058,bonneau-maynard-etal-2006-results,0,\N,Missing
I11-1058,C98-1031,0,\N,Missing
I11-1058,bick-2004-named,0,\N,Missing
I13-1125,2009.mtsummit-posters.14,0,0.0207206,"the standard approach builds and compares context vectors for each word of the source and target languages. A particularity of this approach is that, to enable the comparison of context vectors, it requires the existence of a seed bilingual dictionary to translate source context vectors. The use of the bilingual dictionary is problematic when a word has several translations, whether they are synonymous or 2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) 952 International Joint Conference on Natural Language Processing, pages 952–956, Nagoya, Japan, 14-18 October 2013. Once translated into the target language, the context vectors disambiguation process intervenes. This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words. For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polysemous word’s translations senses. We hypothesize that a word is monosemic if it is associated to only one entry in the bilingual dictionary. We checked this assump"
I13-1125,P95-1050,0,0.200321,"ext vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The main work in this research area could be seen as an extension of Harris’s distributional hypothesis (Harris, 1954). It is based on the simple observation that a word and its translation are likely to appear in similar contexts across languages (Rapp, 1995). Based on this assumption, the alignment method, known as the standard approach builds and compares context vectors for each word of the source and target languages. A particularity of this approach is that, to enable the comparison of context vectors, it requires the existence of a seed bilingual dictiona"
I13-1125,P04-1067,0,0.082163,"Missing"
I13-1125,hazem-morin-2012-adaptive,0,0.0165341,"t prominent translations of each polysemous unit wp , an average similarity is computed for each translation wpj of wp : PN SemSim (wi , wpj ) j Ave Sim(wp ) = i=1 (3) N 953 Corpus Corpus French 396, 524 Romanian 22,539 English 524, 805 English 322,507 The resulting bilingual dictionary contains about 136,681 entries for Romanian-English with an average of 1 translation per word. 4.1.3 Evaluation list In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are usually composed of about 100 single terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created a reference list3 for each pair of language. The FrenchEnglish list contains 96 terms extracted from the French-English M E SH and the UMLS thesauri4 . The Romanian-English reference list was created by a native speaker and contains 38 pair of words. Note that reference terms pairs appear at least five times in each part of both comparable corpora. Table 1: Comparable corpora sizes in term of words. where N is the total number of monosemic words and SemSim is the similarity value of wpj and the ith monosemic word. Hence, according to average rel"
I13-1125,C10-1070,0,0.0173584,"ord. Hence, according to average relatedness values Ave Sim(wpj ), we obtain for each polysemous word wp an ordered list of translations wp1 . . . wpn . This allows us to select translations of words which are more salient than the others to represent the word to be translated. 4 Experiments and Results 4.2 4.1 Resources 4.1.1 Three other parameters need to be set up: (1) the window size, (2) the association measure and the (3) similarity measure. To define context vectors, we use a seven-word window as it approximates syntactic dependencies. Concerning the rest of the parameters, we followed Laroche and Langlais (2010) for their definition. The authors carried out a complete study of the influence of these parameters on the bilingual alignment and showed that the most effective configuration is to combine the Discounted Log-Odds ratio (equation 4) with the cosine similarity. The Discounted Log-Odds ratio is defined as follows: Comparable corpora We conducted our experiments on two FrenchEnglish and Romanian-English comparable corpora specialized on the breast cancer domain. Both corpora were extracted from Wikipedia1 . We consider the topic in the source language (for instance cancer du sein [breast cancer]"
I13-1125,C10-1073,0,0.0420861,"Missing"
I13-1125,C02-2020,1,\N,Missing
I13-1125,P94-1019,0,\N,Missing
L16-1366,I13-1077,0,0.0232293,"o rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the"
L16-1366,W15-4660,1,0.410631,"such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a differe"
L16-1366,L16-1505,1,0.819925,"terally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a different problem: the identification o"
L16-1366,W09-3102,1,0.945102,"sed by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how fami"
L16-1366,W07-1007,0,0.0400165,"luation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus. Keywords: Technicality of Medical Terms; Learning to rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical s"
L16-1366,W10-3304,0,0.0619834,"Missing"
L16-1505,L16-1366,1,0.819925,"terms over more technical terms. For this purpose, each set of terms sharing the same UMLS CUI is sorted by degree of technicality: e.g., for concept C0036973 (‘shiver’), grelottements is the less technical term, and frissonnement is the most technical. This degree of technicality was computed by comparing the probabilities of a term according to two language models respectively trained on a technical corpus of medical articles (CRTT)2 and on a non-technical corpus of online medical forums3 . The degree of technicality of a term is computed as the likelihood ratio of these two probabilities (Bouamor et al., 2016). To generate a lay variant of a term, its CUI is determined and the least technical term for this CUI is chosen. Additionally, a manually created list of {technical, lay} term pairs is used for terms lacking a UMLS CUI, or terms for which no degree of technicality could be computed because they were unseen in our training corpora: e.g., naus´ees et vomissements (‘nausea and vomiting’) refers to NVPO (‘PONV’, ‘Postoperative Nausea and Vomiting’). 5. Results and Evaluation The system has been tested on three patient cases with project partners and during public demonstrations, and a first evalu"
L16-1505,W15-4660,1,0.545287,"ly been addressed recently (McCray et al., 2000; Zeng-Treitler et al., 2007). Virtual patients (VP) are interactive systems and require managing terms—e.g. by formalizing ontological concepts (Nirenburg et al., 2008)—and a Natural Language Understanding (NLU) module. The NLU component may rely on text meaning representations for resolving paraphrases (Nirenburg et al., 2009) or a corpus of questions and answers curated by an expert (Kenny et al., 2008). We are developing a conversational agent to be used in a simulated consultation with a VP, where the system aims at training medical doctors (Campillos-Llanos et al., 2015). Users (medical students or doctors) interact with the VP to collect information that allows them to provide a correct diagnosis. Medical trainers define each e-learning case beforehand by entering the VP profile data in a clinical record (e.g. symptoms or medical history). Managing linguistic and terminological variation is crucial to match a user’s question to a term in the clinical record and to select suitable terms for answer generation. This paper gives an overview of the difficulties (Section 2.) and strategies applied in both analysis (Section 3.) and generation (Section 4.). We also"
L16-1505,W08-1507,0,0.07858,"Missing"
L18-1025,D15-1301,0,0.0261746,"to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definition of replicability or"
L18-1025,J92-1002,0,0.025332,"anguage-related value that stimulated an enormous amount of academic work, some of which has been evaluated with respect to the extent to which it does or does not reproduce the values reported in (Shannon, 1951). For example, (Cover and King, 1978) used a very different method from Shannon’s original one and found a value of 1.3 bits for the entropy of written English. The paper explicitly states that this value “agrees well with Shannon’s estimate,” suggesting that the authors considered their value to have reproduced Shannon’s original value in (Shannon, 1951)3 . In a very different tone, (Brown et al., 1992) reported an upper bound of exactly 1.75 bits, but did not explicitly compare that to previous findings, although it is clear from the paper that they considered it different from—and better than—previously reported values. As the authors put it: We see this paper as a gauntlet thrown down before the computational linguistics community. A relevant value from our papers that was not reproduced is the mean value for the frequency of negation. We reported this in our papers (Cohen et al., 2010) and (Cohen et al., 2017a). They were different by roughly a factor of 2, even though we used the same c"
L18-1025,daelemans-hoste-2002-evaluation,0,0.0820242,"as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificia"
L18-1025,P13-1166,0,0.316125,"Missing"
L18-1025,H90-1013,0,0.482748,", it can be quite difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computati"
L18-1025,H94-1017,0,0.0985049,"difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics"
L18-1025,W16-6110,1,0.905358,"Missing"
L18-1025,J05-1004,0,0.0161754,"e processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificial intelligence, theoretical linguistics, and cognitive science (Cori et al., 2002). (See also (Cori and L´eon, 2002) for a dis"
L18-1025,W10-1726,0,0.114498,"initions related to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definiti"
L18-1543,W16-2922,0,0.0911438,"Missing"
L18-1543,W17-2312,1,0.572879,"Missing"
L18-1543,W17-2310,0,0.140613,"Missing"
L18-1582,E12-1058,0,0.0318444,"dology choices for automating the literature screening, and to find ways to improve the quality of constructing datasets used to train such retrieval methods. Second, we experiment on an existing reference dataset and introduce a new, complementary dataset. 3681 2. Dataset Yearbook Related Work Methods for automation have been attempted with varying degrees of success in technology assisted review in several topics in biomedicine (O’Mara-Eves et al., 2015). Technology assisted review has also been implemented in other fields with similarly stringent recall requirements, such as patent search (Stein et al., 2012), and electronic discovery (Grossman and Cormack, 2011). Automated document discovery is typically cast as a ranking or classification problem (O’Mara-Eves et al., 2015). Common methods for automation include Support Vector Machines and variants of Naive Bayes, including Complement Naive Bayes (Matwin et al., 2010), and Multinomial Naive Bayes (Matwin and Sazonova, 2012). Other methods have been tried, including Voting Perceptrons (Cohen et al., 2006), Decision Trees (Bekhuis and DemnerFushman, 2010), Evolutional S VM (Bekhuis and DemnerFushman, 2010), WAODE (Bekhuis and Demner-Fushman, 2010),"
L18-1605,S16-1081,0,0.0234795,"ome past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly availa"
L18-1605,W17-2508,0,0.0208688,"by human review of samples of resulting sequences of two sentences. • A configuration for indexing and search in the Solr search engine, typically based on a tokenizer, stop words, and possibly more language components. These datasets were used in the BUCC 2017 and 2018 Shared Tasks (Zweigenbaum et al., 2017; Zweigenbaum et al., 2018). Participants were taskeed with detecting in a bilingual pair of corpora the inserted parallel sentences. Three of the four language pairs were addressed by the participants in 2017: French, German, and Chinese, with a maximum F-score of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . I"
L18-1605,W16-2347,0,0.026976,"n the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described"
L18-1605,W03-1722,0,0.0180004,"eport here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which strategy to apply (full=short, default=large, search=multiple solutions). We attempted to avoid these considerations by working directly with characters. This was initially motivated by the technical choice of Solr (v6.4.0), whos"
L18-1605,N04-1034,0,0.166203,"Missing"
L18-1605,W15-3411,1,0.839088,"ora with known parallel sentence pairs. We therefore needed to prevent as much as possible naturally occurring parallel sentence pairs from remaining in our monolingual corpora. The strategy we adopted in this purpose was to desynchronize our comparable corpora. Since we started from Wikipedia articles in two languages, we knew that interlinked articles would be highly likely to contain such parallel sentences: this is indeed a property that is often desired by past work on parallel sentence extraction. This is also how our previous shared task on detection of comparable texts has been setup (Sharoff et al., 2015): the gold standard was based on the iwiki links. In contrast to such work, we built pairs of monolingual corpora which never contained two interlinked Wikipedia articles. This was also in line with our desideratum not to include meta-information on the sentences, such as being found in two interlinked articles. The main drawback in doing so is that the most comparable pairs of documents for a given language pair are removed from the pairs of corpora we built: only one out of two interlinked pages can be kept in one of our corpora. This reduces the comparability of our datasets. However, the t"
L18-1605,N10-1063,0,0.0345766,"they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the o"
L18-1605,I05-3027,0,0.0204923,"of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which"
L18-1605,P03-1010,0,0.0953731,"r criteria such as domain, genre, time period. In contrast to parallel corpora, they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, pla"
L18-1605,W17-2512,1,0.775299,"wever, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described in (Zweigenbaum et al., 2017). The present paper provides more detail about our motivation and design criteria, about the rationale we followed to implement these design criteria, and about the processing of the Chinese part of the corpus. 2. A Dataset for Parallel Sentence Extraction from Comparable Corpora 2.1. Desiderata for a Dataset for the Task We aimed to build a bilingual corpus to measure progress on the identification of parallel sentences in monolingual corpora. This led us to the following desiderata and design choices. No metadata. We wish to focus on the cross-language comparison of sentence contents. Instea"
P13-2133,C02-2020,1,0.736512,"rd approach is the bilingual dictionary. Its use is problematic when a word has several translations, whether they are synonymous or polysemous. For instance, the French 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements ha"
P13-2133,P04-1067,0,0.877152,"Missing"
P13-2133,hazem-morin-2012-adaptive,0,0.0535024,"epresentative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standar"
P13-2133,C10-1070,0,0.247056,"terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains. The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6 . The second list contains 79 terms extracted from the FrenchEnglish MESH and the UMLS thesauri7 . Note that reference terms pairs appear more than five times in each part of both comparable corpora. Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. We followed (Laroche and Langlais, 2010) to define these parameters. They carried out a complete study of the influence of these parameters on the bilingual alignment. The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7. (2) where N is the total number of monosemic words in each context vector and SemSim is the similarity value of wpj and the ith monosemic word. Hence, according to average similarity values Ave Sim(wpj ), we obtain for each polysemous word wp an ordered list of translations wp1 . . . wpn . 4 Experiments and Results 4.1"
P13-2133,C10-1073,0,0.284,"Missing"
P13-2133,W11-1205,0,0.13144,"seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standard approach but weigh the different translations according to their frequency in the target corpus. Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 3.1 path-based semantic similarity measures denoted PATH,W UP (Wu and Palmer, 1994) and L EA COCK (Leacock and Chodorow, 1998). PATH is a baseline that is equal to the inverse of the shortest path between two words. W UP finds the dep"
P13-2133,2009.mtsummit-posters.14,0,0.332928,"oblematic when a word has several translations, whether they are synonymous or polysemous. For instance, the French 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attem"
P13-2133,P95-1050,0,0.284557,"n from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source and target vec"
P13-2133,P94-1019,0,\N,Missing
P18-2090,P16-1186,0,0.0325694,"Missing"
P18-2090,C00-2137,0,0.0972755,"Missing"
P18-2090,J15-4004,0,0.0632687,"Missing"
P18-2090,W18-1702,1,0.765497,"ndom walks after removing self-loops. Table 2: Best parameters models and the accuracy of word2vec skip-gram are statistically significant (p < 0.0001). The time complexity when using our modified negative sampling distribution is similar to that of the original skip-gram negative sampling except that the distribution from which negative examples are sampled is different for each token. We pre-compute this distribution off-line for each token so that the added complexity is proportional to the size of the vocabulary. Specifically, precomputing the co-occurrences and graphs using corpus2graph (Zhang et al., 2018) takes about 2.5 hours on top of 8 hours for word2vec alone on the entire Wikipedia corpus using 50 logical cores on a server with 4 Intel Xeon E5-4620 processors : the extra cost is not excessive. Let us take a closer look at each graph-based model. First, the word context distribution based model: we find that all else being equal, replacing zero values gives better performance. We believe a reason may be that for a training word, all the other words should have a probability to be selected as negative examples—the job of noise distributions is to assign these probabilities. We note that for"
P18-2090,E17-2003,0,0.0287293,"Missing"
P18-2090,P14-5010,0,0.00664903,"Missing"
P19-2041,Q17-1010,0,0.0482427,"ms better than the contextual embeddings alone and improves upon static embeddings trained on a large in-domain corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements t"
P19-2041,W16-4202,0,0.0151782,"tatic forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test files for a total of 30,94"
P19-2041,D14-1162,0,0.0846276,"extual embeddings (ELMo) from the general domain. We also show that this combination performs better than the contextual embeddings alone and improves upon static embeddings trained on a large in-domain corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can"
P19-2041,N18-1202,0,0.331675,"main corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements to strategies involving contextual embeddings. 2 Related Work Former work by Roberts (2016) analyzed the"
P19-2041,W18-2501,0,0.0232897,"., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test files for a total of 30,946 + 45,404 ⇡ 76,000 sequences 4 . 3.2 Task and Model The goal of the Clinical Concept Detection task is to extract three types of medical entities: problems (e.g. the name of a disea"
P19-2041,W16-4208,0,0.0175127,"ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements to strategies involving contextual embeddings. 2 Related Work Former work by Roberts (2016) analyzed the trade-off between corpus size and similarity when training word embeddings for a clinical entity recognition task. The author’s conclusion was that while embeddings trained with word2vec on indomain texts performed generally better, a combination of both in-domain and general domain em1 Python code for reproducing our experiments is available at: https://github.com/helboukkouri/ acl_srw_2019 295 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 295–301 c Florence, Italy, July 28 - August 2, 2019. 2019 Associa"
P19-2041,P16-1128,0,0.0482051,"Missing"
P19-2041,D18-1176,0,0.0206433,"representations pre-trained on MIMIC-III, proving once more the value of large in-domain corpora (Si et al., 2019).2 While interesting for the clinical domain, these strategies may not always be applicable to other specialized fields since large in-domain corpora like MIMIC-III will rarely be available. To deal with this issue, we explore embedding combinations3 . In this respect, we consider both static forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During tra"
P19-2041,N16-1030,0,0.0412958,"t, we consider both static forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test f"
P19-2041,W17-0212,0,0.0189816,"omain corpus. Similar corpora will often be available in other specialized domains as it is always possible to build a corpus from the training documents. Concatenation a simple concatenation of vectors coming from two different embeddings. This is denoted X Y (e.g. i2b2 Wikipedia). Mixture in the particular case where ELMo embeddings were combined with word2vec vectors, we can directly add the word2vec embedding in the linear combination of ELMo. We denote this combination strategy X+ +Y (e.g. ELMo small+ +i2b2). Then, we also train embeddings on each of two general-domain corpora: Wikipedia (2017) encyclopedia articles from the 01/10/2017 data dump6 . This is a large (2 billion tokens) corpus from the general domain that has limited coverage of the medical field. The mixture method generalizes the way ELMo representations are combined. Given a word w, if we denote the three internal representations produced by ELMo (i.e. the CharCNN, 1st bi-LSTM and 2nd bi-LSTM representations) by h1 , h2 , h3 , we recall that the model computes the word’s embedding as: Gigaword (2003) newswire text data from many sources including the New York Times. This is a large (2 billion tokens) corpus from the"
W02-0304,1997.iwpt-1.3,0,0.01589,"Missing"
W02-0304,W98-1504,0,0.271191,"Missing"
W02-0304,A97-1016,0,\N,Missing
W02-0304,J95-4004,0,\N,Missing
W02-1403,P98-1082,0,0.0303293,". Various methods have been proposed to discover relations between terms (see Jacquemin and Bourigault (2002) for a review). We divide them into internal and external methods, in the same way as McDonald (1993) for proper names. Internal methods look at the constituency of terms, and compare terms based on the words they contain. Term matching can rely directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefenstette, 1994). The detection of appropriate syntactic patterns of cooccurrence is another method to uncover relations between terms in corpora (Hearst, 1992; Séguéla and Aussenac, 1999). In previous work we applied lexical methods to identify relations between terms on the basis on their content words, taking morphological variants into account. Our goal was then to assess the feasibility of such structuri"
W02-1403,C92-2082,0,0.020099,"directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefenstette, 1994). The detection of appropriate syntactic patterns of cooccurrence is another method to uncover relations between terms in corpora (Hearst, 1992; Séguéla and Aussenac, 1999). In previous work we applied lexical methods to identify relations between terms on the basis on their content words, taking morphological variants into account. Our goal was then to assess the feasibility of such structuring by studying it on an existing, hierarchically structured terminology. Ignoring this existing structure and starting from the set of its terms, we attempt to discover hierarchical term-toterm links and compare them with the preexisting relations. Our goal in the present paper is to analyze ‘new’ relations. ‘New’ means that these induced relati"
W02-1403,W93-0104,0,0.010883,"ckground Terminology structuring, i.e., organizing a set of terms through semantic relations, is one of the difficult issues that have to be addressed when building terminological resources. These relations include subsumption or hyperonymy (the is-a relation), meronymy (part-of and its variants), as well as other, diverse relations, sometimes called ‘transversal’ (e.g., cause, or the general see also). Various methods have been proposed to discover relations between terms (see Jacquemin and Bourigault (2002) for a review). We divide them into internal and external methods, in the same way as McDonald (1993) for proper names. Internal methods look at the constituency of terms, and compare terms based on the words they contain. Term matching can rely directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefen"
W02-1403,C98-1079,0,\N,Missing
W04-1807,C92-2082,0,0.603889,"cent families of works: (i) Cartier (1997), (ii) Pearson (1996) and Rebeyrolle (2000), (iii) Muresan and Klavans (2002). They have used respectively “contextual exploration”, lexicosyntactic patterns and linguistic analysis and rules. The former one extracts defining statements on the basis of the match of linguistic clues, when they are relayed in the sentence by some linguistic rules. These rules are developped by the author, withing the schema defined in the “contextual exploration” methodology (Desclés, 1996). Pearson (1996) and Rebeyrolle (2000) have followed the methodology described by Hearst (1992), up to now mainly applied to discover hyponymous terms. It consists in describing the lexico-syntactic context of an occurrence of a pair of terms known to share a semantic relation. Modelling the context in which they occur provides a “pattern” to apply to the corpus, in order to extract other pairs of terms connected by the same relation. Pearson and Rebeyrolle have modelled lexico-syntactic contexts around lexical clues interpreted as “definition markers”. Rebeyrolle, working on French, evaluated the different pattern types she modelled, across different corpora: 57 she obtained a precisio"
W04-1807,muresan-klavans-2002-method,0,0.35404,"(1985), Flowerdew (1992), Sager (2001) and Meyer (2001). Another type of interesting existing work is about typologies of definitions: Martin (1983), Chukwu and Thoiron (1989) and Auger (1997), amongst others, provide, in their classifications of definitions, linguistic clues to find defining statements in corpus. We propose to integrate the typologies that we mention in section 2.2, along with the linguistic clues they give: the definition markers. And, at last, some works have already focused on mining definitions from corpora, including Cartier (1997), Pearson (1996), Rebeyrolle (2000) and Muresan and Klavans (2002), mostly through the use of lexical definition markers. These works provide us with methodological guidelines and another set of lexical markers for our own experiment. As (Pearson (1996); Rebeyrolle (2000)), our method is based on lexico-syntactic patterns, so that we can build on the work on French language by Rebeyrolle (2000). We extended her work in two respects: an analysis of the parenthesis as low-level linguistic clue for definitions, and the concomitant extraction of the semantic relation involved in a “defining expression”, along with the extraction of the definition itself. Previou"
W09-2701,J07-1004,0,0.0545716,"Missing"
W09-2701,J07-1005,0,0.0593275,"Missing"
W09-2701,W04-0509,0,0.0362692,"uilt, offline (Fleischman et al., 2003; Sang et al., 2005; Delbecque et al., 2005) or dynamically. In medical QA systems, both document analysis and question analysis nearly always rely on extensive knowledge of domain concepts and relations, e.g. as provided by the UMLS knowledge sources (McCray and Nelson, 1995). More than named entities, systems need to detect mentions of concepts (Aronson, 2001) and their relations (Rindflesch et al., 2005). Besides, taking into account the structure of documents such as scientific articles or encyclopedia entries may help focus on more relevant sections (Niu and Hirst, 2004; Sang et al., 2005). Finally, answers to complex medical questions often need to span more than one sentence. Extractive summarization is performed both from single documents (DemnerFushman and Lin, 2007) and from multiple documents (Fiszman et al., 2008). Knowledge and Reasoning for Processing Medical Questions Medical question-answering has to address questions other than the usual factual questions of most QA evaluations. This calls for different question classifications (Ely et al., 2000; Yu et al., 2005), especially to determine whether a given question can be answered using medical know"
W09-2701,P03-1001,0,0.0174316,"genbaum et al., 2007). 1 2 Knowledge and Reasoning for Finding Medical Answers Answers to medical questions should be searched in the most reliable data available. When data exist in structured knowledge bases (e.g. a drug compendium), it may be more appropriate to query such knowledge bases directly. Therefore an approach akin to that of Start/Omnibase (Lin and Katz, 2003) may be indicated. When answers are to be found in a collection of documents, as is the case in traditional question-answering systems, a representation of the information contained in these documents can be built, offline (Fleischman et al., 2003; Sang et al., 2005; Delbecque et al., 2005) or dynamically. In medical QA systems, both document analysis and question analysis nearly always rely on extensive knowledge of domain concepts and relations, e.g. as provided by the UMLS knowledge sources (McCray and Nelson, 1995). More than named entities, systems need to detect mentions of concepts (Aronson, 2001) and their relations (Rindflesch et al., 2005). Besides, taking into account the structure of documents such as scientific articles or encyclopedia entries may help focus on more relevant sections (Niu and Hirst, 2004; Sang et al., 2005"
W09-2701,W03-1310,0,\N,Missing
W09-3102,P01-1008,0,0.407768,"ext corpora. A few paraphrase acquisition approaches used plain monolingual corpora to detect paraphrases, such as (Jacquemin, 1999) who detects term variants or (Pasca and Dienes, 2005) who extract paraphrases from random Web documents. This type of corpus does not insure the actual existence of paraphrases and a majority of methods have relied on corpora with a stronger similarity between the documents, thus likely to provide a greater amount of paraphrases. Some paraphrase approaches used monolingual parallel corpora, i.e. different translations or versions of the same texts. For instance (Barzilay and McKeown, 2001) detected paraphrases in a corpus of English translations of literary novels. However such corpora are not easily available and approaches which rely instead on other types of corpora are actively investigated. Bilingual parallel corpora have been exploited for acquiring paraphrases in English (Bannard and Callison-Burch, 2005) and French (Max, 2008). Comparable corpora are another useful source of paraphrases. In this regard, only closely related corpora have been used, especially and almost exclusively corpora of news sources reporting the Whereas multilingual comparable corpora have been us"
W09-3102,I05-1011,0,0.184873,"ext. In this work1 , we are interested in using comparable corpora to extract paraphrases. Paraphrases are useful to various applications, including information retrieval (Ibrahim et al., 2003), information extraction (Shinyama and Sekine, 2003), document summarization (Barzilay, 2003) and text simplification (Elhadad and Sutaria, 2007). Several methods have been designed to extract paraphrases, many of them dealing with comparable text corpora. A few paraphrase acquisition approaches used plain monolingual corpora to detect paraphrases, such as (Jacquemin, 1999) who detects term variants or (Pasca and Dienes, 2005) who extract paraphrases from random Web documents. This type of corpus does not insure the actual existence of paraphrases and a majority of methods have relied on corpora with a stronger similarity between the documents, thus likely to provide a greater amount of paraphrases. Some paraphrase approaches used monolingual parallel corpora, i.e. different translations or versions of the same texts. For instance (Barzilay and McKeown, 2001) detected paraphrases in a corpus of English translations of literary novels. However such corpora are not easily available and approaches which rely instead o"
W09-3102,P95-1050,0,0.0777602,"lections of texts sharing common characteristics. Very often comparable corpora consist of texts in two (or more) languages that address the same topic without being translations of each other. But this notion also applies to monolingual texts. In a monolingual context, comparable corpora can be texts from different sources (such as articles from various newspapers) or from different genres (such as specialized and lay texts) but dealing with the same general topic. Comparable corpora have been used to perform several Natural Language Processing tasks, such as extraction of word translations (Rapp, 1995; Chiao and Zweigenbaum, 2002) in a multilingual context or acquisition of 1 This paper is an extension of the work presented in (Deléger and Zweigenbaum, 2008a) and (Deléger and Zweigenbaum, 2008b), more specifically, a new corpus is added, an additional type of paraphrase (based on neoclassical compounds) is extracted and the evaluation is more relevant. 2 Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 2–10, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP on monolingual comparable corpora of specialized and lay medical French documents"
W09-3102,W03-1609,0,0.402153,"their lay versions). The other two corpora (on nicotine addiction and diabetes), however, were built from heterogeneous sources through a restricted search and are less similar. We first queried two health search engines (the health Web portals CISMeF3 and HON4 ) with key words. Both allow the user to search for documents targeted to a population (e.g., patient-oriented documents). We also queried known relevant websites for documents dealing with our chosen topics. Those were same events. (Barzilay and Lee, 2003) generated paraphrase sentences from news articles using finite state automata. (Shinyama and Sekine, 2003) extracted paraphrases through the detection of named entities anchors in a corpus of Japanese news articles. In the medical domain, (Elhadad and Sutaria, 2007) worked with a comparable, almost parallel, corpus of medical scientific articles and their lay versions to extract paraphrases between specialized and lay languages. We aim at detecting paraphrases in medical corpora in the same line as (Elhadad and Sutaria, 2007) but for French. This type of paraphrases would be a useful resource for text simplification or to help authoring medical documents dedicated to the general public. However, i"
W09-3102,W07-1007,0,0.377581,"are less similar. We first queried two health search engines (the health Web portals CISMeF3 and HON4 ) with key words. Both allow the user to search for documents targeted to a population (e.g., patient-oriented documents). We also queried known relevant websites for documents dealing with our chosen topics. Those were same events. (Barzilay and Lee, 2003) generated paraphrase sentences from news articles using finite state automata. (Shinyama and Sekine, 2003) extracted paraphrases through the detection of named entities anchors in a corpus of Japanese news articles. In the medical domain, (Elhadad and Sutaria, 2007) worked with a comparable, almost parallel, corpus of medical scientific articles and their lay versions to extract paraphrases between specialized and lay languages. We aim at detecting paraphrases in medical corpora in the same line as (Elhadad and Sutaria, 2007) but for French. This type of paraphrases would be a useful resource for text simplification or to help authoring medical documents dedicated to the general public. However, in a French medical context, it is difficult to obtain comparable corpora of documents with a high level of similarity, such as pairs of English scientific artic"
W09-3102,J97-1003,0,0.0208114,"rresponding verbs (Hathout et al., 2002) to detect such pairs in the corpus segments. These noun-verb pairs served as anchors for the detection of paraphrases. In order to design paraphrasing patterns we extracted all pairs of deverbal noun and verb with their contexts from the development corpus. The study of such pairs with their contexts allowed us to establish a set of lexico-syntactic paraphrasing patterns11 . An example of such patterns can be seen in Table 1. 1. as multiple topics are usually addressed in a single text, we performed topic segmentation on each text using the TextTiling (Hearst, 1997) segmentation tool. A segment may consist of one or several paragraphs; 2. we then tried to identify pairs of text segments addressing similar topics and likely to contain paraphrases. For this we used a common, vector-based measure of text similarity: the cosine similarity measure which we computed for each pair of topic segments in the cross-product of both corpus sides (each segment was represented as a bag of words); 5 http://www.has-sante.fr/ http://www.inpes.sante.fr/ 7 http://www.doctissimo.fr/ 8 http://www.tabac-info-service.fr/ 9 http://www.stop-tabac.ch/ 10 http://www.diabete.qc.ca/"
W09-3102,W03-1608,0,0.230922,"Missing"
W09-3102,P99-1044,0,0.746843,"hinyama and Sekine, 2003) in a monolingual context. In this work1 , we are interested in using comparable corpora to extract paraphrases. Paraphrases are useful to various applications, including information retrieval (Ibrahim et al., 2003), information extraction (Shinyama and Sekine, 2003), document summarization (Barzilay, 2003) and text simplification (Elhadad and Sutaria, 2007). Several methods have been designed to extract paraphrases, many of them dealing with comparable text corpora. A few paraphrase acquisition approaches used plain monolingual corpora to detect paraphrases, such as (Jacquemin, 1999) who detects term variants or (Pasca and Dienes, 2005) who extract paraphrases from random Web documents. This type of corpus does not insure the actual existence of paraphrases and a majority of methods have relied on corpora with a stronger similarity between the documents, thus likely to provide a greater amount of paraphrases. Some paraphrase approaches used monolingual parallel corpora, i.e. different translations or versions of the same texts. For instance (Barzilay and McKeown, 2001) detected paraphrases in a corpus of English translations of literary novels. However such corpora are no"
W09-3102,J03-3001,0,0.0373851,"sages which potentially convey comparable information (Section 2.2); and (iii) what sorts of paraphrases can be collected between these two types of discourse, which is addressed in Section 2.3, through the identification of two kinds of paraphrases: nominalization paraphrases and paraphrases of neo-classical compounds. An evaluation of the method (Section 2.4) is conducted and results are presented (Section 3) and discussed (Section 4). 2 2.1 Material and Methods Building comparable corpora of lay and specialized texts Today, a popular way of acquiring a corpus is collecting it from the Web (Kilgarriff and Grefenstette, 2003), as it provides easy access to an unlimited amount of documents. Here we focus 2 http://www.sor-cancer.fr/ http://www.cismef.org/ 4 http://www.hon.ch/ 3 3 French governmental websites, including that of the HAS5 which issues guidelines for health professionals, and that of the INPES6 which provides educational material for the general public; as well as health websites dedicated to the general public, including Doctissimo7 , Tabac Info Service8 , Stoptabac9 and Diabète Québec10 . The corpus dealing with the topic of diabetes served as our development corpus for the first type of paraphrases w"
W09-3102,N03-1003,0,\N,Missing
W09-3102,P05-1074,0,\N,Missing
W11-0207,embarek-ferret-2008-learning,0,0.0864559,"Missing"
W11-0207,C02-1054,0,0.0996403,"Missing"
W11-0207,P04-1055,0,\N,Missing
W11-0411,W10-1804,0,0.10543,"ation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Following (Bonneau-Maynard et al., 2005; Alex et al., 2010), the annotation guidelines were first written from December 2009 to May 2010 by three researchers managing the manual annotation campaign. During guidelines production, we evaluated the feasibility of this specific annotation task and the usefulness of the guidelines by annotating a small part of the target corpus. Then, these guidelines were delivered to the annotators. They consist of a description of the objects to annotate, general annotation rules and principles, and more than 250 prototypical and real examples extracted from the corpus (Rosset et al., 2010). Rules are important to set t"
W11-0411,J08-4004,0,0.16077,"rics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give us about the annotation. Markables Annotators Both institutes F"
W11-0411,bick-2004-named,0,0.312622,"undheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as"
W11-0411,J96-2004,0,0.23313,"inally, we merged the results with the anno97 5.2 Metrics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give"
W11-0411,W09-3002,0,0.0410686,"we presented an extension of the traditional named entity categories to new types (functions, civilizations) and new coverage (expressions built over a substantive). We created guidelines that were used by graduate annotators to annotate a broadcast news corpus. The organizers also annotated a small part of the corpus to build a mini reference corpus. We evaluated the human annotations with our mini-reference corpus: the actual computed κ is between 0.71 et 0.85 which, given the complexity of the task, seems to indicate a good annotation quality. Our results are consistent with other studies (Dandapat et al., 2009) in demonstrating that human annotators’ training is a key asset to produce quality annotations. 99 We also saw that guidelines are never fixed, but evolve all along the annotation process due to feedback between annotators and organizers; the relationship between guidelines producers and human annotators evolved from “parent” to “peer” (Akrich and Boullier, 1991). This evolution was observed during the annotation development, beyond our expectations. These data have been used for the 2011 Quaero Named Entity evaluation campaign. Extensions and revisions are planned. Our first goal is to add a"
W11-0411,desmet-hoste-2010-towards,0,0.0216477,"rent classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Followin"
W11-0411,doddington-etal-2004-automatic,0,0.157033,", some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact databa"
W11-0411,C02-1130,0,0.0613942,"cations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific do"
W11-0411,W09-3025,1,0.778972,"ed. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain bette"
W11-0411,C96-1079,0,0.902879,"luation campaign on named entity extraction aiming at building a fact database in the news domain, the first step being to define what kind of entities are needed. This campaign focused on broadcast news corpora in French. While traditional named entities include three major classes (persons, locations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; G"
W11-0411,I05-1058,0,0.0948165,"roaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard"
W11-0411,sekine-nobata-2004-definition,0,0.926265,"entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be a"
W11-0411,vilnat-etal-2010-passage,0,\N,Missing
W12-1101,S10-1004,0,0.173659,"Missing"
W12-1101,W12-1102,0,0.0186244,"Missing"
W12-3606,bick-2004-named,0,0.0325063,"(CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 20"
W12-3606,doddington-etal-2004-automatic,0,0.0405426,"self (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location"
W12-3606,C02-1130,0,0.0265156,"et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the de"
W12-3606,galibert-etal-2010-named,1,0.859086,"can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub"
W12-3606,I11-1058,1,0.836346,"tities on spoken data and the resulting corpus. The training part of the corpus is only composed of broadcast news data while the test corpus is composed of both broadcast news and broadcast conversations data. In order to build a minireference corpus for this annotation campaign (a “gold” corpus), we randomly extracted a sub-corpus from the training one. This sub-corpus was annotated by 6 different annotators following a 4-step procedure. Table 1 gives statistics about training, test and gold corpora. These corpora (“BN” in the remainder of the paper) has been used in an evaluation campaign (Galibert et al., 2011). PP PP Data Training PP Inf. PP P # shows 188 # lines 43,289 # tokens 1,291,225 # entity types 113,885 # distinct types 41 # components 146,405 # distinct comp. 29 Test Gold 18 5,637 108,010 5,523 32 8,902 22 398 11,532 1,161 29 1,778 22 Table 1: Statistics on the annotated BN corpora Structured Named Entities in Old Newspapers We performed the same annotations on a corpus composed of OCRed press archives, henceforth the Old Press (OP) corpus. Human annotation was subcontracted to the same team of annotators as for the BN corpus, thus facilitating the consistency of annotations across corpora"
W12-3606,galibert-etal-2012-extended,1,0.588515,"née e t a p p a r e i l l e r a e n s u i t e nour &lt; loc.adm.town &gt; Toulon. &lt; / loc.adm.town &gt; Figure 4: Example annotated text block Component noisy-entities. When a character recognition error involves an entity boundary, a segmentation error occurs, either between an entity and other tokens, or between several entities and possibly other tokens. To allow the annotators to annotate the entity in that character span, we defined a new component noisy-entities which indicates that an entity is present in the noisy span of characters. A complete description of these adaptations can be found in (Galibert et al., 2012). 3.3 Global corpus extraction Unannotated sub-corpus Global annotated corpus Scientist 1 Scientist 2 Scientist 4 Scientist 3 Adjudication extraction Inter-Annotator Agreement To evaluate the manual annotations of the annotation team (“Global annotated corpus” in Figure 5), we built a mini reference corpus by selecting 255 blocks from the training corpus. We followed the same procedure as the one used for the BN corpus, as illustrated in Figure 5: Adjudication Institute 1 Institute 2 Adjudication Annotated sub-corpus IAA Institutes Adjudication IAA Mini-reference 1. The corpus is annotated ind"
W12-3606,C96-1079,0,0.645168,"he point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of"
W12-3606,W11-0411,1,0.930226,", as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and"
W12-3606,grover-etal-2008-named,0,0.179562,"corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since"
W12-3606,A00-1044,0,0.0706869,"ora: the pre-existing broadcast news corpus and this new corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described"
W12-3606,sekine-nobata-2004-definition,0,0.07436,"992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 2006) added categories"
W12-5108,D09-1050,0,0.0301563,"Missing"
W12-5108,besancon-etal-2010-lima,1,0.845855,"Missing"
W12-5108,boulaknadel-etal-2008-multi,0,0.0421588,"Missing"
W12-5108,N10-1029,0,0.124822,"Missing"
W12-5108,A94-1006,0,0.419027,"Missing"
W12-5108,W04-3250,0,0.263946,"Missing"
W12-5108,2005.mtsummit-papers.11,0,0.0448373,"Missing"
W12-5108,N03-1017,0,0.0276194,"Missing"
W12-5108,P93-1003,0,0.0392538,"Missing"
W12-5108,2005.mtsummit-posters.11,0,0.304752,"Missing"
W12-5108,W06-2402,0,0.0536135,"Missing"
W12-5108,E09-1057,0,0.0406034,"Missing"
W12-5108,2006.amta-papers.11,0,0.0308064,"Missing"
W12-5108,P03-1021,0,0.0303012,"Missing"
W12-5108,W10-4006,0,0.0245231,"Missing"
W12-5108,P02-1040,0,0.0840765,"Missing"
W12-5108,W09-2907,0,0.364864,"Missing"
W12-5108,2007.jeptalnrecital-long.37,0,0.0306627,"Missing"
W12-5108,2006.amta-papers.25,0,0.0550019,"Missing"
W12-5108,vintar-fiser-2008-harvesting,0,0.0542712,"Missing"
W12-5108,O04-2001,0,0.0554197,"Missing"
W13-2001,W11-1802,1,0.929598,"tand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST’13 web site. 2.1 Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al., 2008) that represents domain knowledge of NFκB proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al., 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al., 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more “real” task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the coreference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the eve"
W13-2001,P05-1022,0,0.0190745,"Date 4 Participation GE 1-2-3 EVEX BioNLP-ST’13 organization BioNLP-ST’13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were TEES-2.1 • • BioSEM • NCBI • DlutNLP • HDS 4NLP • NICTA • USheff • UZH • HCMUS • • • • • CG PC GRO GRN BB 1 - 2-3 • • NaCTeM • • NCBI • RelAgent • UET-NII • ISI • OSEE U. of Ljubljana K.U. Leuven IRISATexMex Boun • • • • • • • • • • • • • • LIPN • LIMSI • • • Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTur"
W13-2001,W11-0214,1,0.829223,"t Error Rate (Makhoul et al., 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures. Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al., 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al., 2011b; Ohta et al., 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities. 2.4 Gene Regulation Network in Bacteria (GRN) 2.6 Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats wi"
W13-2001,W12-4304,1,0.844492,"Missing"
W13-2001,W09-1401,1,0.905104,"Missing"
W13-2001,W11-1801,1,0.712795,"Missing"
W13-2001,J08-1002,0,\N,Missing
W13-2001,W11-0215,1,\N,Missing
W13-2001,W11-1810,1,\N,Missing
W13-2321,J93-2004,0,0.0481147,"n all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annot"
W13-2321,W09-3002,0,0.126125,"speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annotation guidelines for nam"
W13-2321,I11-1142,1,0.822824,"Leixa , Olivier Galibertγ , Pierre Zweigenbaumα . α LIMSI–CNRS β Universit´e Paris-Sud γ LNE δ LPP, Universit´e Sorbonne Nouvelle  ELDA {rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr leixa@elda.org, olivier.galibert@lne.fr 2011),1 and which we used in contrastive studies of news texts in French (Rosset et al., 2012). We want to rely on the same named entity definitions for studies on two types of data we did not cover: parliament debates (Europarl corpus) and regional, contemporary written news (L’Est R´epublicain), both in French. To help the annotation process we could reuse our system (Dinarelli and Rosset, 2011), but needed first to examine whether a system trained on one type of text (our first Broadcast News data) could be used to produce a useful pre-annotation for different types of text (our two corpora). We therefore set up the present study in which we aim to answer the following questions linked to this point and to related annotation issues: Abstract Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design incl"
W13-2321,W10-1807,0,0.523869,"want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotated manually. Neverthel"
W13-2321,C12-1055,1,0.796018,"ould you say that one has been easier to annotate than the other? The Europarl corpus is more difficult to annotate in the sense that the existing types and components do not always match the realities found in the corpus, either because their definitions 4. Concerning the annotation manual, are there topics that you would like to change, or correct? In the same way, which named entities caused you the most difficulties to deal with? All 8 annotators answered these questions. We summarize below what we found in their answers. 2 This feeling is supported by results about ambiguity presented in Fort et al. (2012). 172 there has been a difference between novice and expert annotators. Both groups agreed on the same difficulties, pointed at the same errors, and criticized the same entities, saying that their definitions needed to be clarified. cannot apply exactly, or because the required types and components are missing (mainly for frequencies: “five times per year”). The other half of the annotators did not feel any specific difficulties in annotating one corpus or the other. According to them, both corpora are the same in terms of register and sentence structure. 6 In this section we provide results o"
W13-2321,W09-3003,0,0.0351183,"es and components), we want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotat"
W13-2321,I11-1058,1,0.890374,"Missing"
W13-2321,W07-1516,0,0.0591516,"Missing"
W13-2321,W11-0411,1,0.821738,"eement and that full pre-annotation yields the best result. We observe that, as expected, pre-annotation leads human annotators to obtain higher consistency. Table 2: F-measure and Slot Error Rate achieved by the automatic system on each kind of annotation and on in-domain broadcast data We also computed inter-annotator agreement (IAA) for each corpus considering two groups of annotators, experts and novices. We consider that the inter-annotator agreement is somewhere between the F-measure and the standard IAA considering as markables all the units annotated by at least one of the annotators (Grouin et al., 2011). We computed Scott’s Pi (Scott, 1955), and Cohen’s Kappa (Cohen, 1960). The former considers 5 Subjective assessment An important piece of information in any annotation campaign is the feelings of the annotators about the task. This can give interesting clues about the expected quality of their work and on the usefulness of the pre-annotation step. We asked the annotators a few questions concerning several features of this project, such as the annotation 171 1 5.1.1 Press: Cohen&apos;s kappa Press: F-measure Europarl: Cohen&apos;s kappa Europarl: F-measure 0.9 Most of the annotators preferred the corpo"
W13-2321,W12-3606,1,0.874172,"Missing"
W13-2503,C02-2020,1,0.839754,"teration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is suffic"
W13-2503,P04-1067,0,0.0882668,"Missing"
W13-2503,hazem-morin-2012-adaptive,0,0.0527117,"Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as ‘anchor points’. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the sour"
W13-2503,C10-1070,0,0.0902848,"(2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only tra"
W13-2503,C10-1073,0,0.0383228,"Missing"
W13-2503,W11-1205,0,0.0127304,"ilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors. 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. A large number of Word Sense Disambiguation WSD techniques were previously proposed in the literature. The most popular ones are those that compute semantic similarity with the help of existing thesauri such as WordNet (Fellbaum, 1998). This resource groups English words into sets of synonyms called synsets, provides short, general definitions and records various semantic relations (hypernymy, meronymy, etc.) between these synonym sets. This thesaurus has been applied to many tasks relying on"
W13-2503,J03-1002,0,0.00648395,"rd approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure. The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. On two specialized French-English comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Bilingual lexicons play a vital role in many Natural Language Processing applications such as Machine Translation (Och and Ney, 2003) or CrossLanguage Information Retrieval (Shi, 2009). Research on lexical extraction from multilingual corpora have largely focused on parallel corpora. The scarcity of such corpora in particular for specialized domains and for language pairs not involving English pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora are comprised of texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The main work in this research area could be seen as an extension of Harris"
W13-2503,2009.mtsummit-posters.14,0,0.0925898,"ainder of the paper is organized as follows: Section 2 presents the standard approach and recalls in some details previous work addressing the task of bilingual lexicon extraction from comparable corpora. In section 3 we present our context disambiguation process. Before concluding and presenting directions for future work, we describe in section 4 the experimental protocol we followed and discuss the obtained results. 2 Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as ‘anchor points’. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilin"
W13-2503,P95-1050,0,0.14801,"Missing"
W13-2503,P99-1067,0,0.0734271,"of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 17 Word to be translated (source language) Bilingual Dictionary WordNet Context Vectors (Target language) Building Context Vector Context vector Disambiguated Context vector Translated Context vector Figure 1: Overall architecture of the lexical extraction approach 3 Context Vector Disambiguation context vector. There is a relatively large number of word-to-word similarity metrics that were previously proposed in the literature, ranging from path-length measures computed on semantic networks, to metrics based on"
W13-2503,P94-1019,0,\N,Missing
W14-6301,W14-6306,0,0.0610287,"Missing"
W14-6301,W14-6304,0,0.0536407,"Missing"
W14-6301,W14-6308,0,0.035181,"Missing"
W15-3411,W15-3413,0,0.0497045,"ld possibly provide more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold sta"
W15-3411,W15-3412,0,0.0703684,"more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold standard. Systems were requ"
W16-3001,J08-1002,0,\N,Missing
W16-3001,W13-2002,0,\N,Missing
W16-3001,W13-2007,0,\N,Missing
W16-3001,P05-1045,0,\N,Missing
W16-3001,H05-1091,0,\N,Missing
W16-3001,W12-3621,1,\N,Missing
W16-5107,N13-1073,0,0.0169323,"), where the suicide must be coded by taking into account the specific circumstance that lead to it (here, strangulation). In such cases we kept the input statements separate. The most generic statement (e.g. suicide) was considered inconclusive and did not receive a code assignment while the ‘head’ statement (e.g. ligature strangulation, which provided the defining information for code assignment) was aligned with the output code. To align the statements, we used a model originally intended for bilingual word alignment in parallel sentences: a log-linear reparameterization of the IBM2 model (Dyer et al., 2013). The alignments were produced from the computed clauses without allowing for null alignment in order to satisfy our constraints, and with a Dirichlet prior to favor diagonal alignments. The model underperforms on multi-word segments as it relies on co-occurrence counts of raw and computed causes, which are very sparse. To overcome this problem, both causes were pre-processed by removing stopwords and applying stemming. Next, the Damerau-Levenshtein distance between two segments was linearly combined with the occurrence count to act as a prior on the alignment probabilities. 4 Results We appli"
W16-5107,W11-1801,0,0.0175283,". It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), monitoring disease activity (Koopman et al., 2015a), or predicting cancer incidence through retrospective and prospective studies (Bedford et al., 2014). Nevertheless, use"
W16-5107,W07-1013,0,0.573981,"ed the coder’s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), moni"
W16-5109,W06-1632,0,0.0262479,"ly a small piece of preprocessing in a larger natural language processing pipeline whose adaptation to a given clinical task generally already requires some human annotation effort, a supervised method requiring more human annotation is not desirable. We therefore endeavored to investigate methods which require no human annotation to perform this task. 2 Related work The problem of &lt;EOL> classification seems to be little explored in natural language processing (NLP), and the section that Smith (2011) dedicates to segmentation does not mention it. Some NLP research (Sporleder and Lapata, 2006; Filippova and Strube, 2006) has addressed paragraph segmentation from a quite different perspective: given a text split into sentences, determine paragraph boundaries. However, they started from texts where sentence boundaries were given, and the input texts were assumed to be “clean” from the point of view of &lt;EOL> marks (i.e., either sentence boundaries are deterministically marked by &lt;EOL>s or by XML markup). A few papers on clinical NLP have recently addressed it and proposed methods based upon heuristics and knowledge about the usual format of the texts (Zweigenbaum and Grouin, 2014) or supervised machine learning"
W16-5109,tepper-etal-2012-statistical,0,0.0278098,"of MA or MAB to an automatically detected +wrap subset. A most interesting perspective is the study of the interaction of &lt;EOL> classification with sentence segmentation. On the one hand, as suggested by one of the reviewers, sentence segmentation might be used as a baseline for &lt;EOL> classification, all the more in texts where paragraphs typically end with a period. On the other hand, the study of the impact of &lt;EOL> classification on sentence segmentation is one of the motivations for the present work, and constitutes our next step. As suggested by another reviewer, section title detection (Tepper et al., 2012) can also help paragraph segmentation. As a matter of fact, it was part of the heuristics used in (Zweigenbaum and Grouin, 2014), where it helped to avoid pasting a title (possibly with no final period) to the next line. 6 Conclusion We presented a method which uses self-training and co-training to classify &lt;EOL>s with no human annotation, based on available token and line length features. It achieves high &lt;EOL> classification Fmeasures on i2b2 clinical texts which incur paragraph folding, and can also detect texts which are not subject to this phenomenon. In future work, we plan to test MB as"
W16-5109,D14-1187,0,0.0139509,"lows them to re-train the classifier then to iterate until convergence, according to the expectation-maximization algorithm (EM). The method we propose below to train an &lt;EOL> classifier is related to this principle, but does not need an initial human annotation. Elkan and Noto (2008) propose a non-iterative method for this purpose, but it assumes that the annotated examples are drawn randomly from the positive examples, which is not the case in our situation. Yet another path would consist in considering the &lt;EOL> annotations as ambiguous (both &lt;SP> and &lt;TUB>) and in applying the methods of (Wisniewski et al., 2014). However, this would create a systematic dependency between these two classes in these annotations, a situation in which learning is not guaranteed (Bordes et al., 2010). 3 3.1 Material and methods Corpora We target here clinical texts with a complex mixture of formats. However, we also test our methods on more controlled corpora which we have in several formats. The controlled-format corpora are made of six plain text e-books by Jules Verne in four languages from the Gutenberg project (http://www. gutenberg.net), which we split into chapters. Each of their paragraphs is split into multiple l"
W16-5112,S12-1008,0,0.0184955,"ssing will occur for that document pair and the information is effectively lost for the information extraction process. In this paper we present and evaluate the text reuse detection tool in isolation and discuss its strengths and weaknesses. 2 Background A traditional approach for the detection of verbatim copying1 is to compute the similarity between the source and target text as the proportion of substring sequences that the two texts have in common. These substring sequences can either be defined as character n-grams (Cohen et al., 2013), words (Wrenn et al., 2010), or word n-grams (Adeel Nawab et al., 2012). These methods are mainly based on fingerprinting and hashing techniques, i.e. the documents are represented as sets of unique digital signatures, and are highly precise but are not robust to much surface variation. Some methods, however, are adapted to deal with insertions and deletion of words or characters. For example, as an extension of the ‘longest common substring’ algorithm (Gusfield, 1997), which calculated text similarity as the length of the longest continuous sequence of characters normalized by the sum of the document lengths, Wise et al. (1996) developed the ‘Greedy String Tilin"
W16-5112,P02-1020,0,0.0151576,"f subsequent text mining processes, such as encoding errors, missing files, OCR errors, etc. One interesting issue in cumulatively constructed text corpora is the problem of ‘text reuse’. Text reuse is defined here as the intentional or unintentional reusing of existing text (fragments) to create a new text, for example, by copy-pasting text fragments from one document to fit into a new document; or by adapting a report and saving both the old and the new version as separate documents. Text reuse is a complex phenomenon which has been studied in multiple settings such as newspaper journalism (Clough et al., 2002), programming code (Ohno and Murao, 2009), the analysis of text reuse in blogs and web pages (Abdel Hamid et al., 2009), etc. It is quite prevalent in the medical domain (Wrenn et al., 2010) and often seen as a negative factor: Cohen et al. (2013) found that copy-pasting practices in US hospitals have a significant negative impact on the accuracy of the subsequent text mining systems on the clinical notes. However, when text reuse is considered as a diachronic phenomenon, it has some interesting aspects. By identifying which text (fragments) have been reused we can follow the flow of informati"
W16-5112,W15-2603,1,0.887071,"Missing"
W16-6113,W11-1801,0,0.0804193,"Missing"
W16-6113,W07-1013,0,0.119201,"sease classes in the ICD-10 classification) best represent the contents of a given text (e.g., a patient discharge summary). It can be decomposed into the detection of text mentions of biomedical concepts of the suitable types (entity recognition) and the determination of the target concepts (concept normalization) which best represent the text mentions in the context of the source text and the given use case. The state of the art of biomedical entity recognition and biomedical concept normalization has been established and published in a number of shared tasks which addressed clinical texts (Pestian et al., 2007; Uzuner et al., 2007; Uzuner et al., 2011; Suominen et al., 2013), biomedical literature (Kim et al., 2011; N´edellec et al., 2015), sometimes in multiple languages (Suominen et al., 2013; N´ev´eol et al., 2016). This paper focuses on ICD-10 coding. ICD coding has been studied in the past (e.g., as early as (Wingert et al., 1989)), but only recently has a large dataset been released for ICD-10 coding of death certificates (N´ev´eol et al., 2016). In that context, N´ev´eol et al. (2016) mention that participants in the CLEF eHealth 2016 ICD-10 coding task either used dictionary-based methods o"
W17-2343,L16-1505,1,0.831443,"ght to have specific patterns (e.g. recurrent ngrams, question roots or domain semantic labels), which make it possible to formalise rules. In our system, rules are formalised based on the semantic annotation of questions.1 For example, a rule processing the combination of S YMPTOM and F REQUENCY labels interprets the input as a query on the frequency of a symptom. Accordingly, the VP agent will answer with a fixed type 1 4 Data sources and preparation 4.1 The semantic labels we use encode domain data (DISEASE), miscellanea (e.g. time or quantity) and question type or tense: e.g. Q PASTYESNO (Campillos et al., 2016). Data sources We collected French language questions from books aimed at medical consultation and clinical examination (Bates and Bickley, 2014; Epstein et al., 2015), as well as resources for medical translation (Coud´e et al., 2011; Pastore, 2015).2 We also collected questions from 25 transcribed doctor-patient interactions performed by human standardized patients (i.e. actors simulating medical consultations). 4.2 Task description Example of questions Do you cough every day ? Are your parents still alive ? of reply instantiated with the corresponding data in the record. We hypothesize that"
W17-2343,W16-2922,0,0.0134934,"utational complexity). As our data were scarce, we used word vectors pretrained in a large domain corpus from the European Medicines Agency,3 which amounts to more than 16 million tokens after tokenization. Several parameter values were tested: window size of 2, 4, 6, 8 and 10, vector dimension of 50, 100 and 300, use of 3-grams or 3-character-grams, number of negative samples (5, 10 or 20), learning rate (0.1 and 0.05) and sub-sampling threshold (1e-3 and 1e-4). We only tested the skip-gram architecture since it has 3 http://opus.lingfil.uu.se/EMEA.php/ been observed to yield better results (Chiu et al., 2016). The minimum word count was fixed to 1, given the scarcity of our labelled data. We did not use semantic annotation to create word vectors. 6 Results and discussion Table 3 breaks down our results (reported as F1score) in the training set (top of the table) with different parameter combinations and non-neural classifiers. The weighted average F1-score was computed based upon both F1-scores of classifying RBPS and OPS types of questions. The best combinations of parameters found in the training set were applied to the test set; their results are placed at the bottom of the table. Note that a b"
W17-2343,J07-1005,0,0.133932,"assifier achieved promising results without it. 1 Introduction Previous work on question classification has mostly been undertaken within the framework of question answering (hereafter, QA) tasks, where classification is but one step of the overall process. Other steps are linguistic/semantic question processing, answer retrieval and generation by integrating data; indeed, these make QA a different task to that of standard information retrieval. Biomedical QA (Zweigenbaum, 2003) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question typ"
W17-2343,W01-1203,0,0.149194,"be handled by a different strategy. What is needed in this context is a way to determine whether a given question should be transmitted to the rule-based system or to a fallback strategy. This is the goal of the present research, which is tackled as a binary classification task. Figure 1 is a schema of the processing steps we address in this work (note that we do not represent other stages such as dialogue management). Guiding the processing of input questions is a common step in QA systems. Questions may be filtered through an upfront classifier based on machine-learning techniques, parsing (Hermjakob, 2001), regular expressions and syntactic rules, or hybrid methods (Lally et al., 2012). To achieve that, a question analysis process might precede, which may involve detecting lexical answer types, question targets or the question focus. Our VP system relies on named entity recognition and domain semantic labels in the question analysis. The results we report seem to show that leveraging this semantic information was beneficial for the classification step. We also tested a neural method without the semantic information, and indeed did not achieve the best performance (despite having promising resul"
W17-2343,W15-0611,0,0.413282,"03) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question types need accurate classification to search the data in the clinical record. In this context, question classification has aimed at identifying detailed question types (Jaffe et al., 2015). In contrast, we consider a situation where we already have a rule-based question analysis system that classifies questions according to the semantic function or content (in order to restrict the search for data in the patient record and reply coherently). This strategy works well a"
W17-2343,D14-1181,0,0.00501679,"presentations of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type"
W17-2343,W06-1303,0,0.0482284,"antic level (Slaughter et al., 2006; Roberts and Demner-Fushman, 2016). We refer to (Athenikos and Han, 2010; Neves and Leser, 2015), respectively, for state-of-the-art reviews of QA for biomedicine and biology. Questions are generally classified into Yes/No, Factoid/List and Definition/summary. Questions to a virtual patient have been addressed by mapping the user input to a set of predefined questions (Jaffe et al., 2015), as is done in a large subset of recent general-domain QA work which queries lists of frequently asked questions (FAQs) and returns their associated predetermined answers (Leuski et al., 2006; Nakov et al., 2016). Our setting is different in two ways: first, we do not rely on a FAQ but instead generate answers based on the question and on the contents of the virtual patient’s record; second, we already perform fine-grained question classification with a rule-based system (Campillos et al., 2015), and aim to determine whether a given question should be referred to this rule-based strategy or deserves to be handled by a fallback strategy. 2.2 Approaches Across the mentioned tasks, machine-learning methods for classifying questions range from hierarchical classifiers (Li and Roth, 20"
W17-2343,C02-1150,0,0.235043,"ki et al., 2006; Nakov et al., 2016). Our setting is different in two ways: first, we do not rely on a FAQ but instead generate answers based on the question and on the contents of the virtual patient’s record; second, we already perform fine-grained question classification with a rule-based system (Campillos et al., 2015), and aim to determine whether a given question should be referred to this rule-based strategy or deserves to be handled by a fallback strategy. 2.2 Approaches Across the mentioned tasks, machine-learning methods for classifying questions range from hierarchical classifiers (Li and Roth, 2002) to linear support vector machines (SVM, hereafter) (Zhang and Lee, 2003). The benefit of using semantic features to improve question classification varies across experiments. For example, (Roberts et al., 2014a) reported improvements when classifying a dataset of consumer-related topics. They used an SVM with combinations of features including semantic information, namely Unified MediR (Bodenreider, 2004) Secal Language System mantic Types and Concept Unique Identifiers. For their part, (Patrick and Li, 2012) used SNOMED categories. They reported improvements in classification through models"
W17-2343,J07-1004,0,0.0238553,"Missing"
W17-2343,D15-1279,0,0.0260311,"ing to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type is not feasible in the long term. Thus, we need a classifier to dis"
W17-2343,S16-1083,0,0.0725338,"Missing"
W17-2343,D14-1162,0,0.0816632,"type of the semantic information used in each task might explain these results. The impact of using semantic features is a point we explore in the present work in the context of questions to a virtual patient. Neural network representations and classifiers are more and more applied to natural language processing (Bengio et al., 2003; Collobert et al., 2011). Word embeddings—i.e. vector representations of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This"
W17-2343,W14-3405,0,0.341942,". 1 Introduction Previous work on question classification has mostly been undertaken within the framework of question answering (hereafter, QA) tasks, where classification is but one step of the overall process. Other steps are linguistic/semantic question processing, answer retrieval and generation by integrating data; indeed, these make QA a different task to that of standard information retrieval. Biomedical QA (Zweigenbaum, 2003) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question types need accurate classification to se"
W17-2343,W16-6106,0,0.0267548,"ng rate; SAMP: sampling threshold Table 4: Results of the best tested models (neural approach) fier) and also used a neural model to classify questions. However, our results agree with the observation that restricted-domain QA is less affected by data-intensive methods, but depend on refined language processing methods (Moll´a and Vicedo, 2007)—in this type of system, accurate semantic annotation. On the other hand, the neural method seems promising in this kind of classification task, and how to use domain semantic information with it requires further exploration, in line with current works (Yu et al., 2016). We also need to pretrain vectors on domain data of different nature (e.g. clinical records) to confirm our results. Finally, other methods for computing vector representations of sentences deserve to be explored. 7 Conclusions For the task of optimizing question processing in a VP natural language system, we reported the improvement of using the semantic information in the question analysis step as a feature for question classification. This is likely due to the idiosyncrasy of our task, where the dialogue system makes use of semantic rules for processing input questions. We are nonetheless"
W17-2343,N16-1178,0,0.0131972,"ns of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type is not feasible in t"
W17-2510,N10-1063,0,0.221619,", one must find a way to compare sentences in two different languages, for instance by first translating one language into the other. Another issue is sentence similarity: how do we define and calculate sentence similarity? The last issue is the existence of too many possible sentence combinations: theoretically, for each sentence in a source monolingual corpus, every sentence in the target monolingual corpus could be used to generate a source-target sentence pair for subsequent parallel sentence identification, which would create a quadratic number of candidate sentence pairs. Previous work (Smith et al., 2010; Munteanu and Marcu, 2005) on parallel sentence extraction from comparable corpora has used external clues for this purpose. (Smith et al., 2010) bootstrapped the process with document-level sentence alignment. (Munteanu and Marcu, 2005) leveraged the publication date of newspaper articles to trim down the number of candidate sentence pairs. These selection methods are not suitable for the BUCC 2017 shared task as no meta-information is provided on the documents from which the corpus sentences are extracted. In this context, we test how similar methods fare without any metainformation. In thi"
W17-2510,P04-1023,0,0.158733,"Missing"
W17-2510,2005.mtsummit-papers.11,0,0.196707,"Missing"
W17-2510,E09-1057,0,0.044733,"Missing"
W17-2510,W15-3413,0,0.0172123,". 1 Pierre Zweigenbaum1 2 LRI, Univ. Paris-Sud, CNRS, Universit´e Paris-Saclay Orsay, France pz@limsi.fr Introduction Parallel sentences are used in many natural language processing applications, particularly for automatic terminology extraction (Lefever et al., 2009) and statistical machine translation (Koehn, 2005; Callison-Burch et al., 2004). However, such resources are scarce for many language pairs and domains. Comparable corpora are sets of texts in two or more languages that are selected according to similar specifications, but are not translations of each other (Sharoff et al., 2013; Morin et al., 2015). Nevertheless, parallel sentences, i.e., sentence pairs that are good translations of each other, can occur naturally in such corpora. Therefore many approaches have been proposed to spot parallel sentences in comparable corpora (Munteanu et al., 2004; Smith et al., 2010). Extracting parallel sentences from comparable monolingual corpora is a very challenging task. According to the shared task web page,1 The aim of the Building and Using Comparable Corpora (BUCC) 2017 shared task is to quantitatively evaluate competing methods for extracting parallel sentences from comparable monolingual corp"
W17-2510,N04-1034,0,0.305005,"ever et al., 2009) and statistical machine translation (Koehn, 2005; Callison-Burch et al., 2004). However, such resources are scarce for many language pairs and domains. Comparable corpora are sets of texts in two or more languages that are selected according to similar specifications, but are not translations of each other (Sharoff et al., 2013; Morin et al., 2015). Nevertheless, parallel sentences, i.e., sentence pairs that are good translations of each other, can occur naturally in such corpora. Therefore many approaches have been proposed to spot parallel sentences in comparable corpora (Munteanu et al., 2004; Smith et al., 2010). Extracting parallel sentences from comparable monolingual corpora is a very challenging task. According to the shared task web page,1 The aim of the Building and Using Comparable Corpora (BUCC) 2017 shared task is to quantitatively evaluate competing methods for extracting parallel sentences from comparable monolingual corpora, 1 https://comparable.limsi.fr/bucc2017/ bucc2017-task.html 51 Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 51–55, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics for Chinese-E"
W17-2510,J05-4003,0,0.184017,"Missing"
W17-2512,W17-2511,0,0.141723,"ts for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf"
W17-2512,W09-3109,0,0.0709534,"Missing"
W17-2512,N04-1034,0,0.216543,"detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and comparability issues. We"
W17-2512,S16-1081,0,0.426211,"llel sentence spotting task. 1 Introduction Shared tasks and the associated datasets have proved their worth as a driving force in a number of subfields of Natural Language Processing. However, very few shared tasks were organized on the topic of comparable corpora. Therefore, we endeavored to design and organize shared tasks as companions of the BUCC workshop se1 https://comparable.limsi.fr/bucc2017/ bucc2017-task.html 60 Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 60–67, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (Agirre et al., 2016), and WMT’s bilingual document alignment (Buck and Koehn, 2016). The present paper reports the actual organization of the task as a companion to the BUCC 2017 workshop. We describe the final method we used to prepare bilingual corpora in four language pairs: Chinese-English, French-English, GermanEnglish, and Russian-English (Section 2), the evaluation method (Section 3), the participants’ systems (Section 4), the results they obtained (Section 5), and conclude (Section 6). 2 formed the actual insertion after all parallel sentence pairs were thus processed. Additionally, a different distributi"
W17-2512,W17-2508,0,0.183214,"nsertion were then performed on each split separately. Since the training and test sets for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the J"
W17-2512,W15-3411,1,0.785307,"Missing"
W17-2512,P16-1189,0,0.0982588,"Missing"
W17-2512,P03-1010,0,0.129546,"f et al., 2015) tackled the detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and co"
W17-2512,W17-2510,1,0.72964,"he same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf.idf: LIMSI) and with a trained classifier (RALI, LIMSI)."
W17-2512,W17-2509,0,\N,Missing
W17-2512,W16-2347,0,\N,Missing
W18-1702,N07-1013,0,0.101593,"Missing"
W18-1702,C04-1194,0,0.0363861,"ee-level progressive calculation design, rebuilding networks with different configurations is even faster as it does not need to start all over again. This tool also works with other graph libraries such as igraph, NetworkX and graph-tool as a front end providing data to boost network generation speed. 1 Pierre Zweigenbaum LIMSI, CNRS, Universit´e Paris-Saclay Orsay, France pz@limsi.fr Introduction Word co-occurrence networks are widely used in graph-based natural language processing methods and applications, such as keyword extraction (Mihalcea and Tarau, 2004) and word sense discrimination (Ferret, 2004). A word co-occurrence network is a graph of word interactions representing the co-occurrence of words in a corpus. An edge can be created when two words co-occur within a sentence; these words are possibly non-adjacent, with a maximum distance (in number of words, see Section 2.2) defined by a parameter dmax (Cancho and Sol´e, 2001). In an alternate definition, an edge can be created when two words co-occur in a fixed-sized sliding window moving along the entire document or sentences (Rousseau and Vazirgiannis, 2013). Despite different methods of forming edges, the structure of the network fo"
W18-1702,W04-3252,0,0.064116,"ta within hours. And thanks to its nodes-edges-weight three-level progressive calculation design, rebuilding networks with different configurations is even faster as it does not need to start all over again. This tool also works with other graph libraries such as igraph, NetworkX and graph-tool as a front end providing data to boost network generation speed. 1 Pierre Zweigenbaum LIMSI, CNRS, Universit´e Paris-Saclay Orsay, France pz@limsi.fr Introduction Word co-occurrence networks are widely used in graph-based natural language processing methods and applications, such as keyword extraction (Mihalcea and Tarau, 2004) and word sense discrimination (Ferret, 2004). A word co-occurrence network is a graph of word interactions representing the co-occurrence of words in a corpus. An edge can be created when two words co-occur within a sentence; these words are possibly non-adjacent, with a maximum distance (in number of words, see Section 2.2) defined by a parameter dmax (Cancho and Sol´e, 2001). In an alternate definition, an edge can be created when two words co-occur in a fixed-sized sliding window moving along the entire document or sentences (Rousseau and Vazirgiannis, 2013). Despite different methods of f"
W18-1702,Q14-1019,0,0.0198886,"ion and the matrix-type representation, the comis just one of the most common ways used in ing subsection uses the random walk algorithm as graph-based natural language processing applicaan example. tions. We also support other (built-in and user3.2 Random walk defined) definitions of the weight. For instance, Random walks (Aldous and Fill, 2002) are widely when calculating the sum of co-occurrences, we used in graph-based natural language processcan assign different weights to co-occurrences acing tasks, for instance word-sense disambiguacording to the word pair distance, to make the retion (Moro et al., 2014) and text summarizasulting edge weight more sensitive to the word pair tion (Erkan and Radev, 2004; Zhu et al., 2007). distance information. The core of the random walk related algorithms For a large corpus, we may not need all edges calculation is the transition matrix P . to generate the final network. Based on the word count information from Section 2.2, we may seIn the random walk scenario, starting from an lect those nodes whose total frequency is greater initial vertex u, we cross an edge attached to u that leads to another vertex, say v (v can be u itthan or equal to min count, or the m"
