2008.amta-papers.12,P07-1005,0,0.0211714,"native in GEN(x) that is most similar to such a reference translation. In the latter case, y is often called the oracle-best (or simply oracle) translation of x. The components GEN(·), Φ, and α define a mapping from an input x to an output y ∗ through y ∗ = arg max y ∈ GEN(x) Φ(x, y) · α, (1) P where Φ(x, y)·α = j αj Φj (x, y), with j indexing the feature dimensions, is the inner product. Since y is a word-sequence, (1) is called global linear model to emphasize that the maximization is jointly over the entire sentence y, not locally over each word/phrase in y (as done in (Zens and Ney, 2006; Chan et al., 2007; Carpuat and Wu, 2007)). The learning task is to obtain the “optimal” parameter vector α from training examples, while the decoding task is to search, given an x, for the maximizer y ∗ of (1). These tasks are discussed next in Section 2.2 and Section 2.3, respectively. 2.2 Parameter Estimation Methods Given a set of training examples, the choice of α may be guided, for instance, by an explicit criterion such as maximizing, among distributions in an exponential family parameterized by Φ, the conditional log-likelihood of y i given xi . Algorithms that determine α in this manner typically opera"
2008.amta-papers.12,P07-1002,0,0.01916,"ting, the minimum error rate training algorithm (Och, 2003) has become the de facto standard in SMT systems. Smith and Eisner (2006) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g., (Liang et al., 2006; Blunsom et al., 2008)), at each iteration of training. In the latter case, they resort to a relative weak baseline for computational efficiency. For example, Liang et al. (2006) use a monotone translation system, while Blunsom et al. (2008) do not use a language model and they train the baseline system only on"
2008.amta-papers.12,P05-1033,0,0.0451893,"carry out systematic experiments on several benchmark tests for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method. 1 Introduction Recent research in statistical machine translation (SMT) has made remarkable progress by evolving from word-based translation (Brown et al., 1993), through flat phrase-based translation (Koehn et al., 2003) and hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007), to syntax-based translation (Galley et al., 2006). These systems usually contain three major components: a translation model, a word-reordering model, and a language model. In this paper, we mainly focus on improving the language model (LM). A language model constitutes a crucial component in many other tasks such as automatic speech recognition, handwriting recognition, optical character recognition, etc. It assigns a priori probabilities to word sequences. In general, we expect a low probability for an ungrammatical or implausible word sequence. Normally, a language model A"
2008.amta-papers.12,J07-2003,0,0.740407,"tematic experiments on several benchmark tests for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method. 1 Introduction Recent research in statistical machine translation (SMT) has made remarkable progress by evolving from word-based translation (Brown et al., 1993), through flat phrase-based translation (Koehn et al., 2003) and hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007), to syntax-based translation (Galley et al., 2006). These systems usually contain three major components: a translation model, a word-reordering model, and a language model. In this paper, we mainly focus on improving the language model (LM). A language model constitutes a crucial component in many other tasks such as automatic speech recognition, handwriting recognition, optical character recognition, etc. It assigns a priori probabilities to word sequences. In general, we expect a low probability for an ungrammatical or implausible word sequence. Normally, a language model A regular LM obta"
2008.amta-papers.12,W02-1001,0,0.203446,"MTA conference, Hawaii, 21-25 October 2008] language modeling has been investigated by Stolcke and Weintraub (1998), Chen et al. (2000), Kuo et al. (2002), and Roark et al. (2007) on various speech recognition tasks. It is scientifically interesting to see whether such techniques lead to improvement in an SMT task, given the substantial task differences. We do so in this paper. We investigate application of the discriminative n-gram language modeling framework of Roark et al. (2007) to a large-scale SMT task. Our discriminative language model is trained using an averaged perceptron algorithm (Collins, 2002). In our task, there are millions of training examples available, and many of them may not be beneficial due to various reasons including noisy reference translations resulting from automatic sentence-alignment of a document-aligned bilingual text corpus. Moreover, our discriminative model contains millions of features, making standard perceptron training prohibitively expensive. To address these two issues, we propose a novel data selection method that strives to obtain a comparable/better model using only a fraction of the training data. We carry out systematic experiments on a state-of-the-"
2008.amta-papers.12,P06-1121,0,0.0347731,"ts for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method. 1 Introduction Recent research in statistical machine translation (SMT) has made remarkable progress by evolving from word-based translation (Brown et al., 1993), through flat phrase-based translation (Koehn et al., 2003) and hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007), to syntax-based translation (Galley et al., 2006). These systems usually contain three major components: a translation model, a word-reordering model, and a language model. In this paper, we mainly focus on improving the language model (LM). A language model constitutes a crucial component in many other tasks such as automatic speech recognition, handwriting recognition, optical character recognition, etc. It assigns a priori probabilities to word sequences. In general, we expect a low probability for an ungrammatical or implausible word sequence. Normally, a language model A regular LM obtained through the MLE technique is task-independent"
2008.amta-papers.12,N03-1017,0,0.0122335,"leads to good models using a fraction of the training data. We carry out systematic experiments on several benchmark tests for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method. 1 Introduction Recent research in statistical machine translation (SMT) has made remarkable progress by evolving from word-based translation (Brown et al., 1993), through flat phrase-based translation (Koehn et al., 2003) and hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007), to syntax-based translation (Galley et al., 2006). These systems usually contain three major components: a translation model, a word-reordering model, and a language model. In this paper, we mainly focus on improving the language model (LM). A language model constitutes a crucial component in many other tasks such as automatic speech recognition, handwriting recognition, optical character recognition, etc. It assigns a priori probabilities to word sequences. In general, we expect a low probability for an ungrammatical or"
2008.amta-papers.12,W08-0402,1,0.880307,"Missing"
2008.amta-papers.12,P06-1096,0,0.0371272,"6) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g., (Liang et al., 2006; Blunsom et al., 2008)), at each iteration of training. In the latter case, they resort to a relative weak baseline for computational efficiency. For example, Liang et al. (2006) use a monotone translation system, while Blunsom et al. (2008) do not use a language model and they train the baseline system only on sentences whose length is less than 15 words and whose reference is reachable by the decoder. In comparison, in this pape"
2008.amta-papers.12,D07-1104,0,0.027391,"ng Data We compile a parallel dataset consisting of various corpora distributed by the Linguistic Data Consortium (LDC) for NIST MT evaluation. The selected subset has about 1M sentence pairs, corresponding to about 28M words in each language. To train the English language model, we use a 130M word subset of the English Gigaword corpus (LDC2007T07) and the English side of the parallel corpora. 5.2 Baseline Systems Training We partition the parallel text into 30 nonoverlapping sections, and train 30 baseline SMT systems. We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (MERT) (Och, 2003) to obtain wordalignments, translation models, language models, and the optimal weights for combining these models, respectively. The baseline LM is a trigram2 LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The MERT is performed on the NIST MT’03 evaluation data set. As mentioned in Section 3, we carry out the leaveone-out training for both LM and TM. When training an LM, we need to exclude one portion (among the 30 portions) of the English side of the parallel corpora. To do this efficientl"
2008.amta-papers.12,P03-1021,0,0.243413,"gorithm Figure 1 depicts the perceptron algorithm (Roark et al., 2007). Given a set of training examples, the algorithm sequentially iterates over the examples, and adjust the parameter vector α. After iterating over the training data a few times, an averaged model, defined as αavg T N 1X 1 X i αt = T N t=1 Baseline Feature: We first define a baseline feature Φ0 (x, y), to be the score assigned to y by the baseline SMT system. This score itself is often a linear combination of several models, with the relative weights among these models obtained via some minimum error rate training procedure (Och, 2003). (2) i=1 is computed and is used for testing, where αti represents the parameter vector after seeing the i-th example in the t-th iteration, N represents the size of the training set, and T is the number of iterations the perceptron algorithm runs. Discriminative n-gram Features: The count of each n-gram in y constitutes a feature. E.g., the first n-gram feature may be, Φ1 (x, y) = Count of the bigram “the of” in y. Normally, the baseline SMT system also employs an n-gram LM, and the baseline score Φ0 (x, y) is thus not independent of the n-gram features. However, we do not change the paramet"
2008.amta-papers.12,P08-1024,0,0.02736,"le, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g., (Liang et al., 2006; Blunsom et al., 2008)), at each iteration of training. In the latter case, they resort to a relative weak baseline for computational efficiency. For example, Liang et al. (2006) use a monotone translation system, while Blunsom et al. (2008) do not use a language model and they train the baseline system only on sentences whose length is less than 15 words and whose reference is reachable by the decoder. In comparison, in this paper, we focus exclusively on the language model features using an n-best reranking method. Moreover, our improvement is over a full-grown state-of-the-art hierarchical machine translation system for a known difficult task (i.e., translation from Chinese to English). While all the above discriminative models are global (meaning the optimization is over the whole output sequence), it is also possible to deco"
2008.amta-papers.12,J93-2003,0,0.0132547,"this context, we propose a novel data selection method that leads to good models using a fraction of the training data. We carry out systematic experiments on several benchmark tests for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method. 1 Introduction Recent research in statistical machine translation (SMT) has made remarkable progress by evolving from word-based translation (Brown et al., 1993), through flat phrase-based translation (Koehn et al., 2003) and hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007), to syntax-based translation (Galley et al., 2006). These systems usually contain three major components: a translation model, a word-reordering model, and a language model. In this paper, we mainly focus on improving the language model (LM). A language model constitutes a crucial component in many other tasks such as automatic speech recognition, handwriting recognition, optical character recognition, etc. It assigns a priori probabilities to word sequences. In g"
2008.amta-papers.12,D07-1007,0,0.0242414,"at is most similar to such a reference translation. In the latter case, y is often called the oracle-best (or simply oracle) translation of x. The components GEN(·), Φ, and α define a mapping from an input x to an output y ∗ through y ∗ = arg max y ∈ GEN(x) Φ(x, y) · α, (1) P where Φ(x, y)·α = j αj Φj (x, y), with j indexing the feature dimensions, is the inner product. Since y is a word-sequence, (1) is called global linear model to emphasize that the maximization is jointly over the entire sentence y, not locally over each word/phrase in y (as done in (Zens and Ney, 2006; Chan et al., 2007; Carpuat and Wu, 2007)). The learning task is to obtain the “optimal” parameter vector α from training examples, while the decoding task is to search, given an x, for the maximizer y ∗ of (1). These tasks are discussed next in Section 2.2 and Section 2.3, respectively. 2.2 Parameter Estimation Methods Given a set of training examples, the choice of α may be guided, for instance, by an explicit criterion such as maximizing, among distributions in an exponential family parameterized by Φ, the conditional log-likelihood of y i given xi . Algorithms that determine α in this manner typically operate in batch mode—they r"
2008.amta-papers.12,P02-1040,0,0.0798058,"his leave-one-out training for the LM component but not the acoustic models, we do so for both the language and translation models. This is because both the TM and LM in an SMT system are non-parametric and equally susceptible to over-fitting. In a large-scale task like statistical machine translation, Step 2 is the most time-consuming step, as millions of parallel sentences need to be decoded, each taking several CPU-seconds. In Step 3, we find the oracle translation in each N best list by using an automatic sentence-level metric of similarity to the reference translation, specifically BLEU (Papineni et al., 2002).1 The oracle translation is used to provide supervision during discriminative training. Step 4 is optional, and we discuss its benefits in Section 4. The details of Step 5 have already been discussed in Section 2. Discriminative LM Training for SMT The discriminative LM reranking framework of Section 2 is quite general and may be applied in many language modeling tasks. In this section, we discuss how this framework is applied in SMT. The training for our discriminative LM involves five major steps: 4 Data Selection for Discriminative LMs After decoding (Step 2) and oracle-finding (Step 3), w"
2008.amta-papers.12,N04-1023,0,0.0637161,"experimental results. In the context of an SMT task, discriminative training methods have been applied both in a smallscale setting (i.e, finding optimal weights among a small set of generative models) and in a large-scale setting (i.e., training optimal weights for millions of features). In the small-scale setting, the minimum error rate training algorithm (Och, 2003) has become the de facto standard in SMT systems. Smith and Eisner (2006) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g.,"
2008.amta-papers.12,P06-2101,0,0.0386192,"0 0.8 1.4 1.2 77.8 43.5 24.7 14.2 77.7 43.6 25.0 14.6 -0.1 0.2 1.2 2.8 79.7 42.6 23.3 12.8 79.9 43.4 24.0 13.5 0.3 1.9 3.0 5.5 slightly improves the model quality as shown in our experimental results. In the context of an SMT task, discriminative training methods have been applied both in a smallscale setting (i.e, finding optimal weights among a small set of generative models) and in a large-scale setting (i.e., training optimal weights for millions of features). In the small-scale setting, the minimum error rate training algorithm (Och, 2003) has become the de facto standard in SMT systems. Smith and Eisner (2006) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Li"
2008.amta-papers.12,N06-1012,0,0.0426976,"l features. 2.3.2 Reranking as Decoding Given the feature- and the parameter-vectors, the total score assigned to an output y ∈ GEN(x) for a given input x is X S(x, y) = βΦ0 (x, y) + αj Φj (x, y) , (3) j∈[1,F ] where β is the weight for the baseline feature and F is the number of discriminative n-gram features. To 135 [8th AMTA conference, Hawaii, 21-25 October 2008] find the optimal weight β for the baseline feature, one could simply treat Φ0 as a feature in the discriminative language model (DLM) and set the value of β via the perceptron algorithm. This, however, may lead to under-training (Sutton et al., 2006) of the discriminative n-gram features in the DLM: the baseline feature is strongly indicative of the overall goodness of y for x, relative to any single discriminative n-gram feature which indicates the local goodness of y. Therefore, we use a fixed value for β during the training of the DLM, as suggested by Roark et al. (2007). The decoding task therefore is to search for the y that maximizes S(x, y) in (1). Figure 2 illustrates our language model reranking algorithm: it performs a linear scan of the N -best list and maintains the best variant y ∗ in terms of S(x, ·). 3 system that excluded"
2008.amta-papers.12,P06-1091,0,0.0178355,"s). In the small-scale setting, the minimum error rate training algorithm (Och, 2003) has become the de facto standard in SMT systems. Smith and Eisner (2006) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g., (Liang et al., 2006; Blunsom et al., 2008)), at each iteration of training. In the latter case, they resort to a relative weak baseline for computational efficiency. For example, Liang et al. (2006) use a monotone translation system, while Blunsom et al. (2008) do not use a language model and they train"
2008.amta-papers.12,D07-1080,0,0.0149709,"ed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whether using a fixed n-best (e.g., Watanabe et al. (2007)) or running an endto-end decoding (e.g., (Liang et al., 2006; Blunsom et al., 2008)), at each iteration of training. In the latter case, they resort to a relative weak baseline for computational efficiency. For example, Liang et al. (2006) use a monotone translation system, while Blunsom et al. (2008) do not use a language model and they train the baseline system only on sentences whose length is less than 15 words and whose reference is reachable by the decoder. In comparison, in this paper, we focus exclusively"
2008.amta-papers.12,W06-3108,0,0.020119,"l human or the alternative in GEN(x) that is most similar to such a reference translation. In the latter case, y is often called the oracle-best (or simply oracle) translation of x. The components GEN(·), Φ, and α define a mapping from an input x to an output y ∗ through y ∗ = arg max y ∈ GEN(x) Φ(x, y) · α, (1) P where Φ(x, y)·α = j αj Φj (x, y), with j indexing the feature dimensions, is the inner product. Since y is a word-sequence, (1) is called global linear model to emphasize that the maximization is jointly over the entire sentence y, not locally over each word/phrase in y (as done in (Zens and Ney, 2006; Chan et al., 2007; Carpuat and Wu, 2007)). The learning task is to obtain the “optimal” parameter vector α from training examples, while the decoding task is to search, given an x, for the maximizer y ∗ of (1). These tasks are discussed next in Section 2.2 and Section 2.3, respectively. 2.2 Parameter Estimation Methods Given a set of training examples, the choice of α may be guided, for instance, by an explicit criterion such as maximizing, among distributions in an exponential family parameterized by Φ, the conditional log-likelihood of y i given xi . Algorithms that determine α in this man"
2008.amta-papers.12,D07-1055,0,0.0124724,".7 42.6 23.3 12.8 79.9 43.4 24.0 13.5 0.3 1.9 3.0 5.5 slightly improves the model quality as shown in our experimental results. In the context of an SMT task, discriminative training methods have been applied both in a smallscale setting (i.e, finding optimal weights among a small set of generative models) and in a large-scale setting (i.e., training optimal weights for millions of features). In the small-scale setting, the minimum error rate training algorithm (Och, 2003) has become the de facto standard in SMT systems. Smith and Eisner (2006) propose an annealed minimum risk approach, while Zens et al. (2007) give a systematic experimental comparison for different training criteria. Shen et al. (2004) use perceptron-inspired algorithms to tune weights for tens of features, and the model is used to rerank a N -best as we have done here. Large-scale discriminative training has been attempted by several groups. They differ in the features used, for example, reordering features (Tillmann and Zhang, 2006; Chang and Toutanova, 2007), translation model features (Blunsom et al., 2008), and both translation and language models features (Liang et al., 2006; Watanabe et al., 2007). They also differ in whethe"
2012.amta-papers.3,D10-1061,0,0.0357829,"Missing"
2012.amta-papers.3,J07-2003,0,0.058508,"(10) so that X hx − m0i , x − m0i i x∈Ci = X hx − mi + ∆i , x − mi + ∆i i x∈Ci = X (hx − mi , x − mi i + h∆i , ∆i i x∈Ci +2h∆i , x − mi i) (11) Denote Bi = P h∆i , ∆i i, which is easy to compute, and let Si = hx − mi , x − mi i, then x∈Ci 2 X 1 0 S |Ci0 |i i=1  2 X X 1  hx − mi , x − mi i+ = 0 |Ci | i=1 x∈Ci  X h∆i , x − mi i + Ai + |Ci |Bi  2 tr{Sw0 } = 5 Experimental Results In this section we report our experimental results on the NIST Urdu-English and Chinese-English MT tasks. We used the open source translation software Joshua (Li, 2009) which implemented the Hiero translation model (Chiang, 2007), and used BLEU as the evaluation metric. We experimented on both the human-labeled parallel corpus(supervised data) and the “hallucinated” parallel corpus(semisupervised data) created by the “round-trip” method introduced in section 3. x∈Ci  2 X 1 X = hx − mi , x − mi i+ |Ci0 | i=1 x∈Ci 2h∆i , X  (x − mi )i + Ai + |Ci |Bi  5.1 Datasets x∈Ci = 2 X i=1  1 X hx − mi , x − mi i+ |Ci0 | 2h∆i , x∈Ci X  x − |Ci |mi )i + Ai + |Ci |Bi  x∈Ci  2 X X 1  = Si + 2h∆i , x − |Ci |mi )i+ 0 |Ci | i=1 x∈Ci Ai + |Ci |Bi ) 2 X 1 = (Si + Ai + |Ci |Bi ) |Ci0 | Now that Q(D) can be updated online, it is na"
2012.amta-papers.3,W02-1001,0,0.0316905,"rue outputs. Of course, to derive the monolingual rules, we need an existing parallel corpus to extract the set of bilingual rules. It can be seen that both semi-supervised training methods require only the target side samples y ∈ Y, and a set of confusions CON(y) can be generated to mimic the true confusions GEN(x). We then make use of the CON(y) together with the true outputs y for discriminative raining. In our work presented in this paper, we use the nbest lists translated from the hallucinated inputs by the baseline system as the confusion set CON(y), and we use the perceptron algorithm (Collins, 2002) for n-best list re-ranking, similar to the framework proposed in (Li, 2008). 4 Selective Sampling for Training As mentioned in section 1, facing vast amounts of data we want to prioritize good samples and discard those which are harmful to discriminative training. In this section, we propose a criterion for selecting samples which makes the training more effective. Discriminative training essentially aims to reward features indicating good translations and penalize those indicating bad ones. A good training set therefore should meet the following conditions: 1. High corpus-level oracle score:"
2012.amta-papers.3,N09-1047,0,0.0558362,"Missing"
2012.amta-papers.3,P09-1021,0,0.392036,"Missing"
2012.amta-papers.3,D11-1085,1,0.881934,"Missing"
2012.amta-papers.3,2008.amta-papers.12,1,0.872545,"Missing"
2012.amta-papers.3,W09-0424,1,0.891691,"Missing"
2012.amta-papers.3,C10-2075,1,0.861746,"Missing"
2012.amta-papers.3,P06-1096,0,0.0614642,"Missing"
2012.amta-papers.3,P03-1021,0,0.175081,"e for training and bad ones harmful to the training. How to select training samples from vast amount of data can greatly affect the training performance. In this paper we propose a method for selecting samples that are most suitable for discriminative training according to a criterion measuring the dataset quality. Our experimental results show that by adding samples to the training set selectively, we are able to exceed the performance of system trained with the same amount of samples selected randomly. 1 Introduction Discriminative training methods have been successfully applied to MT tasks(Och, 2003; Shen, 2004; Liang, 2006; Li, 2008). Compared with generative training methods, discriminative training can easily incorporate task-specific knowledge represented by features, and it aims to directly minimize the specific task error instead of maximizing the likelihood. To take the advantage of discriminative training methods, people developed rich feature sets which usually include thousands or even millions of features. In order to reach reliable parameter estimations for so many features, discriminative training for MT must be scaled to large amount of training data. (Li, 2008) proposed a"
2012.amta-papers.3,1993.mtsummit-1.24,0,0.193829,"Missing"
2012.amta-papers.3,P04-1007,0,0.0855505,"Missing"
2012.amta-papers.3,N04-1023,0,0.0837295,"Missing"
2012.amta-papers.3,2006.amta-papers.25,0,0.0786887,"Missing"
2013.iwslt-papers.14,2005.iwslt-1.20,0,0.0595346,"er, and a host of factors that alter how an individual speaks (such as heartrate, stress, emotional state). Machine translation accuracy is affected by different factors, such as domain (e.g., newswire, medical, SMS, speech), register, and the typological differences between the languages. Because these technologies are imperfect themselves, their inaccuracies tend to multiply when they are chained together in the task of speech translation. Cross-lingual speech applications are typically built by combining speech recognition and machine translation systems, each trained on disparate datasets [1, 2]. The recognizer makes mistakes, passing text to the MT system with vastly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful f"
2013.iwslt-papers.14,D08-1027,0,0.0480385,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,D09-1030,1,0.309927,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,N10-1024,1,0.431154,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,N12-1006,1,0.287152,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,P11-1122,1,0.487136,"rily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceeded 25 words, it was split on the next utterance boundary. We pr"
2013.iwslt-papers.14,W12-3152,1,0.743004,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,2005.mtsummit-papers.11,0,0.0360278,"d practice, we took steps to deter wholesale use of automated translation services by our translators. • Utterances were presented as images rather than text; this prevented cutting and pasting into online translation services.5 • We obtained translations from Google Translate for the utterances before presenting them to workers. HITs which had a small edit distance from these translations were manually reviewed and rejected if they were too similar (in particular, if they contained many of the same errors). • We also included four consecutive short sentences from the Europarl parallel corpus [17] in each HIT. HITs which had low overlap with the reference translations of these sentences were manually reviewed and rejected if they were of low quality. We obtained four redundant translations of sixty randomly chosen conversations from the Fisher corpus. In total, 115 workers completed 2463 HITs, producing 46,324 utterance-level translations and a little less than half a million words. 2.3. Selection of Preferred Translators We then extended a strategy devised by [16] to select highquality translators from the first round of translations. We designed a second-pass HIT which was used to ra"
2013.iwslt-papers.14,W13-2226,1,0.868991,"performance of the MT system, and we report experiments varying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty •"
2013.iwslt-papers.14,J07-2003,0,0.0593645,"arying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Inter"
2013.iwslt-papers.14,P08-1115,0,0.0451524,"we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Interface Transcript 1-best Lattice Oracle Path Euro 41.8 24.3 32.1 LDC 58.7 35.4 37.1 46.2 ASR 54.6 34.7 35.9 44.3 LDC +ASR 58.7 35.5 36.8 46.3 Table 4: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems. Training set Interface Tra"
2013.iwslt-papers.14,N12-1047,0,0.0637002,"isher Train, as described above. • ASR. An in-domain model trained on pairs of Spanish ASR outputs and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Trans"
2013.iwslt-papers.14,P02-1040,0,0.106207,"and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Transcript 1-best Lattice Reference 1-best → MT Lattice → MT 1-best → Google sí hablar de cuáles y cosas"
2014.iwslt-papers.13,2005.iwslt-1.20,0,0.0317057,"stems for spontaneous, conversational, human-human speech. In contrast to machine directed or scripted conversations (broadcast news), most conversational speech has by nature, variability in recording environment and vocal registers and a high number of disfluencies and out-of-vocabulary words. It also exhibits difficult challenges associated with code switching and regional dialects. This directly relates to an increase of difficulty for both ASR and SMT systems. Since SLT systems are generally built by feeding the output of the ASR system to an SMT system, each trained on separate datasets [1, 2], errors produced by the systems compound. With respect to Egyptian Arabic specifically, unscripted, spontaneous, telephone conversations have been available through the Callhome Egyptian Arabic corpus (speech and transcripts) since 1997. However, since this dataset did not come with translations for the transcriptions in Arabic, researchers had to resort to using out-of-domain data to train the SMT systems. Transcripts for spontaneous conversations (speech), vary significantly from transcripts for scripted conversations and informal written conversations (web, forum, SMS, chat). To bridge thi"
2014.iwslt-papers.13,N10-1024,1,0.808571,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D08-1027,0,0.0103059,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D09-1030,1,0.792712,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,P11-1122,1,0.929626,"ntional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each utterance in the corpus contains channel and segment information. These were inco"
2021.mtsummit-research.1,N19-1388,0,0.0193293,"rs. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et"
2021.mtsummit-research.1,D19-1165,0,0.0164252,"the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-lea"
2021.mtsummit-research.1,D19-1146,0,0.0180657,"al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-learn parameter initialization for child models using trained-high resource parent models for this task. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 1 Figure 1: The multi-arm bandit agents’ (MAB) interface with the NMT system. In this paper, we build upon the framework for learning curricula introduced in Kumar et al. (2019) and attempt to alleviate the problem of observation sparsity by learning more robust policies from multiple training runs. We use contextual multi-arm b"
2021.mtsummit-research.1,P13-2119,0,0.0253054,"learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual"
2021.mtsummit-research.1,C16-1299,0,0.0215611,"can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While stan"
2021.mtsummit-research.1,D19-1632,1,0.896888,"Missing"
2021.mtsummit-research.1,Q17-1024,0,0.0154309,"he right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph e"
2021.mtsummit-research.1,N19-1208,1,0.914862,"gence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how i"
2021.mtsummit-research.1,2015.iwslt-evaluation.11,0,0.049697,"ranslation system using contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an ext"
2021.mtsummit-research.1,P10-2041,0,0.0675284,"low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in"
2021.mtsummit-research.1,N19-4009,0,0.0281775,"mini-batch. The reward obtained for this action is the delta-validation perplexity post update, the improvement in perplexity on the validation set in a finite window. The exploration strategy is the linearly-decaying epsilon-greedy strategy (Kuleshov and Precup, 2014). The contextual MABs are implemented as simple feed-forward neural networks which take the observation vector as input and produce a distribution over two states representing the bins. If we choose to exploit this learned policy, the bin with maximum probability mass is selected for sampling. 3 Experiment Setup We use Fairseq (Ott et al., 2019) for all our NMT experiments and the our NMT systems are configured to replicate the setup described in Guzmán et al. (2019). The grid search experiments search over the the range [0, 1] for sampling in increments of 0.1. The pruned tree-search uses a beam width of 1. The phase duration for tree-search is set to one epoch of NMT training. We use either 5 or 10 concurrent contextual MABs which are implemented as two 256-dimensional feed forward neural networks trained using RMSProp with a learning rate of 0.00025 and a decay of 0.95. Rewards for the agent (validation delta-perplexity) are provi"
2021.mtsummit-research.1,N19-1189,1,0.838007,"Missing"
2021.mtsummit-research.1,D16-1163,0,0.0203494,", 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-learn parameter initialization for child models using trained-high resource parent models for this task. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 1 Figure 1: The multi-arm bandit agents’ (MAB) interface with the NMT system. In this paper, we build upon the framework for learning curricula introduced in Kumar et al. (2019) and attempt to alleviate the problem of observation sparsity by learning more robust policies from multiple training runs. We use c"
C10-2075,P05-1074,0,0.0655539,"pecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives"
C10-2075,N06-1003,0,0.0291331,"y to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives for each other. The motivati"
C10-2075,N09-1025,0,0.0284366,"few non-identity rules. See (Li, 2010) for further discussion. 659 of N (y). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be efficient and the actual MT decoding can use t"
C10-2075,J07-2003,0,0.518374,"008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (CG), illustrated in this paper for a Hiero system (Chiang, 2007). From the bilingual synchronous context-free grammar (SCFG) used in Hiero, we extract a monolingual SCFG, with rules of the kind, X → hstrong tea, powerful teai or 656 Coling 2010: Poster Volume, pages 656–664, Beijing, August 2010 X → hin X1 , in the X1 i. Thus our CG is also an SCFG that generates pairs of English sentences that differ from each other in ways that alternative English hypothesis considered during translation would differ from each other. This CG is then used to “translate” each sentence in the LM training corpus into what we call its confusion set — a set of other “sentences"
C10-2075,2005.iwslt-1.1,0,0.0566575,"y the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task. Secondly, when tuning θ, CE uses the maximum likelihood training, but we use the minimum 660 risk training of (5). Since our training uses a taskspecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a"
C10-2075,D09-1005,1,0.846575,"). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be efficient and the actual MT decoding can use them conveniently. The objective of (5) is differentiable and thus we can o"
C10-2075,2008.amta-papers.12,1,0.88869,"criptive models of text. However, in a task like Chinese-to-English MT, the de facto role of the LM is to discriminate among the alternative English translations being contemplated by the MT system for a particular Chinese input sentence. We call the set of such alternative translations a confusion set. Since a confusion set is typically a minuscule subset of the set of all possible word sequences, it is arguably better to train the LM parameters so as to make the best candidate in the confusion set more likely than its competitors, as done by Roark et al. (2004) for speech recognition and by Li and Khudanpur (2008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (CG), illustrated in this paper for a Hiero system (Chia"
C10-2075,W09-0424,1,0.919782,"Missing"
C10-2075,W07-0716,0,0.0153995,"mum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives for each other. The motivation and goal are also di"
C10-2075,P03-1021,0,0.106266,"small test set and hence has few non-identity rules. See (Li, 2010) for further discussion. 659 of N (y). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be effici"
C10-2075,N09-1024,0,0.0361715,"e to English). Specifically, we can add the discriminative model into an MT pipeline as a feature, and tune its weight relative to other models in the MT system, including the baseline n-gram LM. 4 Related and Similar Work The detailed relation between the proposed procedure and other language modeling techniques has been discussed in Sections 1 and 2. Here, we review two other methods that are related to our method in a broader context. 4.1 Unsupervised Training of Global Log-linear Models Our method is similar to the contrastive estimation (CE) of Smith and Eisner (2005) and its successors (Poon et al., 2009). In particular, our confusion grammar is like a neighborhood function in CE. Also, our goal is to improve both efficiency and accuracy, just as CE does. However, there are two important differences. First, the neighborhood function in CE is manually created based on human insights about the particular task, while our neighborhood function, generated by the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task. Secondly, when tuning θ, CE uses the ma"
C10-2075,W04-3219,0,0.0334591,"raining uses a taskspecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad ("
C10-2075,P04-1007,0,0.29948,"as the whole-sentence model are generative or descriptive models of text. However, in a task like Chinese-to-English MT, the de facto role of the LM is to discriminate among the alternative English translations being contemplated by the MT system for a particular Chinese input sentence. We call the set of such alternative translations a confusion set. Since a confusion set is typically a minuscule subset of the set of all possible word sequences, it is arguably better to train the LM parameters so as to make the best candidate in the confusion set more likely than its competitors, as done by Roark et al. (2004) for speech recognition and by Li and Khudanpur (2008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (C"
C10-2075,P05-1044,1,0.818509,"actual MT decoding (e.g., translating Chinese to English). Specifically, we can add the discriminative model into an MT pipeline as a feature, and tune its weight relative to other models in the MT system, including the baseline n-gram LM. 4 Related and Similar Work The detailed relation between the proposed procedure and other language modeling techniques has been discussed in Sections 1 and 2. Here, we review two other methods that are related to our method in a broader context. 4.1 Unsupervised Training of Global Log-linear Models Our method is similar to the contrastive estimation (CE) of Smith and Eisner (2005) and its successors (Poon et al., 2009). In particular, our confusion grammar is like a neighborhood function in CE. Also, our goal is to improve both efficiency and accuracy, just as CE does. However, there are two important differences. First, the neighborhood function in CE is manually created based on human insights about the particular task, while our neighborhood function, generated by the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task."
C10-2075,D11-1085,1,\N,Missing
D11-1085,P08-1024,0,0.0401701,"ur approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which"
D11-1085,N09-1025,0,0.0625912,"T systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a sco"
D11-1085,J07-2003,0,0.581901,"ness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is ins"
D11-1085,W02-1001,0,0.20038,"N 1 2 ing − M j=1 log pφ (xj |yj ) + 2σ 2 kφk2 on some bilingual data, with the regularization coefficient σ 2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij . All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi . 3.3 The Forward Translation System δθ and The Loss Function L(δθ (xi ), yi ) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, 8 In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model pφ (x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. 2006) can be formalized in this framework by choosing different functions for δθ and L(δθ (xi ), yi ). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δθ and L(δθ (xi ), yi ) we considered in our investigation. 3.3.1 Deterministic Decoding A si"
D11-1085,P08-1115,0,0.0269094,"for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often n"
D11-1085,2005.iwslt-1.1,0,0.0473602,"|y) and pθ (y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train θ, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 5.1.1 Baseline Systems IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram languag"
D11-1085,P06-1121,0,0.0422989,"γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approac"
D11-1085,W05-1506,0,0.0238305,"isner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating pφ (x |yi ) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xij . We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations sh"
D11-1085,N07-1018,0,0.0210271,"imations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation ("
D11-1085,N03-1017,0,0.00578651,"ore(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly th"
D11-1085,D09-1005,1,0.926783,"criminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be proba"
D11-1085,W09-0424,1,0.90745,"Missing"
D11-1085,P09-1067,1,0.797285,"wed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracti"
D11-1085,C10-2075,1,0.85854,"ed by the reverse Hiero system; each translation phrase (or rule) corresponding to a hyperedge. To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of xi . We can do that by considering only forward translations that respect the hypergraph structure of Xi . The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y"
D11-1085,P06-1096,0,0.256824,"Missing"
D11-1085,D08-1076,0,0.0341042,"pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. On"
D11-1085,N06-1045,0,0.0232079,"he forward translations. Still, even with the worse imputation (in the case of “NLM”), our forward translations improve as we add more monolingual data. 5.5.2 Imputation with Different k-best Sizes In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence. This is the 1-best approximation of section 3.4. Table 5 shows (in the fully unsupervised case) that the performance does not change much as k increases.16 This may be because that the 5-best sentences are likely to be quite similar to one another (May and Knight, 2006). Imputing a longer k-best list, a sample, or a lattice for xi (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust. 6 Conclusions In this paper, we present an unsupervised discriminative training method that works with missing inputs. The key idea in our method is to use a reverse model to impute the missing input from the observed output. The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e., 16 In the present experiments, however, we simply weighted"
D11-1085,P03-1021,0,0.401482,"guage model pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003)"
D11-1085,2001.mtsummit-papers.68,0,0.0244807,"English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracting a high-scoring translation of x. The goal of discriminative training is to minimize the expected loss of δθ (·), under a given taskspecific loss function L(y 0 , y) that measures how 2 Note that the extra monolingual data is used only for tuning the model weights, but not for inducing new phrases or rules. 921 bad it would be to output y 0 when the correct output is y. For an MT system that is judged by the BLEU metric (Papineni et al., 2001), for instance, L(y 0 , y) may be the negated BLEU score of y 0 w.r.t. y. To be precise, the goal3 is to find θ with low Bayes risk, θ∗ = argmin θ X p(x, y) L(δθ (x), y) (1) x,y where p(x, y) is the joint distribution of the inputoutput pairs.4 The true p(x, y) is, of course, not known and, in practice, one typically minimizes empirical risk by replacing p(x, y) above with the empirical distribution p˜(x, y) given by a supervised training set {(xi , yi ), i = 1, . . . , N }. Therefore, θ∗ = argmin θ = argmin θ X p˜(x, y) L(δθ (x), y) x,y N 1 X L(δθ (xi ), yi ). N (2) i=1 The search for θ∗ typi"
D11-1085,P06-2101,1,0.955514,"ing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ"
D11-1085,D08-1065,0,0.0319576,"a theoretical contribution, and we do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper. ambiguous weighted finite-state automaton Xi , (b) the forward translation system δθ is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al. (2008) that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al. (2009b) show how to approximate a hypergraph representation of pφ (x |yi ) by an unambiguous WFSA. One could then apply the construction to this WFSA12 , obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi ∈ Xi , it must parse i"
D11-1085,D07-1080,0,0.0393648,"development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be t"
D11-1085,P02-1040,0,\N,Missing
D11-1104,J96-1002,0,0.0151449,"Missing"
D11-1104,D07-1090,0,0.0616768,"plexity comes from processing negatives examples for each binary classifier. Therefore, we can achieve the same level of speedup as standard subsampling by only subsampling negative examples, and most importantly, it allows us to keep all the existent patterns(positive examples) in the training data. Of course, negative examples are important and even in the binary case, we benefit from including more of them, but since we have so many of them, they might not be as critical as positive examples in determining the distribution. A similar conclusion can be drawn from Google’s work on large LMs (Brants et al., 2007). Not having to properly smooth the LM, they are still able to benefit from large volumes of web text as training data. It is probably more important to have a high n-gram coverage than having a precise distribution. The explanation here might lead us to wonder whether for the multi-class problem, subsampling the terms in the normalizer would achieve the same results. More specifically, instead of summing over 1135 all words in the vocabulary, we may choose to only consider α of them. In fact, the short-list approach in (Schwenk, 2007) and the adaptive importance sampling in (Bengio and Seneca"
D15-1218,P08-1115,0,0.211997,"tice input for the SMT system from P˜ASR,M T . 2.2.1 A discussion about related techniques 1. Decoding (Translation) : Our model closely resembles a featurized finite-state transducer based translation model. If we replace the ˜ M T )∗ with output alphabet of the acceptor (W the target side phrases, we will actually get output in the target language. Even though this model does not explicity include reordering, the coarse-grained decoder has access to information that can enable better decisions about which hypotheses are better for the downstream process (translation). 2. Lattice Decoding : (Dyer et al., 2008b) suggests passing the entire word lattice to the SMT system. However, even if these lattices are not pruned, a beam based decoder might not consider hypotheses that our model may produce through coarse-grained decoding. 3. Language model re-scoring : One may use a bigger source language model to re-score the ASR lattice (or an N -best list). This however, does not consider any SMT features in re-scoring. With our model, we can simply use this as an additional feature. ˜ ASR ◦ S)) P˜ASR = det(min(L ˜ MT Now, we use the weighted phrase acceptor W 4 to bring in the SMT features . Composing this"
D15-1218,P07-2045,0,0.00459846,"Missing"
D15-1218,C04-1168,0,0.024945,"volves training an Automatic Speech Recognition (ASR) system to transcribe speech to text in the source language. Step two involves extracting an appropriate form of the ASR output to translate. We will refer to this step as the Speech-Translation interface. In the simplest scenario, the ASR 1best output can be used as the source text to translate. It may be useful to consider alternative ASR hypotheses and these take the form of an N -best list or a word-lattice. An N -best list can be included easily into the tuning and the decoding process of a statistical machine translation (SMT) system (Zhang et al., 2004). Several researchers have proposed solutions to incorporating lattices and 1. Using downstream information : Hypothesis selection for the input to the SMT system should be done jointly by the ASR and the SMT systems. That is, there may exist hypotheses that a trained SMT system may find easier to translate and produce better translations for than the ones that are deemed best based on the ASR acoustic and language model scores. Incorporation of knowledge from the downstream process (translation) is vital to selecting translation options, and subsequently producing better translations. 2. Coar"
D15-1218,2013.iwslt-papers.14,1,0.905793,"Missing"
H01-1050,1998.amta-tutorials.5,0,\N,Missing
H01-1050,W98-1005,0,\N,Missing
H01-1050,J95-4004,0,\N,Missing
H01-1050,A97-1029,0,\N,Missing
L16-1210,bazillon-etal-2008-manual,0,0.0345947,"alysis of the reading and recording errors. In the case of Mixer-6, the sentence prompts served as a strict transcript from which we could detect deviations in the speech output after aligning the transcript to the audio. This alignment and detection process easily extends to other large corpora of read speech. It could also extend to cases in which the speech may be highly constrained by a script, but with some deviations expected, such as in broadcast news. In high quality speech, such as broadcast news, even an automatic decoding of the speech greatly facilitates human transcription (e.g., Bazillon et al., 2008). The additional component of automatic error detection, or evaluation of the alignment goodness (via sclite), identifies the location of deviant segments. The human auditor can directly target these hypothesized errors for manual transcription, minimizing time spent listening to correctly aligned audio, and in the process, minimize manual effort. Overall, this methodology contributes to the further advances made in human-machine collaboration for improved speech transcription (e.g., Roy and Roy, 2009), particularly with respect to auditing the corpus transcription for use in speech research."
L16-1210,cieri-etal-2004-mixer,0,0.0394898,"and SR research. Introduction Speech corpora for research on speaker recognition (SR) are particularly challenging to design and collect. Best practices in this field call for accurate demographics, multiple sessions per speaker at certain intervals, control of speech style, signal quality, channels, transducers, and many other factors, all with accurate documentation. The NIST Speaker Recognition evaluations (SRE) have provided de facto standards for such corpora over the last two decades, and the Mixer series of corpora were collected to meet SRE requirements over a period of several years (Cieri et al., 2004). They exercised a variety of conditions relevant to their intended applications: multiple languages, channels, environments, noises, speaking styles and many others. Mixer-6 followed the same protocols as the other Mixers in the collection of phone calls, but added two additional recording conditions of an interview and read speech portion (for a detailed description of the design and methods, see Brandschain et al., 2013). For forensic, phonetic, and engineering research, this corpus can be very valuable, especially with regards to speaker identification and variability. In comparison to oth"
L16-1210,H92-1073,0,0.363817,"her removed from more naturally occurring speech. Relatedly, other speech corpora may provide comparable numbers of speakers (e.g. TIMIT; Garofolo et al., 1993), but fewer data points per speaker; transcribed spontaneous speech (e.g., Buckeye Corpus; Pitt et al., 2005), but fewer speakers; or, transcribed spontaneous speech, but again fewer number of data points per speaker (e.g., transcribed portion of the Switchboard corpus; Godfrey et al., 1992; Greenberg et al., 1996). The Wall Street Journal Corpus may fulfill similar criteria with a large quantity of transcribed data from many speakers (Paul and Baker, 1992), but as the same set of sentences was always used, the lexical and prosodic factors of the Mixer-6 corpus are relatively matched across talkers. This quality is especially beneficial for many questions of phonetic, forensic, and engineering research, a few of which are discussed above. 6.2 Methodology Potential and Application The methodology employed can potentially be applied to auditing other speech corpora. Given that the task at hand was read speech, many researchers may make the simplifying assumption that the text was read correctly in all cases. As mentioned before, however, a greater"
N03-2026,H01-1033,1,\N,Missing
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N07-1032,H05-1050,1,0.929858,"then be necessary in order to perform well on new test collections. In this paper, we show how to perform such a tuning in the context of unsupervised document clustering, by (i) introducing a degree of freedom, α, into two leading informationtheoretic clustering algorithms, through the use of generalized mutual information quantities; and (ii) selecting the value of α based on clusterings of similar, but supervised document collections (crossinstance tuning). One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use “strapping” (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, and then selects the α with the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. ∗ This work was partially supported by the DARPA GALE program (Contract No HR0011-06-2-0001) and by the JHU WSE/APL Partnership¯ Fund. Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learn"
N07-1032,P95-1026,0,0.116775,"h the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. ∗ This work was partially supported by the DARPA GALE program (Contract No HR0011-06-2-0001) and by the JHU WSE/APL Partnership¯ Fund. Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises. A variety of algorithms (e.g., bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), alternating structure optimization (Ando and Zhang, 2005), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples. Of special interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhan"
N09-2003,J07-2003,0,0.486782,"m a hypergraph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations. We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components. 1 Introduction A hypergraph, as demonstrated by Huang and Chiang (2007), is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired"
N09-2003,W07-0414,1,0.747436,"omputing the similarity of any one hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable. Therefore, efficient algorithms that can exploit the structure of the hypergraph are required. We present an efficient oracle extraction algorithm, which involves two key ideas. Firstly, we view the oracle extraction as a bottom-up model scoring process on a hypergraph, where the model is “trained” on the reference translation(s). This is similar to the algorithm proposed for a lattice by Dreyer et al. (2007). Their algorithm, however, requires maintaining a separate dynamic programming state for each distinguished sequence of “state” words and the number of such sequences can be huge, making the search very slow. Secondly, therefore, we present a novel look-ahead technique, called equivalent oracle-state maintenance, to merge multiple states that are equivalent for similarity computation. Our experiments show that the equivalent oraclestate maintenance technique significantly speeds up (more than 40 times) the oracle extraction. Efficient oracle extraction has at least three important application"
N09-2003,P08-1067,0,0.0214826,"much. Hypothesis space 1-best (Baseline) 500-unique-best Hypergraph 500-best oracles MT’04 35.7 44.0 52.8 53.2 MT’05 32.6 41.2 51.8 52.2 MT’06 28.3 35.1 37.8 38.0 Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. 3.2 Discriminative Hypergraph-Reranking Oracle extraction is a critical component for hypergraph-based discriminative reranking, where millions of model parameters are discriminatively tuned to prefer the oracle-best hypotheses over others. Hypergraph-reranking in MT is similar to the forest-reranking for monolingual parsing (Huang, 2008). Moreover, once the oracle-best hypothesis is identified, discriminative models may be trained on hypergraphs in the same way as on n-best lists (cf e.g. Li and Khudanpur (2008b)). The results in Table 4 demonstrate that hypergraph-reranking with a discriminative LM or TM improves upon the baseline models on all three test sets. Jointly training both the LM and TM likely suffers from over-fitting. 12 MT’04 35.7 35.9 36.1 36.0 MT’05 32.6 33.0 33.2 33.1 MT’06 28.3 28.2 28.7 28.6 Table 4: BLEU scores after discriminative hypergraphreranking. Only the language model (LM) or the translation model"
N09-2003,W05-1506,0,0.0943868,"ding or a hypergraph-rescoring stage. In the latter case, if the hypergraph generated by the first-stage decoding does not have a set of DP states that is a superset of the DP states required for oracle extraction, we need to split the items of the firststage hypergraph and create new items with sufficiently detailed states. It is worth mentioning that if the hypergraph items contain the state information necessary for extracting the oracle-best hypothesis, it is straightforward to further extract the k-best hypotheses in the hypergraph (according to BLEU) for any k ≥ 1 using the algorithm of Huang and Chiang (2005). Item Item A Item B Item C |h| 5 10 17 |e r| 6.2 9.8 18.3 matches (3, 2, 2, 1) (8, 7, 6, 5) (12, 10, 9, 6) log BLEU -0.82 -0.27 -0.62 Table 1: Example computation when items A and B are combined by a rule to produce item C. |e r |is the approximated reference length as described in the text. 2.2 EQ-L-STATE (em 1 ) 1 2 3 4 5 6 7 els ← em 1 for i ← m to 1  right to left i if IS - A - SUFFIX(e1 ) break  stop reducing els else els ← ei−1  reduce state 1 return els Equivalent Oracle State Maintenance Figure 1: Equivalent Left LM State Computation. The process above, while able to extract the or"
N09-2003,P07-1019,0,0.0460671,"thesis from a hypergraph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations. We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components. 1 Introduction A hypergraph, as demonstrated by Huang and Chiang (2007), is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired"
N09-2003,N03-1017,0,0.0120385,"Missing"
N09-2003,W08-0402,1,0.936152,"ove is O(m ˜ 2 ), where m ˜ is the number of unique n-grams in the reference translations. Clearly, m ˜  m by several orders of magnitude, leading to effectively much fewer items to process in the chart. One may view this idea of maintaining equivalent states more generally as an outside look-ahead during bottom-up inside parsing. The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the inside parsing eliminates or collapses the state into a coarser state. The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme. 3 Experimental Results We report experimental results on a Chinese to English task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007). 3.1 Goodness of the Oracle-Best Translations Table 2 reports the average speed (seconds/sentence) for oracle extraction. Hypergraphs were generated with a trigram LM and expanded on the fly for 4gram BLEU computation. Basic DP 25.4 sec/sent Collapse equiv. states 0.6 sec/sent speed-up × 42 Test Set Baseline Discrim. LM Discrim. TM Discrim. TM+LM Tab"
N09-2003,2008.amta-papers.12,1,0.887943,"ove is O(m ˜ 2 ), where m ˜ is the number of unique n-grams in the reference translations. Clearly, m ˜  m by several orders of magnitude, leading to effectively much fewer items to process in the chart. One may view this idea of maintaining equivalent states more generally as an outside look-ahead during bottom-up inside parsing. The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the inside parsing eliminates or collapses the state into a coarser state. The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme. 3 Experimental Results We report experimental results on a Chinese to English task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007). 3.1 Goodness of the Oracle-Best Translations Table 2 reports the average speed (seconds/sentence) for oracle extraction. Hypergraphs were generated with a trigram LM and expanded on the fly for 4gram BLEU computation. Basic DP 25.4 sec/sent Collapse equiv. states 0.6 sec/sent speed-up × 42 Test Set Baseline Discrim. LM Discrim. TM Discrim. TM+LM Tab"
N09-2003,2001.mtsummit-papers.46,0,0.0533683,"system produces a set of translations, which are then grafted to form a confusion network. The confusion network is then rescored, often employing additional (language) models, to select the final translation. When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system. However, some translations in the confusion network may not be reachable by some component systems, in which case a system’s score for the most similar reachable translation serves as a good approximation. Multi-source Translation: In a multi-source translation task (Och and Ney, 2001) the input is given in multiple source languages. This leads to a situation analogous to system combination, except that each component translation system now corresponds to a specific source language. 2 Oracle Extraction on a Hypergraph In this section, we present the oracle extraction algorithm: it extracts one or more translations in a hypergraph that have the maximum BLEU score1 with respect to the corresponding reference translation(s). The BLEU score of a hypothesis h relative to a reference r may be expressed in the log domain as,   X 4 |r| 1 log BLEU(r, h) = min 1− , 0 + log pn . |h|"
N09-2003,P02-1040,0,0.076574,"generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired translations, with similarity computed via some automatic 9 metric such as BLEU (Papineni et al., 2002). Such maximally similar translations will be called oraclebest translations, and the process of extracting them oracle extraction. Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable. Therefore, efficient algorithms that can exploit the structure of the hypergraph are required. We present an efficient oracle extraction algorithm, which involves two key ideas. Firstly, we view the oracle extrac"
N09-2003,P07-1040,0,0.0791747,"Missing"
N10-1046,J92-4003,0,0.0957293,"s For class-based unigrams, P (Q|S) is computed using only the cluster labels of the query terms as Y P (Q|S) = P (qi |Cqi , S)P (Cqi |S), (2) i=1...M where Cqi is the cluster that contains qi and P (qi |Cqi , S) is the emission probability of the ith query term given its cluster and the sentence. P (Cqi |S) is analogous to the sentence model P (qi |S) in (1), but is based on clusters instead of terms. To calculate P (Cqi |S), each cluster is considered an atomic entity, with Q and S interpreted as sequences of such entities. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002). The algorithm requires an input corpus statistics in the form hw, w0 , fww0 i, where fww0 is the number of times the word w0 is seen in the context w. Both w and w0 are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, a bottom-up approach is used to merge the pair of clusters that minimizes the loss in Average Mutual Information (AMI) between the word cluster Cw0 and its context cluster Cw . Different words seen in the same contexts are good candidates for merger, as are different context"
N10-1046,J90-1003,0,0.417354,"not consider whole documents, and only concerns itself with the number of times that two words occur in the same sentence. Co-occurrence in a Window of Text The window-wise co-occurrence statistic is an even narrower notion of context, considering only terms in a window surrounding w0 . Specifically, a window of a fixed size is moved along the text, and fww0 is set as the number of times both w and w0 appear in the window. Since the window size is a free parameter, different sizes may be applied. In our experiments we use two window sizes, 2 and 5, that have been studied in related research (Church and Hanks, 1990). Co-occurrence in a Syntactic Relationship Another notion of word similarity derives from having the same syntactic relationship with the context w. This syntax-wise co-occurrence statistic is similar to the sentence-wise co-occurrence, in that co-occurrence is defined at the sentence level. However, in contrast to the sentence-wise model, w and w0 are said to co-occur only if there is a syntactic relation between them in that sentence. E.g., this type of co-occurrence can help cluster nouns that are used as objects of same verb, such as ‘tea’, ‘water’, and ‘cola,’ which all are used with the"
N10-1046,kaisser-lowe-2008-creating,0,0.0252002,"4,084,473 12,343,947 46,307,650 14,093,661 Google n-grams Google n-grams window-5 window-2 12,005,479 328,431,792 Table 1: Statistics for different notions of co-occurrence. Derivatives of the TREC QA Data Sets The set of questions from the TREC 2006 QA track1 was used as the test data to evaluate our models, while the TREC 2005 set was used for development. The TREC 2006 QA task contains 75 questionseries, each on one topic, for a total of 403 factoid questions which is used as queries for sentence retrieval. For sentence-level relevance judgments, the Question Answer Sentence Pair corpus of Kaisser and Lowe (2008) was used. All the documents that contain relevant sentences are from the NIST AQUAINT1 corpus. QA systems typically employ sentence retrieval after initial, high quality document retrieval. To simulate this, we created a separate search collection for each question using all sentences from all documents relevant to the topic (question-series) from which the question was derived. On average, there are 17 relevant documents per topic, many not relevant to the question itself: they may be relevant to another question. So the sentence search collection is realistic, even if somewhat optimistic. 4"
P08-2021,W07-0414,1,0.83118,"2 shows tercomTERs of invWER-oracles (as computed by the aforementioned Dyna program) and oracle BLEU scores of the confusion networks. The confusion networks were generated using 9 MT systems applied to the Chinese GALE 2007 Dev set, which consists of roughly 550 Newswire segments, and 650 Weblog segments. The confusion networks which were generated with the ITGbased alignments gave significantly better oracle tercomTERs (significance tested with a Fisher sign test, p − 0.02) and better oracle BLEU scores. The BLEU oracle sentences were found using the dynamic-programming algorithm given in Dreyer et al. (2007) and measured using Philipp Koehn’s evaluation script. On the other hand, a comparison between the 1-best paths did not reveal significant differences that would favor one approach or the other (either in terms of tercomTER or BLEU). We also tried to understand which alignment method gives higher probability to paths “close” to the corresponding oracle. To do that, we computed the probability that a random path from a confusion network is within x edits from its oracle. This computation was done efficiently using finite-state-machine operations, and did not involve any randomization. Prelimina"
P08-2021,H05-1036,1,0.783103,"imple ITG that has one nonterminal and whose terminal symbols allow insertion, deletion, and substitution. The minimum-cost ITG tree can be found by dynamic programming. This leads to invWER (Leusch et al., 2003), which is defined as the minimum number of edits (insertions, deletions, substitutions and block shifts allowed by the ITG) needed to convert one string to another. In this paper, the minimuminvWER alignments are used for generating confusion networks. The alignments are found with a 11rule Dyna program (Dyna is an environment that facilitates the development of dynamic programs—see (Eisner et al., 2005) for more details). This program was further sped up (by about a factor of 2) with an A∗ search heuristic computed by additional code. Specifically, our admissible outside heuristic for aligning two substrings estimated the cost of aligning the words outside those substrings as if reordering those words were free. This was complicated somewhat by type/token issues and by the fact that we were aligning (possibly weighted) lattices. Moreover, the same Dyna program was used for the computation of the minimum invWER path in these confusion networks (oracle path), without having to invoke tercom nu"
P08-2021,2003.mtsummit-papers.32,0,0.803311,"because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths. System-level combination of translation outputs is a promising path towards such improvements. Yet there are some significant hurdles in this p"
P08-2021,E06-1005,0,0.266143,"choice among several phrases. Note that our contributions in this paper could be applied to arbitrary lattice topologies.) For example, Bangalore et al. (2001) show how to build a confusion network following a multistring alignment procedure of several MT outputs. The procedure (used primarily in biology, (Thompson et al., 1994)) yields monotone alignments that minimize the number of insertions, deletions, and substitutions. Unfortunately, monotone alignments are often poor, since machine translations (particularly from different models) can vary significantly in their word order. Thus, when Matusov et al. (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment. (TER is defined as the minimum number of inser81 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81–84, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tions, deletions, substitutions and block shifts between two strings.) A remarkable feature of th"
P08-2021,P07-1040,0,0.571055,"ment step is difficult because different MT approaches usually reorder the translated words differently. Training the selection step is difficult because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengt"
P08-2021,2006.amta-papers.25,0,0.391486,"ated words differently. Training the selection step is difficult because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths. System-level combination of translation outputs is a promising path towards su"
P08-2021,J97-3002,0,0.0363107,"k to be moved, it should have an exact match in its new position. However, this sometimes leads to counter-intuitive sequences of edits; for instance, for the sentence pair 82 “thomas jefferson says eat your vegetables” “eat your cereal thomas edison says”, tercom finds an edit sequence of cost 5, instead of the optimum 3. Furthermore, the block selection is done in a greedy manner, and the final outcome is dependent on the shift order, even when the above constraints are imposed. An alternative to tercom, considered in this paper, is to use the Inversion Transduction Grammar (ITG) formalism (Wu, 1997) which allows one to view the problem of alignment as a problem of bilingual parsing. Specifically, ITGs can be used to find the optimal edit sequence under the restriction that block moves must be properly nested, like parentheses. That is, if an edit sequence swaps adjacent substrings A and B of the original string, then any other block move that affects A (or B) must stay completely within A (or B). An edit sequence with this restriction corresponds to a synchronous parse tree under a simple ITG that has one nonterminal and whose terminal symbols allow insertion, deletion, and substitution."
P09-1067,N03-1017,0,0.0413473,"in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec,"
P09-1067,W08-0402,1,0.591858,"pruning, which might be sensible for their relatively small task (e.g., input sentences of < 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9 A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then"
P09-1067,W09-0424,1,0.0903774,"Missing"
P09-1067,D07-1104,0,0.0215993,"ese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 5.3 Results of Different Variational Decoding Table 2 presents the BLEU results under different ways in using the variational models, as discussed in Section 3.2."
P09-1067,P05-1010,0,0.0117112,"in principle applied in a hypergraph with spurious ambiguity. 598 et al. (2008). In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q ∗ (r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Decoding scheme MT’04 Viterbi 35.4 MBR (K=1000) 35.8 Crunching (N =10000) 35.7 Crunching+MBR (N =10000) 35.8 Variational (1to4gram+wp+vt) 36.6 Table 1: BLEU scores for Viterbi, Crunching, MBR, and variational decoding. All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05). In each column, we boldface the best result as well as"
P09-1067,N06-1045,0,0.362842,"y systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec, Singapore, 2-7 August 2009. 2009 ACL an"
P09-1067,J05-2002,0,0.0329211,"Missing"
P09-1067,P00-1056,0,0.038583,"Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 5.3 Results of Different Variational Decoding Table 2 presents the BLEU results under different ways in using the variational models"
P09-1067,P03-1021,0,0.168639,"et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 3.2.3 Decoding with q ∗ When translating x at runtime, the q ∗ constructed from HG(x) will be used as a surrogate for p during decoding. We want its most probable string: y ∗ = argmax q ∗ (y) whose edges correspond to n-grams (weighted with negative log-probabilities) and whose vertices correspond to (n −"
P09-1067,2001.mtsummit-papers.68,0,0.0732892,"number of hyperedges). y ∗ = argmax q ∗ (y) (14) y∈T(x) This ensures that y ∗ is a valid string in the original hypergraph HG(x), which will tend to rule out inadequate translations like “the mouse.” If our sole objective is to get a good approximation to p(y |x), we should just use a single n-gram model q ∗ whose order n is as large as possible, given computational constraints. This may be regarded as favoring n-grams that are likely to appear in the reference translation (because they are likely in the derivation forest). However, in order to score well on the BLEU metric for MT evaluation (Papineni et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or"
P09-1067,C96-2215,0,0.222629,"Missing"
P09-1067,P06-2101,1,0.849939,"t, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 3.2.3 Decoding with q ∗ When translating x at runtime, the q ∗ constructed from HG(x) will be used as a surrogate for p during decoding. We want its most probable string: y ∗ = argmax q ∗ (y) whose edges correspond to n-grams (weighted with negative log-probabilities) and whose vertices correspond to (n − 1)-grams. However, because q ∗ only approximates p, y"
P09-1067,D08-1065,0,0.670982,"otal probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. Our particular variational distributions are parameterized as n-gram models. We also analytically show that interpolating these n-gram models for different n is similar to minimumrisk decoding for BLEU (Tromble et al., 2008). Experiments show that our approach improves the state of the art. 1 Introduction Ambiguity is a central issue in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of int"
P09-1067,W06-3110,0,0.0137793,"n-gram types of different n, into several subsets Wn , each of which contains only the n-grams with a given length n. We can now rewrite (19) as follows, X θn · gn (y |x) (21) y ∗ = argmax Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): X l(y, y 0 )p(y 0 |x) y ∗ = argmin R(y) = argmin y y w∈N y y0 (17) n by assuming θw = θ|w |and, ( |y| if n = 0 gn (y |x) = P (22) w∈Wn g(w |x)cw (y) if n > 0 11 It would also be possible to interpolate with the N -best approximations (see Section 2.4), with some complications. 12 Zens and Ney (2006) use a similar decision rule as here and they also use posterior n-gram probabilities as feature functions, but their model estimation and decoding are over an N -best, which is trivial in terms of computation. 13 Already at (14), we explicitly ruled out translations y having no derivation at all in the hypergraph. However, suppose the hypergraph were very large (thanks to a large or smoothed translation model and weak pruning). Then (14)’s heuristic would fail to eliminate bad translations (“the mouse”), since nearly every string y ∈ Σ∗ would be derived as a translation with at least a tiny p"
P09-1067,P08-1025,0,0.0247397,"def entropy H(p, q) = − y p(y |x) log q(y), and 20 Both H(p, q) and Hd (p) involve an expectation over exponentially many derivations, but they can be computed in time only linear in the size of HG(x) using an expectation semiring P (Eisner, 2002). In particular, H(p, q) can be found as − d∈D(x) p(d |x) log q(Y(d)). 600 Many interesting research directions remain open. To approximate the intractable MAP decoding problem of (2), we can use different variational distributions other than the n-gram model of (11). Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008). We might also attempt to minimize KL(q k p) rather than KL(p k q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 2005). One could also augment our n-gram models with non-local string features (Rosenfeld et al., 2001) provided that the expectations of these features could be extracted from the hypergraph. Variational inference can also be exploited to solve many other intractable problems in MT (e.g., word/phrase alignment and system combination). Finally, our method can be used for tasks be"
P09-1067,P03-1006,0,0.0398513,"whole translation string in the dynamic programming state. They alleviate the computation cost somehow by using aggressive beam pruning, which might be sensible for their relatively small task (e.g., input sentences of < 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9 A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weigh"
P09-1067,P08-1024,0,0.150027,"decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3 A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4 Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). Maximum A Posterior (MAP) Decoding For a given input sentence x, a decoding method identifies a particular “best” output string y ∗ . The maximum a posteriori (MAP) decision rule is y ∗ = argmax p(y |x) #$ Figure 2: Tree ambiguity in syntax-based MT: two derivation trees yield the same translation string. In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure"
P09-1067,J07-2003,0,0.813337,"processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec, Singapore, 2-7"
P09-1067,P09-1064,0,0.314107,"In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q ∗ (r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Decoding scheme MT’04 Viterbi 35.4 MBR (K=1000) 35.8 Crunching (N =10000) 35.7 Crunching+MBR (N =10000) 35.8 Variational (1to4gram+wp+vt) 36.6 Table 1: BLEU scores for Viterbi, Crunching, MBR, and variational decoding. All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05). In each column, we boldface the best result as well as all results that are statistically indistinguishable from it. In MBR, K is t"
P09-1067,P02-1001,1,0.263978,"(Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then possible to obtain expected transition counts in Q (where the expectation is taken under p), or other sufficient statistics needed to estimate φ. This takes only time O(|HG(x)|) when L is a left-to-right refinement of Q (meaning that any two prefix strings that reach the same state in L also reach the same state in Q), for then intersecting L or HG(x) with Q does not split any states. That is the case when L and Q are respectively pure m-gram and n-gram models with m ≥ n, as assumed in (12) and Figure 4. It is also the case when Q is a pure n-gram model and L is constructed not to"
P09-1067,W96-0214,0,0.1543,"Missing"
P09-1067,P07-1019,0,0.00627607,"scaling factor to adjust the sharpness of the distribution, the score s(x, y, d) is a learned linear combination of features of the triple (x, y, d), and Z(x) is a normalization constant. Note that p(y, d |x) = 0 if y 6= Y(d). Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.3 However, both |D(x)| and |T(x) |may be exponential in |x|. Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3 A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4 Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). Maximum A Posterior (MAP) Decoding For a given input sentence x, a decoding method identifies a particular “best” output string y ∗ . The maximum a"
P09-1067,P02-1040,0,\N,Missing
P09-1067,N04-1022,0,\N,Missing
P09-4007,P05-1032,1,0.805694,"ssor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optim"
P09-4007,J07-2003,0,0.827653,"lation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was suppo"
P09-4007,P03-2041,0,0.0474557,"ize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3 The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computatio"
P09-4007,P06-1121,0,0.410414,"grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Resear"
P09-4007,W05-1506,0,0.0961856,"t this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedin rwth cmu-statxfer BLEU-4 31.14 26.89 26.86 26.52 25.96 25.51 25.44 24.89 23.65 • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. Table 1: BLEU scores for top primary systems on the WMT-09 French-English Task from CallisonBurch et al. (2009), who also provide human evaluation results. 2.1 • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations"
P09-4007,P09-1067,1,0.853412,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,P07-2045,1,0.0131579,"lkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1 Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedi"
P09-4007,P06-1077,0,0.259566,"rsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GA"
P09-4007,D07-1104,0,0.0157455,"d computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using th"
P09-4007,W08-0402,1,0.798672,"Missing"
P09-4007,P03-1021,0,0.0153466,"guage suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable"
P09-4007,P05-1034,0,0.0603092,"hronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the D"
P09-4007,N09-2003,1,0.88482,"Missing"
P09-4007,P07-1065,0,0.0575308,"Missing"
P09-4007,W09-0424,1,0.2272,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,W09-0401,1,\N,Missing
P09-4007,D08-1076,0,\N,Missing
P12-1019,P05-1063,0,0.714277,"how that up-training leads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices an"
P12-1019,D09-1116,0,0.031144,"eads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N"
P12-1019,N10-1115,0,0.0160197,"states occur when Q is empty and S contains a single tree (the output). Ω is determined by the set of dependency labels r ∈ R and one of three transition types: • Shift: remove the head of Q (wj ) and place it on the top of S as a singleton tree (only wj .) • Reduce-Leftr : replace the top two trees in S (s0 and s1 ) with a tree formed by making the root of s1 a dependent of the root of s0 with label r. • Reduce-Rightr : same as Reduce-Leftr except reverses s0 and s1 . Table 1 shows the kernel features used in our dependency parser. See Sagae and Tsujii (2007) for a complete list of features. Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. Substructure sharing reduces these steps for equivalent states, which are persistent throughout a candidate set. Note that there are far fewer kernel features than total features, hence the hash function calculation is very fast. We summarize substructure sharing for dependency parsing in Algorithm 1. We extend the definition of states to be {S, Q, p} where p denotes the score of the state: the probability of the action sequence that resulted in the current state. Also, fol179 while Heap 6= ∅ do πcurrent ←Hea"
P12-1019,P10-1110,0,0.0315158,"r multi-class classification, so pg (ωi |πi ) = pg (f (π) ◦ ωi ), where ◦ is a conjunction operation. In this way, states can be summarized by features. Equivalent states are defined as two states π and 0 π with an identical feature representation: π ≡ π0 iff f (π) = f (π 0 ) If two states are equivalent, then g imposes the same distribution over actions. We can benefit from this substructure redundancy, both within and between hypotheses, by saving these distributions in memory, sharing a distribution computed just once across equivalent states. A similar idea of equivalent states is used by Huang and Sagae (2010), except they use equivalence to facilitate dynamic programming for shift-reduce parsing, whereas we generalize it for improving the processing time of similar hypotheses in general models. Following Huang and Sagae, we define kernel features as the smallest set of atomic features ˜f (π) such that, ˜f (π) = ˜f (π 0 ) ⇒ π ≡ π0. H caches equivalent states in a hypothesis set and resets for each new utterance. For each state, we first check H for equivalent states before computing the action distribution; each cache hit reduces decoding time. Distributing hypotheses wi across different CPU thread"
P12-1019,D10-1002,0,0.100775,"illation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM. Word-lattices for discriminative training and rescoring come from this baseline ASR system.6 The longspan discriminative LM’s baseline feature weight (α0 ) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter. While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l3 ) time to parse a sentence of length l. Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem.7 We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS). 6.1 Results Before we demonstrate the speed of our models, we show th"
P12-1019,D10-1069,0,0.104863,"ities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntact"
P12-1019,P06-2089,0,0.0210613,"equivalence. The tagger is deterministic (greedy) in that it only considers the best tag at each step, so we do not store scores. However, this tagger uses a depth3 Sagae and Tsujii (2007) use a beam strategy to increase speed. Search space pruning is achieved by filtering heap states for probability greater than 1b the probability of the most likely state in the heap with the same number of actions. We use b = 100 for our experiments. 4 We note that while we have demonstrated substructure sharing for dependency parsing, the same improvements can be made to a shift-reduce constituent parser (Sagae and Lavie, 2006). w1 t1 w2 t2 ··· ··· wi ti 2 2 wi ti 1 wi wi+1 t1i t1i+1 t2i t2i+1 |T | ti+1 wi+2 wi+3 1 ti |T | lookahead search Figure 2: POS tagger with lookahead search of d=1. At wi the search considers the current state and next state. first search lookahead procedure to select the best action at each step, which considers future decisions up to depth d5 . An example for d = 1 is shown in Figure 2. Using d = 1 for the lookahead search strategy, we modify the kernel features since the decision for wi is affected by the state πi+1 . The kernel features in position i should be ˜f (πi ) ∪ ˜f (πi+1 ): ˜f (π"
P12-1019,D07-1111,0,0.275071,"Association for Computational Linguistics, pages 175–183, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include"
P12-1019,W11-0328,0,0.164455,"al modifica175 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175–183, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic"
P14-1063,W02-1001,0,0.297449,"Missing"
P14-1063,P05-1022,0,0.0620973,"Missing"
P14-1063,N12-1023,0,0.0539721,"Missing"
P14-1063,D08-1024,0,\N,Missing
P14-1063,P05-1012,0,\N,Missing
P14-1063,N13-1134,0,\N,Missing
P14-1063,N13-1052,0,\N,Missing
P14-1124,C00-1027,0,0.0716847,"happening with infrequent words, and why does this matter for term detection? To look at the data from a different perspective, we consider the random variable k, which is the number of times a word occurs in a particular document. In Figure 5 we plot the following ratio, which Church and Gale (1995) define as burstiness : fw Ew [k|k > 0] = DFw (3) as a function of fw . We denote this as E[k] and can interpret burstiness as the expected word count given we see w at least once. In Figure 5 we see two classes of words emerge. A similar phenomenon is observed concerning adaptive language models (Church, 2000). In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model (Jelinek, 1997). Likewise, Katz attempts to capture Figure 5: Tagalog burstiness. these two classes in his G model of word frequencies (1996). For the first class, burstiness increases slowly but steadily as w occurs more frequently. Let us label these Class A words. Since our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents Looking close to the y-axis in Figure 5, we ob"
P14-1124,P99-1022,0,0.861461,"Missing"
P14-1124,W06-1644,0,0.674262,"Missing"
W00-0504,P99-1027,0,0.0327709,"ed words may be helpful. We thus plan to exp]Iore the potential for integrated sequential model]ling of both words and syllables [Meng et al., 20013]. 4. Multiseale Embedded Translation Figures 1 and 2 illustrate two translingual retrieval strategies. In query translation, English text queries are transformed into Mandarin and then used to retrieve Mandarin documents. For document translation, Mandarin documents are translated into English before they are indexed and then matched with English queries. McCarley has reported improved effectiveness from techniques that couple the two techniques [McCarley, 1999], but time constraints may limit us to explonng only the query translation strategy dunng the six-week Workshop. 4,1 W o r d T r a n s l a t i o n While we make use of sub-word transcription to smooth out-of-vocabulary(OOV) problems in speech recognition as described above, and to alleviate the OOV problem :for translation as we discuss in the next section, accurate translation generally relies on the additional information available at the word and phrase levels. Since the &quot;bag of words&quot; information retrieval techniques do not incorporate any meaningful degree of language understanding to as"
W00-0504,O99-2001,1,0.804809,"t takes place frequently...) The above considerations lead to a number of techniques we plan to use for our task. We concentrate on three equally critical problems related to our theme of translingual speech retrieval: (i) indexing Mandarin Chinese audio with word and subword units, (ii) translating variable-size units for cross-language information retrieval, and (iii) devising effective retrieval strategies for English text queries and Mandarin Chinese news audio. 3. Multiscale Audio Indexing A popular approach to retrieval is to apply spoken document Large-Vocabulary s Examples drawn from [Meng and Ip, 1999]. Continuous Speech Recognition (LVCSR) 9 for audio indexing, followed by text retrieval techniques. Mandarin Chinese presents a challenge for word-level indexing by LVCSR, because of the ambiguity in tokenizing a sentence into words (as mentioned earlier). Furthermore, LVCSR with a static vocabulary is hampered by the out-of-vocabulary (OOV) problem, especially when searching sources with topical coverage as diverse as that found in broadcast news. By virtue of the monosyllabic nature of the Chinese language and its dialects, the syllable inventory can provide a complete phonological coverag"
W00-0504,P97-1017,0,0.139368,"Missing"
W00-0504,1998.amta-tutorials.5,0,\N,Missing
W00-0504,W98-1005,0,\N,Missing
W00-0504,J95-4004,0,\N,Missing
W00-0504,J98-4003,0,\N,Missing
W00-0504,X93-1008,0,\N,Missing
W00-0504,A97-1029,0,\N,Missing
W03-1003,J93-2003,0,0.00613502,"Missing"
W03-1003,P00-1056,0,0.0310459,"h to CLIR. 2.3 Obtaining Stochastic Translation Lexicons The translation lexicons K w and &apos;  may be created out of an available electronic translation lexicon, with multiple translations of a word being treated as equally likely. Stemming and other morphological analyses may be applied to increase the vocabulary-coverage of the translation lexicons. Alternately, they may also be obtained automatically from a parallel corpus of translated and sentence-aligned Chinese-English text using statistical machine translation techniques, such as the publicly available GIZA++ tools (Och and Ney, 2000), as done by Khudanpur and Kim (2002). Unlike standard MT systems, however, we apply the translation models to entire articles, one word at a time, to get a bag of translated words — cf. (1) and (3). Finally, for truly resource deficient languages, one may obtain a translation lexicon via optical character recognition from a printed bilingual dictionary (cf. Doerman et al (2002)). This task is arguably easier than obtaining a large LM training corpus. 3 Cross-Lingual Lexical Triggers It seems plausible that most of the information one gets from the cross-lingual unigram LM of (1) is in the for"
W03-1003,H01-1035,0,0.0368945,"Missing"
W03-1003,W97-1014,0,\N,Missing
W03-1508,J93-2003,0,0.00275436,"a phonemic representation using the Festival1 speech synthesis system. 2. Translation of the English phoneme sequence into a sequence of generalized initials and finals or GIFs — commonly used sub-syllabic units for expressing pronunciations of Chinese characters. 3. Transformation of the GIF sequence into a sequence of pin-yin symbols without tone. 4. Translation of the pin-yin sequence to a character sequence. Steps 1. and 3. are deterministic transformations, while Steps 2. and 4. are accomplished using statistical means. The IBM source-channel model for statistical machine translation (P. Brown et al., 1993) plays a central role in our system. We therefore describe it very briefly here for completeness. In this model, a word foreign language sentence          is modeled as the output of a “noisy channel” whose input is its correct  -word English translation       , and having observed the channel output  , one seeks a posteriori the most likely English sentence                    The translation model     is estimated from a paired corpus of foreign-language sentences and their English translations, and the language model    is trained from Engl"
W03-1508,C00-1056,0,0.64405,"nite state transducers that implement transformation rules for back-transliteration from Japanese to English have been described by Knight and Graehl (1997), and extended to Arabic by Glover-Stalls and Knight (1998). In both cases, the goal is to recognize words in Japanese or Arabic text which hapFigure 1: Four steps in English-to-Chinese transliteration of names. pen to be transliterations of English names. If the orthography of a language is strongly phonetic, as is the case for Korean, then one may use relatively simple hidden Markov models to transform English pronunciations, as shown by Jung et al (2000). The work closest to our application scenario, and the one with which we will be making several direct comparisons, is that of Meng et al (2001). In their work, a set of hand-crafted transformations for locally editing the phonemic spelling of an English word to conform to rules of Mandarin syllabification are used to seed a transformation-based learning algorithm. The algorithm examines some data and learns the proper sequence of application of the transformations to convert an English phoneme sequence to a Mandarin syllable sequence. Our paper describes a data driven counterpart to this tec"
W03-1508,W00-0504,1,0.891204,"Missing"
W03-1508,W98-1005,0,\N,Missing
W03-1508,P97-1017,0,\N,Missing
W06-3309,N04-1015,0,0.577447,"erages this knowledge— MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction—provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Discriminative approaches (especially SVMs) have been shown to be very effective for many supervised classification tasks; see, for example, (Joachims, 1998; Ng and Jordan, 2001). However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under cert"
W06-3309,P02-1047,0,0.0315097,"y a supervised approach. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are"
W06-3309,W00-1302,0,0.0187171,"heir work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are at least competitive with Support Vector Machines from a pu"
W07-0414,W05-1506,0,0.05168,"r each sentence of the test set, a lattice is created in the way described in Section 4, with parameters optimized on a small heldout set.7 For each sentence, the 1000-best candidates according to the lattice scores are extracted. We take the 10-best oracle candidates, according to the reference, and use a B LEU decoder to create the best permutation of each of them and pick the best one. Using this procedure, we make sure that we get the highest-scoring unreordered candidates and choose the best one among their oracle reorderings. Table 2 6 We use a straightforward adaption of Algorithm 3 in Huang and Chiang (2005) 7 We fill the initial phrase and word lattice with the 20 best candidates, using phrases of 3 or less words. 108 10 15 20 25 Sentence length 30 35 40 Figure 6: Reordering oracle scores for different sentence lengths. See also Table 2. 6 Discussion The empirical results show that reordering under sufficiently permissive constraints can improve a monotone baseline oracle by more than 7.5 B LEU points. This gap between choosing the best unreordered sentences versus choosing the best optimally reordered sentences is small for short sentences and widens dramatically (more than nine B LEU points) f"
W07-0414,H05-1021,0,0.4754,"lation task and show that reordering under the ITG constraints can improve over the baseline by more than 7.5 B LEU points. 1 Introduction Reordering the words and phrases of a foreign sentence to obtain the target word order is a fundamental, and potentially the hardest, problem in machine translation. The search space for all possible permutations of a sentence is factorial in the number of words/phrases; therefore a variety of models have been proposed that constrain the set of possible permutations by allowing certain reorderings while disallowing others. Some models (Brown et al. (1996), Kumar and Byrne (2005)) allow words to change place with their local neighbors, but disallow global We present a novel technique to compare achievable translation accuracies under different reordering constraints. While earlier work has trained and tested instantiations of different reordering models and then compared the translation results (Zens and Ney, 2003) we provide a more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms. Our technique attempts to answer the question: What is the highest B LEU score that a given translation system coul"
W07-0414,P06-1091,0,0.418803,"and stored only as boundary words of covered paths/ constituents. Therefore we cannot discount a locally attached word that has already been attached elsewhere to an alternative path/constituent. However, clipping affects most heavily the unigram scores which are constant, like the length of the sentence.4 4 Since the sentence lengths are constant for all reorderings of a given sentence we can in our experiments also ignore the brevity penalty which cancels out. If the input consists of sev107 We also adopt the approximation that treats every sentence with its reference as a separate corpus (Tillmann and Zhang, 2006) so that ngram counts are not accumulated, and parallel processing of sentences becomes possible. Due to these two approximations, our method is not guaranteed to find the best reordering defined by the reordering constraints. However, we have found on our heldout data that an oracle that does not accumulate n-gram counts is only minimally worse than an oracle that does accumulate them (up to 0.25 B LEU points).5 If, in addition, clipping is ignored, the resulting oracle stays virtually the same, at most 0.02 B LEU points worse than the oracle found otherwise. All results in this paper are com"
W07-0414,P06-1123,0,0.0347305,"ns and Ney (2003) perform a comparison of different reordering strategies. Their study differs from ours in that they use reordering models trained on real data and may therefore be influenced by feature selection, parameter estimation and other training-specific issues. In our study, only the baseline translation model is trained on data. Zens et al. (2004) conduct a study similar to Zens and Ney (2003) and note that the results for the ITG reordering constraints were quite dependent on the very simple probability model used. Our study avoids this issue by using the B LEU oracle approach. In Wellington et al. (2006), hand-aligned data are used to compare the standard ITG constraints to ITGs that allow gaps. 8 Conclusions We have presented a training-independent method to compare different reordering constraints for machine translation. Given a sentence in foreign word order, its reference translation(s) and reordering constraints, our dynamic-programming algorithms efficiently find the oracle reordering that has the approximately highest B LEU score. This allows evaluating different reordering constraints experimentally, but abstracting away from specific features, the probability model or training metho"
W07-0414,J97-3002,0,0.239581,"equence abcd cannot be reordered to cadb, for example. MJ1 and MJ-2 are shown in Figure 1. could 8 to-me 2 2.2 3 (a) The sentence in foreign word order. you Reordering constraints restrict the movement of words or phrases in order to reach or approximate the word order of the target language. Some of the constraints considered in this paper were originally proposed for reordering words, but we will describe all constraints in terms of reordering phrases. Phrases are units of consecutive words read off a phrase translation table. 2.1 if ITG Constraints The Inversion Transduction Grammar (ITG) (Wu, 1997), a derivative of the Syntax Directed Transduction Grammars (Aho and Ullman, 1972), constrains the possible permutations of the input string by defining rewrite rules that indicate permutations of the string. In particular, the ITG allows all permutations defined by all binary branching structures where the children of any constituent may be swapped in order. The ITG constraint is different from the other reordering constraints presented in that it is not based on finite-state operations. An Model MJ-1 MJ-2 IBM(2) IBM(4) IBM(4) (prune) ITG ITG (prune) # perm. 13 52 32 384 42 394 78 “Best” sent"
W07-0414,P06-1066,0,0.0365219,"Missing"
W07-0414,P03-1019,0,0.441736,"ations of a sentence is factorial in the number of words/phrases; therefore a variety of models have been proposed that constrain the set of possible permutations by allowing certain reorderings while disallowing others. Some models (Brown et al. (1996), Kumar and Byrne (2005)) allow words to change place with their local neighbors, but disallow global We present a novel technique to compare achievable translation accuracies under different reordering constraints. While earlier work has trained and tested instantiations of different reordering models and then compared the translation results (Zens and Ney, 2003) we provide a more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms. Our technique attempts to answer the question: What is the highest B LEU score that a given translation system could reach when using reordering constraints X? Using this oracle approach, we abstract away from issues that are not inherent in the reordering constraints, but may nevertheless influence the comparison results, such as model and feature design, feature selection, or parameter estimation. In fact, we compare several sets of reordering constra"
W07-0414,W05-0834,0,0.266989,"rom a simple baseline translation model. For each reordering paradigm, we take the candidate translations, get the best oracle reorderings under the given reordering constraints and pick the best sentence according to the B LEU score. The baseline translation system is created using probabilistic word-to-word and phrase-to-phrase taeral sentences of different lengths (see fn. 1) then the brevity penalty can be built in by keeping track of length ratios of attached phrases. 5 The accumulating oracle algorithm makes a greedy decision for every sentence given the ngram counts so far accumulated (Zens and Ney, 2005). The result of such a greedy oracle method may depend on the order of the input sentences. We tried 100 shuffles of these and received 100 very similar results, with a variance of under 0.006 B LEU points. The non-accumulating oracles use an epsilon value (1−10 ) for zero counts. 5 and Figure 6 show the resulting B LEU scores for different sentence lengths. Table 3 shows results of the ITG runs with different length bounds ρ. The average phrase length in the candidate translations of the test set is 1.42 words. Oracle decodings under the ITG and under IBM(4) constraints were up to 1000 times"
W07-0414,C04-1030,0,0.0910608,"s successful in reordering, improving the monotone baseline by only about 2.5 B LEU points at best, but is the best choice if speed is an issue. As described above, the reorderings defined by the local constraints MJ-1 and MJ-2 are subsets of IBM(2) and IBM(3). We did not test IBM(3), but the values can be interpolated between IBM(2) and IBM(4). The ITG constraints do not belong in this family of finite-state contraints; they allow reorderings that none of the other methods allow, and vice versa. The fact that ITG constraints can reach such high translation accuracies supports the findings in Zens et al. (2004) and is an empirical validation of the ITG hypothesis. The experiments with the constrained ITG show the effect of reorderings spanning different lengths (see Table 3). While most reorderings are shortdistance (&lt;5 phrases) a lot of improvements can still be obtained when ρ is increased from length 5 to 10 and even from 10 to 20 phrases. 7 Related Work There exist related algorithms that search the space of reorderings and compute B LEU oracle approxi109 Len. ρ=0 ρ=5 ρ=10 ρ=20 ρ=30 ρ=40 26–30 18.31 24.07 26.40 26.79 26.85 26.85 31–35 18.94 25.10 27.21 28.00 28.09 28.11 36–40 18.22 24.46 26.66 2"
W08-0402,J07-2003,0,0.401479,"able Decoder for Parsing-based Machine Translation with Equivalent Language Model State Maintenance Zhifei Li and Sanjeev Khudanpur Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218, USA zhifei.work@gmail.com and khudanpur@jhu.edu Abstract ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important firststep towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder. Our decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We describe a scalable decoder for parsingbased mac"
W08-0402,P03-2041,0,0.0175935,"und d to be reordered around of in the translation. The rule weight is omitted in this example. A bilingual SCFG derivation is analogous to a monolingual CFG derivation. It begins with a pair of aligned start symbols. At each step, an aligned pair of nonterminals is rewritten as the two corresponding components of a single rule. In this sense, the derivations are generated synchronously. Our decoder presently handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars (Eisner, 2003; Chiang, 2006). 2.2 MT Decoding as Chart Parsing Given a source-language sentence f ∗ , the decoder must find the target-language yield e(D) of the best 11 derivation D among all derivations with sourcelanguage yield f (D) = f ∗ , i.e. e ∗ µ = e arg max D : f (D)=f ∗ ¶ w(D) , (2) where w(D) is the composite weight of D. The parser may be treated as a deductive proof system (Shieber et al., 1995). Formally (cf. (Chiang, 2007)), a parser defines a space of weighted items, with some items designated as axioms and some as goals, and a set of inference rules of the form I1 : w1 · · · Ik : wk φ, I:"
W08-0402,P06-1121,0,0.0220669,"decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to impleStraightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity. Therefore, it is important to develop efficient methods for LM integration. We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs. Specifically, instead of maintaining a separate state for each distinguished sequence of “state” words, we merge multiple states that can be made equival"
W08-0402,W05-1506,0,0.0901444,"representing a set of likely hypotheses D in (2). Briefly, a hyper-graph is a set of vertices and hyper-edges, with each hyper-edge connecting a set of antecedent vertices to a consequent vertex, and a special vertex designated as the target vertex. In parsing parlance, a vertex corresponds to an item in the chart, a hyper-edge corresponds to a SCFG rule with the nonterminals on the right-side replaced by back-pointers to antecedent items, and the target 12 vertex corresponds to the goal item2 . Given a hyper-graph for a source-language sentence f ∗ , we use the k-best extraction algorithm of Huang and Chiang (2005) to extract its k most likely translations. Moreover, since many different derivations D in (2) may lead to the same target-language yield e(D), we adopt the modification described in Huang et al. (2006) to efficiently generate the unique k best translations of f ∗ . 3 Parallel and Distributed Computing Many applications of parsing-based MT entail the use of SCFGs extracted from millions of bilingual sentence pairs and LMs extracted from billions of words of target-language text. This requires the decoder to make use of distributed computing to spread the memory required to load large-scale SC"
W08-0402,P07-1019,0,0.0336906,"es the translation grammar. w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(·) provides the LM probability for all complete m-grams in a string, while the function q(·) elides symbols whose m-grams have been accounted for by p(·). Details about the functions p(·) and q(·) are provided in Section 4. 2.3 Pruning in a Decoder Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large vocabularies TT and detailed nonterminal sets. In our decoder, we incorporate two pruning techniques described by (Chiang, 2007; Huang and Chiang, 2007). For beam pruning, in each cell, we discard all items whose weight is worse, by a relative threshold β, than the weight of the best item in the same cell. If too many items pass the threshold, a cell only retains the top-b items by weight. When combining smaller items to obtain a larger item by applying an inference rule, we use cube-pruning to simulate kbest extraction in each destination cell, and discard combinations that lead to an item whose weight is worse than the best item in that cell by a margin of . 2.4 k-best Extraction Over Hyper-graphs For each source-language sentence f ∗ , th"
W08-0402,2006.amta-papers.8,0,0.101232,"model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to impleStraightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity. Therefore, it is important to develop efficient methods for LM integration. We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs. Specifically, instead of maintaining a separate state"
W08-0402,P06-1077,0,0.0102475,"r language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to impleStraightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity. Therefore, it is important to develop efficient methods for LM integration. We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs. Specifically, instead of maintaining a separate state"
W08-0402,D07-1104,0,0.254006,"rate the back-off weights of the “LM histories” of the suffix of em−1 that went miss1 ing due to left LM state reduction. 15 We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT evaluation. The parallel data we select contains about 570K Chinese-English sentence pairs, adding up to about 19M words on each side. To train the English language models, we use the English side of the parallel text and a subset of the English Gigaword corpus, for a total of about 130M words. We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al., 2003) to obtain wordalignments, a translation model, language models, and the optimal weights for combining these models, respectively. distributed LM architecture5 as discussed in Section 3.2. As shown in Table 2, the 7-gram distributed language model (DLM) significantly improves translation performance over the 5-gram LM. However, decoding is significantly slower (12.2 sec/sent when using the non-parallel decoder) due to the added network communication overhead. 5.2 Improvements in Decoding Speed Table 2: Distr"
W08-0402,P03-1021,0,0.0359985,"Missing"
W08-0402,P00-1056,0,0.0511255,"ion that some care must be taken later to incorporate the back-off weights of the “LM histories” of the suffix of em−1 that went miss1 ing due to left LM state reduction. 15 We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT evaluation. The parallel data we select contains about 570K Chinese-English sentence pairs, adding up to about 19M words on each side. To train the English language models, we use the English side of the parallel text and a subset of the English Gigaword corpus, for a total of about 130M words. We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al., 2003) to obtain wordalignments, a translation model, language models, and the optimal weights for combining these models, respectively. distributed LM architecture5 as discussed in Section 3.2. As shown in Table 2, the 7-gram distributed language model (DLM) significantly improves translation performance over the 5-gram LM. However, decoding is significantly slower (12.2 sec/sent when using the non-parallel decoder) due to the added network communication overhead. 5.2 Im"
W08-0402,P02-1040,0,0.104171,"Missing"
W08-0402,P05-1034,0,0.0536714,"be made equivalent for language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to impleStraightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity. Therefore, it is important to develop efficient methods for LM integration. We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs. Specifically, instead of maintaini"
W08-0402,N07-1063,0,0.101952,"criminative training experiments. 10 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 Parsing-based MT Decoder In this section, we discuss the core algorithms implemented in our decoder. These algorithms have been discussed by Chiang (2007) in detail, and we recapitulate the essential parts here for completeness. 2.1 Grammar Formalism Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Following the notation in Venugopal et al. (2007), a probabilistic SCFG comprises a set of source-language terminal symbols TS , a set of target-language terminal symbols TT , a shared set of nonterminal symbols N , and a set of rules of the form X → hγ, α, ∼, wi , (1) where X ∈ N , γ ∈ [N ∪TS ]∗ is a (mixed) sequence of nonterminals and source terminals, α ∈ [N ∪TT ]∗ is a sequence of nonterminals and target terminals, ∼ is a one-to-one correspondence or alignment between the nonterminal elements of γ and α, and w ≥ 0 is a weight assigned to the rule. An illustrative rule for Chinese-to-English translation is N P → h N P0 d N P1 , N P1 of N"
W08-0402,W06-1626,0,0.0150877,"LM into memory only once. This multithreading provides a very significant speed-up. 3.2 Distributed Language Models It is not possible in some cases to load a very large LM into memory on a single machine, particularly if the SCFG is also very large. In other cases, loading the LM each time the decoder runs may be too time-consuming relative to the time required for decoding itself, such as in iterative decoding with updated combination weights during minimum errorrate training. It is therefore desirable to have dedicated servers to load parts of the LM3 — an idea that has been exploited by (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007). Our implementation can load a (partitioned) LM on different servers before initiating decoding. The decoder remotely calls the servers to obtain individual LM probabilities, and linearly interpolates them on the fly using a given set of interpolation weights. With this architecture, one can deal with a very large target-language text corpus by splitting it into many parts and training separate LMs from each. The runtime interpolation capability may also be used for LM adaptation, e.g. for building document-specific language models. To mitigate potent"
W08-0402,D07-1090,0,\N,Missing
W09-0424,P05-1032,1,0.752623,"s also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all rules from the training set. The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set. However, we are currently extending the decoder to directly access the suffix array. This will allow the decoder at runtime to efficiently extract exactly those rules needed to tra"
W09-0424,W08-0309,1,0.277708,"lable as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 137 System Joshua Baseline Minimum Bayes Risk Rescoring Deterministic Annealing Variational Decoding of 21.2 million English sentences with half a billion words. We used SRILM to train a 5-gram language model using a vocabulary containing the 500,000 most frequent words in this corpus. Note that we did not use the English side of the parallel corpus as language model training data. To tune the system parameters we used News Test Set from WMT08 (Callison-Burch et al., 2008), which consists of 2,051 sentence pairs with 43 thousand English words and 46 thousand French words. This is in-domain data that was gathered from the same news sources as the WMT09 test set. 3.2 BLEU-4 25.92 26.16 25.98 26.52 Table 1: The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In t"
W09-0424,D07-1104,0,0.412715,"e. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of"
W09-0424,J07-2003,0,0.947277,"ovided as an abstract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and"
W09-0424,moore-2002-fast,0,0.0291243,"d the Olympic Committee. The crawl gathered approximately 40 million files, consisting of over 1TB of data. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We wrote set of simple heuristics to transform French URLs onto English URLs, and considered matching documents to be translations of each other. This yielded 2 million French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section"
W09-0424,P03-2041,0,0.0958636,"For our submission, we used k = 20, which resulted in 1.5 million (out of 23 million) sentence pairs being selected for use as training data. There were 30,037,600 English words and 30,083,927 French words in the subsampled training corpus. 2.2 2.3 Decoding Algorithms2 Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Currently, it only handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs (e.g., (Galley et al., 2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm. Specifically, the decoding algorithm maintains a chart, which contains an array of cells. Each cell in turn maintains a list of proven items. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translat"
W09-0424,J03-1002,0,0.00743813,"es on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objecti"
W09-0424,P06-1121,0,0.562847,"ize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms des"
W09-0424,P03-1021,0,0.692054,"other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two direct"
W09-0424,P05-1034,0,0.103948,"stract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the es"
W09-0424,W05-1506,0,0.084474,"ls on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivations from the hypergraph. Parallel and distributed decoding: We also implement parallel decoding and a distributed language model by exploiting multi-core and multi-processor architectures and distributed computing techniques. More details on these two features are provided by Li and Khudanpur (2008b). 2.4 updates, each reflecting a greedy selection of the dimension giving the most gain. Each iteration also optimizes several random “intermediate initial” points in addition to the one surviving from the previous iteration, as an approximation to performing"
W09-0424,P06-2101,0,0.0477005,"raining (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness 4 C"
W09-0424,N03-1017,0,0.0293738,"oaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all ru"
W09-0424,P07-2045,1,0.0364964,"well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the to"
W09-0424,P07-1065,0,0.0441818,"distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring function in Java. This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams. This native implementation is more scalable than the basic Java LM implementation. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). 2.5 WMT-09 Translation Task Results Minimum Error Rate Training Johsua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu. The optimization consists of a series of line-optimizations along the dimensions corresponding to the parameters. The search across a dimension uses the efficient method of Och (2003). Each iteration of our MERT implementation consists of multiple weight 3 This feature allows users to easily try the Joshua toolkit without installing the SRILM toolkit and compiling the n"
W09-0424,2005.mtsummit-papers.11,0,0.0944271,"llion French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section 2.1. For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09. This data consisted Language Models In addition to the distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring func"
W09-0424,N04-1022,0,0.0715322,"r training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct deri"
W09-0424,2008.amta-papers.12,1,0.948747,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W08-0402,1,0.946992,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W09-0424,1,0.108172,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P09-1067,1,0.881207,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P06-1077,0,0.253036,"new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007)"
W09-0424,D08-1076,0,\N,Missing
W10-1718,N09-1025,0,0.0465037,"Missing"
W10-1718,J07-2003,0,0.2479,"Missing"
W10-1718,P08-1115,0,0.0560574,"Missing"
W10-1718,J99-4004,0,0.114206,"Missing"
W10-1718,D09-1005,1,0.852808,"Missing"
W10-1718,W08-0402,1,0.900437,"Missing"
W10-1718,N09-2003,1,0.885137,"Missing"
W10-1718,W09-0424,1,0.907315,"Missing"
W10-1718,P09-1067,1,0.884344,"Missing"
W10-1718,D07-1104,0,0.0414902,"Missing"
W10-1718,P03-1021,0,0.11921,"Missing"
W10-1718,2006.amta-papers.25,0,0.0712727,"Missing"
W10-1718,W06-3119,0,0.208519,"Missing"
W10-1718,N04-1035,0,\N,Missing
W10-1718,W10-1726,1,\N,Missing
W10-1718,P02-1001,0,\N,Missing
W12-2707,P05-1063,0,0.0705001,"Missing"
W12-2707,D09-1116,0,0.0443834,"Missing"
W12-2707,D09-1087,0,0.0607762,"Missing"
W12-2707,J93-2004,0,0.0425648,"Missing"
W12-2707,D07-1111,0,0.0299004,"r proceeds leftto-right, and only hypothesized structures based on w1 , . . . , wi−1 are used by the SLM to predict wi . Similarly, the specific features used by the parser are also not important: more noteworthy is that the SLM uses (h.w−3 , h.w−2 , h.w−1 ) and their POS tags to predict wi . The question is whether this yields lower perplexity than predicting wi from (wi−3 , wi−2 , wi−1 ). For the sake of completeness, we next describe the parser and SLM in some detail, but either may be skipped without loss of continuity. The Parser: We use the shift-reduce incremental dependency parser of (Sagae and Tsujii, 2007), which constructs a tree from a transition sequence governed by a maximum-entropy classifier. Shiftreduce parsing places input words into a queue Q and partially built structures are organized by a stack S. Shift and reduce actions consume the queue and build the output parse on the stack. The classifier g assigns probabilities to each action, and the probability of a state pg (π) can be computed as the product of the probabilities of a sequence of actions that resulted in the state. The parser therefore provides (multiple) syntactic analyses of the history w1 , . . . , wi−1 at each word posi"
W12-2707,W11-0328,0,0.016954,"lary we use in BN experiments has about 84K words. • WSJ setup : The training text consists of about 37M words. We use eval92+eval93 (10K words) as our evaluation set and dev93 (9K words) serves as our development set for interpolating SLMs with the baseline 4-gram model. In both cases, we sample about 20K sentences from the training text (we exclude them from training data) to serve as our heldout data for applying the bucketing algorithm and estimating λ’s. To apply the dependency parser, all the data sets are first converted to Treebank-style tokenization and POStagged using the tagger of (Tsuruoka et al., 2011)2 . Both the POS-tagger and the shift-reduce dependency parser are trained on the Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and the WSJ Penn Treebank (after converting them to dependency trees) which consists of about 1.2M tokens. Finally, we train a modified kneser-ney 4-gram LM on the tokenized training text to serve as our baseline LM, for both experiments. 5.2 Results and Analysis Table 2 shows the perplexity results for BN and WSJ experiments, respectively. It is evident that the 4gram baseline for BN is stronger than the 40M case of Table 1. Yet, the interpolated S"
