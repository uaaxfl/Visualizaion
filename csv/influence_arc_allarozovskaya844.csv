2020.bea-1.21,W18-6111,0,0.272919,"the state-of-the-art results on the CoNLL datasets were obtained using SMT and NMT approaches. The systems are typically trained on a combination of NUCLE and the English part of the Lang-8 corpus (Mizumoto et al., 2012), even though the latter is known to contain noise, as it is only partially corrected. Minimally-Supervised and Data-Augmented GEC There has been a lot of work on generating synthetic training data for GEC. The approaches can be broken down into those that attempt to make use of additional resources (e.g. Wikipedia edits) or noisify correct English data via artificial errors. Boyd (2018) augmented training data with edits extracted from Wikipedia revision history in German. The edits were classified and only those relating to GEC were kept. Wikipedia edits are extracted from the revision history using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014). The contribution of the resulting edits is demonstrated using a multilayer convolutional encoderdecoder neural network model that we also use in this work. Mizumoto et al. (2011) extracted a Japanese learners corpus from the revision log of Lang-8 (about 1 million sentences) and implemented a character-based machine-translatio"
2020.bea-1.21,W19-4406,0,0.0120873,"tten text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a lack of standardised experimental settings: Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have The goal of this paper is to compare the techniques for generating synthetic data used by the UEDIN-MS and Kakao&Brain systems. The"
2020.bea-1.21,P17-1074,0,0.0378574,"ve within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the other variables, such as model choice, hyperparameters, and the choice of native data. We train NMT systems and evaluate our models on two learner corpora, the W&I+LOCNESS corpus introduced in BEA-2019, and the FCE corpus (Yannakoudakis et al., 2011). Using the automatic error type tool ERRANT (Bryant et al., 2017), we also show performance evaluation by error type on the two corpora. The paper makes the following contributions: (1) we provide a fair comparison of two methods for generating synthetic parallel data for GEC, using two evaluation datasets; (2) we find that the two methods train different complementary systems and target different types of errors: while the Inverted Spellchecker approach is good at identifying spelling errors, the Patterns+POS approach is better at correcting errors relating to grammar, such as noun number, verb agreement, and verb tense; (3) overall, the Patterns+POS metho"
2020.bea-1.21,W19-4423,0,0.421514,"d with correcting grammatical errors in written text. Current GEC systems, namely those leveraging statistical and neural machine translation, require large quantities of annotated training data, which can be expensive or impractical to obtain. This research compares techniques for generating synthetic data utilized by the two highest scoring submissions to the restricted and low-resource tracks in the BEA-2019 Shared Task on Grammatical Error Correction. 1 Introduction The two top scoring systems in the Restricted and Low Resource tracks, UEDIN-MS (Grundkiewicz et al., 2019) and Kakao&Brain (Choe et al., 2019), outperformed the other teams by a large margin in both tracks; furthermore, both systems made use of artificial data for training their NMT systems, but they generated artificial data in different ways. Interestingly, in the Restricted Track, both of the systems scored on par, while in the Low Resource Track Kakao&Brain exhibited a larger gap in performance (a drop of more than 10 points compared to the Restricted track) vs. 4 points for UEDIN-MS. While both teams used the same model architecture, transformerbased neural machine translation (NMT) (Vaswani et al., 2017), in addition to the di"
2020.bea-1.21,W19-4427,0,0.113181,"Missing"
2020.bea-1.21,P13-2121,0,0.0234378,"his method on in-domain W&I+LOCNESS evaluation data. To explore this hypothesis, we analyze these models’ performance with respect to ERRANT error types in Section 6. We also note that, when a spellchecker is added, performance is improved substantially for the Patterns+POS methods (5 and 7 points, respectively, on W&I+LOCNESS and FCE datasets). In conapproach using a transformer architecture, but in preliminary results it did not outperform the CNN. The language model (LM) is a 5-gram LM trained on the publicly available WMT News Crawl corpus (233 million sentences), using the KenLM toolkit (Heafield et al., 2013). We also use an off-the-shelf speller (Flor, 2012; Flor and Futagi, 2012) as a pre-processing step (prior to running the grammar correction system). We include results with and without the use of the speller. Most of the experiments (except experiment 1, as shown below) are performed using 2 million sentences (50 million tokens) from the WMT News Crawl corpus. We use this data to create artificially noised source data with the noising techniques described above. For the Inverted Spellchecker method, we use the same error rate of 0.15 used by the authors in their original system (the error rat"
2020.bea-1.21,P17-1070,0,0.0707626,"in the data generation methods, the systems used different training scenarios, hyperpameter values, and training corpora of native data. Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a lack of standardised experimental settings: Although"
2020.bea-1.21,P11-1092,0,0.0340954,"sing Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014). The contribution of the resulting edits is demonstrated using a multilayer convolutional encoderdecoder neural network model that we also use in this work. Mizumoto et al. (2011) extracted a Japanese learners corpus from the revision log of Lang-8 (about 1 million sentences) and implemented a character-based machine-translation model. The other approach of generating parallel data creates artificial errors in well-formed native data. This approach was shown to be effective within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the other variables, such as model choice, hyperparameters, and the choice of native data. We train NMT systems and evaluate our models on two learner corpora, the W&I+LOCNESS corpus introduced in BEA-2019, and the FCE corpus (Yannakoudakis et al., 2011). Using the automatic error type tool ERRANT (Bryant et al., 2017), we also show performance evaluation by error type on the two corpor"
2020.bea-1.21,W14-1703,0,0.0463164,"Missing"
2020.bea-1.21,D12-1052,0,0.0245436,"terns+POS approach. In the next section, we discuss related work. Section 3 gives an overview of the W&I+LOCNESS and FCE learner datasets. Section 4 describes the error generation methods. Experiments are presented in section 5. Section 6 analyzes the results. Section 7 concludes the paper. 2 Related Work Progress in English GEC Earlier GEC approaches focused on English as a Second Language Learners and made use of linear machine learning algorithms and developed classifiers for specific error types, such as articles, prepositions, or noun number (Gamon, 2010; Rozovskaya and Roth, 2010, 2014; Dahlmeier and Ng, 2012). The classifiers can be trained on native English data, learner data, or a combination thereof. The CoNLL shared tasks on English grammar correction provided a first large annotated corpus 3 The Learner Datasets We make use of two publicly-available datasets of learner texts for evaluation – the W&I+LOCNESS and the FCE – described below. The BEA-2019 Shared Task introduced a new parallel corpus designed to represent a wide range of English proficiency levels. The W&I+LOCNESS corpus consists of handannotated data drawn from two sources. The Cambridge English Write & Improve (W&I) data 199 Type"
2020.bea-1.21,W13-1703,0,0.0576579,"hat were added to the artificially-generated data. Our goal in combining synthetic data with learner data is to evaluate to contribution of synthetic data (generated in different ways) in various scenarios with in-domain and out-domain learner data available. The additional learner training data comes from the publiclyavailable learner corpora of various sizes and various sources of data: the W&I+LOCNESS and the FCE training partitions (treated as in-domain for the respective evaluation datasets), the Lang-8 corpus (Mizumoto et al., 2012), and the NUCLE corpus from the CoNLL-2014 shared task (Dahlmeier et al., 2013) (both treated as out-of-domain for the two datasets). These learner corpora were also allowed for use in the Restricted track. Statistics for the amounts of data can be seen in Table 2. The first experiment trains models on 50M tokens of artificial data generated by each noising method. The second experiment adds W&I+LOCNESS training data to the artificial data. Experiment 3 adds the FCE training set to the artificial data. In the fourth experiment, we add the entirety of the annotated training corpora (FCE, Lang-8, and NUCLE) consisting of 13.5 million tokens to the initial artificially-gene"
2020.bea-1.21,D16-1161,0,0.013095,"anslation (NMT) (Vaswani et al., 2017), in addition to the differences in the data generation methods, the systems used different training scenarios, hyperpameter values, and training corpora of native data. Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a"
2020.bea-1.21,E14-3011,0,0.0213495,"Missing"
2020.bea-1.21,W12-2006,0,0.020895,"sh GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a lack of standardised experimental settings: Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have The goal of this paper is to compare the techniques for generating synthetic data used by the UEDIN-MS and Kakao&Brain systems. The method used in the UEDIN-MS system utilizes confusion sets generated by a spel"
2020.bea-1.21,E14-3013,0,0.0805019,"ewicz and Junczys-Dowmunt, 2014). The contribution of the resulting edits is demonstrated using a multilayer convolutional encoderdecoder neural network model that we also use in this work. Mizumoto et al. (2011) extracted a Japanese learners corpus from the revision log of Lang-8 (about 1 million sentences) and implemented a character-based machine-translation model. The other approach of generating parallel data creates artificial errors in well-formed native data. This approach was shown to be effective within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the other variables, such as model choice, hyperparameters, and the choice of native data. We train NMT systems and evaluate our models on two learner corpora, the W&I+LOCNESS corpus introduced in BEA-2019, and the FCE corpus (Yannakoudakis et al., 2011). Using the automatic error type tool ERRANT (Bryant et al., 2017), we also show performance evaluation by error type on the two corpora. The paper makes the f"
2020.bea-1.21,C12-2084,0,0.131296,"ner data for training (NUCLE, (Ng et al., 2014)), as well as two test sets. All the data was produced by learners of English studying at the National University of Singapore (majority of which were native speakers of Chinese). The statistical machine translation approach was shown to be successful in the CoNLL-2014 competition for the first time (Junczys-Dowmunt and Grundkiewicz, 2014). Since then, the state-of-the-art results on the CoNLL datasets were obtained using SMT and NMT approaches. The systems are typically trained on a combination of NUCLE and the English part of the Lang-8 corpus (Mizumoto et al., 2012), even though the latter is known to contain noise, as it is only partially corrected. Minimally-Supervised and Data-Augmented GEC There has been a lot of work on generating synthetic training data for GEC. The approaches can be broken down into those that attempt to make use of additional resources (e.g. Wikipedia edits) or noisify correct English data via artificial errors. Boyd (2018) augmented training data with edits extracted from Wikipedia revision history in German. The edits were classified and only those relating to GEC were kept. Wikipedia edits are extracted from the revision histo"
2020.bea-1.21,I11-1017,0,0.0773883,"hes can be broken down into those that attempt to make use of additional resources (e.g. Wikipedia edits) or noisify correct English data via artificial errors. Boyd (2018) augmented training data with edits extracted from Wikipedia revision history in German. The edits were classified and only those relating to GEC were kept. Wikipedia edits are extracted from the revision history using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014). The contribution of the resulting edits is demonstrated using a multilayer convolutional encoderdecoder neural network model that we also use in this work. Mizumoto et al. (2011) extracted a Japanese learners corpus from the revision log of Lang-8 (about 1 million sentences) and implemented a character-based machine-translation model. The other approach of generating parallel data creates artificial errors in well-formed native data. This approach was shown to be effective within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the"
2020.bea-1.21,W12-2012,0,0.0283114,"hesis, we analyze these models’ performance with respect to ERRANT error types in Section 6. We also note that, when a spellchecker is added, performance is improved substantially for the Patterns+POS methods (5 and 7 points, respectively, on W&I+LOCNESS and FCE datasets). In conapproach using a transformer architecture, but in preliminary results it did not outperform the CNN. The language model (LM) is a 5-gram LM trained on the publicly available WMT News Crawl corpus (233 million sentences), using the KenLM toolkit (Heafield et al., 2013). We also use an off-the-shelf speller (Flor, 2012; Flor and Futagi, 2012) as a pre-processing step (prior to running the grammar correction system). We include results with and without the use of the speller. Most of the experiments (except experiment 1, as shown below) are performed using 2 million sentences (50 million tokens) from the WMT News Crawl corpus. We use this data to create artificially noised source data with the noising techniques described above. For the Inverted Spellchecker method, we use the same error rate of 0.15 used by the authors in their original system (the error rate is chosen to simulate the error rate of the learner data). The same prob"
2020.bea-1.21,N16-1133,0,0.0171021,"in addition to the differences in the data generation methods, the systems used different training scenarios, hyperpameter values, and training corpora of native data. Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a lack of standardised experimen"
2020.bea-1.21,N10-1019,0,0.0404523,"eficial, and is especially helpful for the Patterns+POS approach. In the next section, we discuss related work. Section 3 gives an overview of the W&I+LOCNESS and FCE learner datasets. Section 4 describes the error generation methods. Experiments are presented in section 5. Section 6 analyzes the results. Section 7 concludes the paper. 2 Related Work Progress in English GEC Earlier GEC approaches focused on English as a Second Language Learners and made use of linear machine learning algorithms and developed classifiers for specific error types, such as articles, prepositions, or noun number (Gamon, 2010; Rozovskaya and Roth, 2010, 2014; Dahlmeier and Ng, 2012). The classifiers can be trained on native English data, learner data, or a combination thereof. The CoNLL shared tasks on English grammar correction provided a first large annotated corpus 3 The Learner Datasets We make use of two publicly-available datasets of learner texts for evaluation – the W&I+LOCNESS and the FCE – described below. The BEA-2019 Shared Task introduced a new parallel corpus designed to represent a wide range of English proficiency levels. The W&I+LOCNESS corpus consists of handannotated data drawn from two sources."
2020.bea-1.21,W14-1701,0,0.0403211,"framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field after a long hiatus, as recent GEC systems have become difficult to evaluate given a lack of standardised experimental settings: Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have The goal of this paper is to compare the techniques for generating synthetic data used by the UEDIN-MS and Kakao&Brain systems. The method used in the UEDIN-MS system utilizes confusion sets generated by a spellchecker, while th"
2020.bea-1.21,N10-1018,1,0.58112,"is especially helpful for the Patterns+POS approach. In the next section, we discuss related work. Section 3 gives an overview of the W&I+LOCNESS and FCE learner datasets. Section 4 describes the error generation methods. Experiments are presented in section 5. Section 6 analyzes the results. Section 7 concludes the paper. 2 Related Work Progress in English GEC Earlier GEC approaches focused on English as a Second Language Learners and made use of linear machine learning algorithms and developed classifiers for specific error types, such as articles, prepositions, or noun number (Gamon, 2010; Rozovskaya and Roth, 2010, 2014; Dahlmeier and Ng, 2012). The classifiers can be trained on native English data, learner data, or a combination thereof. The CoNLL shared tasks on English grammar correction provided a first large annotated corpus 3 The Learner Datasets We make use of two publicly-available datasets of learner texts for evaluation – the W&I+LOCNESS and the FCE – described below. The BEA-2019 Shared Task introduced a new parallel corpus designed to represent a wide range of English proficiency levels. The W&I+LOCNESS corpus consists of handannotated data drawn from two sources. The Cambridge English Writ"
2020.bea-1.21,P11-1093,1,0.784891,"from the revision history using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014). The contribution of the resulting edits is demonstrated using a multilayer convolutional encoderdecoder neural network model that we also use in this work. Mizumoto et al. (2011) extracted a Japanese learners corpus from the revision log of Lang-8 (about 1 million sentences) and implemented a character-based machine-translation model. The other approach of generating parallel data creates artificial errors in well-formed native data. This approach was shown to be effective within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the other variables, such as model choice, hyperparameters, and the choice of native data. We train NMT systems and evaluate our models on two learner corpora, the W&I+LOCNESS corpus introduced in BEA-2019, and the FCE corpus (Yannakoudakis et al., 2011). Using the automatic error type tool ERRANT (Bryant et al., 2017), we also show performance evaluation by erro"
2020.bea-1.21,Q14-1033,1,0.864856,"Missing"
2020.bea-1.21,D14-1102,0,0.0172846,"IN-MS. While both teams used the same model architecture, transformerbased neural machine translation (NMT) (Vaswani et al., 2017), in addition to the differences in the data generation methods, the systems used different training scenarios, hyperpameter values, and training corpora of native data. Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was"
2020.bea-1.21,P11-1019,0,0.193897,"errors in well-formed native data. This approach was shown to be effective within the classification framework (Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011; Felice and Yuan, 2014). on POS-based confusions. Henceforth, we refer to these as Inverted Spellchecker method and Patterns+POS method, respectively. To ensure a fair comparison of the methods, we control for the other variables, such as model choice, hyperparameters, and the choice of native data. We train NMT systems and evaluate our models on two learner corpora, the W&I+LOCNESS corpus introduced in BEA-2019, and the FCE corpus (Yannakoudakis et al., 2011). Using the automatic error type tool ERRANT (Bryant et al., 2017), we also show performance evaluation by error type on the two corpora. The paper makes the following contributions: (1) we provide a fair comparison of two methods for generating synthetic parallel data for GEC, using two evaluation datasets; (2) we find that the two methods train different complementary systems and target different types of errors: while the Inverted Spellchecker approach is good at identifying spelling errors, the Patterns+POS approach is better at correcting errors relating to grammar, such as noun number, v"
2020.bea-1.21,N16-1042,0,0.050441,"s used the same model architecture, transformerbased neural machine translation (NMT) (Vaswani et al., 2017), in addition to the differences in the data generation methods, the systems used different training scenarios, hyperpameter values, and training corpora of native data. Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text. More recently, significant progress has been made, especially in English GEC, within the framework of statistical machine translation (SMT) and neural machine translation (NMT) approaches (Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018). The success of these approaches can be partially attributed to the availability of several large training sets. In the most recent Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019), which continued the tradition of earlier GEC competitions (Dale et al., 2012; Ng et al., 2014), all of the 24 participating teams applied NMT and/or SMT approaches. One of the goals of BEA-2019 was to re-evaluate the field"
2021.eacl-main.231,P15-2097,0,0.0184758,"of the system outputs, whereas we are creating golds by directly correcting the hypotheses output by the system. We note that there have been other proposals that argue that this principle still does not make the output fluent and propose generating references based on fluency (Sakaguchi et al., 2016). As suggested by Choshen and Abend (2018b), correcting for fluency further increases the space of valid corrections for a sentence, and we do not attempt to do this in this work. Reference-based evaluations include several measures, such as the MaxMatch scorer M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015), ERRANT (Bryant et al., 2017), and Imeasure (Felice and Briscoe, 2015). These met2687 rics have some commonalities, e.g. both MaxMatch and ERRANT measure precision, recall, and F-score. M 2 has been used with different beta parameter values, the default is beta = 0.5, weighting precision twice as high as recall, which is more common than assigning equal weights and has been shown to have stronger correlation with human ratings (Grundkiewicz et al., 2015). GLEU focuses on the fluency aspect – it is an extension of the BLEU metric in Machine Translation (Papineni et al., 2002). I-measure emphas"
2021.eacl-main.231,E17-2037,0,0.0131704,"upy. A system is evaluated using reference-based metrics where for each source sentence there is at least one corresponding corrected version that was generated by a human expert. We refer to this corrected version as Reference Gold (RG). The set of possible correct versions for a given source sentence is very large – possibly infinite – and any single reference gold is just a single point in that space. Most of the GEC evaluation sets contain one RG for each source sentence, although some (English) datasets contain more (CoNLL-test has 2 and an additional set of 8 generated later, and JFLEG (Napoles et al., 2017) has 4 fluency-based references). System performance is computed by scoring the topranked hypothesis H1 for each sentence against the corresponding RG. In addition to RGs, we create for each pair of (source, Hi ), where Hi is the system hypothesis, • • • • • S – original text Hi – hypothesis at rank i RG – reference gold CGi – closest gold to hypothesis Hi Gold edit – an edit between a source sentence and an RG or CG Proposed edit – an edit between a source sentence and a system hypothesis Correct edit – a proposed edit that is also a gold edit relative to a system hypothesis and specific refe"
2021.eacl-main.231,W13-3601,0,0.0740022,"Missing"
2021.eacl-main.231,W14-1701,0,0.0245585,"(hypotheses at ranks 1 and 10, H1 and H10 ); the reference gold (RG), and two additional golds generated on top of each of the hypotheses (CG1 and CG10 ). edit), while Dist (H10 , CG10 ) is 0. The three golds – two CGs and the RG – illustrate the notion of semantic equivalence (multiple ways of correcting the same source sentence, while preserving its meaning), not reflected in the standard evaluation. 3.2 Experimental Setup We perform experiments on 2 English and 2 Russian datasets, using diverse NMT GEC model frameworks. The English datasets include the commonly used benchmarks – CoNLL-14 (Ng et al., 2014; Dahlmeier et al., 2013), and the BEA corpus (Bryant et al., 2019). The Russian datasets include the RULEC-GEC corpus (Rozovskaya and Roth, 2019) (henceforth RULEC) and another dataset of Russian learner writing that has been recently collected from the online language learning platform Lang-8 (Mizumoto et al., 2011) and annotated by native speakers.2 We refer to this dataset as Lang8. CoNLL-14 contains two primary RGs against which the systems are standardly evaluated, while the other datasets include one RG for each sentence. We report results using one RG for each dataset for uniformity, a"
2021.findings-acl.359,I17-4001,0,0.0235004,"released new evaluation data, both from learners of English and native speakers. Napoles et al. (2019) further addressed the issue of robustness of GEC models, by proposing novel evaluation metrics, and also released a diverse GEC dataset. GEC on Other Languages Two most prominent attempts at GEC in other languages include shared tasks on Arabic and Chinese text correction. In Arabic, a large-scale corpus (2M words) was collected and annotated as part of the QALB project (Zaghouani et al., 2014). There have also been three shared tasks on Chinese grammatical error diagnosis (Lee et al., 2016; Rao et al., 2017, 2018). In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling) (Imamura et al., 2012; Israel et al., 2013; de Ilarraza et al., 2008; Vincze et al., 2014). The most relevant to us is the work of Rozovskaya and Roth (2019) that made available an annotated corpus of Russian learner essays. The data released in Rozovskaya and Roth (2019) is relatively uniform, as it is all produced by native English speakers, whereas the RU-Lang8 data comes from a diverse set of speakers. 3 The RU-Lang8 Dataset RU-"
2021.findings-acl.359,P11-1093,1,0.723905,"sian GEC, which is more diverse linguistically and contains data of different genre of writing, compared to the existing resource RULEC. We make the resource available for research purposes;1 (2) We provide benchmark results on this new corpus, using state-of-the-art models that are trained on synthetic data and learner data; (3) We provide an error analysis showing that most of the grammar errors are still challenging for the current systems. 2 Related Work Progress in English GEC There has been a lot of work on grammatical error correction, but most of the research has been done on English (Rozovskaya and Roth, 2011; Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Rozovskaya and Roth, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018; Kaneko et al., 2020). Recently, state-of-the-art results were obtained using statistical and neural machine translation approaches. The systems are typically trained on a combination of native data with synthetic er1 https://github.com/arozovskaya/RU-Lang8 4103 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4103–4111 August"
2021.findings-acl.359,P16-1208,1,0.842283,";1 (2) We provide benchmark results on this new corpus, using state-of-the-art models that are trained on synthetic data and learner data; (3) We provide an error analysis showing that most of the grammar errors are still challenging for the current systems. 2 Related Work Progress in English GEC There has been a lot of work on grammatical error correction, but most of the research has been done on English (Rozovskaya and Roth, 2011; Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Rozovskaya and Roth, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018; Kaneko et al., 2020). Recently, state-of-the-art results were obtained using statistical and neural machine translation approaches. The systems are typically trained on a combination of native data with synthetic er1 https://github.com/arozovskaya/RU-Lang8 4103 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4103–4111 August 1–6, 2021. ©2021 Association for Computational Linguistics rors and naturally-occurring learner data from NUCLE (Dahlmeier et al., 2013) and the English part of the Lang-8 corpus (Mizumoto"
2021.findings-acl.359,Q19-1001,1,0.900523,"been done. More importantly, as learner data in other language is very hard to come by and expensive to annotate, few benchmark corpora exist in other languages. We develop a benchmark corpus for Russian learner data, by providing expert quality annotations for a subset of the Russian subcorpus of Lang8, henceforth RU-Lang8 dataset. Lang-8 (Mizumoto et al., 2012) is a dataset collected from a language learning website and partially corrected Alla Rozovskaya Department of Computer Science Queens College (CUNY) New York, NY arozovskaya@qc.cuny.edu by native language volunteers. In Russian GEC, Rozovskaya and Roth (2019) recently released an annotated learner corpus, RULEC. The expert annotations that we provide will allow researchers to use the created corpus as another evaluation benchmark corpus for non-English GEC. As we show, RU-Lang8 is more diverse than RULEC in terms of the first language backgrounds, and the genre of writing. We benchmark two state-of-the-art neural machine translation models on the new corpus: a convolutional neural network (CNN) and a Transformer model. The paper makes the following contributions: (1) We generate gold annotations for Lang-8 data to create an additional evaluation d"
2021.findings-acl.359,2021.eacl-main.231,1,0.807342,"Missing"
2021.findings-acl.359,D14-1102,0,0.0272532,"erse linguistically and contains data of different genre of writing, compared to the existing resource RULEC. We make the resource available for research purposes;1 (2) We provide benchmark results on this new corpus, using state-of-the-art models that are trained on synthetic data and learner data; (3) We provide an error analysis showing that most of the grammar errors are still challenging for the current systems. 2 Related Work Progress in English GEC There has been a lot of work on grammatical error correction, but most of the research has been done on English (Rozovskaya and Roth, 2011; Susanto et al., 2014; Yuan and Briscoe, 2016; Hoang et al., 2016; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Rozovskaya and Roth, 2016; Jianshu et al., 2017; Chollampatt and Ng, 2018; Kaneko et al., 2020). Recently, state-of-the-art results were obtained using statistical and neural machine translation approaches. The systems are typically trained on a combination of native data with synthetic er1 https://github.com/arozovskaya/RU-Lang8 4103 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4103–4111 August 1–6, 2021. ©2021 Assoc"
2021.findings-acl.359,vincze-etal-2014-automatic,0,0.0276674,"er Languages Two most prominent attempts at GEC in other languages include shared tasks on Arabic and Chinese text correction. In Arabic, a large-scale corpus (2M words) was collected and annotated as part of the QALB project (Zaghouani et al., 2014). There have also been three shared tasks on Chinese grammatical error diagnosis (Lee et al., 2016; Rao et al., 2017, 2018). In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling) (Imamura et al., 2012; Israel et al., 2013; de Ilarraza et al., 2008; Vincze et al., 2014). The most relevant to us is the work of Rozovskaya and Roth (2019) that made available an annotated corpus of Russian learner essays. The data released in Rozovskaya and Roth (2019) is relatively uniform, as it is all produced by native English speakers, whereas the RU-Lang8 data comes from a diverse set of speakers. 3 The RU-Lang8 Dataset RU-Lang8 was created using data collected as part of the Lang-8 corpus (Mizumoto et al., 2012). The Lang-8 learner corpus is a dataset compiled from a language learning website.2 It contains data from learners of a variety of foreign languages and is weakly"
D10-1094,P01-1005,0,0.0269081,"er is organized as follows. First, we describe related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural language by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for contexts"
D10-1094,W07-1604,0,0.113414,"Missing"
D10-1094,C10-2031,0,0.0269314,"iency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For e"
D10-1094,W07-1607,0,0.0131151,"Missing"
D10-1094,C08-1022,0,0.390829,"Missing"
D10-1094,I08-1059,0,0.106651,"used on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or al"
D10-1094,N10-1019,0,0.7264,"akes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a pot"
D10-1094,han-etal-2010-using,0,0.541148,"pted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction."
D10-1094,P03-2026,0,0.189556,"spelling errors – More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/"
D10-1094,J08-2005,1,0.730673,"lso comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic 8 ThreshAll is not possible with this training option, as the system never proposes a correction that is not in L1ConfSet(pi ). 9 LBJ code is available at http://cogcomp.cs. illinois.edu/page/software 967 Regression, while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applications (Punyakanok et al., 2008). 6 Results and Discussion Table 4 shows performance of the four systems by the source language. For each source language, the methods that restrict candidate sets in training or testing outperform the baseline system NegAll-Clean-ThreshAll that does not restrict candidate sets. The NegAll-ErrorL1-NoThresh system performs better than the other three systems for all languages, except for Italian. In fact, for the Czech speaker data, all systems other than NegAll-ErrorL1NoThresh, have a precision and a recall of 0, since no errors are detected10 . Source lang. CH CZ IT RU SP System Acc. P R NegA"
D10-1094,W10-1004,1,0.824013,"ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco"
D10-1094,N10-1018,1,0.795238,"ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco"
D10-1094,C08-1109,0,0.28795,"r a set of prepositions for the preposition correction problem. Each occurrence of a candidate word in text is represented as a vector of features. A classifier is trained on a large corpus of error-free text. Given text to correct, for each word in text that belongs to the confusion set the classifier is used to predict the most likely candidate in the confusion set given the word’s context. In the same spirit, models for correcting ESL errors are generally trained on well-formed native text. Han et al. (2006) train a maximum entropy model to correct article mistakes. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. Gamon et al. (2008) train a decision tree model and a language model to correct errors in article and preposition usage. Bergsma et al. (2009) propose a Na¨ıve Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prep"
D10-1094,P10-2065,0,0.367897,"1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the no"
D13-1074,P06-1032,0,0.122418,"Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence"
D13-1074,D07-1001,0,0.0255979,"nnotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), and (2) a model that join"
D13-1074,D12-1052,0,0.55997,"tatistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts with the noun number error: a correction system"
D13-1074,W13-1703,0,0.196576,"Missing"
D13-1074,W11-2838,0,0.488,"e, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault a"
D13-1074,W12-2006,0,0.262607,"nsiders interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010"
D13-1074,C08-1022,0,0.23815,"Missing"
D13-1074,I08-1059,0,0.0301249,"011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentenc"
D13-1074,N10-1019,0,0.206966,"al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues"
D13-1074,P03-2026,0,0.294222,"atical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. B"
D13-1074,P08-1021,0,0.0200433,"ction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts wi"
D13-1074,P11-2089,1,0.912829,"Missing"
D13-1074,de-marneffe-etal-2006-generating,0,0.0113291,"Missing"
D13-1074,D11-1022,0,0.0261226,"ve data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)),"
D13-1074,W13-3601,0,0.246487,"Missing"
D13-1074,P11-1094,0,0.0522924,"not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University"
D13-1074,J08-2005,1,0.732485,"ry recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, ("
D13-1074,W04-2401,1,0.908343,": (1) Data: until very recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear progra"
D13-1074,W10-1004,1,0.869357,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,D10-1094,1,0.833405,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,N10-1018,1,0.898646,"and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi"
D13-1074,P11-1093,1,0.877471,"the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Na¨ıve Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 2 ∅ denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. Example “They believe that such situation must be avoided.” “Nevertheless , electric cars is still regarded as a great trial innovation.” “Every students have appointments with the head of the department.” Predictions made by the Illinois system such situation → such a situations cars is → car are No change Table 3: Examples of predictions of the Illinois system that combines independently-trained models. The words that are selected as inpu"
D13-1074,C08-1109,0,0.0160111,"riff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleadi"
D13-1074,P13-1143,0,0.170403,"e input from participants. The number of mistakes in the revised test data is slightly higher. contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett"
D13-1074,W12-2032,1,\N,Missing
D13-1074,W13-3602,1,\N,Missing
D13-1074,W11-2843,1,\N,Missing
E14-1038,N10-1019,0,0.102404,"e preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on"
E14-1038,P01-1005,0,0.102126,"Missing"
E14-1038,P11-1092,0,0.257009,"t with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most co"
E14-1038,P03-2026,0,0.364715,"Missing"
E14-1038,W13-3603,0,0.101105,"Missing"
E14-1038,W11-2838,0,0.179927,"Missing"
E14-1038,W12-2006,0,0.158705,"S tag and dependency of the governor of the preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of th"
E14-1038,C08-1022,0,0.168511,"Missing"
E14-1038,P08-1021,0,0.10352,"ancing.” “You ask me for some informations*/information- here they*/it are*/is.” “Nobody {has to be}*/{should be} late.” Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus. 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT section of the Gigaword corpus and contains about 2,600 of frequently-occurring words tagged as VB; for example, (3) will add shop but not shopping, but (4) will add both. For methods (3) and (4), we developed verbMorph,5 a tool that performs morphological analysis on verbs and is used to lemmatize verbs and to generate morphological variants. The module makes uses of (1) the verb lemma l"
E14-1038,I08-1059,0,0.528693,"ite Form choice that encompass the most common grammatical verb problems for ESL learners. The first two examples show mistakes on verbs that function as main verbs in a clause: sentence (1) shows an example of subject-verb Agreement error; (2) is an example of a Tense mistake where the ambiguity is between {will find} (Future tense) Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et 358 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–367, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and find (Present tense). Examples (3) and (4) display Form mistakes: confusing the infinitive and gerund forms in (3) and including an inflection on an infinitive verb in (4). This paper addresses the specific challenges of verb error correction that have not been addressed previously – identifying candidates for mistakes and determining which class of errors is pres"
E14-1038,P11-1121,0,0.147689,"svivek@cs.stanford.edu Abstract al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assume prior knowledge of the mistake type; however, identifying the specific category of a verb error is nontrivial, since the surface form of the verb may be ambiguous, especially when that verb is used incorrectly. Consider the following examples of verb mistakes: Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studi"
E14-1038,W13-3601,0,0.294354,"Missing"
E14-1038,D10-1032,0,0.0651336,"Missing"
E14-1038,W10-1004,1,0.910661,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,N10-1018,1,0.883695,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,P11-1093,1,0.953366,"a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mista"
E14-1038,P12-2039,0,0.0417645,"POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on features or on the spe"
E14-1038,P10-2065,0,0.230951,"Missing"
E14-1038,P11-1019,0,0.118417,"Missing"
E14-1038,W13-3602,1,\N,Missing
J17-4002,K15-1010,0,0.0507327,"Missing"
J17-4002,P15-1068,0,0.0280758,"Missing"
J17-4002,N13-1055,0,0.50182,"ower than the MT approach of Junczys-Dowmunt and Grundkiewicz (2016), the combined system uses a much weaker MT component (which scores 39.48 F0.5, when used by itself). We expect that a combined system that used a better MT component would perform significantly better than Junczys-Dowmunt and Grundkiewicz (2016) and, in addition, would handle better some types of mistakes that MT systems do not do well on. Adaptation Using Artificial Errors Several other researchers study the effect of the adaptation framework that uses artificial errors. The two most closely related studies are the works by Cahill et al. (2013) and Felice and Yuan (2014). Cahill et al. researched the effects of different training paradigms with different data sets on the preposition error correction task. They show improvements when using artificial errors, although their selection models were not optimized with respect to F-score (typically, models trained on well-edited text use a threshold, as we do in this work, because 752 Rozovskaya, Roth, and Sammons Adapting to Learner Errors with Minimal Supervision otherwise these models tend to have extremely low precision, which negatively affects the F-score). Because the original model"
J17-4002,W07-1604,0,0.0553449,"Missing"
J17-4002,P11-1092,0,0.184477,"r evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale"
J17-4002,D12-1052,0,0.0514367,"features. Parameter Tuning Ten percent of the training data is used to optimize the inflation rate for the AP-adapted models (0.8 for articles and prepositions, and 0.85 for verb agreement). 4.3 Note on Evaluation Metrics Various metrics have been proposed and used in error correction. These can be broken down roughly into metrics that compute the accuracy of the system and those that use the F-measure. The HOO competitions adopted F1, which can take into account both precision and recall of the systems; the M2 scorer used in CoNLL is also F-based but can take into account phrase-based edits (Dahlmeier and Ng 2012). Overall, accuracy and F-measure are equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an"
J17-4002,W13-1703,0,0.192291,"Missing"
J17-4002,W12-2006,0,0.0686499,"Missing"
J17-4002,W11-2838,0,0.234377,"b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale, Anisimoff, and Narroway 2012; Ng et al. 2013, 2014), makes use of a machine-learning classifier paradigm and is based on the methodology for correcting context-sensitive spelling mistakes made by native speakers. With the exception that learners and native writers exhibit errors of different types, most of the grammar and usage mistakes made by non-native speakers of English also fall into the category of context-sensitive errors that result in valid English words (e.g., articles or prepositions) being confused. Traditionally, following the work on context-sensitive spelling, classifie"
J17-4002,W07-1607,0,0.0850399,"Missing"
J17-4002,C08-1022,0,0.199451,"Missing"
J17-4002,N15-1060,0,0.0362442,"se-based edits (Dahlmeier and Ng 2012). Overall, accuracy and F-measure are equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an I-measure that is accuracy-based and Napoles et al. (2015) proposed a variation of the BLEU metric used in Machine Translation (called GLEU). Further, Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) also compare the outputs of the systems in the CoNLL shared task against human judgments and show that the need for new metrics is motivated by a lack of correlation between F1 and human judgments. However, the newly proposed GLEU metric does not fare much better in terms of correlation with human judgments than the F-measure and the I-measure. Developing a"
J17-4002,E14-3013,0,0.355184,"of Junczys-Dowmunt and Grundkiewicz (2016), the combined system uses a much weaker MT component (which scores 39.48 F0.5, when used by itself). We expect that a combined system that used a better MT component would perform significantly better than Junczys-Dowmunt and Grundkiewicz (2016) and, in addition, would handle better some types of mistakes that MT systems do not do well on. Adaptation Using Artificial Errors Several other researchers study the effect of the adaptation framework that uses artificial errors. The two most closely related studies are the works by Cahill et al. (2013) and Felice and Yuan (2014). Cahill et al. researched the effects of different training paradigms with different data sets on the preposition error correction task. They show improvements when using artificial errors, although their selection models were not optimized with respect to F-score (typically, models trained on well-edited text use a threshold, as we do in this work, because 752 Rozovskaya, Roth, and Sammons Adapting to Learner Errors with Minimal Supervision otherwise these models tend to have extremely low precision, which negatively affects the F-score). Because the original models were not optimized, it is"
J17-4002,W14-1702,0,0.0128573,"relevant confusion set. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the application of the selection training approach to ESL error correction, a model is tailored toward one mistake type (e.g., errors involving preposition usage) and is trained on well-formed native English text with features defined based on the surr"
J17-4002,W09-2112,0,0.070649,"from the FCE corpus. S and INF stand for third person singular and bare verb form, respectively. The left column shows the correct verb. Each row shows the author’s verb choices for that label and Prob(source|label). The numbers next to the targets show the count of the label (or source) in the data set. Label S (640) INF (2,192) WAS (593) WERE (157) Sources S (627) INF (2,205) WAS (601) WERE (149) 0.961 0.005 - 0.039 0.995 - 0.997 0.064 0.003 0.936 is ensured by generating artificial mistakes using the confusion matrix. The idea of using artificial errors goes back to Izumi et al. (2003) and Foster and Andersen (2009). The approach discussed here refers to the adaptation method originally proposed in Rozovskaya and Roth (2010b) and its modified version in Rozovskaya, Sammons, and Roth (2012). In Rozovskaya and Roth (2010b), artificial errors are generated using the distribution of naturally-occurring errors. This version of the approach suffers from the low recall problem, as discussed subsequently. Rozovskaya, Sammons, and Roth (2012) describe a more general method of using artificial errors for adaptation and also solve the low recall problem. This is the method we describe here. Generating Artificial Er"
J17-4002,N10-1019,0,0.42737,"d tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in te"
J17-4002,W11-1422,0,0.0179001,"ically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions (Dale and Kilgarriff 2011; Dale, Anisimoff, and Narroway 2012; Ng et al. 2013, 2014), makes use of a machine-learning classifier paradigm and is based on the methodology for correcting co"
J17-4002,I08-1059,0,0.58607,"these methods ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these"
J17-4002,D15-1052,0,0.0329163,"Missing"
J17-4002,han-etal-2010-using,0,0.0320065,"Missing"
J17-4002,P03-2026,0,0.473253,"putational Linguistics Volume 43, Number 4 the Illinois system that implements these methods ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a na"
J17-4002,W14-1703,0,0.0208495,"et. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the application of the selection training approach to ESL error correction, a model is tailored toward one mistake type (e.g., errors involving preposition usage) and is trained on well-formed native English text with features defined based on the surrounding context. Task-specific confusion"
J17-4002,D16-1161,0,0.208624,"on the CoNLL corpus, the performance is quite poor (28.25). Thus, all other MT systems use an additional source of supervision—either the Cambridge Learner corpus (CLC), which is a larger version of FCE, or the publicly available Lang-8 corpus. The Lang-8 corpus used in the literature has several versions, depending on when it was collected and how much noise has been removed from it. The smallest version used by Hoang, Chollampatt, and Ng (2016) and Chollampatt, Taghipour, and Ng (2016) contains about 11M words; the one in Mizumoto and Matsumoto (2016) is about twice as large; the version in Junczys-Dowmunt and Grundkiewicz (2016) contains about 30M words; the MT component in Rozovskaya and Roth (2016) has 48M words. Because of these differences in size and quality, the comparisons are not fair. For instance, Junczys-Dowmunt and Grundkiewicz (2016) report an F0.5 score of 52.21 using a version closer to 50M words, which was used in 751 Computational Linguistics Volume 43, Number 4 Table 19 State-of-the-art systems on CoNLL-2014 data set. The systems are divided into three categories: classifiers, MT, and combined. For each system, we also show the size of the annotated learner data used to train the system. Depending o"
J17-4002,P08-1021,0,0.465416,"next section. These error patterns may be prominent across multiple first languages or be first-language dependent. The effect of “language transfer”—applying knowledge from the native language, when learning a foreign language—has been the subject of considerable study in the second-language acquisition literature (Odlin 1989; Gass and Selinker 1992; Montrul 2000; Montrul and Slabakova 2002; Oh and Zubizaretta 2003; Ionin, Zubizarreta, and Bautista 2008). These facts have also been confirmed empirically by studies that quantitatively examine learner corpora (Han, Chodorow, and Leacock 2006; Lee and Seneff 2008). For example, speakers of languages that do not have a determiner system (e.g., Russian) tend to make 4–5 times more article mistakes in English than speakers whose first language has articles (Rozovskaya and Roth 2010b). In addition to the error regularities due to first language influence, some confusions are much more likely to occur than others across multiple first languages. For example, regardless of the first language, ESL writers are 38 times more likely to incorrectly use “in” rather than “by” in place of the correct word “on” (Table 2, Section 3.1). Training for Correction Tasks De"
J17-4002,C12-2084,0,0.0177187,"al, contextual information, such as the POS tags of the head nouns for article errors, the surface form of the verbs for verb agreement mistakes, and semantic classes of the nouns for preposition mistakes. The adaptation method is evaluated in the context of training statistical machine translation systems for grammar correction, so that work is not directly comparable to ours. Furthermore, no improvements are reported of the baseline of training on a corpus containing natural errors. Large-scale Error-annotated Corpora Two large-scale error-annotated corpora became available recently—Lang-8 (Mizumoto et al. 2012) and corpus Wikirev (Cahill et al. 2013)—that several researchers have used in developing grammar correction systems. Similar to results reported in this work, Cahill et al. (2013) show that using these corpora with artificially generated errors or naturally occurring errors typically outperforms models trained in the selection paradigm.9 We wish to emphasize that the focus of the current work is on methods that use minimal supervision, when error-annotated data are not available, such as low-frequency errors and languages other than English. 7. Discussion and Conclusion This article addressed"
J17-4002,P13-1112,0,0.0464128,"Missing"
J17-4002,P15-2097,0,0.0254983,"equivalent, with the exception of tuning; F-measure is flexible in that it allows for calibrating precision and recall. In CoNLL-2014, F0.5 was used (precision was weighed twice as high as recall). 737 Computational Linguistics Volume 43, Number 4 Three papers have been published recently that addressed the appropriateness of commonly used metrics for error correction; two of these also proposed new metrics that are different from the standard F1 adopted in the shared tasks but are variations of accuracy or F-measure. Felice and Briscoe (2015) proposed an I-measure that is accuracy-based and Napoles et al. (2015) proposed a variation of the BLEU metric used in Machine Translation (called GLEU). Further, Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) also compare the outputs of the systems in the CoNLL shared task against human judgments and show that the need for new metrics is motivated by a lack of correlation between F1 and human judgments. However, the newly proposed GLEU metric does not fare much better in terms of correlation with human judgments than the F-measure and the I-measure. Developing a new, more appropriate metric is an involved issue that is beyond the sc"
J17-4002,W14-1701,0,0.046607,"be error-free, where each target word occurrence (e.g., peace) is treated as a positive training example for the corresponding word. Given a text to correct, for each confusable word, the task is to select the most likely candidate from the relevant confusion set. The machine learning classifier approach has been and remains one of the prevalent methods in ESL error correction, as is evidenced by the competitions devoted to grammatical error correction: HOO-2011 (Dale and Kilgarriff 2011), HOO-2012 (Dale, Anisimoff, and Narroway 2012), CoNLL-2013 (Ng et al. 2013), and CoNLL-2014 shared tasks (Ng et al. 2014). Thanks to these competitions, the field has also seen a number of alternative approaches. For example, the CoNLL shared tasks made available a large annotated learner data set, that enabled the machine translation approach (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2014) that showed competitive performance in CoNLL-2014. In this work, our focus is on the classifier-based approach with an emphasis on techniques that allow for building robust models by leveraging large amounts of native English data without the use of expensive annotation. The Selection Training Paradigm In the appl"
J17-4002,W13-3601,0,0.111029,"Missing"
J17-4002,W13-3602,1,0.925905,"Missing"
J17-4002,W14-1704,1,0.929151,"of training either on annotated or native data alone. This is because, in contrast to training on annotated ESL data, the adaptation approach only requires a small amount of annotation to estimate the parameters related to error regularities, while context parameters can be learned from native data. In sum, the proposed methods allow us to combine the advantages of training on native and annotated learner data. The proposed adaptation framework was implemented as part of the Illinois system that came first in several text correction competitions, including the prestigious CoNLL shared tasks (Rozovskaya et al. 2014, 2013). We further evaluate the proposed approaches and study the effect of adaptation when using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages. This article unifies and significantly extends material that appeared previously in Rozovskaya and Roth (2010b, 2011, 2014), and Rozovskaya, Sammons and Roth (2012). The novel contribution is concentrated in Section 5, and evaluates the adaptation approach by comparing performance when error statistics are drawn from the writer’s native (target) language data vs. from"
J17-4002,D10-1094,1,0.83317,"rror correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions"
J17-4002,N10-1018,1,0.621593,"rror correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.2 1. Introduction This article addresses the problem of correcting grammatical context–sensitive mistakes made by English as a Second Language (ESL) writers, a subject that has recently attracted significant attention (Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault, Foster, and Chodorow 2010; Gamon 2010; Rozovskaya and Roth 2010a,b, 2011; Dahlmeier and Ng 2011, 2012). ESL error correction is an important problem because the majority of people who write in English today are not native English speakers (Gamon 2011). Representative ESL errors—a verb agreement mistake, an unnecessary article, and an incorrect choice of a preposition— are illustrated in Figure 1.3 The source word (the word chosen by the ESL writer) and the label (the correct choice, as specified by a native English speaker) are emphasized. A standard approach to dealing with these errors, which also proved highly successful in text correction competitions"
J17-4002,P11-1093,1,0.939673,"ion algorithms that we develop for this purpose. After we describe how error patterns are learned, two adaptation methods are presented. The proposed methods are designed to work with two state-of-the-art machine learning algorithms: The artificial errors adaptation method is applicable for a discriminative learning algorithm (implemented here within Averaged Perceptron – henceforth AP), and the priors adaptation method for the Naive Bayes (NB) algorithm. These two algorithms demonstrated superior performance in a study that compared several learning frameworks for ESL error correction tasks (Rozovskaya and Roth 2011). This study also included language models and other count-based methods. 3.1 Learning Error Patterns Error patterns are extracted from annotated learner data; these error patterns are referred to as error statistics. As we show here, unlike the context parameters, the error statistics are very simple, and thus we only need a small annotated sample to estimate them. Given a specific task, we collect all source/label pairs from the annotated sample, where both the source and the label belong to the confusion set, and generate two 729 Computational Linguistics Volume 43, Number 4 Table 1 Confusi"
J17-4002,Q14-1033,1,0.838077,"the frequencies of the respective target words in WikiNYT. To compare with the WikiNYT, Web1T contains on the order of 10,000 more training examples for each error type. Using the two corpora allows us to evaluate the proposed adaptation methods when applying two state-of-the-art machine learning algorithms and to demonstrate how to take advantage of the benefits provided by each data source and each machine learning framework. On WikiNYT, we train discriminatively using the AP algorithm and rich syntactic features shown to be useful for article and verb agreement errors (Lee and Seneff 2008; Rozovskaya and Roth 2014) and for preposition errors (Tetreault, Foster, and Chodorow 2010). Because of the special format of the Web1T corpus, it is difficult to generate rich feature annotations for this data, and to make use of a discriminative classifier on this corpus, as one would have to limit the surrounding context to two words on each side of the mistake. Because we wish to make use of the context features that extend beyond the two-word window, it is only possible to use count-based methods (e.g., NB or language models). We thus train the NB algorithm. Learner Data Several annotated learner data sets were m"
J17-4002,P16-1208,1,0.787987,"Missing"
J17-4002,W11-2843,1,0.841078,"nd other fine-grained adaptations, see Section 5. 4.2 Experimental Set-up Depending on what training data source is used, we refer to the models as follows: 1. Native-trained models: trained on native English data in the selection paradigm 2. ESL-trained models: trained on annotated learner data in the correction training paradigm 3. Adapted models: trained on native English data and adapted using annotated learner data Features AP models trained on WikiNYT use rich features tailored to each error type. These features were used in the components of the Illinois system in several shared tasks (Rozovskaya et al. 2011, 2013) and are presented in Appendix Tables A.1, A.2, A.3, and A.4 for convenience. The Web1T models are trained on word n-gram features. Parameter Tuning Ten percent of the training data is used to optimize the inflation rate for the AP-adapted models (0.8 for articles and prepositions, and 0.85 for verb agreement). 4.3 Note on Evaluation Metrics Various metrics have been proposed and used in error correction. These can be broken down roughly into metrics that compute the accuracy of the system and those that use the F-measure. The HOO competitions adopted F1, which can take into account bot"
J17-4002,W12-2032,1,0.918662,"Missing"
J17-4002,D14-1102,0,0.107547,"Missing"
J17-4002,C08-1109,0,0.232257,". Note that because native data do not have information about learner errors, the model can only use contextual cues. Thus, when the resulting classifier is applied to non-native text, the most appropriate preposition is selected based exclusively on the surrounding context, similar to a cloze task where one needs to “guess” a word that has been replaced by blank in a sentence. An error is flagged if this most likely candidate is different from the author’s choice (Eeg-Olofsson and Knuttson 2003; Izumi et al. 2003; Han, Chodorow, and Leacock 2006; De Felice and Pulman 2008; Gamon et al. 2008; Tetreault and Chodorow 2008; Tetreault, Foster, and Chodorow 2010). We call this general approach to error correction the selection training paradigm. In realistic ESL situations, however, the scenario is different and there is additional information that could be used by a correction system beyond that used in the selection paradigm. The ESL learner writes a text, and typically makes mistakes on 1 The Illinois system ranked first in all metrics in the CoNLL-2013 competition and scored second and first on original and revised annotation metrics, respectively, in the 2014 competition. 2 This article unifies and significa"
J17-4002,P10-2065,0,0.102557,"Missing"
J17-4002,P11-1019,0,0.0649247,"this data, and to make use of a discriminative classifier on this corpus, as one would have to limit the surrounding context to two words on each side of the mistake. Because we wish to make use of the context features that extend beyond the two-word window, it is only possible to use count-based methods (e.g., NB or language models). We thus train the NB algorithm. Learner Data Several annotated learner data sets were made available recently, including the data set used in the HOO competition, the CoNLL data set (Dahlmeier, Ng, and Wu 2013), and the FCE data set (Yannakoudakis, Briscoe, and Medlock 2011), a subset of the Cambridge Learner corpus. Because we want to explore the effects of first language backgrounds, we use the FCE corpus in this work. This corpus contains data from learners of multiple language backgrounds, including information on the first language of the writer. The work described in Section 5 makes use of this information. We discuss other corpora and approaches in Section 6. The FCE corpus contains 1,244 essays (500,000 words) produced by learners of 16 first language backgrounds. The data set is fully corrected and error tagged. For the key adaptation experiments, we spl"
J17-4002,N16-1042,0,0.0636681,"s, as some error phenomena, such as article and verb agreement errors, are better handled using classifiers (Rozovskaya and Roth 2016). Regarding the amount of supervision, MT systems require significant annotation: All of the recently published MT systems are trained using Lang-8 corpus, which contains between 11M and 48M words of annotated learner data, depending on the corpus version and the pre-processing that was performed (Susanto, Phandi, and Ng 2014; Chollampatt, Taghipour, and Ng 2016; Hoang, Chollampatt, and Ng 2016; JunczysDowmunt and Grundkiewicz 2016; Mizumoto and Matsumoto 2016; Yuan and Briscoe 2016). Although most of the recent publications report results on the CoNLL-2014 test set, those systems are evaluated on global behavior (i.e., the whole corpus) and do not focus or evaluate performance on specific error phenomena, as we do in this work. Further, because we care about specific first-language backgrounds, CoNLL is not an appropriate data set to use: The FCE corpus is the only one that contains data from learners of multiple language backgrounds and information on the first language of the writer, used in Section 5. Therefore, in this work we make use of the FCE data set. However, w"
J17-4002,P01-1005,0,\N,Missing
J17-4002,N16-1133,0,\N,Missing
K16-2016,P06-4018,0,0.108716,"connectives in text. This is a binary classifier that, given a connective word or phrase (e.g. but or if . . . then) determines whether the connective functions as a discourse connective in the specific context. We use the training 116 data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014; Wang and Lan, 2015). Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006). 4.2 of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in Wang and Lan (2015) and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features: Identifying Arg1 Position For explicit relati"
K16-2016,D14-1008,0,0.0234248,"o determine what sense (e.g. Comparison.Contrast, Expansion.Conjunction, etc.) an explicit relation conveys. A 3-level sense hierarchy has been defined in PDTB, which has four top-level senses: Comparison, Contingency, Expansion, and Temporal. We use lexical and syntactic features based on previous work and also introduce new features: Explicit Relations: Argument Extraction SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014), without the joint inference and enhance it using features in Wang and Lan (2015). This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in Wang and Lan (2015) and add novel features. To identify candidate constituents, we follow Kong et al. (2014), where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in Wang and Lan (2015). Based on earlier work (Wang and Lan, 2015; Lin et al., 2014), we implement the followi"
K16-2016,D09-1036,0,0.0705856,"Missing"
K16-2016,P09-1077,0,0.636079,"Our design most closely resembles Identifying Explicit Connectives The purpose of the explicit connective classifier is to identify discourse connectives in text. This is a binary classifier that, given a connective word or phrase (e.g. but or if . . . then) determines whether the connective functions as a discourse connective in the specific context. We use the training 116 data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014; Wang and Lan, 2015). Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006). 4.2 of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in Wang and Lan (2015) and add novel features. The novel features are the"
K16-2016,prasad-etal-2008-penn,0,0.233809,"Missing"
K16-2016,rizzolo-roth-2010-learning,0,0.0487943,"Missing"
K16-2016,E14-1068,0,0.0887846,"n Lin et al. (2009) and Wang and Lan (2015) and augment these with novel features. Implicit Arg1 Extractor: The Implicit Arg1 extractor employs a rich set of features. Most of these are similar to those presented for PS Arg1 and PS Arg2 extractors in that we take into account POS information, punctuation symbols that occur on the boundaries of the constituents, as well as dependency relations in the constituent itself. One key distinction of how we define the depenNon-Explicit Sense Classifier Following previous work on non-explicit sense classification (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2014), we define four sets of binary feature groups: Brown clustering pairs, Brown clustering arguments, first-last words, and production rules. Dependency rules and polarity features were also extracted, but did not improve the results and were removed from the final model. A cutoff of 5 was used to prune all of the features. Additionally, Mutual Information (MI) was 118 Features P R F1 Base features 90.96 90.14 90.55 + new features 91.88 91.05 91.46 dency relation features is that, in contrast to prior work that treats each dependency relation as a separate binary feature, we only consider the fi"
K16-2016,K15-2002,0,0.138745,"ional Linguistics, pages 115–121, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Figure 1: Overview of the system architecture. 2. In China, a great number of workers are engaged in pulling out the male organs of rice plants using tweezers, and one-third of rice produced in that country is grown from hybrid seeds. Implicit=on the other hand At Plant Genetic Systems, researchers have isolated a pollen-inhibiting gene that can be inserted in a plant to confer male sterility. (Comparison.Contrast) [wsj 0209] the pipeline proposed by the top system last year (Wang and Lan, 2015), in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice. The overall architecture of the system is shown in Figure 1. Given the input text, the connective classifier identifies explicit discourse connectives. Next, the position classifier is invoked that determines for each explicit relation whether Arg1 is located in the same sentence as Arg2 (SS) or in a previous sentence (PS). The following three modules – SS Arg1/Arg2 Extractor, PS Arg1 Extractor, and PS Arg2 Extractor – extract text spans of the respect"
K16-2016,K15-2001,0,0.0495515,"d et al., 2008). Our system was trained on the training partition and tuned using the development data. Results in the paper are reported for the development and the test sets from PDTB, as well as for the blind test. 3 4 System Components This section describes each component of the pipeline and introduces novel features. System Description 4.1 The system consists of multiple modules that are applied in a pipeline fashion. This architecture is a standard approach that was originally proposed in Lin et al. (2014) and was followed with slight variations by systems in the last year competition (Xue et al., 2015). Our design most closely resembles Identifying Explicit Connectives The purpose of the explicit connective classifier is to identify discourse connectives in text. This is a binary classifier that, given a connective word or phrase (e.g. but or if . . . then) determines whether the connective functions as a discourse connective in the specific context. We use the training 116 data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work"
K16-2016,K16-2001,0,0.0347033,"Missing"
N10-1018,P06-1032,0,0.230314,"to be as similar as possible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native English speakers; we show th"
N10-1018,C08-1022,0,0.710307,"Missing"
N10-1018,W09-2112,0,0.25149,"sible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native English speakers; we show that training on data that con"
N10-1018,I08-1059,0,0.364367,"aining on data with artificial errors is beneficial when compared to utilizing clean data. More importantly, error statistics have not been considered for error correction tasks. Lee and Seneff (2008) examine statistics on article and preposition mistakes in the JLE corpus. While they do not suggest a specific approach, they hypothesize that it might be helpful to incorporate this knowledge into a correction system that targets these two language phenomena. 3.2 Approaches to Detecting Article Mistakes Automated methods for detecting article mistakes generally use a machine learning algorithm. Gamon et al. (2008) use a decision tree model and a 5-gram language model trained on the English Gigaword corpus (LDC2005T12) to correct errors in English article and preposition usage. Han et al. (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. Yi et al. (2008) propose a web count-based system to correct determiner errors. In the above approaches, the classifiers are trained on native data. Therefore the classifiers cannot use the 1 http://www.cambridge.org/elt original article that the writer used as a feature. Han et al. (2006) use the source article at evaluation time and propose a"
N10-1018,P03-2026,0,0.609892,"Missing"
N10-1018,W00-0708,0,0.533452,"Missing"
N10-1018,W10-1004,1,0.589588,"Missing"
N10-1018,W08-1205,0,0.106771,"Missing"
N10-1018,N07-2045,0,0.0264026,"Missing"
N10-1018,J08-2005,1,\N,Missing
N10-1018,I08-2082,0,\N,Missing
P11-1093,P96-1041,0,0.0381487,"ior(p) = P C(p) q∈Conf Set C(q) , = log(prior(p)) + X + log(P (f |p)) (3) f ∈F (S,p) NB weights and its free coefficient are also summarized in Table 2. 2.4 SumLM For candidate p, SumLM (Bergsma et al., 2009)6 produces a score by summing over the logs of all feature counts: X g(S, p) = log(C(f )) f ∈F (S,p) = X log(P (f |p)C(p)) f ∈F (S,p) (2) = |F (S, p)|C(p) + X log(P (f |p)) f ∈F (S,p) where C(p) and C(q) denote the number of times preposition p and q, respectively, occurred in the training data. We implement a count-based LM with Jelinek-Mercer linear interpolation as a smoothing method5 (Chen and Goodman, 1996), where each n-gram length, from 1 to n, is associated with an interpolation smoothing weight λ. Weights are optimized on a held-out set of ESL sentences. Win2 and Win3 features correspond to 4-gram LMs and Win4 to 5-gram LMs. Language models are trained with SRILM (Stolcke, 2002). 4 LBJ can be downloaded from http://cogcomp.cs. illinois.edu. 5 Unlike other LM methods, this approach allows us to train LMs on very large data sets. Although we found that backoff LMs may perform slightly better, they still maintain the same hierarchy in the order of algorithm performance. 927 where C(f ) denotes"
P11-1093,W07-1604,0,0.0341386,"r even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outperform a maximum entropy classifier (Gamon, 2010). However, the language model was trained on the Gigaword corpus, 17 · 109 words (Linguistic Data Consortium, 2003), a corpus several orders of magnitude larger than the corpus used to train the classifier. Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10, 000 times smaller (Chodorow et al., 2007)1 . In this work, we compare four popular learning methods applied to the problem of correcting preposition and article errors and evaluate on a common ESL data set. We compare two probabilistic approaches – Na¨ıve Bayes and language modeling; a discriminative algorithm Averaged Perceptron; and a count-based method SumLM (Bergsma et al., 2009), which, as we show, is very similar to Na¨ıve Bayes, but with a different free coefficient. We train our models on data from several sources, varying training sizes and feature sets, and show that there are significant differences in the performance of t"
P11-1093,C10-2031,0,0.0774458,"rs exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of"
P11-1093,C08-1022,0,0.536614,"Missing"
P11-1093,I08-1059,0,0.422305,"ge of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confus"
P11-1093,N10-1019,0,0.607863,", models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of a confusable word in text is represen"
P11-1093,han-etal-2010-using,0,0.235479,"or ESL and native data. Percentage of test n-gram features that occurred in training. Native refers to data from Wikipedia and NYT. B09 refers to statistics from Bergsma et al. (2009). Roth, 2010a). For instance, a Chinese learner of English might say “congratulations to this achievement” instead of “congratulations on this achievement”, while a Russian speaker might say “congratulations with this achievement”. A system performs much better when it makes use of knowledge about typical errors. When trained on annotated ESL data instead of native data, systems improve both precision and recall (Han et al., 2010; Gamon, 2010). Annotated data include both the writer’s preposition and the intended (correct) one, and thus the knowledge about typical errors is made available to the system. Another way to adapt a model to the first language is to generate in native training data artificial errors mimicking the typical errors of the non-native writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). Henceforth, we refer to this method, proposed within the discriminative framework AP, as AP-adapted. To determine typical mistakes, error statistics are collected on a small set of annotated ESL senten"
P11-1093,J08-2005,1,0.623978,"e the regularized version of AP in Learning Based Java4 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with a generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training. It also has been shown to produce stateof-the-art results on many natural language applications (Punyakanok et al., 2008). 2.3 Na¨ıve Bayes NB is another linear model, which is often hard to beat using more sophisticated approaches. NB architecture is also particularly well-suited for adapting the model to the first language of the writer (Section 4). Weights in NB are determined, similarly to LM, by the feature counts and the prior probability of each candidate p (Eq. (2)). For each candidate p, NB computes the joint probability of p and the feature space F , assuming that the features are conditionally independent given p: Y P (f |p)} g(S, p) = log{prior(p) · f ∈F (S,p) 2.2 Language Modeling Given a feature f"
P11-1093,W10-1004,1,0.85896,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,D10-1094,1,0.560607,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,N10-1018,1,0.858911,"and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010). 924 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Although the choice of a particular learning algorithm differs, with the exception of decision trees (Gamon et al., 2008), all algorithms used are linear learning algorithms, some discriminative (Han et al., 2006; Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b), some probabilistic (Gamon et al., 2008; Gamon, 2010), or “counting” (Bergsma et al., 2009; Elghaari et al., 2010). While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features. Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data. Several conclusions have been made when comparing systems developed for ESL correction tasks. A language model was found to outpe"
P11-1093,C08-1109,0,0.700232,"rors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, ther"
P11-1093,P10-2065,0,0.514208,"larities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods. 1 Introduction There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, In (1) the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there (Carlson et al., 2001; Golding and Roth, 1999). A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}. Each occurrence of a confusable word in tex"
P11-1093,P03-2026,0,\N,Missing
P11-2089,W10-0731,0,0.0164388,"lice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in"
P11-2089,D09-1030,0,0.0334174,"ferent groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in ne"
P11-2089,W10-0708,0,0.00908944,"e there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of"
P11-2089,C08-1022,0,0.0718184,"Missing"
P11-2089,W10-0713,0,0.00614558,"s the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s A"
P11-2089,I08-1059,0,0.0380613,"of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambigu"
P11-2089,N10-1019,0,0.0860787,"Missing"
P11-2089,W10-0717,0,0.0141234,"ated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) a"
P11-2089,N10-1024,0,0.0278613,"orpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL)."
P11-2089,W10-1004,1,0.378088,"Missing"
P11-2089,D10-1094,1,0.419692,"Missing"
P11-2089,C08-1109,1,0.664593,"ies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009),"
P11-2089,W10-1006,1,0.836109,"allison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than sim1 ple rule violations such as number agreement. As a There has been a rec"
P11-2089,W10-0725,0,0.0169367,"rent evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 20"
P11-2089,N10-1057,0,0.046017,"Missing"
P11-2089,W10-4236,0,\N,Missing
P14-2027,P01-1005,0,0.0263224,"Spelling Error Correction model, which uses supervised learning to map input characters into output characters in context. The approach has the following characteristics: Character-level Corrections are learned at the character-level1 using a supervised sequence labeling approach. Generalized The input space consists of all characters, and a single classifier is used to learn 2 Related Work Most earlier work on automatic error correction addressed spelling errors in English and built models of correct usage on native English data (Kukich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Arabic spelling correction has also received considerable interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese s"
P14-2027,habash-etal-2012-conventional,1,0.900559,"Missing"
P14-2027,N13-1044,1,0.906303,"Missing"
P14-2027,D12-1052,0,0.0226669,"models of correct usage on native English data (Kukich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Arabic spelling correction has also received considerable interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of"
P14-2027,D12-1138,0,0.028716,"e recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013). The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.2 Eskander et al. (2013) analyzed the data to identify the seven most common types of errors. They developed seven classifiers and appli"
P14-2027,P11-1038,0,0.0363372,"phic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013). The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.2 Eskander et al. (2013) analyzed the data to identify the seven most common types of errors. They developed seven classifiers and applied them to the data in succession. This makes the approach tailored to the specific data set in use and limited to a specific set of errors. In this work, a single model is considered for all types of errors. The model c"
P14-2027,N13-1066,1,0.860407,"acter-level1 using a supervised sequence labeling approach. Generalized The input space consists of all characters, and a single classifier is used to learn 2 Related Work Most earlier work on automatic error correction addressed spelling errors in English and built models of correct usage on native English data (Kukich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Arabic spelling correction has also received considerable interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistic"
P14-2027,I08-2131,0,0.0314932,"aracteristics: Character-level Corrections are learned at the character-level1 using a supervised sequence labeling approach. Generalized The input space consists of all characters, and a single classifier is used to learn 2 Related Work Most earlier work on automatic error correction addressed spelling errors in English and built models of correct usage on native English data (Kukich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Arabic spelling correction has also received considerable interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland,"
P14-2027,N10-1019,0,0.0255857,"ish and built models of correct usage on native English data (Kukich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Arabic spelling correction has also received considerable interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on"
P14-2027,N01-1025,0,0.246898,"Missing"
P14-2027,P11-1088,1,0.832313,"sh correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013). The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.2 Eskander et al. (2013) analyzed the data to identify the seven most common types of errors. They developed seven classifiers and applied them to the data in succession. This makes th"
P14-2027,D13-1008,0,0.0226538,"Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013). The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.2 Eskander et al. (2013) analyzed the data to identify the seven most common types of errors. They developed seven classifiers and applied them to the data in succession. This makes the approach tailored to the specific data set in use and limited to a specific set of errors. In this work, a single model is considered for all types of errors. The model considers every character in the input text for a possible spelli"
P14-2027,P11-1093,1,0.871573,"14). Supervised spelling correction approaches trained on paired examples of errors and their corrections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; 1 We use the term ‘character’ strictly in the alphabetic sense, not the logographic sense (as in the Chinese script). 161 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–167, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Input k o r e c t d Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011). In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013). The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.2 Eskander et al. (2013) analyzed t"
P14-2027,zaghouani-etal-2014-large,1,\N,Missing
P16-1208,P01-1005,0,0.060046,"translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data a"
P16-1208,N10-1019,0,0.462467,"following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation: role of learn"
P16-1208,P11-1092,0,0.11127,"= arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation: role of learner data in training the"
P16-1208,D12-1052,0,0.821128,"sets are also from the student population studying at the same University but annotated separately. We report results on the CoNLL-2014 test. The annotation includes specifying the relevant correction as well as the information about each error type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors are related to grammar but also include mechanical, collocation, and other errors. An F-based scorer, named M2, was used to score the systems (Dahlmeier and Ng, 2012). The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall. Two types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train th"
P16-1208,W13-1703,0,0.365025,"3 presents error analysis. In Section 4, we develop classifier and MT systems that make use of the strengths of each framework. Section 5 shows how to combine the two approaches. Section 6 concludes. 2 Related Work We first introduce the CoNLL-2014 shared task and briefly describe the state-of-the-art GEC systems in the competition and beyond. Next, an overview of the two leading methods is presented. 2.1 CoNLL-2014 shared task and approaches CoNLL-2014 training data (henceforth CoNLLtrain) is a corpus of learner essays (1.2M words) written by students at the National University of Singapore (Dahlmeier et al., 2013), corrected and error-tagged. The CoNLL-2013 test set was included in CoNLL-2014 and is used as development. Both the development and the test sets are also from the student population studying at the same University but annotated separately. We report results on the CoNLL-2014 test. The annotation includes specifying the relevant correction as well as the information about each error type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors"
P16-1208,W11-2838,0,0.0275862,"Missing"
P16-1208,W12-2006,0,0.0326593,"Missing"
P16-1208,P11-2071,0,0.0134095,"incorrect adjectival form, an error that is typically not modeled with standard classifiers. 3.2 Generalizability Because MT systems extract error/correction pairs from phrase-translation tables, they can only identify erroneous surface forms observed in training and propose corrections that occurred with the corresponding surface forms. Crucially, in a standard MT scenario, any resulting translation consists of “matches” mined from the translation tables, so a standard MT model lacks lexical abstractions that might help generalize, thus out-of-vocabulary words is a well-known problem in MT (Daume and Jagarlamudi, 2011). While more advanced MT models can abstract by adding higher-level Error type Orthog./punc. (Mec) Article (ArtOrDet) Preposition (Prep) Noun number (Nn) Verb tense (Vt) Subj.-verb agr. (SVA) Verb form (Vform) Word form (Wform) AMU (MT) P R F0.5 61.6 16.3 39.6 38.0 10.9 25.4 54.9 10.4 29.5 49.6 43.2 48.2 30.2 9.3 20.8 48.3 14.9 33.3 40.5 16.8 31.8 59.0 36.6 52.6 CUUI (Classif.) P R F0.5 53.3 8.7 26.4 31.8 47.9 34.0 31.7 8.8 20.9 42.5 46.2 43.2 61.1 5.4 19.9 57.7 57.7 57.7 69.2 15.1 40.3 60.0 13.5 35.6 Table 6: Performance of MT and classifier systems from CoNLL-2014 on common errors. features"
P16-1208,W01-0502,1,0.719233,"Missing"
P16-1208,C08-1022,0,0.29075,"Missing"
P16-1208,W14-1702,0,0.436542,"grammar and usage mistakes that are not addressed by standard proofing tools. Recently, there has been a spike in research on grammatical error correction (GEC), correcting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the"
P16-1208,W12-2012,0,0.078999,"tems in this work. Comparison to existing state-of-the-art. System Performance P R F0.5 MT is trained on CoNLL-train MT 43.34 11.81 28.25 Spelling+MT 49.86 16.36 35.37 Article+MT 45.11 13.99 31.22 Verb agr.+MT 46.36 14.63 32.33 Art.+Verb agr.+Spell+MT 52.07 20.89 40.10 MT is trained on Lang-8 MT 66.15 15.11 39.48 Spelling+MT 65.87 16.94 41.75 Article+MT 63.81 17.70 41.95 Verb. agr.+MT 66.09 18.01 43.08 Art.+Verb agr.+Spell+MT 64.13 22.15 46.51 Table 13: Pipelines: select classifiers and MT. compile a list of patterns using CoNLL training data. We also use an off-the-shelf speller (Flor, 2012; Flor and Futagi, 2012). Results are shown in Table 12. Performance improves by almost 5 and 7 points for the native-trained system and for the best configuration of classifiers with supervision. Both systems also outperform the top CoNLL system, by 1 and 6 points, respectively. The result of 43.11 by the best classifier configuration substantially outperforms the existing state-of-the-art: a combination of two MT systems and two classifier systems, and MT with re-ranking (Susanto et al., 2014; Mizumoto and Matsumoto, 2016). 5 Combining MT and Classifier Systems Since MT and classifiers differ with respect to the ty"
P16-1208,I08-1059,0,0.383036,"ear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions"
P16-1208,han-etal-2010-using,0,0.0573728,"ce of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors in"
P16-1208,W09-0408,0,0.0627986,"Missing"
P16-1208,P13-2121,0,0.0132949,"able 6. 4.1 Machine Translation Systems A key advantage of the MT framework is that, unlike with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input. We build two MT systems that differ only in the use of parallel data: the CoNLL2014 training data and Lang-8. Our MT systems are trained using Moses (Koehn et al., 2007) and follow the standard approach (Junczys-Dowmunt and Grundkiewicz, 2014; Susanto et al., 2014). Both systems use two 5-gram language models – English Wikipedia and the corrected side of CoNLL-train – trained with KenLM (Heafield et al., 2013). Table 9 reports the performance of the systems. As shown, performance increases by more than 11 points when a larger parallel corpus is used. The best MT system outperforms the top CoNLL system by 2 points. 4.2 Classifiers We now present several classifier systems, exploring the two important properties of the classification framework – the ability to train without super2210 Parallel data CoNLL-train Lang-8 CoNLL-2014 top 1 Performance P R F0.5 43.34 11.81 28.25 66.15 15.11 39.48 39.71 30.10 37.33 System Classifiers (learner) Classifiers (native) MT CoNLL-2014 top 1 CoNLL-2014 top 2 CoNLL-20"
P16-1208,P03-2026,0,0.117514,"h by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizabili"
P16-1208,W14-1703,0,0.518218,"orrecting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al.,"
P16-1208,N03-1017,0,0.0303842,"rs of the translation model are estimated from a parallel corpus, i.e. the set of foreign sentences and their corresponding translations into the target language. In error correction, the task is cast as translating from erroneous learner writing into corrected well-formed English. The MT approach relies on the availability of a parallel corpus for learning the translation model. In case of error correction, a set of learner sentences and their corrections functions as a parallel corpus. State-of-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon (Koehn et al., 2003). The resulting lexicon consists of a list of pairs (seqf , seqe ) where seqf is a sequence of one or more foreign words, seqe is a predicted translation. Each pair comes with an associated score. At decoding time, all phrases from sentence f are collected with their corresponding translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = a"
P16-1208,P07-2045,0,0.0255458,"ponents and show how to exploit the strengths of each framework in combination. Table 8 summarizes the data used. Results are reported with respect to all errors in the test data. This is different from performance for individual errors in Table 6. 4.1 Machine Translation Systems A key advantage of the MT framework is that, unlike with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input. We build two MT systems that differ only in the use of parallel data: the CoNLL2014 training data and Lang-8. Our MT systems are trained using Moses (Koehn et al., 2007) and follow the standard approach (Junczys-Dowmunt and Grundkiewicz, 2014; Susanto et al., 2014). Both systems use two 5-gram language models – English Wikipedia and the corrected side of CoNLL-train – trained with KenLM (Heafield et al., 2013). Table 9 reports the performance of the systems. As shown, performance increases by more than 11 points when a larger parallel corpus is used. The best MT system outperforms the top CoNLL system by 2 points. 4.2 Classifiers We now present several classifier systems, exploring the two important properties of the classification framework – the ability to"
P16-1208,de-marneffe-etal-2006-generating,0,0.0450643,"Missing"
P16-1208,N16-1133,0,0.423966,"hermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al., 2014; Mizumoto and Matsumoto, 2016). The purpose of this work is to gain a better understanding of the values offered by each method and to facilitate progress on the task, building on the advantages of each approach. Through better understanding of the methods, we exploit the strengths of each technique and, building on existing architecture, develop superior systems within 2205 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2205–2215, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics each framework. Further combination of these systems yields e"
P16-1208,I11-1017,0,0.355369,"wo types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners’ Corpus and the Lang-8 corpus (Mizumoto et al., 2011), respectively. Many teams also used native English datasets. The most common ones are the Web1T corpus (Brants and Franz, 2006), the CommonCrawl dataset, which is similar to Web1T, and the English Wikipedia. Several teams used off-the-shelf spellcheckers. In addition, Susanto et al. (2014) made an attempt at combining MT and classifiers. They used CoNLL-train and Lang-8 as non-native data and English Wikipedia as native data. We believe that the reason this study did not yield significant improvements (Table 1) is that individual strengths of each framework have not been fully exploited. Furt"
P16-1208,W13-3601,0,0.130237,"Missing"
P16-1208,W14-1701,0,0.698888,"or type. The tagset consists of 28 categories. Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The majority of these errors are related to grammar but also include mechanical, collocation, and other errors. An F-based scorer, named M2, was used to score the systems (Dahlmeier and Ng, 2012). The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall. Two types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data (Ng et al., 2014). The approaches varied widely: classifiers, MT, rules, hybrid systems. Table 3 summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors. The third system used MT. As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners’ Corpus and the Lang-8 corpus (Mizumoto et al., 2011), respectively. Many teams also used native English datasets. The most common ones are the Web1T corpus (Brants and Franz, 200"
P16-1208,P02-1038,0,0.0245171,"f-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon (Koehn et al., 2003). The resulting lexicon consists of a list of pairs (seqf , seqe ) where seqf is a sequence of one or more foreign words, seqe is a predicted translation. Each pair comes with an associated score. At decoding time, all phrases from sentence f are collected with their corresponding translations observed in training. These are scored together with the language modeling scores and may include other features. The phrasebased approach by Koehn et al. (2003) uses a loglinear model (Och and Ney, 2002), and the best correction maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice an"
P16-1208,P11-1094,0,0.0198862,"or type, a confusion set is specified and includes a list of confusable words. For some errors, confusion sets are constructed using a closed list (e.g. prepositions). For other error types, NLP tools are required. To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems an"
P16-1208,rizzolo-roth-2010-learning,1,0.266301,"32.15 17.96 27.76 38.41 23.05 33.89 43.34 11.81 28.25 39.71 30.10 37.33 41.78 24.88 36.79 41.62 21.40 35.01 Table 10: Classifier systems trained with and without supervision. Learner data refers to CoNLL-train. Native data refers to Web1T. The MT system uses CoNLL-train for parallel data. yakanok and Roth, 2001), a syntactic parser (Klein and Manning, 2003) and a dependency converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares th"
P16-1208,D10-1094,1,0.642674,"is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact. (2) Generalizibility refers to t"
P16-1208,N10-1018,1,0.666684,"is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact. (2) Generalizibility refers to t"
P16-1208,P11-1093,1,0.752827,"lassifiers. Training without supervision is possible in the classification framework, as follows. For a given mistake type, e.g. preposition, a classifier is trained on native data that is assumed to be correct; the classifier uses context words around each preposition as features. The resulting model is then applied to learner prepositions and will predict the most likely preposition in a given context. If the preposition predicted by the classifier is different from what the author used in text, this preposition is flagged as a mistake. We refer the reader to Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) for a description of training classifiers with and without supervision for error correction tasks. Below, we address two questions related to the use of supervision: • Training with supervision: When training using learner data, how does a classifier-based system compare against an MT system? • Training without supervision: How well can we do by building a classifier system with native data only, compared to MT and classifier-based systems that use supervision? Our classifier system is based on the implementation framework of the second CoNLL-2014 system (Rozovskaya et al., 2014) and consists"
P16-1208,D13-1074,1,0.826188,"other error types, NLP tools are required. To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task. In the classifier paradigm, there are various algorithms – generative (Gamon, 2010; Park and Levy, 2011), discriminative (Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010), and joint approaches (Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2013). Earlier works trained on native data (due to lack of annotation). Later approaches incorporated learner data in training in various ways (Han et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010a; Dahlmeier and Ng, 2011). 3 Error Analysis of MT and Classifiers This section presents error analysis of the MT and classifier approaches. We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a system to identify and c"
P16-1208,Q14-1033,1,0.922797,"cy converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train. The first classifier system performs comparably to the MT system (27.76 vs. 28.25), however, the native-trained classifier system outperforms both, and does not use any annotated data. The native-trained classifier system would place fourth in CoNLL-2014. 4.2.2 Flexibility We now explore another advantage of the classifier-based a"
P16-1208,W11-2843,1,0.882666,"to CoNLL-train. Native data refers to Web1T. The MT system uses CoNLL-train for parallel data. yakanok and Roth, 2001), a syntactic parser (Klein and Manning, 2003) and a dependency converter (Marneffe et al., 2006). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm (Rizzolo and Roth, 2010) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the features used can be found in Rozovskaya and Roth (2014). We also add several novel ideas that are described below. Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLLtrain) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train. The first classifier system performs comparably to the MT system (27.76 vs. 28.25), however, the native-trained classifier system outperforms both, and"
P16-1208,D14-1102,0,0.267908,"as a Second Language, including four shared tasks: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). These shared tasks facilitated progress on the problem within the framework of two leading methods – machine learning classification and statistical machine translation (MT). The top CoNLL system combined a rule-based module with MT (Felice et al., 2014). The second system that scored almost as highly used machine learning classification (Rozovskaya et al., 2014), and the third system used MT (Junczys-Dowmunt and Grundkiewicz, 2014). Furthermore, Susanto et al. (2014) showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited. Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task. Table 1 shows existing state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of <3 points) were published since then (Susanto et al., 2014; Mizumoto and Matsumoto, 2016)."
P16-1208,P10-2065,0,0.0875681,"orrection maximizes the following: e∗ = arg max P (e|f ) (1) e M X λm hm (e, f )) = arg max exp( e m=1 where hm is a feature function, such as language model score and translation scores, and λm corresponds to a feature weight. The classifier approach is based on the contextsensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction (Izumi et al., 2003; Han et al., 2006; Gamon et al., 2008; Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011; Dahlmeier and Ng, 2012). The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data. 2207 Property (1a) Error coverage: ability to address a wide variety of error phenomena MT +All errors occurring in the training data are automatically covered (1b) Error complexity: ability to handle complex and interacting mistakes that go beyond word boundaries (2) Generalizability: going beyond the error confusions observed in training (3) Supervision/Annotation:"
P16-1208,W14-1704,1,\N,Missing
Q14-1033,W13-3608,0,0.097315,"Missing"
Q14-1033,N13-1055,0,0.026784,"OS and syntactic knowledge, training on learner data is advantageous. Finally, for verb form errors, there is an advantage when training on a lot of native data, although the difference is not as substantial as for noun errors. This suggests that unlike agreement mistakes that are better addressed using syntax, form errors, similarly to nouns, benefit from training on a lot of data with n-gram features. To summarize, choice of the training data is an important consideration for building a robust system. Researchers compared native- and learnertrained models for prepositions (Han et al., 2010; Cahill et al., 2013), while the analysis in this work addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and"
Q14-1033,P96-1041,0,0.0649767,"1 29.40 32.41 34.40 33.53 36.42 08.46 12.16 Table 6: Comparison of learning models. Web1T corpus. Modules that are part of the Illinois submission are marked with an asterisk. Source ED INF ING S ED 0.99675 0.00177 0.00124 0.00054 Candidates INF ING 0.00192 0.00103 0.99630 0.00168 0.00447 0.99407 0.00544 0.00132 S 0.00030 0.00025 0.00022 0.99269 Table 7: Priors confusion matrix used for adapting NB. Each entry shows Prob(candidate|source), where source corresponds to the verb form chosen by the author. with SRILM (Stolcke, 2002) using Jelinek-Mercer linear interpolation as a smoothing method (Chen and Goodman, 1996). On the CoNLL test data, NB outperforms LM on all errors; on the FCE corpus, NB is superior on all errors, except preposition errors, where LM outperforms NB only very slightly. We attribute this to the fact that the preposition problem has more labels; when there is a big confusion set, more features have default smooth weights, so there is no advantage to running NB. We found that with fewer classes (6 rather than 12 prepositions), NB outperforms LM. It is also possible that when we have a lot of labels, the theoretical difference between the algorithms disappears. Note that NB can be impro"
Q14-1033,C12-1038,0,0.0612245,"Missing"
Q14-1033,P11-1092,0,0.337864,"error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and learner data in training, see Gamon (2010) and Dahlmeier and Ng (2011). Error Art. Prep. Noun Agr. Form All Illinois submission Model F1 AP-infl. 33.50 NB-adapt. 07.10 NB 42.60 NB 26.14 NB 14.50 31.43 This work Model F1 AP-infl. 33.50 LM 12.09 NB 42.60 AP-infl. 27.93 NB-adapt. 18.35 31.75 Table 15: Results on CoNLL of the Illinois system (without post-processing) and this work. NB and LM models are trained on Web1T; AP models are trained on NUCLE. Modules different from the Illinois submission are in bold. different from the 12.14 in Table 8); the agreement classifier is trained on the learner data using AP with rich features and error inflation; the form classi"
Q14-1033,D12-1052,0,0.247026,"t additional experiments that further analyze each dimension. While a direct comparison with other systems is not always possible due to other differences between the systems, we believe that these results are still useful. Table 5 lists systems used for comparion. It is important to note that the dimensions are not independent. For instance, there is a correlation between algorithm choice and training data. 7 The tool and more detail about it can be found at http://cogcomp.cs.illinois.edu/page/publication view/743 Results are reported on the test data using F1 computed with the CoNLL scorer (Dahlmeier and Ng, 2012). Error-specific results are generated based on the output of individual modules. Note that these are not directly comparable to error-specific results in the CoNLL overview paper: the latter are approximate as the organizers did not have the error type information for corrections in the output. The complete system includes the union of corrections made by each of these modules, where the corrections are applied in order. Ordering overlapping candidates8 might potentially affect the final output, when modules correctly identify an error but propose different corrections, but this does not happ"
Q14-1033,W13-1703,0,0.203789,"Missing"
Q14-1033,W11-2838,0,0.084967,"describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance. 1 Dan Roth Department of Computer Science University of Illinois Urbana, IL 61801 danr@illinois.edu Introduction The field of text correction has seen an increased interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated"
Q14-1033,W12-2006,0,0.0843321,"are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance. 1 Dan Roth Department of Computer Science University of Illinois Urbana, IL 61801 danr@illinois.edu Introduction The field of text correction has seen an increased interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated in the task developed a wide a"
Q14-1033,W01-0502,1,0.803217,"Missing"
Q14-1033,W09-2112,0,0.150223,"be noted that although inflation also decreases precision it is still helpful. In fact, because of the low error rates, performance on the CoNLL dataset with natural errors is very poor, often resulting in F1 being equal to 0 due to no errors being detected. Inflation vs. Sampling To demonstrate the impact of error inflation, we compare it against sampling, an approach used by other teams – e.g. HIT – that improves recall by removing correct examples in training. The HIT article model is similar to the 9 The idea of using artificial errors goes back to Izumi et al. (2003) and was also used in Foster and Andersen (2009). The approach discussed here refers to the adaptation method in Rozovskaya and Roth (2010b) that generates artificial errors using the distribution of naturally-occurring errors. Error Art. Prep. Agr. Form Model AP (natural errors) AP (infl. const. 0.9)* AP (natural errors) AP (infl. const. 0.7) AP (natural errors) AP (infl. const. 0.8) AP (natural errors) AP (infl. const. 0.9) F1 CoNLL 07.06 24.61 0.0 07.37 0.0 17.06 0.0 10.53 Error FCE 27.65 30.96 14.69 34.77 08.05 31.03 01.56 09.43 Art. Agr. trained on learner data with word n-gram features and the source feature. Inflation constant shows"
Q14-1033,N10-1019,0,0.103835,"rk addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that directly combine native and learner data in training, see Gamon (2010) and Dahlmeier and Ng (2011). Error Art. Prep. Noun Agr. Form All Illinois submission Model F1 AP-infl. 33.50 NB-adapt. 07.10 NB 42.60 NB 26.14 NB 14.50 31.43 This work Model F1 AP-infl. 33.50 LM 12.09 NB 42.60 AP-infl. 27.93 NB-adapt. 18.35 31.75 Table 15: Results on CoNLL of the Illinois system (without post-processing) and this work. NB and LM models are trained on Web1T; AP models are trained on NUCLE. Modules different from the Illinois submission are in bold. different from the 12.14 in Table 8); the agreement classifier is trained on the learner data using AP with rich features and erro"
Q14-1033,han-etal-2010-using,0,0.0708563,"ver, when we add POS and syntactic knowledge, training on learner data is advantageous. Finally, for verb form errors, there is an advantage when training on a lot of native data, although the difference is not as substantial as for noun errors. This suggests that unlike agreement mistakes that are better addressed using syntax, form errors, similarly to nouns, benefit from training on a lot of data with n-gram features. To summarize, choice of the training data is an important consideration for building a robust system. Researchers compared native- and learnertrained models for prepositions (Han et al., 2010; Cahill et al., 2013), while the analysis in this work addresses five error types – showing that errors behave differently – and evaluates on two corpora.13 6 Discussion In Table 15, we show the results of the system, where the best modules are selected based on the performance on the training data. We also show the Illinois modules (without post-processing). The following changes are made with respect to the Illinois submission: the preposition system is based on an LM and enhanced to handle spurious preposition errors (thus the Illinois result of 7.10 shown here is 13 For studies that direc"
Q14-1033,P03-2026,0,0.193888,"recall and, consequently, F1. It should be noted that although inflation also decreases precision it is still helpful. In fact, because of the low error rates, performance on the CoNLL dataset with natural errors is very poor, often resulting in F1 being equal to 0 due to no errors being detected. Inflation vs. Sampling To demonstrate the impact of error inflation, we compare it against sampling, an approach used by other teams – e.g. HIT – that improves recall by removing correct examples in training. The HIT article model is similar to the 9 The idea of using artificial errors goes back to Izumi et al. (2003) and was also used in Foster and Andersen (2009). The approach discussed here refers to the adaptation method in Rozovskaya and Roth (2010b) that generates artificial errors using the distribution of naturally-occurring errors. Error Art. Prep. Agr. Form Model AP (natural errors) AP (infl. const. 0.9)* AP (natural errors) AP (infl. const. 0.7) AP (natural errors) AP (infl. const. 0.8) AP (natural errors) AP (infl. const. 0.9) F1 CoNLL 07.06 24.61 0.0 07.37 0.0 17.06 0.0 10.53 Error FCE 27.65 30.96 14.69 34.77 08.05 31.03 01.56 09.43 Art. Agr. trained on learner data with word n-gram features a"
Q14-1033,W13-3603,0,0.045134,"Missing"
Q14-1033,P08-1021,0,0.0530787,"n on learner data with rich features. The word n-gram and POS agreement features are the same as those in the article module. Syntactic features encode properties of the subject of the verb and are presented in Rozovskaya et al. (2014b, Table 7) and Appendix Table A.18; these are based on the syntactic parser (Klein and Manning, 2003) and the dependency converter (Marneffe et al., 2006). Table 11 shows that adding rich features is helpful. Notably, adding deeper syntactic knowledge to the agreement module is useful, although parse features are likely to contain more noise.11 Foster (2007) and Lee and Seneff (2008) observe a degrade in performance on syntactic parsers due to grammatical noise that also includes agreement errors. For articles, we chose to add syntactic knowledge from shallow parse as it is likely to be sufficient for articles and more accurate than full-parse features. Candidate Identification for errors on open-class 10 Feature engineering will also be relevant when training on a native corpus that has linguistic annotation. 11 Parse features have also been found useful in preposition error correction (Tetreault et al., 2010). words is rarely discussed but is a crucial step: it is not p"
Q14-1033,P11-2089,1,0.920942,"Missing"
Q14-1033,de-marneffe-etal-2006-generating,0,0.130055,"Missing"
Q14-1033,W13-3601,0,0.196437,"Missing"
Q14-1033,W14-1701,0,0.138751,"rners. Three competitions devoted to error correction for non-native writers took place recently: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). The most recent and most prominent among these, the CoNLL-2013 shared task, covers several common ESL errors, including article and preposition usage mistakes, mistakes in noun number, and various verb errors, as illustrated in Fig. 1.1 Seventeen teams that 1 The CoNLL-2014 shared task that completed at the time of writing this paper was an extension of the CoNLL-2013 competition (Ng et al., 2014) but addressed all types of errors. The Illinois-Columbia submission, a slightly extended version of the participated in the task developed a wide array of approaches that include discriminative classifiers, language models, statistical machine-translation systems, and rule-based modules. Many of the systems also made use of linguistic resources such as additional annotated learner corpora, and defined highlevel features that take into account syntactic and semantic knowledge. Even though the systems incorporated similar resources, the scores varied widely. The top system, from the University"
Q14-1033,rizzolo-roth-2010-learning,1,0.845713,"although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first language backgrounds, most of the preposition errors are replacements, i.e., where the 6 421 Determiner Errors The variants “a” and “an” are collapsed to one class. “Hence, the environmental factors also *contributes/ contribute to variou"
Q14-1033,W10-1004,1,0.944277,"rst language (Gass and Selinker, 1992; Ionin et al., 2008). There are different ways to adapt a model that depend on the type of training data (learner or native) and the algorithm choice. The key application of adaptation is for models trained on native English data, because the learned models do not know anything about the errors learners make. With adaptation, models trained on native data can use the author’s word (the source word) as a feature and thus propose a correction based on what the author originally wrote. This is crucial, as the source word is an important piece of information (Rozovskaya and Roth, 2010b). Below, several adaptation techniques are summarized and evaluated. The Illinois system makes use of adaptation in the article model via the inflation method and adapts its NB preposition classifier trained on Web1T with the priors method. Adapting NB The priors method (Rozovskaya and Roth, 2011, Sec. 4) is an adaptation technique for a NB model trained on native English data; it is based on changing the distribution of priors over the correction candidates. Candidate prior is a special parameter in NB; when NB is trained on native data, candidate priors correspond to the relative frequenci"
Q14-1033,N10-1018,1,0.931993,"rst language (Gass and Selinker, 1992; Ionin et al., 2008). There are different ways to adapt a model that depend on the type of training data (learner or native) and the algorithm choice. The key application of adaptation is for models trained on native English data, because the learned models do not know anything about the errors learners make. With adaptation, models trained on native data can use the author’s word (the source word) as a feature and thus propose a correction based on what the author originally wrote. This is crucial, as the source word is an important piece of information (Rozovskaya and Roth, 2010b). Below, several adaptation techniques are summarized and evaluated. The Illinois system makes use of adaptation in the article model via the inflation method and adapts its NB preposition classifier trained on Web1T with the priors method. Adapting NB The priors method (Rozovskaya and Roth, 2011, Sec. 4) is an adaptation technique for a NB model trained on native English data; it is based on changing the distribution of priors over the correction candidates. Candidate prior is a special parameter in NB; when NB is trained on native data, candidate priors correspond to the relative frequenci"
Q14-1033,P11-1093,1,0.86309,"f the Web 1T 5-gram corpus (henceforth Web1T, (Brants and Franz, 2006)). NARA employs a statistical machine translation model for two error types; two systems have rule-based components for selected errors. Based on the analysis of the Illinois system, we identify the following, inter-dependent, dimensions that will be examined in this work: 1. Learning algorithm: Most of the teams, including Illinois, built statistical models. We show that the choice of the learning algorithm is very important and affects the performance of the system. 2. Adaptation to learner errors: Previous studies, e.g. (Rozovskaya and Roth, 2011) showed that adaptation, i.e. developing models that utilize knowledge about error patterns of the non-native writers, is extremely important. We summarize adaptation techniques proposed earlier and examine their impact on the performance of the system. 3. Linguistic knowledge: It is essential to use some linguistic knowledge when developing error correction modules, e.g., to identify which type of verb System Error Art. Illinois (Rozovskaya et al., 2013) Prep. Noun/Agr./Form NTHU (Kao et al., 2013) All Art./Prep./Noun HIT (Xiang et al., 2013) Agr./Form Art./Prep. NARA (Yoshimoto et al., 2013)"
Q14-1033,D13-1074,1,0.883728,"ate-of-the-art error correction system. In this paper, we identify key principles for building a robust grammatical error correction system and show their importance in the context of the shared task. We do this by analyzing the Illinois system and evaluating it along several dimensions: choice Illinois CoNLL-2013 system, ranked at the top. For a description of the Illinois-Columbia submission, we refer the reader to Rozovskaya et al. (2014a). 2 The state-of-the-art performance of the Illinois system discussed here is with respect to individual components for different errors. Improvements in Rozovskaya and Roth (2013) over the Illinois system that are due to joint learning and inference are orthogonal, and the analysis in this paper still applies there. 3 F1 might not be the ideal metric for this task but this was the one chosen in the evaluation. See more in Sec. 6. 419 Transactions of the Association for Computational Linguistics, 2 (2014) 419–434. Action Editor: Alexander Koller. c Submitted 10/2013; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. of learning algorithm; choice of training data (native or annotated learner data); model adaptation to the mistakes made by"
Q14-1033,W11-2843,1,0.861924,"s an option for a post-processing step where corrections that always result in a false positive in training are ignored but this option is not used here. 4.1 The majority of determiner errors involve articles, although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first language backgrounds, most of"
Q14-1033,W12-2032,1,0.902717,"m. The Illinois system has an option for a post-processing step where corrections that always result in a false positive in training are ignored but this option is not used here. 4.1 The majority of determiner errors involve articles, although some errors also involve pronouns. The Illinois system addresses only article errors. Candidates include articles (“a”,“an”,“the”)6 and omissions, by considering noun-phrase-initial contexts where an article is likely to be omitted. The confusion set for articles is thus {a, the, ∅}. The article classifier is the same as the one in the HOO shared tasks (Rozovskaya et al., 2012; Rozovskaya et al., 2011), where it demonstrated superior performance. It is a discriminative model that makes use of the Averaged Perceptron algorithm (AP, (Freund and Schapire, 1996)) implemented with LBJava (Rizzolo and Roth, 2010) and is trained on learner data with rich features and adaptation to learner errors. See Sec. 5.2 and Sec. 5.3. 4.2 Preposition Errors Similar to determiners, we distinguish three types of preposition mistakes: choosing an incorrect preposition, using a superfluous preposition, and omitting a preposition. In contrast to determiners, for learners of many first lan"
Q14-1033,E14-1038,1,0.793163,"red 25.01 and the median result was 8.48 points.3 These results suggest that there is not enough understanding of what works best and what elements are essential for building a state-of-the-art error correction system. In this paper, we identify key principles for building a robust grammatical error correction system and show their importance in the context of the shared task. We do this by analyzing the Illinois system and evaluating it along several dimensions: choice Illinois CoNLL-2013 system, ranked at the top. For a description of the Illinois-Columbia submission, we refer the reader to Rozovskaya et al. (2014a). 2 The state-of-the-art performance of the Illinois system discussed here is with respect to individual components for different errors. Improvements in Rozovskaya and Roth (2013) over the Illinois system that are due to joint learning and inference are orthogonal, and the analysis in this paper still applies there. 3 F1 might not be the ideal metric for this task but this was the one chosen in the evaluation. See more in Sec. 6. 419 Transactions of the Association for Computational Linguistics, 2 (2014) 419–434. Action Editor: Alexander Koller. c Submitted 10/2013; Revised 6/2014; Publishe"
Q14-1033,P10-2065,0,0.252235,"Missing"
Q14-1033,W13-3616,0,0.036017,"Missing"
Q14-1033,W13-3605,0,0.0248993,"Missing"
Q14-1033,P11-1019,0,0.107752,"ions made by each of these modules, where the corrections are applied in order. Ordering overlapping candidates8 might potentially affect the final output, when modules correctly identify an error but propose different corrections, but this does not happen in practice. Modules that are part of the Illinois submission are marked with an asterisk in all tables. To demonstrate that our findings are not specific to CoNLL, we also show results on the FCE dataset. It is produced by learners from seventeen first language backgrounds and contains 500,000 words from the Cambridge Learner Corpus (CLC) (Yannakoudakis et al., 2011). We split the corpus into two equal parts – training and test. The statistics are shown in Appendix Tables A.16 and A.17. 5.1 Dim. 1: Learning Algorithm Rozovskaya and Roth (2011, Sec. 3) discuss the relations between the amount of training data, learning algorithms, and the resulting performance. They show that on training sets of similar sizes, discriminative classifiers outperform other machine learning methods on this task. Following these results, the Illinois article module that is trained on the NUCLE corpus uses the discriminative approach AP. Most of the other teams that train on the"
Q14-1033,W14-1704,1,\N,Missing
Q14-1033,W13-3602,1,\N,Missing
Q14-1033,W13-3604,0,\N,Missing
Q19-1001,E14-3013,0,0.171247,"aining from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form of artificial errors or by changing the model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign"
Q19-1001,P18-1127,0,0.174583,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,N10-1019,0,0.160887,"focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng,"
Q19-1001,P18-1059,0,0.0560671,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,I08-1059,0,0.052464,"15; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al.,"
Q19-1001,N18-2020,0,0.117356,"1) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to pe"
Q19-1001,P11-1092,0,0.0206157,"nder, and person and agree with the grammatical subject. Other categories for verbs are aspect, tense, and voice. These are typically expressed through morphemes corresponding to functional words in English (shall, will, was, have, had, been, etc.). made by non-native speakers are not random (Montrul and Slabakova, 2002; Ionin et al., 2008), using the potentially erroneous word and the correction provides the models with knowledge about learner error patterns. For this reason, models trained on error-annotated data often outperform models trained on larger amounts of native data (Gamon, 2010; Dahlmeier and Ng, 2011). But this approach requires large amounts of annotated learner data (Gamon, 2010). The minimal supervision approach (Rozovskaya and Roth, 2014; Rozovskaya et al., 2017) incorporates the best of both modes: training on native texts to facilitate the possibility of training from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form o"
Q19-1001,D12-1052,0,0.0221461,"d creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting comple"
Q19-1001,W12-2006,0,0.0997316,"Missing"
Q19-1001,W10-1802,0,0.0737954,"Missing"
Q19-1001,W11-2838,0,0.0745788,"Missing"
Q19-1001,P13-2121,0,0.0295061,"Missing"
Q19-1001,C08-1022,0,0.097737,"Missing"
Q19-1001,C08-2008,0,0.0746635,"Missing"
Q19-1001,dickinson-ledbetter-2012-annotating,0,0.0263887,"dress the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task:"
Q19-1001,P12-2076,0,0.0615788,"rammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction syste"
Q19-1001,N15-1060,0,0.0133458,"of one to five sentences). Mizumoto et al. (2011) present an attempt to extract a Japanese learners’ corpus from the revision log of a language learning Web site (Lang-8). They collected 900K sentences produced by learners of Japanese and implemented a character-based MT approach to correct the 1 https://en.wikipedia.org/wiki/Russian language. 2 This is a standard metric used in grammar correction since the CoNLL shared tasks. Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F0.5 . Other metrics have been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Kore"
Q19-1001,P07-2045,0,0.00859509,"Missing"
Q19-1001,W17-3204,0,0.0132583,"orpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse features is an improvement of 3 to 4 points over the baseline system (they also rely on much large tuning sets, as required for sparse features). The baseline system is essentially the same as that of Susanto et al. (2014). Because our MT result is so much lower than the classification system, we do not expect that adding sparse and dense features will close that gap. approach (Rozovskaya et al., 2017) to simulate lea"
Q19-1001,I13-1199,0,0.0159925,"ve been proposed recently (Felice and Briscoe, 2015; Napoles et al., 2015; Choshen and Abend, 2018a). 3 https://github.com/arozovskaya/RULEC-GEC. 2 errors. The English learner data from the Lang-8 Web site is commonly used as parallel data in English grammar correction. One problem with the Lang-8 data is a large number of remaining unannotated errors. In other languages, attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana"
Q19-1001,P17-1070,0,0.0154963,"performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tok"
Q19-1001,D16-1161,0,0.155647,"elice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), b"
Q19-1001,N18-2046,0,0.092722,"nd Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tokens. Approaches to Text Correction There are currently two well-studied paradigms that achieve competitive results on the task in English–MT and machine learning classification. In the classification approach, error-specific classifiers are built. Given a confusion set, for example {a, the, zero article} for articles, each occurrence of a confusabl"
Q19-1001,I11-1017,0,0.710635,"t and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for o"
Q19-1001,N18-1055,0,0.0645008,"fication methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Junczys-Dowmunt and Grundkiewicz, 2016), but more recently neural MT approaches have shown competitive results on English grammar correction (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018).4 However, neural MT systems tend to require even more supervision. For instance, JunczysDowmunt et al. (2018) adopt the methods developed for low-resource machine translation tasks, but they still require parallel corpora in tens of millions of tokens. Approaches to Text Correction There are currently two well-studied paradigms that achieve competitive results on the task in English–MT and machine learning classification. In the classification approach, error-specific classifiers are built. Given a confusion set, for example {a, the, zero article} for"
Q19-1001,W14-3605,1,0.942416,"ry well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morpholog"
Q19-1001,P15-2097,0,0.252455,"Missing"
Q19-1001,E17-2037,0,0.144591,"Missing"
Q19-1001,W14-1701,0,0.180795,"Missing"
Q19-1001,W15-3204,1,0.93391,"ned types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach that relies on heavy supervision is not viable across languages, genres, and learner backgrounds. Moreover, for languages that are complex morphologically, we may need more data to address the lexical sparsity. Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, wit"
Q19-1001,W13-3601,0,0.103359,"Missing"
Q19-1001,W10-1004,1,0.79392,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P03-1021,0,0.0313127,".0087 Inst. .0002 .0031 0045 .0008 .9511 .0017 Loc. .0007 .0027 .0060 .0031 .0014 .0980 Table 9: Confusion matrix for noun case errors based on the training and development data from the RULEC-GEC corpus. The left column shows the correct case. Each row shows the author’s case choices for that label and P rob(source|label). of RULEC-GEC. We use two 4-gram language models—one is trained on the Yandex corpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse fea"
Q19-1001,D10-1094,1,0.822469,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P02-1040,0,0.10899,"045 .0008 .9511 .0017 Loc. .0007 .0027 .0060 .0031 .0014 .0980 Table 9: Confusion matrix for noun case errors based on the training and development data from the RULEC-GEC corpus. The left column shows the correct case. Each row shows the author’s case choices for that label and P rob(source|label). of RULEC-GEC. We use two 4-gram language models—one is trained on the Yandex corpus, and the other one is trained on the corrected side of the RULEC-GEC training data. Both are trained with KenLM (Heafield et al., 2013). Tuning is done on the development dataset with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric. We note that several neural MT systems have been proposed recently (see Section 2). Because we only have a small amount of parallel data, we adopt the phrase-based MT, as it is known that neural MT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings (Koehn and Knowles, 2017). We also note that Junczys-Dowmunt and Grundkiewicz (2016) present a stronger SMT system for English grammar correction. Their best result that is due to adding dense and sparse features is an improvement of 3 to 4 poi"
Q19-1001,N10-1018,1,0.713855,"model priors. In this work, we use the artificial errors approach; it has been studied extensively for English grammar correction. Several other studies consider the effect of using artificial errors (e.g., Cahill et al., 2013; Felice and Yuan, 2014). 3 3.2 Two annotators, native speakers of Russian with a background in linguistics, corrected a subset of RULEC (12,480 sentences, comprising 206K words). One of the annotators is an English as a Second Language instructor and English–Russian translator. The annotation was performed using a tool built for a similar annotation project for English (Rozovskaya and Roth, 2010a). We refer to the resulting corpus as RULEC-GEC. When selecting sentences to be annotated, we attempted to include a variety of writers from each group (foreign and heritage speakers). The annotated data include 12 foreign and 5 heritage writers. The essays of each writer were sorted alphabetically by the essay file name; the essays for annotation were selected in that order, and the sentences were selected in the order they appear in each essay. We intentionally selected more essays from non-native authors, as we conjectured that these authors would display a greater variety of grammatical"
Q19-1001,P11-1093,1,0.895451,"notating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly"
Q19-1001,W16-6509,0,0.0305538,"at addresses errors in morphology, syntax, and word usage, and takes into account linguistic properties of the Russian language, by emphasizing those that are most commonly misused. The common phenomena were identified through a pilot annotation, and with the help of sample errors that had been collected with the Russian National Corpus in the process of developing a similar annotation of Russian learner texts. The sample errors were made available to us by the authors (Klyachko et al., 2013). This study resulted in an annotated corpus, available for online search at http:// web-corpora.net/ (Rakhilina et al., 2016). Corpus and Annotation We annotated data from the Russian Learner Corpus of Academic Writing (RULEC, 560K words) (Alsufieva et al., 2012), which consists of essays and papers written in a university setting in the United States by students learning Russian as a foreign language and heritage speakers (those who grew up in the United States but had exposure to Russian at home). This closely mirrors the datasets used for English grammar correction. The corpus contains data from 15 foreign language learners and 13 heritage speakers. RULEC is freely available for research use.5 3.1 Russian Grammat"
Q19-1001,Q14-1033,1,0.835863,"ed through morphemes corresponding to functional words in English (shall, will, was, have, had, been, etc.). made by non-native speakers are not random (Montrul and Slabakova, 2002; Ionin et al., 2008), using the potentially erroneous word and the correction provides the models with knowledge about learner error patterns. For this reason, models trained on error-annotated data often outperform models trained on larger amounts of native data (Gamon, 2010; Dahlmeier and Ng, 2011). But this approach requires large amounts of annotated learner data (Gamon, 2010). The minimal supervision approach (Rozovskaya and Roth, 2014; Rozovskaya et al., 2017) incorporates the best of both modes: training on native texts to facilitate the possibility of training from large amounts of data without the need for annotation, but using a modest amount of expensive learner data that contains learner error patterns. Importantly, error patterns can be estimated robustly with a small amount of annotation (Rozovskaya et al., 2017). The error patterns can be provided to the model in the form of artificial errors or by changing the model priors. In this work, we use the artificial errors approach; it has been studied extensively for E"
Q19-1001,P16-1208,1,0.878385,"al annotated learner datasets became available, models were also trained on annotated learner data. More recently, the statistical machine translation (MT) methods, including neural MT, have gained considerable popularity thanks to the availability of large annotated corpora of learner writing (e.g., Yuan and Briscoe, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 2014), Japanese (Mizumoto et al., 2011), and Chinese (Yu et al., 2014), and shared tasks on Arabic (Mohit et al., 2014; Rozovskaya et al., 2015) and Chinese error detection (Lee et al., 2016; Rao et al., 2017). However, building robust models in other languages has been a challenge, since an approach th"
Q19-1001,J17-4002,1,0.944323,"es extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology. 1 Introduction This paper addresses the task of correcting errors in text. Most of the research in the area of grammar error correction (GEC) focused on correcting mistakes made by English language learners. One standard approach to dealing with these errors, which proved highly successful in text correction competitions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014; Rozovskaya et al., 2017), makes use of a machine1 Transactions of the Association for Computational Linguistics, vol. 7, pp. 1–17, 2019. Action Editor: Jianfeng Gao. Submission batch: 4/2018; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. that are specific to these languages. (4) We demonstrate that the classification framework with minimal supervision is particularly useful for morphologically rich languages; they can benefit from large amounts of native data, due to a large variability of word forms, and small amounts of annotation"
Q19-1001,W18-3706,0,0.0406869,"is indicates that the MT system suffers more than classifiers, when the amount of supervision is particularly small, while the morphological complexity of the language is higher. Considering Arabic and Chinese, where the training data is also limited, the results are also much lower than in English. In Arabic, where the supervised learner data includes 43K words, the best reported F-score is 27.32 (Rozovskaya et al., 2015).8 In Chinese, the supervised dataset size is about 50K sentences, and the highest reported scores are 26.93 for detection (Rao et al., 2017) and 17.23 for error correction (Rao et al., 2018), respectively. These results confirm that the approaches that rely on large amounts of supervision do not carry over to low-resource 6.1 Error Analysis To understand the challenges of grammar correction in a morphologically rich language such as Russian, we perform error analysis of the MT system and the classification system that uses minimal supervision. The nature of grammar correction is such that multiple different corrections are often acceptable (Ng et al., 2014). Furthermore, annotators often disagree on what constitutes a mistake, and some gold errors missed by a system may be consid"
Q19-1001,Q16-1013,0,0.02778,"tem that uses CoNLL data obtains an F0.5 score of 36.26 (Rozovskaya and Roth, 2016). In contrast, the classification system for Russian obtains a much lower score of 21.0. This may be due to a larger variety of grammatical phenomena in Russian, lower error rates, and a high proportion of spelling errors (especially among heritage speakers), which we currently do not specifically target. Note also that the CoNLL-2014 results are based on two gold references for each sentence, while we evaluate with respect to one, and having more reference annotations improves performance (Bryant and Ng, 2015; Sakaguchi et al., 2016; Choshen and Abend, 2018b).7 It should also be noted that the gap between the MT system and the classification system when both are trained with limited supervision is larger for Russian (10.6 vs. 20.5) than for English (28.25 vs. 36.26). This indicates that the MT system suffers more than classifiers, when the amount of supervision is particularly small, while the morphological complexity of the language is higher. Considering Arabic and Chinese, where the training data is also limited, the results are also much lower than in English. In Arabic, where the supervised learner data includes 43K"
Q19-1001,I17-4001,0,0.0857053,"Missing"
Q19-1001,P10-2065,0,0.0303007,"has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Cholla"
Q19-1001,W17-1408,0,0.0278391,"attempts at automatic grammar detection and correction have been limited to identifying specific types of misuse (grammar or spelling). Imamura et al. (2012) address the problem of particle error correction for Japanese, and Israel et al. (2013) develop a small corpus of Korean particle errors and build a classifier to perform error detection. De Ilarraza et al. (2008) address errors in postpositions in Basque, and Vincze et al. (2014) study definite and indefinite conjugation usage in Hungarian. Several studies focus on developing spell checkers (Ramasamy et al., 2015; Sorokin et al., 2016; Sorokin, 2017). There has also been work that focuses on annotating learner corpora and creating error taxonomies that do not build a grammar correction system. Dickinson and Ledbetter (2012) present an annotated learner corpus of Hungarian; Hana et al. (2010) and Rosen et al. (2014) build a learner corpus of Czech; and Abel et al. (2014) present KoKo, a corpus of essays authored by German secondary school students, some of whom are non-native writers. For an overview of learner corpora in other languages, we refer the reader to Rosen et al. (2014). 2.2 et al., 2006; Gamon et al., 2008; De Felice and Pulman"
Q19-1001,vincze-etal-2014-automatic,0,0.259444,"Missing"
Q19-1001,P11-1019,0,0.0865019,"Missing"
Q19-1001,D14-1102,0,0.403566,"mon et al., 2008; De Felice and Pulman, 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2012). In the MT approach, the error correction problem is cast as a translation task: namely, translating ungrammatical learner text into wellformed grammatical text, and original learner texts and the corresponding corrected texts act as parallel data. MT systems for grammar correction are trained using 20M–50M words of learner texts to achieve competitive performance. The MT approach has shown state-of-the-art results on the benchmark CoNLL-14 test set in English (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017); it is particularly good at correcting complex error patterns, which is a challenge for the classification methods (Rozovskaya and Roth, 2016). However, phrase-based MT systems do not generalize well beyond the error patterns observed in the training data. Several neural encoder–decoder approaches relying on recurrent neural networks were proposed (Chollampatt et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017). These initial attempts were not able to reach the performance of the state-of-the-art phrase-based MT systems (Ju"
Q19-1001,N16-1042,0,0.0802466,"rill, 2001). In this approach, classifiers are trained for a particular mistake type: for example, preposition, article, or noun number (Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010c,b; Dahlmeier and Ng, 2012). Originally, classifiers were trained on native English data. As several annotated learner datasets became available, models were also trained on annotated learner data. More recently, the statistical machine translation (MT) methods, including neural MT, have gained considerable popularity thanks to the availability of large annotated corpora of learner writing (e.g., Yuan and Briscoe, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018). Classification methods work very well on well-defined types of errors, whereas MT is good at correcting interacting and complex types of mistakes, which makes these approaches complementary in some respects (Rozovskaya and Roth, 2016). Thanks to the availability of large (in-domain) datasets, substantial gains in performance have been made in English grammar correction. Unfortunately, research on other languages has been scarce. Previous work includes efforts to create annotated learner corpora for Arabic (Zaghouani et al., 20"
Q19-1001,P12-2039,0,0.169966,"Missing"
Q19-1001,W08-1205,0,0.11017,"Missing"
Q19-1001,W14-3304,0,\N,Missing
Q19-1001,W17-5037,0,\N,Missing
R09-1069,S07-1085,1,0.868695,"s no systematic study of clusters of closely related and overlapping semantic relations. In this paper we provide an analysis of a set of five most frequently occurring semantic relations which are nearmisses (Part-Whole, Origin-Entity, Purpose) and overlaps (Part-Whole, Measure, Content- Container). Moreover, we compare the performance of three state-of-the-art relation identification systems which employ different feature sets: (1) an improved implementation of a supervised model, SemScat2 [2], (2) the SNoW machine learning architecture [23], and (3) a competitive 1007 SemEval-Task 4 system [1]. The systems were trained and tested on a corpus of 1,000 examples. The results show that in order to identify such nearmisses and overlaps accurately, a semantic relation iden381 International Conference RANLP 2009 - Borovets, Bulgaria, pages 381–387 tification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. The paper is organized as follows. In the next section we present previous work, followed by an analysis of semantic relations. In particular, we provide a classification of clusters of near-miss and overlappi"
R09-1069,W03-1210,1,0.775957,"employing various feature sets. The results show that in order to identify such near-misses and overlaps accurately, a semantic relation identification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. (2) Keywords lexical semantics; semantic relations; machine learning 1 Introduction Although semantic relations have been studied for a long time both in linguistics and natural language processing, they received special attention recently due to research done in various knowledge-rich tasks such as question answering [3, 17], information retrieval [11], and textual entailment [24]. The identification of semantic relations between nominals is the task of recognizing the relationship between two nouns in context. For example, the noun pair (cycling, happiness) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that t"
R09-1069,P07-1072,1,0.898069,"Missing"
R09-1069,J06-1005,1,0.823224,"ations between Nominals The SemEval 2007 task on semantic relations between nominals is to identify the underlying semantic relation between two nouns in the context of a clause. Since there is no concensus on the number and abstraction level of semantic relations, the SemEval effort focused on seven frequently occurring semantic relations listed by many researchers in their lists of relations [20, 6, 8]: Cause– Effect, Instrument–Agency, Product–Producer, Origin– Entity, Theme–Tool, Part–Whole, and Content–Container. The dataset provided consists of a definition file and 140 1 2 Girju et al. [5] trained the annotators providing explicit annotation schemas based on a well defined classification of 6 subtypes of PartWhole relations [25]. This collection is freely available at: http : //apf el.ai.uiuc.edu/resources.html. training and about 70 test sentences for each of the seven relations considered. The definition file for each relation includes a detailed definition, restrictions and conventions, and prototypical positive and near–miss negative examples. For example, the Part–Whole relation is defined as follows [9]: Definition Part–Whole(X, Y) is true for a sentence S that mentions e"
R09-1069,W04-2610,1,0.778865,"used in this research, an evaluation of the frequently occurring set of such semantic relations, and propose a classification of contingency relations. 3.1 SemEval Task 4: Classification of Semantic Relations between Nominals The SemEval 2007 task on semantic relations between nominals is to identify the underlying semantic relation between two nouns in the context of a clause. Since there is no concensus on the number and abstraction level of semantic relations, the SemEval effort focused on seven frequently occurring semantic relations listed by many researchers in their lists of relations [20, 6, 8]: Cause– Effect, Instrument–Agency, Product–Producer, Origin– Entity, Theme–Tool, Part–Whole, and Content–Container. The dataset provided consists of a definition file and 140 1 2 Girju et al. [5] trained the annotators providing explicit annotation schemas based on a well defined classification of 6 subtypes of PartWhole relations [25]. This collection is freely available at: http : //apf el.ai.uiuc.edu/resources.html. training and about 70 test sentences for each of the seven relations considered. The definition file for each relation includes a detailed definition, restrictions and conventi"
R09-1069,S07-1003,1,0.913479,"ss) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that the meaning encoded by the two nouns is not always explicitely stated in context. Despite the encouraging results obtained by the participating systems at the SemEval-2007 - Task 4: Classification of Semantic Relations between Nominals [9], the problem needs further analysis. For example, the set of semantic relations considered for this problem needs to be better understood. Thus, a more thorough analysis of semantic relations needs to be done before building systems capable of recognizing them automatically in context. Particular attention should be given to those semantic relations that are difficult to differentiate (near-misses) and those relations that coexist in some particular contexts (overlaps). Consider for example the following sentences: (1) a. I got home and big he1ibranchesh/e1i had fallen off the he2itreeh/e2i i"
R09-1069,C92-2082,0,0.247058,"Missing"
R09-1069,P06-2064,0,0.0361554,"Missing"
R09-1069,P95-1007,0,0.116436,"Missing"
R09-1069,W04-2609,1,0.898848,"Missing"
R09-1069,P06-1100,0,0.0571105,"Missing"
R09-1069,P06-2105,0,0.0318186,"der to identify such near-misses and overlaps accurately, a semantic relation identification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. (2) Keywords lexical semantics; semantic relations; machine learning 1 Introduction Although semantic relations have been studied for a long time both in linguistics and natural language processing, they received special attention recently due to research done in various knowledge-rich tasks such as question answering [3, 17], information retrieval [11], and textual entailment [24]. The identification of semantic relations between nominals is the task of recognizing the relationship between two nouns in context. For example, the noun pair (cycling, happiness) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that the meaning encoded by the two nouns is not always explici"
R09-1069,J02-3004,0,\N,Missing
R09-1069,W01-0511,0,\N,Missing
S07-1085,A00-2018,0,0.00616465,"is used for V-ing X/Theme, neither X nor Y can be the subject of the sentence, and hence Theme-Tool(X, Y) would be false. This restriction is also captured by the nominalization feature in case X or Y is an agential noun. PP attachment (F9) is defined for NP PP constructions, where the prepositional phrase containing the noun e2 attaches or not to the NP (containing e1 ). The rationale is to identify negative instances where the PP attaches to any other word before NP in the sentence. For example, eat <e1 >pizza</e1 > with <e2 >a fork</e2 >, where with a fork attaches to the verb to eat (cf. (Charniak, 2000)). Furthermore, we implemented and used two semantic role features which identify the semantic role of the phrase in a verb–argument structure, phrase containing either e1 (F10) or e2 (F11). In particular, we focus on three semantic roles: Time, Location, Manner. The feature is set to 1 if the target noun is part of a semantic role phrase and to 0 otherwise. The idea is to filter out near-miss examples, expecially for the Instrument-Agency relation. For this, we used ASSERT, a semantic role labeler developed at the University of Colorado at Boulder2 which was queried through a web interface. I"
S07-1085,W04-2610,1,0.918427,"on for Computational Linguistics No. F1 F2 F3, F4 F5, F6 F7, F8 F9 F10, F11 F12, F13, F14 F15, F16 F17 F18 Feature Definition Feature Set #1: Core features Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1 , e2 ), where e1 is the part and e2 is the whole). Semantic specialization this is the prediction returned by the automatic WordNet IS - A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three possible values: subject, direct object, or neither. PP Attachment applies to NP PP constructions and indicates if the prepositional phrase containing e2 attaches to the NP containing e1 . Semantic Role is concerned with the semantic role of the phrase containing either e1 (F10) or e2 (F11). In"
S07-1085,J06-1005,1,0.863153,"this research. Feature set #1: Core features This set contains six features that were employed in all seven relation classifiers. The features take into consideration only lexico-semantic information 386 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386–389, c Prague, June 2007. 2007 Association for Computational Linguistics No. F1 F2 F3, F4 F5, F6 F7, F8 F9 F10, F11 F12, F13, F14 F15, F16 F17 F18 Feature Definition Feature Set #1: Core features Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1 , e2 ), where e1 is the part and e2 is the whole). Semantic specialization this is the prediction returned by the automatic WordNet IS - A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features Grammatical role describes the grammatical"
S07-1085,W03-1210,1,0.935424,"Missing"
S07-1085,W04-2609,1,0.881787,"3 Total 80 78 93 81 71 72 74 78.4 Base-F 67.8 65.5 80.0 61.5 58.0 53.1 67.9 Base-Acc 51.2 51.3 66.7 55.6 59.2 63.9 51.4 Best features F1, F2, F5, F6, F12–F14 F7, F8, F10, F11, F15–F18 F1–F4, F12–F14 F1, F2, F5, F6, F12–F14 F1–F6, F15, F16 F1–F4 F1–F6, F12–F14 Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macroaveraged for system’s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the baseline accuracy score (majority). lection of 3,129 sentences from Wall Street Journal (Moldovan et al., 2004; Girju et al., 2004) was considered for Part-Whole (1,003), Origin-Entity (167), Product-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5 . Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations rel"
W07-1711,P06-1038,0,0.0146439,"the target. They show that the method is capable of identifying polysemous English words. 2 Levinson (1999) presents an approach to WSD that is evaluated on English and Hebrew. He finds 50 most similar words to the target and clusters them into groups, the number of groups being the number of senses. He reports comparable results for the two languages, but he uses both morphologically and lexically ambiguous words. Moreover, the evaluation methodology focuses on the success of disambiguation for an ambiguous word, and reports the number of ambiguous words that were disambiguated successfully. Davidov and Rappoport (2006) describe an algorithm for unsupervised discovery of word categories and evaluate it on Russian and English corpora. However, the focus of their work is on the discovery of semantic categories and from the results they report for the two languages it is difficult to infer how the languages compare against each other. We conduct a more thorough evaluation. We also Related Work First, we describe several approaches to WSD that are most relevant to the present project: Since we are dealing with languages that do not have many linguistic resources available, we chose a most unsupervised, knowledge"
W07-1711,C04-1112,0,0.0283468,"viewed as clusters of words. 83 2.2 Cross-Linguistic Study of WSD control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more training data. The results indicate an improvement of this approach over classification based on wordforms. 3 Approach Our algorithm relies on the method for selection of relevant contextual terms and on distance measure between them introduced in (Sproat and van Santen, 1998) and on the approach described in (Sch¨utze, 1998), though the details of clustering differ slightly. Th"
W07-1711,J92-1001,0,0.0968866,"evaluation for two languages. Finally, we describe work that is concerned with the role of morphology for the task. 2.1 Approaches to Word Sense Discrimination Pantel and Lin (2002) learn word sense induction from an untagged corpus by finding the set of the most similar words to the target and by clustering the words. Each word cluster corresponds to a sense. Thus, senses are viewed as clusters of words. 83 2.2 Cross-Linguistic Study of WSD control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more"
W07-1711,W97-0322,0,0.0169377,"that we use. We then describe the experiments and the evaluation methodology in Sections 4 and 5, respectively. We conclude with a discussion of the results and directions for future work. Another approach is based on clustering the occurrences of an ambiguous word in a corpus into clusters that correspond to distinct senses of the word. Based on this approach, a sense is defined as a cluster of contexts of an ambiguous word. Each occurrence of an ambiguous word is represented as a vector of features, where features are based on terms occurring in the context of the target word. For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. Sch¨utze (1992) presents a method that explores the similarity between the context terms occurring around the target. This is accomplished by considering feature vectors of context terms of the ambiguous word. The algorithm is evaluated on natural and artificially-constructed ambiguous English words. Sproat and van Santen (1998) introduce a technique for automatic detection of ambiguous words in a corpus and measuring their degree of polysemy. This technique employs a similar"
W07-1711,J98-1004,0,0.0660193,"Missing"
W09-1707,P06-3002,0,0.121361,"t. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the reduced-dimensional space implied by SVD, tokens do indeed cluster intuitively by part-of-speech; and that if context is taken i"
W09-1707,H92-1022,0,0.129679,"ased approach, and is for us a significant reason which recommends the use of DEDICOM. 5 Evaluation For all evaluation described here, we used the CONLL 2000 shared task data (CONLL 2000). This English-language newswire corpus consists of 19,440 types and 259,104 tokens (including punctuation marks as separate types/tokens). Each token is associated with a part-of-speech tag and a chunk tag, although we did not use the chunk tags in the work described here. The tags are from a 44item tagset. The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger (Brill 1992), and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the correlation is likely to be good enough to allow for an informative evaluation of our method. Before discussing the evaluation of unsupervised DEDICOM, let us briefly reconsider the similarities of DEDICOM to the supervised HMM model in the light of actual data in the CONLL corpus. We stated in (5) that X A*DAR*DAA*T. For the CONLL 2000 tagged data, A* is a 19,440 × 44 matrix and R* is a 44 × 44 matrix. Using A*DA and R*DA as emission- and transitionprobability matric"
W09-1707,A88-1019,0,0.19243,"word, and a list of hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He"
W09-1707,J88-1003,0,0.400449,"f hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the"
W09-1707,P93-1034,0,0.489467,"mbiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the reduced-dimensional space implied by SVD, tokens do indeed cluster intuitively by part-of-speech; and"
W09-1707,E95-1020,0,0.0784481,"y, we show how the output of DEDICOM can be evaluated and compared against the more familiar output of supervised HMM-based tagging. 54 Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54–62, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics can be achieved. Whereas the performance of stochastic taggers is generally sub-optimal when the domain of the training data differs from that of the test data, distributional tagging sidesteps this problem, since each corpus can be considered in its own right. Schütze (1995) notes two general drawbacks of distributional tagging methods: the performance is relatively modest compared to that of supervised methods; and languages with rich morphology may pose a challenge.1 In this paper, we present an alternative unsupervised approach to distributional tagging. Instead of SVD, we use a dimensionality reduction technique known as DEDICOM, which has various advantages over the SVD-based approach. Principal among these is that, even though no pre-tagged corpus is required, DEDICOM can easily be used as input to a HMM-based approach (and the two share linear-algebraic si"
W10-1004,W07-1604,0,0.0858951,"Missing"
W10-1004,C08-1022,0,0.63065,"Missing"
W10-1004,I08-1059,0,0.152193,"us types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic"
W10-1004,P03-2026,0,0.445407,"ntroduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it"
W10-1004,P06-1031,0,0.047113,"d Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences writt"
W10-1004,N10-1018,1,0.493309,"ossibly, except for the Cambridge Learner Corpus http://www.cambridge.org/elt 28 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28–36, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of this size may not seem significant in many natural language applications, this is in fact a large corpus for this field, especially considering the effort to correct all mistakes, as opposed to focusing on one language phenomenon. This corpus was used in the experiments described in the companion paper (Rozovskaya and Roth, 2010). The annotation schema that we developed was motivated by our special interest in errors in article and preposition usage, but also includes errors in verbs, morphology, and noun number. The corpus contains 907 article corrections and 1309 preposition corrections, in addition to annotated mistakes of other types. While the focus of the present paper is on annotating ESL mistakes, we have several goals in mind. First, we present the annotation procedure for the task, including an error classification schema, annotation speed, and inter-annotator agreement. Second, we describe a computer progra"
W10-1004,W08-1205,0,0.732346,"he annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writ"
W10-1004,C08-1109,0,\N,Missing
W10-1004,izumi-etal-2004-overview,0,\N,Missing
W10-1004,W07-1600,0,\N,Missing
W11-1904,D08-1031,1,0.660485,"ed reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each pair of mentions, generating a score that indicates their compatibility (Sec. 2.2). Next, at inference stage, a coreference decoder (Sec. 2.3) aggregates these scores into mention"
W11-1904,W11-1901,0,0.117627,"uction The coreference resolution task is challenging, requiring a human or automated reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each pair of mentions, generating a score that indicates their compatibility (Sec. 2.2). Next, at i"
W11-1904,rizzolo-roth-2010-learning,1,0.841039,", and discuss the challenges of resolving coreference for the OntoNotes-4.0 data set. 1 2 Introduction The coreference resolution task is challenging, requiring a human or automated reader to identify denotative phrases (“mentions”) and link them to an underlying set of referents. Human readers use syntactic and semantic cues to identify and disambiguate the referring phrases; a successful automated system must replicate this behavior by linking mentions that refer to the same underlying entity. This paper describes Illinois-Coref, a coreference resolution system built on Learning Based Java (Rizzolo and Roth, 2010), that participated in the “closed” track of the CoNLL-2011 shared task (Pradhan et al., 2011). Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. These protocols were designed to easily Architecture Illinois-Coref follows the architecture used in Bengtson and Roth (2008). First, candidate mentions are detected (Sec. 2.1). Next, a pairwise classifier is applied to each"
W11-2843,W11-2838,0,0.237806,"errors, and punctuation errors. Table 1 lists the error types that our system targets and shows sample errors from the pilot data1 . Introduction 2.1 The Text Correction task addresses the problem of detecting and correcting mistakes in text. This task is challenging, since many errors are not easy to detect, such as context-sensitive spelling mistakes that involve confusing valid words in a language (e.g. “there” and “their”). Recently, text correction has taken an interesting turn by focusing on contextsensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, wh"
W11-2843,N10-1019,0,0.253716,"nown to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). The methods use error statistics based only on a small amount of annotation. The preposition and article systems use these methods with additional improvements. An interesting distinction of the HOO data is that both the pilot and the test fragments are derived from the same set of papers. The size of the pilot data is not sufficient for training a competitive system, 3 http://cogcomp.c"
W11-2843,han-etal-2010-using,0,0.0576922,"ing algorithm is known to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2010b). The methods use error statistics based only on a small amount of annotation. The preposition and article systems use these methods with additional improvements. An interesting distinction of the HOO data is that both the pilot and the test fragments are derived from the same set of papers. The size of the pilot data is not sufficient for training a competitive system, 3 ht"
W11-2843,J08-2005,1,0.678185,"involve articles. compared to a system trained on other data, but we observed only a small improvement when other data were added to the ACL Anthology corpus. The classifiers use features that are based on word n-grams, part-of-speech tags and phrase chunks. The systems use a discriminative learning framework and the regularized version of Averaged Perceptron in Learning Based Java3 (LBJ, (Rizzolo and Roth, 2007)). This linear learning algorithm is known to be among the best linear learning approaches and has been shown to produce state-ofthe-art results on many natural language applications (Punyakanok et al., 2008). 2.1.1 Adaptation to the Error Patterns of the ESL Writers Mistakes made by non-native speakers are systematic and also depend on the first language of the writer (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Injecting knowledge about typical errors into the system improves its performance significantly. While some approaches use this knowledge directly, by training a system on annotated learner data (Han et al., 2010; Gamon, 2010), there is often not enough annotated data for training. In our previous work, we proposed methods to adapt a model to the typical errors of the writers (Rozo"
W11-2843,W10-1004,1,0.645559,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,D10-1094,1,0.829333,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,N10-1018,1,0.707136,"sensitive errors made by English as a Second Language (ESL) writers. The HOO shared task (Dale and Kilgarriff, 2011) focuses on writing mistakes made by non-native writers of English in the context of Natural Language Processing community. This paper presents our entry in the HOO shared task. We target several common types of errors using a combination of discriminative and probabilistic classifiers, together with adaptation techniques 263 Article and Preposition Classifiers We submitted several versions of article and preposition classifiers that build on elements of the systems described in Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2010c). The systems are trained on the ACL Anthology corpus, which contains 10 million articles and 5 million prepositions2 ; some versions also use additional data from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). Our experiments on the pilot data showed a significant performance gain when training on the ACL Anthology corpus, 1 The shared task data are split into pilot and test. Each part consists of text fragments from 19 documents, with one fragment from each document included in pilot and one in tes"
W11-2843,P11-1093,1,0.507326,"Missing"
W12-2032,P01-1005,0,0.0612509,"Missing"
W12-2032,P11-1092,0,0.0292491,"r correction, depending on whether the author’s original word choice is used in training as a feature. In the standard context-sensitive spelling correction paradigm, the decision of the classifier depends only on the context around the author’s word, e.g. article or preposition, and the author’s word itself is not taken into consideration in training. Mistakes made by non-native speakers obey certain regularities (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outperform systems that only look at context. We call the"
W12-2032,W11-2838,0,0.332638,"a and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarriff, 2011; Rozovskaya et al., 2011). We identify the unique characteristics of the error correction task and analyze the limitations of existing approaches to error correction that are due to these characteristics. Based on this analysis, we propose the error inflation method (Sect. 6.2). In this paper, we first briefly discuss the task (Sec272 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272–280, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics tion 2) and present our overall approach (Section 3. Next, we describe the spe"
W12-2032,W12-2006,0,0.33684,"tive mistakes that confuse valid English words and thus cannot be detected without considering the context around the word. Below we show examples of two common ESL mistakes considered in this paper: 1. “Nowadays ∅*/the Internet makes us closer and closer.” 2. “I can see at*/on the list a lot of interesting sports.” In (1), the definite article is incorrectly omitted. In (2), the writer uses an incorrect preposition. This paper describes the University of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010"
W12-2032,N10-1019,0,0.371711,"etween two training paradigms in ESL error correction, depending on whether the author’s original word choice is used in training as a feature. In the standard context-sensitive spelling correction paradigm, the decision of the classifier depends only on the context around the author’s word, e.g. article or preposition, and the author’s word itself is not taken into consideration in training. Mistakes made by non-native speakers obey certain regularities (Lee and Seneff, 2008; Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outp"
W12-2032,han-etal-2010-using,0,0.12426,"Rozovskaya and Roth, 2010a). Adding knowledge about typical errors to a model significantly improves its performance (Gamon, 2010; Rozovskaya and Roth, 2010c; Dahlmeier and Ng, 2011). Typical errors may refer both to speakers whose first language is L1 and to specific authors. For example, non-native speakers whose first language does not have articles tend to make more articles errors in English (Rozovskaya and Roth, 2010a). Since non-native speakers’ mistakes are systematic, the author’s word choice (the source word) carries a lot of information. Models that use the source word in training (Han et al., 2010; Gamon, 2010; Dahlmeier and Ng, 2011) learn which errors are typical for the learner and thus significantly outperform systems that only look at context. We call these models adapted. Training adapted models requires annotated data, since in native English data the source word is always correct and thus cannot be used by the classifier. In this work, we use two methods of adapting a model to typical errors that have been proposed earlier. Both methods were originally developed for models trained on native English data: they use a small amount of annotated ESL data to generate error statistics"
W12-2032,rizzolo-roth-2010-learning,1,0.56539,"he FCE data a held-out set for development. The results in Sections 7 and 8 give performance on this held-out set, where we use the HOO data (1000 files) for training. The actual performance in the task (Section 9) reflects the system trained on the whole set of 1244 documents. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). All article systems are trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). Our preposition systems combine the AP algorithm with the Na¨ıve Bayes (NB) classifier with prior parameters adapted to the learner data (see Section 5). The AP systems are trained using the inflation method (see Section 6.2). We submitted 10 runs. All of our runs achieved comparable performance. Sections 7 and 8 describe our modules. 3 4 2 Task Description System Overview Our system consists of two components that address individually article2 and preposition errors and use the same pre-processing. 1 In addition, the participating teams were allowed to use for training the remaining 244 fil"
W12-2032,W10-1004,1,0.715056,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,D10-1094,1,0.775814,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,N10-1018,1,0.522843,"sity of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very"
W12-2032,P11-1093,1,0.894649,"use of prepositions and determiners (Dale et al., 2012). Fourteen teams took part in the the competition. The scoring included three metrics: Detection, Recognition, and Correction, and our team scored first or second in each metric (see Dale et al. (2012) for details). The UI system consists of two components, a determiner classifier and a preposition classifier, with a common pre-processing step that corrects spelling mistakes. The determiner system builds on the ideas described in Rozovskaya and Roth (2010c). The preposition classifier uses a combined system, building on work described in Rozovskaya and Roth (2011) and Rozovskaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarri"
W12-2032,W11-2843,1,0.434192,"skaya and Roth (2010b). Both the determiner and the preposition systems apply the method proposed in our earlier work, which uses the error distribution of the learner data to generate artificial errors in training data. The original method was proposed for adding artificial errors when training on native English data. In this task, however, we apply this method when training on annotated ESL data. Furthermore, we introduce an improvement that is conceptually simple but very effective and which also proved to be successful in an earlier error correction shared task (Dale and Kilgarriff, 2011; Rozovskaya et al., 2011). We identify the unique characteristics of the error correction task and analyze the limitations of existing approaches to error correction that are due to these characteristics. Based on this analysis, we propose the error inflation method (Sect. 6.2). In this paper, we first briefly discuss the task (Sec272 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272–280, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics tion 2) and present our overall approach (Section 3. Next, we describe the spelling correction module (S"
W12-2032,P11-1019,0,0.0484201,"d the preposition error correction module (Section 8). In Section 9, we present the performance results of our system in the competition. We conclude with a brief discussion (Section 10). The HOO 2012 shared task focuses on correcting determiner and preposition errors made by nonnative speakers of English. These errors are some of the most common and also some of the most difficult for ESL learners (Leacock et al., 2010); even very advanced learners make these mistakes (Rozovskaya and Roth, 2010a). The training data released by the task organizers comes from the publicly available FCE corpus (Yannakoudakis et al., 2011). The original FCE data set contains 1244 essays written by non-native English speakers and is corrected and error-tagged using a detailed error classification schema. The HOO training data contains 1000 of those files.1 The test data for the task consists of an additional set of 100 student essays, different from the 1244 above. Since the HOO task focuses on determiner and preposition mistakes, only annotations marking preposition and determiner mistakes were kept. Note that while the other error annotations were removed, the errors still remain in the HOO data. More details can be found in D"
W12-4513,W11-1904,1,0.870986,"1 Introduction Coreference resolution has been a popular topic of study in recent years. In the task, a system requires to identify denotative phrases (“mentions”) and to cluster the mentions into equivalence classes, so that the mentions in the same class refer to the same entity in the real world. Coreference resolution is a central task in the Natural Language Processing research. Both the CoNLL-2011 (Pradhan et al., 2011) and CoNLL2012 (Pradhan et al., 2012) shared tasks focus on resolving coreference on the OntoNotes corpus. We also participated in the CoNLL-2011 shared task. Our system (Chang et al., 2011) ranked first in two out of four scoring metrics (BCUB and BLANC), and ranked third in the average score. This year, we further improve the system in several respects. In Sec. 2, we describe the Illinois-Coref system for the CoNLL-2011 shared task, which we take as the baseline. Then, we discuss the improvements on mention detection (Sec. 3.1), pronoun resolution (Sec. 3.2), and learning algorithm (Sec. 3.3). 113 Section 4 shows experimental results and Section 5 offers a brief discussion. 2 Baseline System We use the Illinois-Coref system from CoNLL-2011 as the basis for our current system an"
W12-4513,W11-1901,0,0.0900002,"Missing"
W12-4513,W12-4501,0,0.0892953,"Missing"
W12-4513,D08-1031,1,\N,Missing
W13-3602,P08-1021,0,0.116744,"Missing"
W13-3602,P11-1121,0,0.0795709,"Missing"
W13-3602,W13-3601,0,0.365904,"Missing"
W13-3602,rizzolo-roth-2010-learning,1,0.760565,"system on the training data using 5-fold cross-validation (hereafter, “5fold CV”) and in Section 5 we show the results we obtained on test. We close with a discussion focused on error analysis (Section 6) and our conclusions (Section 7). 2 form and subject-verb agreement errors. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). The AP system is trained using the inflation method (Rozovskaya et al., 2012). Our preposition system is a Na¨ıve Bayes (NB) classifier trained on the Google corpus and with prior parameters adapted to the learner data. The other modules – those that correct noun and verb errors – are all NB models trained on the Google corpus. All components take as input the corpus documents preprocessed with a part-of-speech tagger2 and shallow parser3 (Punyakanok and Roth, 2001). Note that the shared task data already contains comparable pre-processing information, in addition to other information, inclu"
W13-3602,W10-1004,1,0.706637,"Missing"
W13-3602,W12-2032,1,0.819811,"CV”) and in Section 5 we show the results we obtained on test. We close with a discussion focused on error analysis (Section 6) and our conclusions (Section 7). 2 form and subject-verb agreement errors. Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth (2010b), Rozovskaya and Roth (2010c) and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), implemented within Learning Based Java (Rizzolo and Roth, 2010). The AP system is trained using the inflation method (Rozovskaya et al., 2012). Our preposition system is a Na¨ıve Bayes (NB) classifier trained on the Google corpus and with prior parameters adapted to the learner data. The other modules – those that correct noun and verb errors – are all NB models trained on the Google corpus. All components take as input the corpus documents preprocessed with a part-of-speech tagger2 and shallow parser3 (Punyakanok and Roth, 2001). Note that the shared task data already contains comparable pre-processing information, in addition to other information, including dependency parse and constituency parse, but we chose to run our own pre-p"
W13-3602,W13-1703,0,0.333024,"Missing"
W13-3602,N10-1019,0,0.343943,"Missing"
W13-3602,N10-1018,1,\N,Missing
W13-3602,P11-1093,1,\N,Missing
W13-3602,D10-1094,1,\N,Missing
W13-3602,W11-2843,1,\N,Missing
W14-1704,N12-1067,0,0.0945153,"Missing"
W14-1704,W13-1703,0,0.553096,"Missing"
W14-1704,W11-2838,0,0.437403,"Missing"
W14-1704,W12-2006,0,0.416041,"Missing"
W14-1704,P08-1118,0,0.0667098,"Missing"
W14-1704,W14-1701,0,0.309089,"Missing"
W14-1704,P11-1093,1,0.64212,"tion on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system – noun number, verb agreement, verb form, and preposition – are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Na¨ıve Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and “Hence, the environmental *factor/factors also *contributes/contribute to various difficulties, *included/including problems in nuclear technology.” Error type Confusion set Noun number {factor, factors} Verb Agreement {contribute, contributes} {included, including, Verb Form includes, include} Table 2: Sample confusion sets for noun number, verb agreement, and verb form. 3 The Baseline System In this section, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to"
W14-1704,D13-1074,1,0.92303,"competition covers all errors occurring in the data. Errors outside the target group were present in the task corpora last year as well, but were not evaluated. Our system extends the one developed by the University of Illinois (Rozovskaya et al., 2013) that placed first in the CoNLL-2013 competition. For this year’s shared task, the system has been extended and improved in several respects: we extended the set of errors addressed by the system, developed a general approach for improving the error-specific models, and added a joint inference component to address interaction among errors. See Rozovskaya and Roth (2013) for more detail. We briefly discuss the task (Section 2) and give an overview of the baseline Illinois system (Section 3). Section 4 presents the novel aspects of the system. In Section 5, we evaluate the complete system on the development data and show the results obtained on test. We offer error analysis and a brief discussion in Section 6. Section 7 concludes. The CoNLL-2014 shared task is an extension of last year’s shared task and focuses on correcting grammatical errors in essays written by non-native learners of English. In this paper, we describe the Illinois-Columbia system that part"
W14-1704,W12-2032,1,0.892017,"Missing"
W14-1704,E14-1038,1,0.918558,"sample confusion sets for noun, agreement, and form errors. Each classifier takes as input the corpus documents preprocessed with a part-of-speech tag2 http://cogcomp.cs.illinois.edu/page/ software view/POS 3 http://cogcomp.cs.illinois.edu/page/ software view/Chunker 1 ∅ denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). correcting verb errors, we refer the reader to Rozovskaya et al. (2014). 4 The Mec error category includes errors in spelling, context-sensitive spelling, capitalization, and punctuation. Our system addresses punctuation errors and capitalization errors. To correct capitalization errors, we collected words that are always capitalized in the training and development data when not occurring sentence-initially. The punctuation classifier includes two modules: a learned component targets missing and extraneous comma usage and is an AP classifier trained on the learner data with error inflation. A second, pattern-based component, complements the AP model: it inserts m"
W14-1704,W13-3602,1,\N,Missing
W14-1704,W13-3601,0,\N,Missing
W14-3605,C12-2011,0,0.247501,"Missing"
W14-3605,W14-3620,0,0.347563,"Missing"
W14-3605,W11-2838,0,0.0776592,"es. Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural ex"
W14-3605,W12-2006,0,0.0702953,"overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these res"
W14-3605,I13-2001,1,0.501252,"ompetitions is adopted: system outputs are compared against gold annotations using Precision, Recall and F1 . Systems are ranked based on the F1 scores obtained on the test set. The QALB Corpus One of the goals of the QALB project is to create a large manually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the After the initial registration, the participants were provided with training and development sets and evaluation scripts. During the test period, the teams were given test data on which they needed to run their systems. Following the announcement of system results, the answer key to the test set was released. Participants authored description papers which will be presented in the Arabic NLP workshop. 40 Statistics Train. Dev. Test Number of docs. 19,41"
W14-3605,W12-5611,0,0.0562666,"Missing"
W14-3605,P05-1071,1,0.0923883,"Missing"
W14-3605,pasha-etal-2014-madamira,1,0.727075,"Missing"
W14-3605,W14-3615,0,0.0699428,"Missing"
W14-3605,W14-3618,1,0.793286,"Missing"
W14-3605,W14-3614,1,0.894222,"Missing"
W14-3605,W14-3621,0,0.0508845,"Missing"
W14-3605,zaghouani-etal-2014-large,1,0.605144,"ade by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these resource production efforts is the creation of robust automatic systems for error correction. 2 Task Description The QALB shared task was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic. The shared task makes use of the QALB corpus (Zaghouani et al., 2014), which is a manually-corrected collection of Arabic texts. The shared task participants were provided with tra"
W14-3605,W14-3617,0,0.0558201,"Missing"
W14-3605,W14-3616,0,0.123166,"Missing"
W14-3605,W14-3619,0,0.107829,"Missing"
W14-3605,W14-1701,0,\N,Missing
W14-3605,N12-1067,0,\N,Missing
W14-3605,W13-3602,1,\N,Missing
W14-3605,W15-3221,0,\N,Missing
W14-3605,W15-3220,0,\N,Missing
W14-3605,W15-3217,0,\N,Missing
W14-3605,W14-3622,1,\N,Missing
W14-3605,W15-3218,0,\N,Missing
W14-3605,W15-3214,0,\N,Missing
W14-3605,I08-2131,0,\N,Missing
W14-3605,W13-3601,0,\N,Missing
W14-3605,W11-2843,1,\N,Missing
W14-3622,P11-1093,1,0.834409,"ew The Columbia University system consists of several components designed to address different types of errors. We submitted three versions of the system. We refer to these as CLMB-1, CLMB-2, and CLMB-3. Table 1 lists all of the components and indicates which components are included in each version. The components are applied in the order shown in the table. Below we describe each component in more detail. 3.1 Maximum Likelihood Model 3.3 Na¨ıve Bayes for Unseen Words The Na¨ıve Bayes component addresses errors for words that were not seen in training. The system uses the approach proposed in Rozovskaya and Roth (2011) that proved to be successful for correcting errors made by English as a Second Language learners. The model operates at the word level and targets word replacement errors that involve single tokens. Candidate corrections are generated using a character confusion table that is based on the training data. The model is a Na¨ıve Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Na¨ıve Bayes component is used in the CLMB-1 system. MADAMIRA Corrector MADAMIRA (Pasha et al., 2014) is a tool"
W14-3622,W11-2602,1,0.834809,"Arabic speakers. Typically, an Arabic speaker has a native proficiency in one of the many Arabic dialects and learns to write and read MSA in a formal setting. For this reason, even in MSA texts produced by native Arabic speakers, one typically finds words and linguistic features specific to the writer’s native dialect that are not found in the standard language. To address such errors, we use Elissa (Salloum and Habash, 2012), which is Dialectal to Standard Arabic Machine Translation System. Elissa uses a rule-based approach that relies on the existence of a dialectal morphological analyzer (Salloum and Habash, 2011), a list of hand-written transfer rules, and dialectal-to-standard Arabic lexicons. Elissa uses different dialect identification techniques to select dialectal words and phrases (dialectal multi-word expressions) that need to be handled. Then equivalent MSA paraphrases of the selected words/phrases are generated and an MSA lattice for each input sentence is constructed. The paraphrases within the lattice are then ranked using language models and the n-best sentences are extracted from lattice. We use 5-gram language models trained using SRILM (Stolcke, 2002) on about 200 million untokenized, A"
W14-3622,N12-1067,0,0.114428,"articipating teams. We have presented three versions of the system; all of these incorporate several components that target different types of mistakes, which we presented and evaluated in this paper. • All sequences of the punctuation marks (., ?, !) that occur between two and six times are merged: e.g ! ! ! → !!!. 4 P 72.22 69.49 69.71 Experimental Results In Section 3, we described the individual system components that address different types of errors. In this section, we show how the system improves when each component is added into the system. System output is scored with the M2 scorer (Dahlmeier and Ng, 2012), the official scorer of the shared task. Table 2 reports performance results of each version of the Columbia system on the development data. Table 3 shows the performance results for the best-performing system, CLMB-1, as each system component is added. Acknowledgments This material is based on research funded by grant NPRP-4-1058-1-168 from the Qatar National Research Fund (a member of the Qatar Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learni"
W14-3622,C12-3048,1,0.854648,"e model prediction. In other words, we only attempt to add missing punctuation. 3.6 Dialectal Usage Corrector Even though the shared task data is written in MSA, MSA is not a native language for Arabic speakers. Typically, an Arabic speaker has a native proficiency in one of the many Arabic dialects and learns to write and read MSA in a formal setting. For this reason, even in MSA texts produced by native Arabic speakers, one typically finds words and linguistic features specific to the writer’s native dialect that are not found in the standard language. To address such errors, we use Elissa (Salloum and Habash, 2012), which is Dialectal to Standard Arabic Machine Translation System. Elissa uses a rule-based approach that relies on the existence of a dialectal morphological analyzer (Salloum and Habash, 2011), a list of hand-written transfer rules, and dialectal-to-standard Arabic lexicons. Elissa uses different dialect identification techniques to select dialectal words and phrases (dialectal multi-word expressions) that need to be handled. Then equivalent MSA paraphrases of the selected words/phrases are generated and an MSA lattice for each input sentence is constructed. The paraphrases within the latti"
W14-3622,W12-5611,0,0.0465787,"Missing"
W14-3622,P14-2027,1,0.839873,"lso performs Alif and Ya spelling correction for the phenomena associated with these letters discussed in Section 2. The corrected form was included among the features and can be used for correcting the input. We use the corrections proposed by MADAMIRA and apply them to the data. As we show in Section 4, while the form proposed by MADAMIRA may not necessarily be correct, MADAMIRA performs at a very high precision. MADAMIRA corrector is used in the CLMB-1 and CLMB-2 systems. 3.4 The GSEC Model The CLMB-3 system implements a Generalized Character-Level Error Correction model (GSEC) proposed in Farra et al. (2014). GSEC is a supervised model that operates at the character level. Because of this, the source and the target side of the training data need to be aligned at the character level. We use the alignment tool Sclite (Fiscus, 1998). The alignment maps each source character to itself, a different character, a pair of characters, or an empty string. For the shared task, punctuation corrections are ignored since punctuation errors are handled by the punctuation corrector described in the following section. It should 161 also be noted that the model was not trained to insert missing characters. The mod"
W14-3622,zaghouani-etal-2014-large,1,0.743188,"Missing"
W14-3622,W14-3605,1,0.812099,"Missing"
W14-3622,W13-3601,0,0.0496276,"Missing"
W14-3622,W14-1701,0,0.0785017,"Missing"
W14-3622,pasha-etal-2014-madamira,1,0.768024,"in Rozovskaya and Roth (2011) that proved to be successful for correcting errors made by English as a Second Language learners. The model operates at the word level and targets word replacement errors that involve single tokens. Candidate corrections are generated using a character confusion table that is based on the training data. The model is a Na¨ıve Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Na¨ıve Bayes component is used in the CLMB-1 system. MADAMIRA Corrector MADAMIRA (Pasha et al., 2014) is a tool designed for morphological analysis and disambiguation of Modern Standard Arabic. MADAMIRA performs morphological analysis in context. This is a knowledge-rich resource that requires a morphological analyzer and a large corpus where every word is marked with its morphological features. The task organizers provided the shared task data pre-processed with MADAMIRA, including all of the features generated by the tool for every word. In addition to the morphological analysis and contextual morphological disambiguation, MADAMIRA also performs Alif and Ya spelling correction for the pheno"
W15-1614,abuhakema-etal-2008-annotating,0,0.311605,"access to detailed error statistics. This can provide learners with a very useful feedback and help them improve their proficiency level. 129 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 129–139, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics These errors may take place in words, phrases, language structures, and the ways words or expressions are used (Granger, 2003). For Arabic, there are few projects that aim at developing Arabic learner corpora and annotating them but most of them are not freely available for users or researchers (Abuhakema et al., 2008; Hassan and Daud, 2011). In this paper, we present our annotation method and our efforts for extending an L1 large scale Arabic language corpus and its manually edited corrections to include annotated non-native Arabic learner text (L2). This work is part of the Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014b), a large-scale error annotation effort that aims to create a manually corrected corpus of errors for a variety of Arabic texts (the target size is 2 million words).1 Our overarching goal is to use our annotated corpus to develop components for automatic detection and"
W15-1614,W13-1703,0,0.0240313,"ar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K"
W15-1614,dickinson-ledbetter-2012-annotating,0,0.136697,"ines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K words of Arabic written materials produced by native speakers of English in the US who learned Ara"
W15-1614,N13-1066,1,0.853751,"ly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing vario"
W15-1614,I08-1059,0,0.0165036,"such as POS, lemma, gender, number or person. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refer"
W15-1614,habash-etal-2012-conventional,1,0.127188,"text uses one of multiple widely acceptable transliterations, the annotators should not modify the word. Punctuation Errors: Punctuation errors should be corrected according to the commonly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic"
W15-1614,N13-1044,1,0.82105,"tal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently oc"
W15-1614,W10-1802,0,0.0226722,"non-native speakers of other languages such as English (Leacock et al., 2010; Rozovskaya and Roth, 2010). Lexical Correction: Finally, if it is impossible to fully correct the word using the previous four steps, there is a clear case of word choice errors and the annotator may have to replace the word used. This can be employed to especially correct inadequate lexical choices or unknown words. In the example given in 5 3. Correct derivation errors; but keep root intact. 4 The minimum edits approach in error correction have already been used in the Error-tagged Learner Corpus of Czech project (Hana et al., 2010) 133 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym ˇ ¯  ˆ ð', yˆ Zø', ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 6 A clitic is a linguistic unit that is pronounced and written like an affix but is grammatically independent. Inflection Error Correction Original ˇ knt qd bdÂnA fy AlςAm AlmADy rHl¯h Alaý mk¯h. Correction ˇ knt qd bdÂt fy AlςAm AlmADy rHl¯h Alaý mk¯h. English Original Correction English Original Correction English Original Correction English  úÍ@ éÊgP  ú"
W15-1614,W14-3603,1,0.49852,"and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotat"
W15-1614,W14-3605,1,0.412475,"us. The results obtained in the evaluation suggest that the annotators produced consistently similar results under the proposed guidelines. We believe that publishing this corpus will give researchers a common development and test set for developing related natural language processing applications. A subset of our L2 corpus will be used as part of the Second QALB Shared Task on Automatic Arabic Error Correction in conjunction with the ACL-2015 Workshop on Arabic NLP.9 This shared task follows the success of the First QALB Shared Task held in conjunction with EMNLP-2014 Workshop on Arabic NLP (Mohit et al., 2014). In the future, we will extend our annotation guidelines to address machine translation output correction (i.e., manual post-editing). We also plan to extend our systems for automatic correction of Arabic language errors (Jeblee et al., 2014; Rozovskaya et al., 2014) to handle L2 data, using the corpus discussed here for training and test purposes. 9 http://www.arabic-nlp.net/wanlp 137 Acknowledgements We thank anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fathy, Hoda Ibrahim, Anissa Jrad, Samah Lakhal, Jihene Wa"
W15-1614,P06-1031,0,0.0280632,"erson. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refers to an annotation error and it is measure"
W15-1614,I13-2001,1,0.525343,"eyeglasses to read the book.’   @Q¯ @ ú Ë è @QÖÏ @ © A  P A¢ JË@ © A ú Ë H@ Table 1: Examples of the different parts of the correction priority order   ¯ h ‘mirror’ was replaced Table 1, the word è @QÖÏ @ AlmrA¯  P A¢ JË@ AlnDArAt ˇ by the word H@ ‘eyeglasses’. alignments starting from document tokenization to after human annotation. 5 6 The Annotation Tool In order to ensure the speed and efficiency of the annotation process, as well as better management, we provide the annotators with a web-based annotation framework, originally developed to manually correct errors in L1 texts (Obeid et al., 2013). The annotation interface allows annotators to perform different actions corresponding to the following types of corrections: (a) edit misspelled words; (b) move words that are not in the right location; (c) add missing words; (d) delete extraneous words; (e) merge words that have been split erroneously; and (f) split words that have been merged erroneously. In our final corpus output format, we record for each annotated file the list of actions taken by the annotator. These actions operate on one or two tokens depending on the action. We also supply token 134 The Annotation Pipeline The anno"
W15-1614,P02-1040,0,0.105018,"ear that ALC is less challenging than ALWC as shown in the IAA of the first round and second rounds. Overall, the high-level of agreement obtained in the second round shows that the annotators produced consistently similar results under the proposed guidelines; and their differences are all within acceptable variation. This of course makes the evaluation of automatic correction harder.8 7 The annotation manager is excluded from this evaluation. This problem might be solved by considering multiple references in the evaluation process similarly to what is done in machine translation evaluation (Papineni et al., 2002). Unfortu8 Original  ®Ö Ï @ úæîDKA ÐA« ú¯ éËA à@ ø ñK@ B@ . ZAKCJË@ ÉJ.¯ àA Anwy An sAnthy AlmqAl¯h fy ςAm AlAnsAn qbl AlθlAθA’. ‘I plan I will be-done the article in the year of humanity before Tuesday.’ Annotator 1 @ ÐA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the year of humanity before Tuesday.’ Annotator 2 @ ÕËA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAlm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the"
W15-1614,pasha-etal-2014-madamira,1,0.792787,"Missing"
W15-1614,W10-1004,1,0.231979,"able at http://reports-archive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. A"
W15-1614,W14-3622,1,0.888722,"Missing"
W15-1614,N13-1036,1,0.851271,"shed novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotators to flag the highly dialectal cases to be reviewed later by the annotation manager. The guidelines classify dialectal word issues into five categories inspired by Habash et al. (2008): dialectal lexical choice, p"
W15-1614,W08-1205,0,0.00798721,"etailed description of ALC is given at: http://www.arabiclearnercorpus.com/ 131 and structures, with some items overused and others significantly underused. They also contain varying degrees of grammatical, orthographic and lexical errors. Moreover, sentences written by Arabic L2 speaker have often a different structure and are not as fluent as sentences produced by a native speaker even when no clear mistakes can be found. Therefore, the correction task is complicated by the fact that the acceptability level of a given sentence differs widely within the native speaker annotators as stated by Tetreault and Chodorow (2008). These issues can be related to linguistic factors such as inter-language (L1 interference), the student’s teaching and learning methodology, and to the translation effect (conscious interference). Thus, correcting the Arabic L2 essays can be a very challenging task that requires a lot of interpretation efforts by the annotators. This will likely lead to lower inter-annotator agreement as there is often many possible ways to correct the L2 errors. In order to annotate the L2 corpus, we use our annotation guidelines designed for L1 (Zaghouani et al., 2014b) and add specific L2 annotation rules"
W15-1614,P11-1019,0,0.0242372,"ive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annot"
W15-1614,zaghouani-etal-2014-large,1,0.588046,"Missing"
W15-1614,zribi-etal-2014-conventional,1,0.33632,"Missing"
W15-1614,W14-3618,1,\N,Missing
W15-3204,W15-3214,0,0.0408316,"Missing"
W15-3204,I08-2131,0,0.0157927,"trated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 26–35, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics 2 Task Description The texts are manually annotated for errors by native Arabic speakers. The annotation begins with an initial automatic pre-processing step. Next, the files are processed with the morphological analysis and"
W15-3204,C12-2011,0,0.0931945,"Missing"
W15-3204,W14-3605,1,0.841304,"al Learning Systems, Columbia University 2 Carnegie Mellon University in Qatar 3 New York University Abu Dhabi 4 Ask.com alla@ccls.columbia.edu,hbouamor@qatar.cmu.edu,nizar.habash@nyu.edu wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu,behrang@cmu.edu Abstract organized within the framework of the Qatar Arabic Language Bank (QALB) project,1 is the first effort aimed at constructing a benchmark data set, which will allow for development and evaluation of automatic correction systems for Arabic. In this paper, we present a summary of the second edition of the QALB competition. The first one – QALB-2014 (Mohit et al., 2014) – took place in conjunction with the Arabic NLP workshop at EMNLP-2014 and focused on errors found in online commentaries produced by native speakers of Arabic. QALB-2014 attracted a lot of attention and resulted in nine systems being submitted with a variety of approaches that included rule-based frameworks, machine-learning classifiers, and statistical machine translation methods. This year’s competition extends the first edition by adding another track that focuses on errors found in essays written by learners of Arabic. Eight teams participated in the competition this year, including seve"
W15-3204,W15-3216,0,0.0195413,"Missing"
W15-3204,W15-3220,0,0.0483413,"Missing"
W15-3204,W15-3217,1,0.853627,"Missing"
W15-3204,W15-3218,0,0.0309508,"Missing"
W15-3204,W15-3221,1,0.873886,"Missing"
W15-3204,W15-3215,0,0.125217,"Missing"
W15-3204,N12-1067,0,0.0704061,"ariety of techniques. For example, the CUFE system extracted rules from the morphological analyzer and learned their probabilities using the training data, while the UMMU system combined statistical machine6 Results In this section, we present the results of the competition. As was done in QALB-2014, we adopted the standard Precision (P), Recall (R), and F1 metric. This metric was also used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Tables 8 and 9 present the official results of the evaluation on the test sets for the Aljazeera data and the L2 data, respectively. The results are sorted according to the F1 scores obtained by the 30 Rank 1 2 3 4 5 6 7 8 9 10 11 12 Team CUFE UMMU-1 GWU UMMU-2 QCRI QCMUQ TECH-2 TECH-1 TECH-3 ARIB-1 ARIB-2 SAHSOH MADAMIRA P 88.85 70.28 74.69 72.69 84.74 71.39 71.20 71.08 69.99 64.50 67.56 81.88 80.32 R 61.76 71.93 67.51 67.52 58.10 65.13 64.94 64.74 60.41 56.50 51.61 40.24 39.98 F1 72.87 71.10 70.92 70.01 68.94 68.12 67.93 67.76 64.85 60.23 58.52"
W15-3204,W11-2838,0,0.388272,"uage. The report includes an overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qa"
W15-3204,I13-2001,1,0.600185,"ts are compared against gold annotations using Precision, Recall and F1 . Systems are ranked based on the F1 scores obtained on the test sets. 3 The QALB Corpus The QALB corpus was created as part of the QALB project. One of the goals of the QALB project is to develop a large manually corrected corpus for a variety of Arabic texts, including texts produced by native and non-native writers, as well as machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013; Zaghouani et al., 2015a). 2 In the shared task, we specified two Add categories: add_before and add_after. Most of the add errors fall into the first category, and we combine these here into a single Add category. 3 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical ˇ order) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional ˇ ¯ symbols: ’ Z, Â @, A @ , A @, wˆ ð', yˆ Zø', ¯h è, ý ø. 4 Tables 1 and 2, and the appendix are reproduced from Mohit et al. (2014) to help explain the format of the files used in QALB-2014 and QALB-2015 sha"
W15-3204,W12-2006,0,0.289248,"n overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings o"
W15-3204,W12-5611,0,0.0600366,"Missing"
W15-3204,pasha-etal-2014-madamira,1,0.852468,"Missing"
W15-3204,zaghouani-etal-2014-large,1,0.849061,"workshop at EMNLP-2014 (Mohit et al., 2014). QALB-2014 addressed errors in online user comments written to Aljazeera articles by native Arabic speakers. This year’s competition includes two tracks – native and non-native. In addition to the Aljazeera commentaries written by native speakers, it also includes texts produced by learners of Arabic as a foreign language (L2). Both the native and the non-native data is written in Modern Standard Arabic and is part of the QALB corpus (see Section 3), a manuallycorrected collection of Arabic texts. The Aljazeera section of the corpus is presented in Zaghouani et al. (2014). The L2 data is extracted from two learner corpora of Arabic – the Arabic Learners Written Corpus (ALWC) (Farwaneh and Tamimi, 2012) and the Arabic Learner Corpus (ALC) (Alfaifi and Atwell, 2012). For details about the L2 data, we refer the reader to Zaghouani et al. (2015a). The shared task participants were provided with training and development data to build their systems, but were also free to make use of additional resources, including corpora, linguistic resources, and software, as long as these were publicly available. For evaluation, a standard framework developed for similar error co"
W15-3204,W15-1614,1,0.759406,"nd the English translation. The errors in the original and the corrected forms are underlined and co-indexed. Table 2 presents a subset of the errors for the example shown in Table 1 along with the error types and annotation actions. The Appendix at the end of the paper lists all annotation actions for that example.4 Essays written by L2 speakers differ from the native texts both because of the genre and the types of mistakes. For this reason, the general QALB L1 annotation guidelines were extended by adding new rules describing the error correction procedure in texts produced by L2 speakers (Zaghouani et al., 2015a). Because the genres are different, the writing styles exhibit different distributions of words, phrases, and structures. Further, while native texts mostly contain orthographic and punctuation mistakes, non-native writings also reveal lexical choice errors, missing and extraneous words (e.g. articles, prepositions), and mistakes in word The QALB-2015 shared task extends QALB2014, the first shared task on Arabic text correction that was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic and took place in conjunction with the Arabic NL"
W15-3204,W15-3219,1,0.765882,"Missing"
W18-2316,P06-4018,0,0.0158218,"Missing"
W18-2316,W12-2405,0,0.0617295,"Missing"
W18-2316,W15-3818,0,0.0429103,"oupling the model to the Boston VA health care System. Previous work has been done which aims to predict patient outcomes using unstructured text. Yamashita et al. (2016) analyzed admission records of 1,222 patients who had a clinical pathway of cerebral infraction. The goal was to develop a method for automatically performing clinical evaluations and to identify early interventions for cases that may have clinically important outcomes. There has been a lot of other related work in the NLP area on unstructured electronic medical records and, in particular, in the clinical domain. For example, Jonnagaddala et al. (2015) developed a model to automatically identify smoking status using a SVM model. Jung et al. (2011) extracted events from clinical notes and used this information to construct a timeline of patient medical history. Both of the above mentioned works also used unstructured clinical notes, but focused on identifying patient history information. Cogley et al. (2012) used machine learning to determine whether a patient experienced a particular medical condition. However, while Cogley et al. (2012) looked at patient history and physical examination reports, we wish to predict disposition from complain"
W18-2316,W11-0219,0,0.0750912,"Missing"
W18-2316,W17-3204,0,0.0145349,"ased Java (Rizzolo, 2011). While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training. We do not use neural network approaches in this work both due to the moderate size of the dataset (neural models have been shown to have a steep learning curve (Koehn and Knowles, 2017) and also because our goal is to develop a model that would be as efficient as possible. We train the classifier on the training partition of the corpus and report results on the test partition. All the data was normalized by removing special characters, lowercased, and POS • Wi−2 , P OSi−2 , Wi−1 , P OSi−1 , Infection, Wi+1 , P OSi+1 , Wi+2 , P OSi+2 • Wi−2 , P OSi−2 , Wi−1 , P OSi−1 , Pain, Wi+1 , P OSi+1 , Wi+2 , P OSi+2 5 Results We evaluate the model using both accuracy and F-score. Table 3 shows accuracy results by disposition type. We note that the most frequent class baseline that corr"
W18-2316,W16-4212,0,0.0279893,"VA healthcare System. However, Peck et al. (2012) focused strictly on predicting admit dispositions only, while we aim to predict all possible outcomes. Furthermore, their results focus strictly on structured fields, such as urgency level, age, sex, chief complaint, and the provider seen, while we work with free text in clinical notes. Another issue with the above model is that by including the provider seen to predict admission, they are tightly coupling the model to the Boston VA health care System. Previous work has been done which aims to predict patient outcomes using unstructured text. Yamashita et al. (2016) analyzed admission records of 1,222 patients who had a clinical pathway of cerebral infraction. The goal was to develop a method for automatically performing clinical evaluations and to identify early interventions for cases that may have clinically important outcomes. There has been a lot of other related work in the NLP area on unstructured electronic medical records and, in particular, in the clinical domain. For example, Jonnagaddala et al. (2015) developed a model to automatically identify smoking status using a SVM model. Jung et al. (2011) extracted events from clinical notes and used"
W19-4407,W13-1703,0,0.0435461,"Missing"
W19-4407,W17-2317,0,0.140228,"to be misspelled. We use a dictionary that consists of 140,000 single words (including inflections), 100,000 multiword terms, and 130,000 names (including names and surnames from various countries). The dictionary includes both American and British spelling variants, common acronyms, and foreign words. The dictionary includes lexica from WordNet,2 the SCOWL project,3 names from US Census Data,4 , Wikipedia lists5 , and various sources on the Web. 4.3 Ranking of Candidate Corrections The ranking step is the most challenging one and is the focus of the most work on non-word spelling correction (Fivez et al., 2017b). Our model uses both the features of the misspelling+candidate pair and the contextual information. The former include orthographic similarity, phonetic similarity, and candidate word frequency. The contextual information includes n-gram support, an estimate of potential re-use of words in text, and word embeddings. The features are listed in Table 2. Orthographic similarity is computed as inverse edit distance, 1/(eDist + 1), where eDist is the edit distance (including transpositions) between the misspelling and the correction candidate (Levenshtein, 1966; Damerau, 1964). 1 https://github."
W19-4407,P00-1037,0,0.806922,"g errors and availability of corpora for such research. 2.1 Data Sets for Spelling Research Traditionally, three areas of research have been particularly interested in spelling errors: information retrieval - for misspellings in queries, English language learning - for misspellings made by language learners, and medical information processing - for misspellings in medical documents. Previous work used either proprietary data sets or artificially generated errors. Flor (2012) evaluated on a large corpus of student essays, but the corpus is not publicly available. Toutanova and Moore (2002) and Brill and Moore (2000) similarly evaluated on proprietary data sets of typos collected from native English texts. Query spelling correction has been an important aspect of research in the domain of information retrieval (Hasan et al., 2015; Chen et al., 77 2.2 Approaches to Spelling Correction TOEFL and GRE exam essays. It significantly outperformed popular spellers like Aspell and the speller in MS Word (Flor and Futagi, 2012). Approaches to correcting non-word spelling errors can be broken down into those that only consider the characteristics of the target token when ranking correction candidates, and those that"
W19-4407,W12-2012,1,0.834923,"Missing"
W19-4407,D07-1019,0,0.0871861,"Missing"
W19-4407,P14-2029,0,0.0743594,"Missing"
W19-4407,P11-1121,0,0.0601661,"Missing"
W19-4407,C90-2036,0,0.311535,"st.aspell.net/dicts/ 4 2010 Surnames, on census.gov 5 https://en.wikipedia.org/wiki/ Category:Names_by_language 79 Phonetic similarity reflects the intuition that a good correction should be phonetically similar to the misspelling. It is computed as 1/(eDistP h + 1), where eDistP h is the edit distance between the phonetic representation of the misspelling and the phonetic representation of the candidate. Phonetic representations are computed using the Double-Metaphone algorithm (Philips, 2000). Candidate frequency. A more frequent word is more likely to be the intended word than a rare word (Kernighan et al., 1990). Unigram word frequency is computed for each candidate using the English Wikipedia corpus. N-gram support. For each correction candidate, all n-grams in the window of four context words on each side are taken into account by the n-gram support feature. We use co-occurrence counts computed from the English Wikipedia corpus and weighted as the Positive Normalized PMI scores (PNPMI). Normalized PMI was introduced by Bouma (2009), we adapt it as: log2 be strengthened if a word like “drive” is found in the vicinity. Given a misspelled token, we define a window of ±15 tokens around it. For every ca"
W19-4407,E17-2037,0,0.0377788,"Missing"
W19-4407,W14-1701,0,0.0543716,"etary corpora of native English texts or artificially generated errors in well-formed texts. While spell checkers today are essential and ubiquitous, dealing with data in a variety of “noisy” domains poses particular challenges to traditional spell checkers. Thus, spelling research has shifted focus primarily to correcting spelling errors in social media data, biomedical texts, and texts written by non-native English writers. Non-native English speakers account for the majority of people writing in English today, and spelling errors are some of the most frequent error types for these writers (Ng et al., 2014). In • We present a corpus of learner essays, TOEFL-Spell, annotated for spelling errors. This corpus can be used as a benchmark corpus to develop state-of-the-art models for spelling correction (Section 3). 76 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 76–86 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics • We develop a minimally-supervised approach to spelling correction that combines contextual and non-contextual information (Section 4). We show that inclusion of word embeddings provides info"
W19-4407,W14-1618,0,0.0150323,"vuSM is a feature that caters for systematic misspellings, when a word is misspelled throughout the essay (Flor, 2012). For each candidate correction, we search in the lists of candidate corrections of other misspelled tokens in the text. Each time the candidate or its inflection is found in another list, the candidate is strengthened with a score of SCC /sqrt(1 + distance), where SCC is the current rescaled overall strength of the corresponding candidate in the other list. Word embeddings have shown a lot of success in many NLP applications, especially for estimation of semantic relatedness (Levy and Goldberg, 2014). We use word embeddings to score the contextual fit of correction candidates in the local context of a misspelling. The idea is that for a misspelling like “roat”, a correction to “road” should 5 Experiments We address the following research questions: • How does the model compare to a baseline system? • What is the contribution of individual features, especially those that provide contextual information? • How much training data is needed to learn a robust model? • How does the model behave on out-of-domain data? 6 https://code.google.com/archive/p/ word2vec 80 5.1 Model Baseline (Flor, 2012"
W19-4407,P06-1129,0,0.135185,"Missing"
W19-4407,P16-1208,1,0.928892,"Missing"
W19-4407,P02-1019,0,0.0986175,"correction of non-word spelling errors and availability of corpora for such research. 2.1 Data Sets for Spelling Research Traditionally, three areas of research have been particularly interested in spelling errors: information retrieval - for misspellings in queries, English language learning - for misspellings made by language learners, and medical information processing - for misspellings in medical documents. Previous work used either proprietary data sets or artificially generated errors. Flor (2012) evaluated on a large corpus of student essays, but the corpus is not publicly available. Toutanova and Moore (2002) and Brill and Moore (2000) similarly evaluated on proprietary data sets of typos collected from native English texts. Query spelling correction has been an important aspect of research in the domain of information retrieval (Hasan et al., 2015; Chen et al., 77 2.2 Approaches to Spelling Correction TOEFL and GRE exam essays. It significantly outperformed popular spellers like Aspell and the speller in MS Word (Flor and Futagi, 2012). Approaches to correcting non-word spelling errors can be broken down into those that only consider the characteristics of the target token when ranking correction"
W19-4407,D09-1093,0,0.0769099,"Missing"
W19-4407,P11-1019,0,0.104184,"Missing"
zaghouani-etal-2014-large,W11-2602,1,\N,Missing
zaghouani-etal-2014-large,W10-1004,1,\N,Missing
zaghouani-etal-2014-large,P05-1071,1,\N,Missing
zaghouani-etal-2014-large,P11-1019,0,\N,Missing
zaghouani-etal-2014-large,P08-2015,1,\N,Missing
zaghouani-etal-2014-large,N13-1036,1,\N,Missing
zaghouani-etal-2014-large,W13-1703,0,\N,Missing
zaghouani-etal-2014-large,shaalan-etal-2012-arabic,0,\N,Missing
zaghouani-etal-2014-large,pasha-etal-2014-madamira,1,\N,Missing
zaghouani-etal-2014-large,dickinson-ledbetter-2012-annotating,0,\N,Missing
zaghouani-etal-2014-large,I13-2001,1,\N,Missing
zaghouani-etal-2014-large,abuhakema-etal-2008-annotating,0,\N,Missing
zaghouani-etal-2014-large,N13-1066,1,\N,Missing
