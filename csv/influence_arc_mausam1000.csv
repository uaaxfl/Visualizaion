2020.acl-main.521,P15-1034,0,0.248647,"aggregation scheme to bootstrap training data by combining extractions from multiple OpenIE systems. • IM O JIE trained on this data establishes a new 1 https://github.com/dair-iitd/imojie SoTA in OpenIE, beating previous systems and also our strong BERT-baseline. 2 Related Work Open Information Extraction (OpenIE) involves extracting (arg1 phrase, relation phrase, arg2 phrase) assertions from a sentence. Traditional open extractors are rule-based or statistical, e.g., Textrunner (Banko et al., 2007), ReVerb (Fader et al., 2011; Etzioni et al., 2011), OLLIE (Mausam et al., 2012), Stanford-IE (Angeli et al., 2015), ClausIE (Del Corro and Gemulla, 2013), OpenIE4 (Christensen et al., 2011; Pal and Mausam, 2016), OpenIE-5 (Saha et al., 2017, 2018), PropS (Stanovsky et al., 2016), and MinIE (Gashteovski et al., 2017). These use syntactic or semantic parsers combined with rules to extract tuples from sentences. Recently, to reduce error accumulation in these pipeline systems, neural OpenIE models have been proposed. They belong to one of two paradigms: sequence labeling or sequence generation. Sequence Labeling involves tagging each word in the input sentence as belonging to the subject, predicate, object o"
2020.acl-main.521,D13-1178,1,0.816681,"s, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task. 1 Introduction Extracting structured information from unstructured text has been a key research area within NLP. The paradigm of Open Information Extraction (OpenIE) (Banko et al., 2007) uses an open vocabulary to convert natural text to semi-structured representations, by extracting a set of (subject, relation, object) tuples. OpenIE has found wide use in many downstream NLP tasks (Mausam, 2016) like multi-document question answering and summarization (Fan et al., 2019), event schema induction (Balasubramanian et al., 2013) and word embedding generation (Stanovsky et al., 2015). Traditional OpenIE systems are statistical or rule-based. They are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They often consist of several components like POS tagging, and syntactic parsing. To bypass error accumulation in such pipelines, end-to-end neural systems have been proposed recently. Recent neural OpenIE methods belong to two categories: sequence labeling, e.g., RnnOIE (Stanovsky et al., 2018) and sequence generation, e.g., CopyAttention (Cui et al., 2018). In principle, generation"
2020.acl-main.521,D19-1651,1,0.92641,"as exaggerated noise in the dataset. We devise an unsupervised Score-andFilter mechanism to automatically select a subset of these extractions that are non-redundant and expected to be of high quality. Our approach scores all extractions with a scoring model, followed by filtering to reduce redundancy. We compare IM O JIE against several neural and non-neural systems, including our extension of CopyAttention that uses BERT (Devlin et al., 2019) instead of an LSTM at encoding time, which forms a very strong baseline. On the recently proposed CaRB metric, which penalizes redundant extractions (Bhardwaj et al., 2019), IM O JIE outperforms CopyAttention by about 18 pts in F1 and our strong BERT baseline by 2 pts, establishing a new state of the art for OpenIE. We release IM O JIE & all related resources for further research1 . In summary, our contributions are: • We propose IM O JIE, a neural OpenIE system that generates the next extraction, fully conditioned on the extractions produced so far. IM O JIE produce a variable number of diverse extractions for a sentence, • We present an unsupervised aggregation scheme to bootstrap training data by combining extractions from multiple OpenIE systems. • IM O JIE"
2020.acl-main.521,P14-1085,1,0.835421,"lues. 5875 4 System Stanford-IE OllIE PropS MinIE OpenIE-4 OpenIE-5 ClausIE CopyAttention RNN-OIE Sense-OIE Span-OIE CopyAttention + BERT IM O JIE Opt. F1 23 41.1 31.9 41.9 51.6 48.5 45.1 35.4 49.2 17.2 47.9 51.6 53.5 Metric AUC Last F1 13.4 22.9 22.5 40.9 12.6 31.8 -∗ 41.9 29.5 51.5 25.7 48.5 22.4 45.1 20.4 32.8 26.5 49.2 -∗ 17.2 -∗ 47.9 32.8 49.6 33.3 53.3 System CopyAttention CoverageAttention CoverageAttention+BERT Diverse Beam Search IM O JIE (w/o BERT) IM O JIE Many downstream applications of OpenIE, such as text comprehension (Stanovsky et al., 2015) and sentence similarity estimation (Christensen et al., 2014), use all the extractions output by the OpenIE system. Last F1 is an important measure for such applications. Comparison Systems We compare IM O JIE against several nonneural baselines, including Stanford-IE, OpenIE-4, OpenIE-5, ClausIE, PropS, MinIE, and OLLIE. We also compare against the sequence labeling baselines of RnnOIE, SenseOIE, and the span selection baseline of SpanOIE. Probably the most closely related baseline to us is the neural generation baseline of CopyAttention. To increase CopyAttention’s diversity, we compare against an English version of Logician, which adds coverage atten"
2020.acl-main.521,P18-2065,0,0.804387,"ma induction (Balasubramanian et al., 2013) and word embedding generation (Stanovsky et al., 2015). Traditional OpenIE systems are statistical or rule-based. They are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They often consist of several components like POS tagging, and syntactic parsing. To bypass error accumulation in such pipelines, end-to-end neural systems have been proposed recently. Recent neural OpenIE methods belong to two categories: sequence labeling, e.g., RnnOIE (Stanovsky et al., 2018) and sequence generation, e.g., CopyAttention (Cui et al., 2018). In principle, generation is more powerful because it can introduce auxiliary words or change word order. However, our analysis of CopyAttention reveals that it suffers from two drawbacks. First, it does not naturally adapt the number of extractions to the length or complexity of the input sentence. Second, it is susceptible to stuttering: extraction of multiple triples bearing redundant information. These limitations arise because its decoder has no explicit mechanism to remember what parts of the sentence have already been ‘consumed’ or what triples have already been generated. Its decoder"
2020.acl-main.521,N19-1423,0,0.0167396,"gracefully. obtain a better precision-recall balance. However, simply concatenating extractions from multiple systems does not work well, as it leads to redundancy as well as exaggerated noise in the dataset. We devise an unsupervised Score-andFilter mechanism to automatically select a subset of these extractions that are non-redundant and expected to be of high quality. Our approach scores all extractions with a scoring model, followed by filtering to reduce redundancy. We compare IM O JIE against several neural and non-neural systems, including our extension of CopyAttention that uses BERT (Devlin et al., 2019) instead of an LSTM at encoding time, which forms a very strong baseline. On the recently proposed CaRB metric, which penalizes redundant extractions (Bhardwaj et al., 2019), IM O JIE outperforms CopyAttention by about 18 pts in F1 and our strong BERT baseline by 2 pts, establishing a new state of the art for OpenIE. We release IM O JIE & all related resources for further research1 . In summary, our contributions are: • We propose IM O JIE, a neural OpenIE system that generates the next extraction, fully conditioned on the extractions produced so far. IM O JIE produce a variable number of dive"
2020.acl-main.521,D11-1142,0,0.46679,"variable number of diverse extractions for a sentence, • We present an unsupervised aggregation scheme to bootstrap training data by combining extractions from multiple OpenIE systems. • IM O JIE trained on this data establishes a new 1 https://github.com/dair-iitd/imojie SoTA in OpenIE, beating previous systems and also our strong BERT-baseline. 2 Related Work Open Information Extraction (OpenIE) involves extracting (arg1 phrase, relation phrase, arg2 phrase) assertions from a sentence. Traditional open extractors are rule-based or statistical, e.g., Textrunner (Banko et al., 2007), ReVerb (Fader et al., 2011; Etzioni et al., 2011), OLLIE (Mausam et al., 2012), Stanford-IE (Angeli et al., 2015), ClausIE (Del Corro and Gemulla, 2013), OpenIE4 (Christensen et al., 2011; Pal and Mausam, 2016), OpenIE-5 (Saha et al., 2017, 2018), PropS (Stanovsky et al., 2016), and MinIE (Gashteovski et al., 2017). These use syntactic or semantic parsers combined with rules to extract tuples from sentences. Recently, to reduce error accumulation in these pipeline systems, neural OpenIE models have been proposed. They belong to one of two paradigms: sequence labeling or sequence generation. Sequence Labeling involves t"
2020.acl-main.521,D19-1428,0,0.0383303,"outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task. 1 Introduction Extracting structured information from unstructured text has been a key research area within NLP. The paradigm of Open Information Extraction (OpenIE) (Banko et al., 2007) uses an open vocabulary to convert natural text to semi-structured representations, by extracting a set of (subject, relation, object) tuples. OpenIE has found wide use in many downstream NLP tasks (Mausam, 2016) like multi-document question answering and summarization (Fan et al., 2019), event schema induction (Balasubramanian et al., 2013) and word embedding generation (Stanovsky et al., 2015). Traditional OpenIE systems are statistical or rule-based. They are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They often consist of several components like POS tagging, and syntactic parsing. To bypass error accumulation in such pipelines, end-to-end neural systems have been proposed recently. Recent neural OpenIE methods belong to two categories: sequence labeling, e.g., RnnOIE (Stanovsky et al., 2018) and sequence generation, e.g., Cop"
2020.acl-main.521,D17-1278,0,0.452001,"penIE, beating previous systems and also our strong BERT-baseline. 2 Related Work Open Information Extraction (OpenIE) involves extracting (arg1 phrase, relation phrase, arg2 phrase) assertions from a sentence. Traditional open extractors are rule-based or statistical, e.g., Textrunner (Banko et al., 2007), ReVerb (Fader et al., 2011; Etzioni et al., 2011), OLLIE (Mausam et al., 2012), Stanford-IE (Angeli et al., 2015), ClausIE (Del Corro and Gemulla, 2013), OpenIE4 (Christensen et al., 2011; Pal and Mausam, 2016), OpenIE-5 (Saha et al., 2017, 2018), PropS (Stanovsky et al., 2016), and MinIE (Gashteovski et al., 2017). These use syntactic or semantic parsers combined with rules to extract tuples from sentences. Recently, to reduce error accumulation in these pipeline systems, neural OpenIE models have been proposed. They belong to one of two paradigms: sequence labeling or sequence generation. Sequence Labeling involves tagging each word in the input sentence as belonging to the subject, predicate, object or other. The final extraction is obtained by collecting labeled spans into different fields and constructing a tuple. RnnOIE (Stanovsky et al., 2018) is a labeling system that first identifies the relati"
2020.acl-main.521,P16-1154,0,0.0431209,"coding has access to all of the previous extractions. We simulate this iterative memory with the help of BERT encoder, whose input includes the [CLS] token and original 5873 Figure 2: Ranking-Filtering subsystem for combining extractions from multiple open IE systems in an unsupervised fashion. (‘Exts’=extractions.) sentence appended with the decoded extractions so far, punctuated by the separator token [SEP] before each extraction. IM O JIE uses an LSTM decoder, which is initialized with the embedding of [CLS] token. The contextualized-embeddings of all the word tokens are used for the Copy (Gu et al., 2016) and Attention (Bahdanau et al., 2015) modules. The decoder generates the tuple one word at a time, producing hreli and hobji tokens to indicate the start of relation and object respectively. The iterative process continues until the EndOfExtractions token is generated. The overall process can be summarized as: 1. Pass the sentence through the Seq2Seq architecture to generate the first extraction. 2. Concatenate the generated extraction with the existing input and pass it again through the Seq2Seq architecture to generate the next extraction. 3. Repeat Step 2 until the EndOfExtractions token i"
2020.acl-main.521,P19-1523,0,0.0607841,"p of the sequential decoding process, for generating the ith extraction, which takes the original sentence and all extractions numbered 1, . . . , i − 1, previously generated, as input. SenseOIE (Roy et al., 2019), improves upon RnnOIE by using the extractions of multiple OpenIE systems as features in a sequence labeling setting. However, their training requires manually annotated gold extractions, which is not scalable for the task. This restricts SenseOIE to train on a dataset of 3,000 sentences. In contrast, our proposed Score-and-Filter mechanism is unsupervised and can scale unboundedly. Jiang et al. (2019) is another labeling system that better calibrates extractions across sentences. SpanOIE (Zhan and Zhao, 2020) uses a span selection model, a variant of the sequence labelling paradigm. Firstly, the predicate module finds the predicate spans in a sentence. Subsequently, the argument module outputs the arguments for this predicate. However, SpanOIE cannot extract nominal relations. Moreover, it bootstraps its training data over a single OpenIE system only. In contrast, IM O JIE overcomes both of these limitations. Sequence Generation uses a Seq2Seq model to generate output extractions one word"
2020.acl-main.521,D12-1048,1,0.883081,"ence, • We present an unsupervised aggregation scheme to bootstrap training data by combining extractions from multiple OpenIE systems. • IM O JIE trained on this data establishes a new 1 https://github.com/dair-iitd/imojie SoTA in OpenIE, beating previous systems and also our strong BERT-baseline. 2 Related Work Open Information Extraction (OpenIE) involves extracting (arg1 phrase, relation phrase, arg2 phrase) assertions from a sentence. Traditional open extractors are rule-based or statistical, e.g., Textrunner (Banko et al., 2007), ReVerb (Fader et al., 2011; Etzioni et al., 2011), OLLIE (Mausam et al., 2012), Stanford-IE (Angeli et al., 2015), ClausIE (Del Corro and Gemulla, 2013), OpenIE4 (Christensen et al., 2011; Pal and Mausam, 2016), OpenIE-5 (Saha et al., 2017, 2018), PropS (Stanovsky et al., 2016), and MinIE (Gashteovski et al., 2017). These use syntactic or semantic parsers combined with rules to extract tuples from sentences. Recently, to reduce error accumulation in these pipeline systems, neural OpenIE models have been proposed. They belong to one of two paradigms: sequence labeling or sequence generation. Sequence Labeling involves tagging each word in the input sentence as belonging"
2020.acl-main.521,N18-2089,0,0.0182981,"esident; US) instead of (Trump; is President of; US) from the sentence 5876 Filtering None Extraction-based Sentence-based Score-And-Filter Opt. F1 49.7 46 49.5 53.5 Metric AUC Last F1 34.5 37.4 29.2 44.9 32.7 48.6 33.3 53.3 Table 6: Performance of IM O JIE on aggregated dataset OpenIE-4+ClausIE+RnnOIE, with different filtering techniques. For comparison, SenseOIE trained on multiple system extractions gives an F1 of 17.2 on CaRB. Figure 3: Precision-Recall curve of OpenIE Systems. “US President Trump...”. Moreover, it is trained only on limited number of pseudo-gold extractions, generated by Michael et al. (2018), which does not take advantage of boostrapping techniques. 6.2 Performance of IM O JIE How does IM O JIE perform compared to the previous neural and rule-based systems? In comparison with existing neural and nonneural systems, IM O JIE trained on aggregated bootstrapped data performs the best. It outperforms OpenIE-4, the best existing OpenIE system, by 1.9 F1 pts, 3.8 pts of AUC, and 1.8 pts of Last-F1. Qualitatively, we find that it makes fewer mistakes than OpenIE-4, probably because OpenIE-4 accumulates errors from upstream parsing modules (see Table 2). IM O JIE outperforms CopyAttention"
2020.acl-main.521,W16-1307,1,0.94465,"can only ensure that the extractions are not exact duplicates. In response, we design the first neural OpenIE system that uses sequential decoding of tuples conditioned on previous tuples. We achieve this by adding every generated extraction so far to the encoder. This iterative process stops when the EndOfExtractions tag is generated by the decoder, allowing it to produce a variable number of extractions. We name our system Iterative MemOry Joint Open Information Extraction (IM O JIE). CopyAttention uses a bootstrapping strategy, where the extractions from OpenIE-4 (Christensen et al., 2011; Pal and Mausam, 2016) are used as training data. However, we believe that training on extractions of multiple systems is preferable. For example, OpenIE-4 benefits from high precision compared to ClausIE (Del Corro and Gemulla, 2013), which offers high recall. By aggregating extractions from both, IM O JIE could potentially 5871 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5871–5886 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Sentence CopyAttention IMOJIE He was appointed Commander of the Order of the British Empire in the 1948 Queen’s Birth"
2020.acl-main.521,D19-1067,0,0.266363,", object or other. The final extraction is obtained by collecting labeled spans into different fields and constructing a tuple. RnnOIE (Stanovsky et al., 2018) is a labeling system that first identifies the relation words and then uses sequence labelling to get their arguments. It is trained on OIE2016 dataset, which postprocesses SRL data for OpenIE (Stanovsky and Dagan, 2016). 5872 Figure 1: One step of the sequential decoding process, for generating the ith extraction, which takes the original sentence and all extractions numbered 1, . . . , i − 1, previously generated, as input. SenseOIE (Roy et al., 2019), improves upon RnnOIE by using the extractions of multiple OpenIE systems as features in a sequence labeling setting. However, their training requires manually annotated gold extractions, which is not scalable for the task. This restricts SenseOIE to train on a dataset of 3,000 sentences. In contrast, our proposed Score-and-Filter mechanism is unsupervised and can scale unboundedly. Jiang et al. (2019) is another labeling system that better calibrates extractions across sentences. SpanOIE (Zhan and Zhao, 2020) uses a span selection model, a variant of the sequence labelling paradigm. Firstly,"
2020.acl-main.521,C18-1194,1,0.914688,"Missing"
2020.acl-main.521,P15-2050,1,0.948812,"a new state of the art for the task. 1 Introduction Extracting structured information from unstructured text has been a key research area within NLP. The paradigm of Open Information Extraction (OpenIE) (Banko et al., 2007) uses an open vocabulary to convert natural text to semi-structured representations, by extracting a set of (subject, relation, object) tuples. OpenIE has found wide use in many downstream NLP tasks (Mausam, 2016) like multi-document question answering and summarization (Fan et al., 2019), event schema induction (Balasubramanian et al., 2013) and word embedding generation (Stanovsky et al., 2015). Traditional OpenIE systems are statistical or rule-based. They are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They often consist of several components like POS tagging, and syntactic parsing. To bypass error accumulation in such pipelines, end-to-end neural systems have been proposed recently. Recent neural OpenIE methods belong to two categories: sequence labeling, e.g., RnnOIE (Stanovsky et al., 2018) and sequence generation, e.g., CopyAttention (Cui et al., 2018). In principle, generation is more powerful because it can introduce auxiliary wo"
2020.acl-main.521,N18-1081,0,0.385039,"nt question answering and summarization (Fan et al., 2019), event schema induction (Balasubramanian et al., 2013) and word embedding generation (Stanovsky et al., 2015). Traditional OpenIE systems are statistical or rule-based. They are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They often consist of several components like POS tagging, and syntactic parsing. To bypass error accumulation in such pipelines, end-to-end neural systems have been proposed recently. Recent neural OpenIE methods belong to two categories: sequence labeling, e.g., RnnOIE (Stanovsky et al., 2018) and sequence generation, e.g., CopyAttention (Cui et al., 2018). In principle, generation is more powerful because it can introduce auxiliary words or change word order. However, our analysis of CopyAttention reveals that it suffers from two drawbacks. First, it does not naturally adapt the number of extractions to the length or complexity of the input sentence. Second, it is susceptible to stuttering: extraction of multiple triples bearing redundant information. These limitations arise because its decoder has no explicit mechanism to remember what parts of the sentence have already been ‘con"
2020.acl-main.521,P19-3007,0,0.0501535,"Missing"
2020.acl-main.521,P17-1099,0,0.0143297,"like existing neural OpenIE systems, IM O JIE produces non-redundant as well as a variable number of OpenIE tuples depending on the sentence, by iteratively generating them conditioned on the previous tuples. Additionally, we also contribute a novel technique to combine multiple OpenIE datasets to create a high-quality dataset in a completely unsupervised manner. We release the training data, code, and the pretrained models.9 IM O JIE presents a novel way of using attention for text generation. Bahdanau et al. (2015) showed that attending over the input words is important for text generation. See et al. (2017) showed that using a coverage loss to track the attention over the decoded words improves the quality of the generated output. We add to this narrative by showing that deep inter-attention between the input and the partially-decoded words (achieved by adding previous output in the input) creates a better representation for iterative generation of triples. This general observation may be of independent interest beyond OpenIE, such as in text summarization. Acknowledgements Mausam is supported by IBM AI Horizons Network grant, an IBM SUR award, grants by Google, Bloomberg and 1MG, and a Visvesva"
2020.acl-main.521,D16-1252,0,0.130585,"se pipeline systems, neural OpenIE models have been proposed. They belong to one of two paradigms: sequence labeling or sequence generation. Sequence Labeling involves tagging each word in the input sentence as belonging to the subject, predicate, object or other. The final extraction is obtained by collecting labeled spans into different fields and constructing a tuple. RnnOIE (Stanovsky et al., 2018) is a labeling system that first identifies the relation words and then uses sequence labelling to get their arguments. It is trained on OIE2016 dataset, which postprocesses SRL data for OpenIE (Stanovsky and Dagan, 2016). 5872 Figure 1: One step of the sequential decoding process, for generating the ith extraction, which takes the original sentence and all extractions numbered 1, . . . , i − 1, previously generated, as input. SenseOIE (Roy et al., 2019), improves upon RnnOIE by using the extractions of multiple OpenIE systems as features in a sequence labeling setting. However, their training requires manually annotated gold extractions, which is not scalable for the task. This restricts SenseOIE to train on a dataset of 3,000 sentences. In contrast, our proposed Score-and-Filter mechanism is unsupervised and"
2020.acl-main.521,P17-2050,1,\N,Missing
2020.acl-main.521,W19-4002,0,\N,Missing
2020.emnlp-main.305,D18-1225,0,0.479321,"3 Recent TKBC Systems Recent work adopts a common style for extending φ(s, r, o) to temporal score φ(s, r, o, t). Lacroix et al. (2020) embed each time instant t to vector t and use the form hs, r, o? , ti (called TNTComplEx). This can be interpreted as any one of s, r, o? becoming t-dependent. Goel et al. (2020) make both subject and object embeddings time-dependent; the ‘diachronic’ embedding e ∈ RD of entity e is characterized by et [d] = ae [d] sin(we [d] t + be [d]), where d ∈ D and the sinusoidal nonlinearity affords the capacity to switch “entity features” on and off with time t. HyTE (Dasgupta et al., 2018) models t ∈ RD , ktk2 = 1 and project all of s, r, o on to t: x ↓ t = x − (x · t)t, where x ∈ {s, r, o}. In all cases, time-dependent entity embeddings are plugged into standard scoring functions like DistMult, CX, or SimplE (Kazemi and Poole, 2018). A very different approach (Garc´ıa-Dur´an et al., 2018) encodes the string representation of relation and time with an LSTM, which is used in TransE (TATransE) or DistMult (TA-DM). These formulations do not directly model recurrences of a relation or interactions (e.g., mutual exclusion) between relations. There is some prior work on explicitly pr"
2020.emnlp-main.305,C16-1161,0,0.373038,"· t)t, where x ∈ {s, r, o}. In all cases, time-dependent entity embeddings are plugged into standard scoring functions like DistMult, CX, or SimplE (Kazemi and Poole, 2018). A very different approach (Garc´ıa-Dur´an et al., 2018) encodes the string representation of relation and time with an LSTM, which is used in TransE (TATransE) or DistMult (TA-DM). These formulations do not directly model recurrences of a relation or interactions (e.g., mutual exclusion) between relations. There is some prior work on explicitly providing ordering constraints between relations (e.g., born, married, died) (Jiang et al., 2016). In contrast, T IME P LEX assumes no such additional engineered inputs; it has explicit components to enable learning of temporal (soft) constraints, as model weights, jointly with embeddings of entities, relations, and time instants. Such constraint based reasoning has also been exploited (in a limited way) for a different task, namely, temporal question answering (Jia et al., 2018). 2 A different TKBC task studies only future fact predictions (Trivedi et al., 2017; Jin et al., 2019). 3734 2.4 Standard Evaluation Schemes This section addresses both issues. Link Prediction: Link prediction qu"
2020.emnlp-main.305,D18-1516,0,0.297932,"Missing"
2020.emnlp-main.305,P15-1067,0,\N,Missing
2020.emnlp-main.305,D16-1260,0,\N,Missing
2020.emnlp-main.305,P18-2013,1,\N,Missing
2020.emnlp-main.306,D11-1142,0,0.5679,"eaffirm the gains. In summary, this paper describes OpenIE6, which • is based on our novel IGL architecture, • is trained with constraints to improve recall, • handles conjunctive sentences with our new stateof-art coordination analyzer, which is 12.3 pts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 20"
2020.emnlp-main.306,P16-1079,0,0.28575,"e IGL gives high precision, we can further improve recall by incorporating (soft) global coverage constraints on this 2-D grid. We use constrained training (Mehta et al., 2018) by adding a penalty term for all constraint violations. This encourages the model to satisfy these constraints during inference as well, leading to improved extraction quality, without affecting running time. Furthermore, we observe that existing neural OpenIE models struggle in handling coordination structures, and do not split conjunctive extractions properly. In response, we first design a new coordination analyzer (Ficler and Goldberg, 2016b). It is built with the same IGL architecture, by interpreting each row in the 2-D grid as a coordination structure. This leads to a new state of the art on this task, with a 12.3 pts improvement in F1 over previous best reported result (Teranishi et al., 2019), and a 1.8 pts gain in F1 over a strong BERT baseline. We then combine the output of our coordination analyzer with our OpenIE extractor, resulting in a further increase in performance (Table 1). Our final OpenIE system — OpenIE6 — consists of IGLbased OpenIE extractor (trained with constraints) and IGL-based coordination analyzer. We"
2020.emnlp-main.306,D16-1003,0,0.39265,"e IGL gives high precision, we can further improve recall by incorporating (soft) global coverage constraints on this 2-D grid. We use constrained training (Mehta et al., 2018) by adding a penalty term for all constraint violations. This encourages the model to satisfy these constraints during inference as well, leading to improved extraction quality, without affecting running time. Furthermore, we observe that existing neural OpenIE models struggle in handling coordination structures, and do not split conjunctive extractions properly. In response, we first design a new coordination analyzer (Ficler and Goldberg, 2016b). It is built with the same IGL architecture, by interpreting each row in the 2-D grid as a coordination structure. This leads to a new state of the art on this task, with a 12.3 pts improvement in F1 over previous best reported result (Teranishi et al., 2019), and a 1.8 pts gain in F1 over a strong BERT baseline. We then combine the output of our coordination analyzer with our OpenIE extractor, resulting in a further increase in performance (Table 1). Our final OpenIE system — OpenIE6 — consists of IGLbased OpenIE extractor (trained with constraints) and IGL-based coordination analyzer. We"
2020.emnlp-main.306,D17-1278,0,0.409475,"tateof-art coordination analyzer, which is 12.3 pts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single labeling to get the extractions. Jiang et al. (2020) extend these to bette"
2020.emnlp-main.306,P16-1228,0,0.0225215,"al., 2019) independently detects coordinator, begin, and end of conjuncts, and does joint inference using Cocke–Younger–Kasami (CYK) parsing over context-free grammar (CFG) rules. Our end-to-end model obtains better accuracy than this approach. Figure 2: 2-D grid for OpenIE with extraction as rows and words as columns. The values represent the labels (S)ubject, (R)elation, (O)bject. The empty cells represent (N)one. Constraints are applied across rows (HVE) and columns (POSC). Constrained Training: Constraining outputs of the model is a way to inject prior knowledge into deep neural networks (Hu et al., 2016; Xu et al., 2018; Nandwani et al., 2019). These constraints can be applied either during training or inference or both. We follow Mehta et al. (2018), which models an output constraint as a differentiable penalty term defined over output probabilities given by the network. This penalty is combined with the original loss function for better training. Bhutani et al. (2019) propose an OpenIE system to get extractions from question-answer pairs. Their decoder enforces vocabulary and structural constraints on the output both during training and inference. In contrast, our system uses constraints o"
2020.emnlp-main.306,N16-1011,1,0.82493,"soft constraints, by imposing additional violation penalties in the loss function. This biases the model to learn to satisfy the constraints, without explicitly enforcing them at inference time. To describe the constraints, we first define the notion of a head verb as all verbs except light verbs (do, be, is, has, etc.). We run a POS tagger on the input sentence, and find all head verbs in the sentence by removing all light verbs.4 For example, 3 IGL is a generalization of Ju et al. (2018). Their model can only label spans which are subsets of one another. 4 We used the light verbs listed by Jain and Mausam (2016). 3751 Figure 4: The final OpenIE system. IGL-CA identifies conjunct boundaries by labeling a 2-D grid. This generates simple sentences and CIGL-OIE emits the final extractions. for the sentence, “Obama gained popularity after Oprah endorsed him for the presidency”, the head verbs are gained and endorsed. In order to cover all valid extractions like (Obama; gained; popularity) and (Oprah; endorsed him for; the presidency), we design the following coverage constraints: • POS Coverage (POSC): All words with POS tags as nouns (N), verbs (V), adjectives (JJ), and adverbs (RB) should be part of at"
2020.emnlp-main.306,2020.acl-main.192,0,0.0174038,"ha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single labeling to get the extractions. Jiang et al. (2020) extend these to better calibrate confidences across sentences. Generation-based systems (Cui et al., 2018; Sun et al., 2018) generate extractions sequentially using seq2seq models. IMoJIE (Kolluru et al., 2020), the current state of art in OpenIE, uses a BERT-based encoder and an iterative decoder that re-encodes the extractions generated so far. This re-encoding captures 3749 dependencies between extractions, increasing overall performance, but also makes it 50x slower than RnnOIE. Recently, span-based models (Jiang et al., 2020) have been proposed, e.g., SpanOIE (Zhan and Zhao, 2020), which"
2020.emnlp-main.306,P17-2049,0,0.171023,"Missing"
2020.emnlp-main.306,2020.acl-main.521,1,0.882656,"free information extraction paradigm that generates extractions of the form (subject; relation; object). Built on the principles of domainindependence and scalability (Mausam, 2016), OpenIE systems extract open relations and arguments from the sentence, which allow them to be 1 *Equal Contribution https://github.com/dair-iitd/openie6 End-to-end neural systems for OpenIE have been found to be more accurate compared to their nonneural counterparts, which were built on manually defined rules over linguistic pipelines. The two most popular neural OpenIE paradigms are generation (Cui et al., 2018; Kolluru et al., 2020) and labeling (Stanovsky et al., 2018; Roy et al., 2019). Generation systems generate extractions one word at a time. IMoJIE (Kolluru et al., 2020) is a state-of-the-art OpenIE system that re-encodes the partial set of extractions output thus far when generating the next extraction. This captures dependencies among extractions, reducing the overall redundancy of the output set. However, this repeated re-encoding causes a significant reduction in speed, which limits use at Web scale. On the other hand, labeling-based systems like RnnOIE (Stanovsky et al., 2015) are much faster (150 sentences pe"
2020.emnlp-main.306,D12-1048,1,0.861715,"is based on our novel IGL architecture, • is trained with constraints to improve recall, • handles conjunctive sentences with our new stateof-art coordination analyzer, which is 12.3 pts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be synta"
2020.emnlp-main.306,D18-1538,0,0.056589,"Missing"
2020.emnlp-main.306,P15-2050,1,0.79876,"s are generation (Cui et al., 2018; Kolluru et al., 2020) and labeling (Stanovsky et al., 2018; Roy et al., 2019). Generation systems generate extractions one word at a time. IMoJIE (Kolluru et al., 2020) is a state-of-the-art OpenIE system that re-encodes the partial set of extractions output thus far when generating the next extraction. This captures dependencies among extractions, reducing the overall redundancy of the output set. However, this repeated re-encoding causes a significant reduction in speed, which limits use at Web scale. On the other hand, labeling-based systems like RnnOIE (Stanovsky et al., 2015) are much faster (150 sentences per second, compared to 3 sentences of IMoJIE) but relatively less accurate. They label each word in the sentence as either S (Subject), R (Relation), O (Object) or N (None) for each extraction. However, as the extractions are predicted independently, this does not model the inherent dependencies among the extractions. We bridge this trade-off though our proposed 3748 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3748–3761, c November 16–20, 2020. 2020 Association for Computational Linguistics Sentence IGL IGL +Con"
2020.emnlp-main.306,N18-1081,0,0.585192,"that generates extractions of the form (subject; relation; object). Built on the principles of domainindependence and scalability (Mausam, 2016), OpenIE systems extract open relations and arguments from the sentence, which allow them to be 1 *Equal Contribution https://github.com/dair-iitd/openie6 End-to-end neural systems for OpenIE have been found to be more accurate compared to their nonneural counterparts, which were built on manually defined rules over linguistic pipelines. The two most popular neural OpenIE paradigms are generation (Cui et al., 2018; Kolluru et al., 2020) and labeling (Stanovsky et al., 2018; Roy et al., 2019). Generation systems generate extractions one word at a time. IMoJIE (Kolluru et al., 2020) is a state-of-the-art OpenIE system that re-encodes the partial set of extractions output thus far when generating the next extraction. This captures dependencies among extractions, reducing the overall redundancy of the output set. However, this repeated re-encoding causes a significant reduction in speed, which limits use at Web scale. On the other hand, labeling-based systems like RnnOIE (Stanovsky et al., 2015) are much faster (150 sentences per second, compared to 3 sentences of"
2020.emnlp-main.306,C18-1326,0,0.262162,"ts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single labeling to get the extractions. Jiang et al. (2020) extend these to better calibrate confidences across sentences. Gene"
2020.emnlp-main.306,W16-1307,1,0.878094,"to improve recall, • handles conjunctive sentences with our new stateof-art coordination analyzer, which is 12.3 pts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single label"
2020.emnlp-main.306,D18-1129,0,0.0228015,"Missing"
2020.emnlp-main.306,2020.findings-emnlp.99,0,0.617968,"Missing"
2020.emnlp-main.306,D19-1067,0,0.689016,"ons of the form (subject; relation; object). Built on the principles of domainindependence and scalability (Mausam, 2016), OpenIE systems extract open relations and arguments from the sentence, which allow them to be 1 *Equal Contribution https://github.com/dair-iitd/openie6 End-to-end neural systems for OpenIE have been found to be more accurate compared to their nonneural counterparts, which were built on manually defined rules over linguistic pipelines. The two most popular neural OpenIE paradigms are generation (Cui et al., 2018; Kolluru et al., 2020) and labeling (Stanovsky et al., 2018; Roy et al., 2019). Generation systems generate extractions one word at a time. IMoJIE (Kolluru et al., 2020) is a state-of-the-art OpenIE system that re-encodes the partial set of extractions output thus far when generating the next extraction. This captures dependencies among extractions, reducing the overall redundancy of the output set. However, this repeated re-encoding causes a significant reduction in speed, which limits use at Web scale. On the other hand, labeling-based systems like RnnOIE (Stanovsky et al., 2015) are much faster (150 sentences per second, compared to 3 sentences of IMoJIE) but relativ"
2020.emnlp-main.306,C18-1194,1,0.890312,"alyzer, which is 12.3 pts better in F1, and • is 10× faster compared to current state of the art and improves F1 score by as much as 4.0 pts. 2 Related Work Banko et al. (2007) introduced the Open Information Extraction paradigm (OpenIE) and proposed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single labeling to get the extractions. Jiang et al. (2020) extend these to better calibrate confidences"
2020.emnlp-main.306,D16-1252,0,0.303591,"ed TextRunner, the first model for the task. Following this, many statistical and rule-based systems have been developed (Fader et al., 2011; Etzioni et al., 2011; Christensen et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal and Mausam, 2016; Stanovsky et al., 2016; Saha et al., 2017; Gashteovski et al., 2017; Saha and Mausam, 2018; Niklaus et al., 2018). Recently, supervised neural models have been proposed, which are either trained on extractions bootstrapped from earlier non-neural systems (Cui et al., 2018), or on SRL annotations adapted for OpenIE (Stanovsky and Dagan, 2016). These systems are primarily of three types, as follows. Labeling-based systems like RnnOIE (Stanovsky et al., 2018), and SenseOIE (Roy et al., 2019) identify words that can be syntactic heads of relations, and, for each head word, perform a single labeling to get the extractions. Jiang et al. (2020) extend these to better calibrate confidences across sentences. Generation-based systems (Cui et al., 2018; Sun et al., 2018) generate extractions sequentially using seq2seq models. IMoJIE (Kolluru et al., 2020), the current state of art in OpenIE, uses a BERT-based encoder and an iterative decode"
2020.emnlp-main.306,I17-1027,0,0.0759711,"s governed by conjunctions (e.g., ‘and’), and splitting conjunctive extractions (see Table 1). We follow CalmIE (Saha and Mausam, 2018), which is part of OpenIE5 system – it splits a conjunctive sentence into smaller sentences based on detected coordination boundaries, and runs OpenIE on these split sentences to increase overall recall. For detecting coordination boundaries, Ficler and Goldberg (2016a) re-annotate the Penn Tree Bank corpus with coordination-specific tags. Neural parsers trained on this data use similarity and replacability of conjuncts as features (Ficler and Goldberg, 2016b; Teranishi et al., 2017). The current state-of-the-art system (Teranishi et al., 2019) independently detects coordinator, begin, and end of conjuncts, and does joint inference using Cocke–Younger–Kasami (CYK) parsing over context-free grammar (CFG) rules. Our end-to-end model obtains better accuracy than this approach. Figure 2: 2-D grid for OpenIE with extraction as rows and words as columns. The values represent the labels (S)ubject, (R)elation, (O)bject. The empty cells represent (N)one. Constraints are applied across rows (HVE) and columns (POSC). Constrained Training: Constraining outputs of the model is a way t"
2020.emnlp-main.306,N19-1343,0,0.188256,"tisfy these constraints during inference as well, leading to improved extraction quality, without affecting running time. Furthermore, we observe that existing neural OpenIE models struggle in handling coordination structures, and do not split conjunctive extractions properly. In response, we first design a new coordination analyzer (Ficler and Goldberg, 2016b). It is built with the same IGL architecture, by interpreting each row in the 2-D grid as a coordination structure. This leads to a new state of the art on this task, with a 12.3 pts improvement in F1 over previous best reported result (Teranishi et al., 2019), and a 1.8 pts gain in F1 over a strong BERT baseline. We then combine the output of our coordination analyzer with our OpenIE extractor, resulting in a further increase in performance (Table 1). Our final OpenIE system — OpenIE6 — consists of IGLbased OpenIE extractor (trained with constraints) and IGL-based coordination analyzer. We evaluate OpenIE6 on four metrics from the literature and find that it exceeds in three of them by at least 4.0 pts in F1. We undertake manual evaluation to reaffirm the gains. In summary, this paper describes OpenIE6, which • is based on our novel IGL architectu"
2020.emnlp-main.711,N19-1423,0,0.0494626,"form the negative examples. Note that our classifier scores each sentence independently and never sees sentences from two paragraphs at the same time. (details in App. A.1) We train two variants of this model: (1) rna (s) is trained to score sentences given a question but no answer (answer is replaced with a [MASK] token); and (2) ra (s) is trained to score sentences given a question and its gold answer. We use rna (s) for relevant sentence selection and ra (s) for support identification. 3.2 Question Answering Module To find answers to questions, we use Wolf et al. (2019)’s implementation of Devlin et al. (2019)’s span prediction model. To achieve our best score, we use their BERT-Large-Cased model with whole-word masking and SQuAD (Rajpurkar et al., 2016) fine-tuning.2 We fine-tune this model on the HotpotQA dataset with input QA context E from rna (s). Since BERT models have a hard limit of 512 word-pieces, we use rna (s) to select the most relevant sentences that can fit within this limit, as described next. (See Appendix A.2 for training details.) To accomplish this, we compute the score rna (s) for each sentence in the input D. Then we add sentences in decreasing order of their scores to the QA"
2020.emnlp-main.711,2020.emnlp-main.710,0,0.710559,"ross time-steps. Xiao et al. (2019) propose a Dynamically Fused Graph Networks (DFGN) model that first creates an entity graph from paragraphs, dynamically extracts sub-graphs, and fuses them with paragraph representations. The Select, Answer, Explain (SAE) model (Tu et al., 2020) also first selects relevant documents and uses them to produce answers and explanations. However, it relies on a self-attention over all document representations to capture potential interactions. Additionally, it relies on a Graph Neural Network (GNN) to answer the questions. Hierarchical Graph Network (HGN) model (Fang et al., 2020) builds a hierarchical graph with three levels: entities, sentences and paragraphs to allow for joint reasoning. DecompRC (Min et al., 2019b) takes a completely different approach of learning to decompose the question (using additional annotations) and then answer the decomposed questions using a standard single-hop RC system. Others such as Min et al. (2019a) have noticed that many HotpotQA questions can be answered just based on a single paragraph. However, they did not consider the support identification task (which we show can also be done independently). While they achieve strong (but not"
2020.emnlp-main.711,2021.ccl-1.108,0,0.1505,"Missing"
2020.emnlp-main.711,P19-1416,0,0.667816,"xes, sentences are scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of “Obama was born in Hawaii.” to the question “Where was the 44th President of USA born?” de"
2020.emnlp-main.711,P19-1613,0,0.481668,"xes, sentences are scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of “Obama was born in Hawaii.” to the question “Where was the 44th President of USA born?” de"
2020.emnlp-main.711,D19-1258,0,0.410308,"ragraph, we show that interaction is actually valuable for QA! Specifically, by using relevant sentences spread across multiple paragraphs, our simple model outperforms previous models with more complex interactions. We thus view Q UARK as a different, stronger baseline for multi-hop QA. In the fullwiki setting, each question has no associated context and models are expected to select paragraphs from Wikipedia. To be able to scale to such a large corpus, the proposed systems often select the paragraphs independent of each other. A recent retrieval method in this setting is Semantic Retrieval (Nie et al., 2019) where first the paragraphs are selected based on the question, followed by individual sentences from these para8840 graphs. However, unlike our approach, they do not use the paragraph context to select the sentences, missing key context needed to identify relevance. 3 Pipeline Model: Q UARK Our model works in three steps. First, we score individual sentences from an input set of paragraphs D based on their relevance to the question. Second, we feed the highest-scoring sentences to a span prediction model to produce an answer to the question. Third, we score sentences from D a second time to i"
2020.emnlp-main.711,P19-1225,0,0.0451822,"tially from using context from the corresponding paragraph. It also shows that running this module a second time, with the chosen answer as input, results in more accurate support identification. 2 Related Work Most approaches for HotpotQA attempt to capture the interactions between the paragraphs by either 1 While their approach selects the paragraphs jointly using the link structure, our sentence selection approach is still independent of the other paragraphs. relying on cross-attention between documents or sequentially selecting paragraphs based on the previously selected paragraphs. While Nishida et al. (2019) also use a standard Reading Comprehension (RC) model, they combine it with a special Query Focused Extractor that identifies relevant sentences by updating a RNN state representation in each step, allowing the model to capture dependencies between sentences across time-steps. Xiao et al. (2019) propose a Dynamically Fused Graph Networks (DFGN) model that first creates an entity graph from paragraphs, dynamically extracts sub-graphs, and fuses them with paragraph representations. The Select, Answer, Explain (SAE) model (Tu et al., 2020) also first selects relevant documents and uses them to pr"
2020.emnlp-main.711,D16-1264,0,0.0346229,"time. (details in App. A.1) We train two variants of this model: (1) rna (s) is trained to score sentences given a question but no answer (answer is replaced with a [MASK] token); and (2) ra (s) is trained to score sentences given a question and its gold answer. We use rna (s) for relevant sentence selection and ra (s) for support identification. 3.2 Question Answering Module To find answers to questions, we use Wolf et al. (2019)’s implementation of Devlin et al. (2019)’s span prediction model. To achieve our best score, we use their BERT-Large-Cased model with whole-word masking and SQuAD (Rajpurkar et al., 2016) fine-tuning.2 We fine-tune this model on the HotpotQA dataset with input QA context E from rna (s). Since BERT models have a hard limit of 512 word-pieces, we use rna (s) to select the most relevant sentences that can fit within this limit, as described next. (See Appendix A.2 for training details.) To accomplish this, we compute the score rna (s) for each sentence in the input D. Then we add sentences in decreasing order of their scores to the QA context E, until we have filled no more than 508 word-pieces (incl. question word-pieces). For every new paragraph considered, we also add its firs"
2020.emnlp-main.711,N18-1059,0,0.0258825,"ARK model, with a question and context paragraphs as input. In both blue boxes, sentences are scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of “Obama was born in Haw"
2020.emnlp-main.711,Q18-1021,0,0.0330821,"Overview of the Q UARK model, with a question and context paragraphs as input. In both blue boxes, sentences are scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance"
2020.emnlp-main.711,P19-1617,0,0.281644,"scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of “Obama was born in Hawaii.” to the question “Where was the 44th President of USA born?” depends on the other r"
2020.emnlp-main.711,D18-1259,0,0.0797862,"n and context paragraphs as input. In both blue boxes, sentences are scored independently from one another. rna (s) and ra (s) use the same model architecture with different weights. Introduction Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018; Khot et al., 2020) and models (Min et al., 2019b; Xiao et al., 2019; Tu et al., 2020) with varying complexities have been proposed over the past few years. Our work focuses on HotpotQA (Yang et al., 2018), which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of “Obama was born in Hawaii.” to the questi"
2020.findings-emnlp.410,W18-5408,0,0.0229175,"ize the forget gate bias to 1, unless specified. For brevity, from hereon we would use ht to → − ← − mean [ ht , ht ]. Below, we formally discuss popular pooling techniques: Related Work Pooling: A wide body of work compares the performance of different pooling techniques in object recognition tasks (Boureau et al., 2010a,b, 2011) and finds max-pooling to generally outperform mean-pooling. However, pooling in natural language tasks is relatively understudied. For some text classification tasks, pooled recurrent architectures (Lai et al., 2015; Zhang and Wallace, 2015; Johnson and Zhang, 2016; Jacovi et al., 2018; Yang et al., 2016a), outperform CNNs and BiLSTMs. Additionally, for textual entailment tasks, Conneau et al. (2017) find that max-pooled representations better capture salient words in a sentence. Our work extends the analysis and examines several pooling techniques, including attention, for BiLSTMs applied to natural language tasks. While past approaches assess the ability of pooling in capturing linguistic phenomena, to the best of our knowledge, we are the first to systematically study the training advantages of various pooling techniques. Attention: First proposed as a way to align targe"
2020.findings-emnlp.410,N03-1031,0,\N,Missing
2020.findings-emnlp.410,P11-1015,0,\N,Missing
2020.findings-emnlp.410,D14-1162,0,\N,Missing
2020.findings-emnlp.410,D13-1170,0,\N,Missing
2020.findings-emnlp.410,N16-1174,0,\N,Missing
2020.findings-emnlp.410,D17-1070,0,\N,Missing
2020.findings-emnlp.410,N19-1357,0,\N,Missing
2020.findings-emnlp.410,W18-5443,0,\N,Missing
2020.findings-emnlp.410,D19-1002,0,\N,Missing
2020.findings-emnlp.410,D19-1567,0,\N,Missing
2021.emnlp-main.357,D18-1547,0,0.0222343,"approaches require dialog state annotations in dialog transcripts. Our work falls under end-to-end approaches, which do not require any such intermediate annotations. We first briefly discuss existing TOD datasets and then review approaches for collecting dialog datasets. Finally, we discuss dialog systems related to F LO N ET. Dialog Datasets: Exisiting TOD datasets can be grouped based on the type of knowledge source on which the dialogs are grounded. Most of the existing datasets are for the recommendation task and grounded on structured KBs. Some notable KBgrounded datasets are MultiWOZ (Budzianowski et al., 2018), Stanford multi domain dataset (Eric et al., 2017), CamRest (Wen et al., 2016), Frames 4349 (El Asri et al., 2017), schema guided dialogs (Rastogi et al., 2020) and taskmaster-1 (Byrne et al., 2019). Kim et al. (2020) augment MultiWOZ with utterances grounded on FAQs. The dialogs in datasets such as ShARC (Saeidi et al., 2018) and doc2dial (Feng et al., 2020) are grounded on snippets from unstructured text documents. To the best of our knowledge, F LO D IAL is the first TOD dataset that is grounded on flowcharts and FAQs. Dialog Data Collection: Crowd sourcing frameworks for creating dialog d"
2021.emnlp-main.357,D19-1459,0,0.0184859,"datasets and then review approaches for collecting dialog datasets. Finally, we discuss dialog systems related to F LO N ET. Dialog Datasets: Exisiting TOD datasets can be grouped based on the type of knowledge source on which the dialogs are grounded. Most of the existing datasets are for the recommendation task and grounded on structured KBs. Some notable KBgrounded datasets are MultiWOZ (Budzianowski et al., 2018), Stanford multi domain dataset (Eric et al., 2017), CamRest (Wen et al., 2016), Frames 4349 (El Asri et al., 2017), schema guided dialogs (Rastogi et al., 2020) and taskmaster-1 (Byrne et al., 2019). Kim et al. (2020) augment MultiWOZ with utterances grounded on FAQs. The dialogs in datasets such as ShARC (Saeidi et al., 2018) and doc2dial (Feng et al., 2020) are grounded on snippets from unstructured text documents. To the best of our knowledge, F LO D IAL is the first TOD dataset that is grounded on flowcharts and FAQs. Dialog Data Collection: Crowd sourcing frameworks for creating dialog datasets can be broadly grouped into three types. (1) Wizard-of-Oz framework (Kelley, 1984) pairs up two crowd-workers who play the roles of user and agent while conversing. The user is provided with"
2021.emnlp-main.357,2020.sigdial-1.35,0,0.0343525,"iew approaches for collecting dialog datasets. Finally, we discuss dialog systems related to F LO N ET. Dialog Datasets: Exisiting TOD datasets can be grouped based on the type of knowledge source on which the dialogs are grounded. Most of the existing datasets are for the recommendation task and grounded on structured KBs. Some notable KBgrounded datasets are MultiWOZ (Budzianowski et al., 2018), Stanford multi domain dataset (Eric et al., 2017), CamRest (Wen et al., 2016), Frames 4349 (El Asri et al., 2017), schema guided dialogs (Rastogi et al., 2020) and taskmaster-1 (Byrne et al., 2019). Kim et al. (2020) augment MultiWOZ with utterances grounded on FAQs. The dialogs in datasets such as ShARC (Saeidi et al., 2018) and doc2dial (Feng et al., 2020) are grounded on snippets from unstructured text documents. To the best of our knowledge, F LO D IAL is the first TOD dataset that is grounded on flowcharts and FAQs. Dialog Data Collection: Crowd sourcing frameworks for creating dialog datasets can be broadly grouped into three types. (1) Wizard-of-Oz framework (Kelley, 1984) pairs up two crowd-workers who play the roles of user and agent while conversing. The user is provided with a goal and the agen"
2021.emnlp-main.357,P02-1040,0,0.110614,"document z. The input to GPT2 is constructed by concatenating all the utterances in the dialog history along with the document-value. GPT2 input is described in detail in Appendix A.2. The response is decoded using beam search. 4.2.3 Pre-training To provide a good initialization to the retriever and the generator, we pre-train both the components separately. For each dialog history and response pair (h, y) in our dataset, we first identify the document over which the response is grounded using weak supervision (Zhao et al., 2020). The document whose document-value has the highest BLEU score (Papineni et al., 2002) w.r.t. the response y is labeled as the pseudo grounded document. 4.2.1 Retrievable Documents The retrievable document set includes all flowchart nodes and all FAQ QA pairs associated with the flowchart. In the original RAG model, each (Wikipedia) document had a single dense embedding, based on which a document was retrieved and used. However, for our setting, the content of a The retriever is pre-trained using a contrastive flowchart node will typically not be explicitly men- loss (Hadsell et al., 2006) by using the pseudo tioned in the dialog history. Instead, the right node grounded docume"
2021.emnlp-main.357,D14-1162,0,0.0844517,"Missing"
2021.emnlp-main.357,N19-1126,1,0.900792,"Missing"
2021.emnlp-main.357,2021.findings-acl.448,1,0.808831,"Missing"
2021.emnlp-main.357,D18-1233,0,0.0275182,"og Datasets: Exisiting TOD datasets can be grouped based on the type of knowledge source on which the dialogs are grounded. Most of the existing datasets are for the recommendation task and grounded on structured KBs. Some notable KBgrounded datasets are MultiWOZ (Budzianowski et al., 2018), Stanford multi domain dataset (Eric et al., 2017), CamRest (Wen et al., 2016), Frames 4349 (El Asri et al., 2017), schema guided dialogs (Rastogi et al., 2020) and taskmaster-1 (Byrne et al., 2019). Kim et al. (2020) augment MultiWOZ with utterances grounded on FAQs. The dialogs in datasets such as ShARC (Saeidi et al., 2018) and doc2dial (Feng et al., 2020) are grounded on snippets from unstructured text documents. To the best of our knowledge, F LO D IAL is the first TOD dataset that is grounded on flowcharts and FAQs. Dialog Data Collection: Crowd sourcing frameworks for creating dialog datasets can be broadly grouped into three types. (1) Wizard-of-Oz framework (Kelley, 1984) pairs up two crowd-workers who play the roles of user and agent while conversing. The user is provided with a goal and the agent is given the knowledge necessary to achieve the goal. (2) Self-dialogs framework (Byrne et al., 2019) require"
2021.emnlp-main.357,D16-1233,0,0.0607356,"Missing"
2021.emnlp-main.357,N19-1026,0,0.0171912,"e paraphrases of each utterance in a dialog outline. Utterances corresponding to each component (problem description, node-edge pairs in the flowchart path and closing) are paraphrased separately and then stitched together to construct a dialog. We define four types of paraphrasing tasks: non-contextual, contextual, problem description and closing tasks. In the non-contextual task, a single utterance from the outline is provided to the crowd workers to paraphrase. We requested the workers to provide two paraphrases for each utterance to improve diversity among paraphrases (Jiang et al., 2017; Yaghoub-Zadeh-Fard et al., 2019). In the contextual task, workers are asked to paraphrase in the context of a specific previously collected paraphrase. Problem descriptions tasks ask the worker to describe the troubleshooting problem using the primary issue and secondary issue as discussed in Section 3.2. In closing task, the worker gracefully terminates the dialog in the context of a troubleshooting solution collected from a non-contextual task. Examples of the four type of tasks can be seen in Figure 2b. As most user responses in a flowchart are yes/no, we design the yes/no paraphrasing task based on a study by Rossen-Knil"
2021.emnlp-main.357,2020.acl-demos.30,0,0.0345374,"rowd-worker to write the entire dialog by playing both user and agent. (3) Dialog paraphrasing framework (Shah et al., 2018) systematically generates a dialog outline (user and agent utterance) and crowdsources paraphrases for each utterance to construct a dialog. We follow this framework for collecting F LO D IAL, as it gives us adequate control over dialog flow so that we can incorporate various utterance types in Table 1. Dialog Systems: Large scale pre-trained language models such as GPT2 (Radford et al., 2019) have been used for response generation in both open domain (Wolf et al., 2019; Zhang et al., 2020; Zhao et al., 2020) and TOD systems (Ham et al., 2020; Hosseini-Asl et al., 2020). A major challenge is GPT2’s limitation on the input size. For our setting, it becomes difficult to feed a long input (flowchart, dialog history, FAQ corpus) to GPT2. We overcome this by following the retrieval augment generation paradigm (Lewis et al., 2020) – we are probably the first to apply it to a dialog setting. The task of zero-shot response generation requires a model to generalize to new domains with just domain descriptors and no training dialogs. Existing approaches (Zhao and Eskenazi, 2018; Wu et al"
2021.emnlp-main.357,W18-5001,0,0.0152673,"et al., 2019; Zhang et al., 2020; Zhao et al., 2020) and TOD systems (Ham et al., 2020; Hosseini-Asl et al., 2020). A major challenge is GPT2’s limitation on the input size. For our setting, it becomes difficult to feed a long input (flowchart, dialog history, FAQ corpus) to GPT2. We overcome this by following the retrieval augment generation paradigm (Lewis et al., 2020) – we are probably the first to apply it to a dialog setting. The task of zero-shot response generation requires a model to generalize to new domains with just domain descriptors and no training dialogs. Existing approaches (Zhao and Eskenazi, 2018; Wu et al., 2019; Rastogi et al., 2020) model slots and intents as domain descriptors. We model flowcharts as domain descriptors and expect the system to generalize to new flowcharts unseen during train. 3 The F LO D IAL Dataset nied with two knowledge sources over which the dialogs are grounded: (1) a set of troubleshooting flowcharts and (2) a set of FAQs which contains supplementary information about the domain not present in the flowchart – both are in English. The data collection process uses the dialog paraphrasing framework (Shah et al., 2018) and is illustrated in Figure 2. At a high"
2021.emnlp-main.357,2020.emnlp-main.272,0,0.198459,"the entire dialog by playing both user and agent. (3) Dialog paraphrasing framework (Shah et al., 2018) systematically generates a dialog outline (user and agent utterance) and crowdsources paraphrases for each utterance to construct a dialog. We follow this framework for collecting F LO D IAL, as it gives us adequate control over dialog flow so that we can incorporate various utterance types in Table 1. Dialog Systems: Large scale pre-trained language models such as GPT2 (Radford et al., 2019) have been used for response generation in both open domain (Wolf et al., 2019; Zhang et al., 2020; Zhao et al., 2020) and TOD systems (Ham et al., 2020; Hosseini-Asl et al., 2020). A major challenge is GPT2’s limitation on the input size. For our setting, it becomes difficult to feed a long input (flowchart, dialog history, FAQ corpus) to GPT2. We overcome this by following the retrieval augment generation paradigm (Lewis et al., 2020) – we are probably the first to apply it to a dialog setting. The task of zero-shot response generation requires a model to generalize to new domains with just domain descriptors and no training dialogs. Existing approaches (Zhao and Eskenazi, 2018; Wu et al., 2019; Rastogi et"
2021.findings-acl.448,D18-1547,0,0.0663031,"Missing"
2021.findings-acl.448,W17-5506,0,0.0231626,"Missing"
2021.findings-acl.448,2020.emnlp-main.281,0,0.197667,"tee Alex Ana Time 10am 2pm 7pm When is my dinner with Alex, the date and time? driver The dinner with Alex is at 10am on 1st Feb. system Figure 1: An example dialog between a driver and a system along with the associated knowledge base. Introduction Task oriented dialog systems interact with users to achieve specific goals such as restaurant reservation or calendar enquiry. To satisfy a user goal, the system is expected to retrieve necessary information from a knowledge base and convey it using natural language. Recently several end-to-end approaches (Bordes and Weston, 2017; Wu et al., 2018; He et al., 2020b; Madotto et al., 2018) have been proposed for learning these dialog systems. Inferring the most relevant KB entities necessary for generating the response is crucial for achieving task success. To effectively scale to large KBs, existing approaches (Wen et al., 2018; Wu et al., 2018) distill the KB by softly filtering irrelevant KB information based on the dialog history. For example, in Figure 1 the ideal filtering technique ∗ D. Raghu is an employee at IBM Research. This work was carried out as part of PhD research at IIT Delhi. is expected to filter just the row 1 as the driver is request"
2021.findings-acl.448,D15-1166,0,0.0302061,"owing Wu et al. (2018), we first generate a sketch response which uses entity type (or sketch) tag in place of an entity. For example, The @meeting with @invitee is at @time is generated instead of The dinner with Alex is at 10pm. When an entity tag is generated, we choose an entity suggested by the context and KB memory pointers. Sketch RNN: We use a GRU to generate the sketch response. At each time t, a generate distribution Pg is computed using the decoder hidden state ht and an attended summary P P of the dialog context gt . The summary gt = i j aij wij , where aij is the Luong attention (Luong et al., 2015) weights over the context word representations (wij ). Context Memory Pointer: At each time t, generate the copy distribution over the context Pcon by performing a multi-hop Luong attention over the context memory. The initial query q0t is set to ht . q0t is then attended over the context to generate an attention distribution a1 and a summarized context gt1 . We represent this as gt1 = Hop(q0t , x). In the next hop the same process is repeated by updating the query q1t = q0t + gt1 . The attention weights after H hops is used for computing the context pointer Pcon as follows: Pcon (yt = w) = X"
2021.findings-acl.448,P18-1136,0,0.037877,"Missing"
2021.findings-acl.448,P02-1040,0,0.112484,"ng Adam optimizer (Kingma and Ba, 2014). The embedding dimensions of the hidden states of encoder and decoder GRU are set to 200 and 100 respectively. Word embeddings are initialized with pre-trained 200d GloVe embeddings (Pennington et al., 2014). Words not in Glove are initialized using Glorot uniform distribution (Glorot and Bengio, 2010). The dropout rate is set to 0.2 and teacher forcing ratio set to 0.9. The best hyper-parameter setting for each dataset and other training details are reported in the Appendix A. Evaluation Metrics: We measure the performance of all the models using BLEU (Papineni et al., 2002), our proposed multiset entity F1 and for completeness the previously used entity F1 (Wu et al., 2018). MultiSet Entity F1 (MSE F1): The entity F1 is used to measure the model’s ability to predict relevant entities from the KB. It is computed by micro averaging over the set of entities in the gold responses and the set of entities in the predicted responses. This metric suffers from two main problems. First, when the gold response has multiple instances of the same entity value, it is accounted for just once in the set representation. For example, in Table 1 the entity value 11am occurs twice"
2021.findings-acl.448,D14-1162,0,0.0854961,"7 66.4 57.5 65.5 65.4 12.7 13.9 17 13.9 17.4 14.4 17.2 16.8 51.9 59.6 54.6 53.7 55.4 62.7 59.0 61.1 59.6 56.7 55.1 59.1 9.1 6.9 9.4 13.6 14.6 30.0 32.4 35.1 35.6 36.5 34.8 35.0 36.0 CDN ET 21.8 68.6 68.4 17.8 62.9 62.9 11.9 38.7 38.6 Table 2: Performance of CDN ET and baselines on the CamRest, SMD and Multi-WOZ 2.1 datasets. Training Details: CDN ET is trained end to end using Adam optimizer (Kingma and Ba, 2014). The embedding dimensions of the hidden states of encoder and decoder GRU are set to 200 and 100 respectively. Word embeddings are initialized with pre-trained 200d GloVe embeddings (Pennington et al., 2014). Words not in Glove are initialized using Glorot uniform distribution (Glorot and Bengio, 2010). The dropout rate is set to 0.2 and teacher forcing ratio set to 0.9. The best hyper-parameter setting for each dataset and other training details are reported in the Appendix A. Evaluation Metrics: We measure the performance of all the models using BLEU (Papineni et al., 2002), our proposed multiset entity F1 and for completeness the previously used entity F1 (Wu et al., 2018). MultiSet Entity F1 (MSE F1): The entity F1 is used to measure the model’s ability to predict relevant entities from the K"
2021.findings-acl.448,D19-1013,0,0.0111812,"ips. Wen et al. (2018) perform distillation using the similarity between dialog history representation and each attribute representation in a KB record, whereas CDN ET uses word based pairwise similarity for distillation. We now briefly discuss approaches that improve other aspects of task oriented dialogs. He et al. (2020c) and He et al. (2020b) model KBs using Relational GCNs (Schlichtkrull et al., 2018).Raghu et al. (2019) provide support for entities unseen during train. Reddy et al. (2019) improve the ability to reason over KB by respecting the relationships between connected attributes. Qin et al. (2019) restricts the response to contain entities from a single KB record. (Qin et al., 2020) handle multiple domains using shared-private networks and He et al. (2020a) optimize their network on both F1 and BLEU. We are the first to propose a pairwise similarity score for KB distillation and a embedding constraint loss to distill irrelevant KB records. 3 as a sequence of utterances {ui }ki=1 and each utterance ui as sequence of words {wij }, and (2) a knowledge base K with M records {rm }M m=1 and each record rm has N key-value attribute pairs n )}N . The network generates the system {(k n , vm n=1"
2021.findings-acl.448,2020.acl-main.565,0,0.0433092,"representation and each attribute representation in a KB record, whereas CDN ET uses word based pairwise similarity for distillation. We now briefly discuss approaches that improve other aspects of task oriented dialogs. He et al. (2020c) and He et al. (2020b) model KBs using Relational GCNs (Schlichtkrull et al., 2018).Raghu et al. (2019) provide support for entities unseen during train. Reddy et al. (2019) improve the ability to reason over KB by respecting the relationships between connected attributes. Qin et al. (2019) restricts the response to contain entities from a single KB record. (Qin et al., 2020) handle multiple domains using shared-private networks and He et al. (2020a) optimize their network on both F1 and BLEU. We are the first to propose a pairwise similarity score for KB distillation and a embedding constraint loss to distill irrelevant KB records. 3 as a sequence of utterances {ui }ki=1 and each utterance ui as sequence of words {wij }, and (2) a knowledge base K with M records {rm }M m=1 and each record rm has N key-value attribute pairs n )}N . The network generates the system {(k n , vm n=1 response Y = (y1 , . . . , yT ) one word at a time. 3.1 Context Encoder: The dialog hi"
2021.findings-acl.448,P17-1099,0,0.0112252,"loss is given by, Ld = − XM m=1 d∗m log(dm ) (6) The overall loss function L = Lg + Lc + Lec + Ld , where Lg and Lc are the cross entropy loss on Pg and Pc respectively. Detailed equations are described in Appendix B. (2) 4 KB Memory Pointer: At each time t, we generate the copy distribution over the KB Pkb using (1) t over the KB record r Luong attention weight βm m and (2) Luong attention weight γnt over attribute keys in a record kn and (3) the distillation weight dm over the KB record rm . The KB pointer Pkb is computed as follows: P The two copy pointers are combined using a soft gate α (See et al., 2017) to get the final copy distribution Pc as follows, (3) Experimental Setup Datasets: We evaluate our model on three datasets – CamRest (Wen et al., 2017), Multi-WOZ 2.1 (WOZ) (Budzianowski et al., 2018) and Stanford Multi-Domain (SMD) Dataset (Eric et al., 2017). Baselines: We compare CDN ET against the following baselines: MLM (Reddy et al., 2019), DSR (Wen et al., 2018), GLMP (Wu et al., 2018), Entity Consistent (Qin et al., 2019), EER (He et al., 2020c), FG2Seq (He et al., 2020b), TTOS (He et al., 2020a) and DFNet (Qin et al., 2020). 5053 Utterance Set MultiSet for which one? I have two, one"
C18-1194,P15-1034,0,0.375438,"Missing"
C18-1194,D16-1006,0,0.0611279,"ions (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Gemulla, 2013) – these outperformed others in a recent large-scale evaluation (Stanovsky and Dagan, 2016). ClausIE can also split some conjunctive clauses, thereby obtaining a higher yield. There hasn’t been any system with a specific focus on conjunctive sentences, which are recently found to be a reason for significant missed recall (Saha et al., 2017). While some of the missed recall is because of conjunctions appearing in arguments of"
C18-1194,D07-1090,0,0.103096,"Missing"
C18-1194,P14-1085,1,0.817439,"ely. C ALM IE(O) particularly achieves 1.8x the yield with a 5 pt precision gain against Open IE 4.2 on a random sample of 100 conjunctive sentences from ClueWeb. Finally, we compare C ALM with Ficler’s system. On the subset of cases where a conjunction coordinates more than two conjuncts, our methods significantly outperform Ficler’s, on an Open IE evaluation. We release our implementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our"
C18-1194,D13-1043,0,0.0390894,"es from ClueWeb. Finally, we compare C ALM with Ficler’s system. On the subset of cases where a conjunction coordinates more than two conjuncts, our methods significantly outperform Ficler’s, on an Open IE evaluation. We release our implementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Ge"
C18-1194,D16-1003,0,0.658912,"hristensen et al., 2011; Pal and Mausam, 2016) and ClausIE (Corro and Gemulla, 2013) to frequently miss important extractions due to conjunctive relation phrases (see Table 1), and occasionally output conjunctive arguments, which are not ideal for readability or downstream applications (Angeli et al., 2015; Stanovsky et al., 2016a). Most modern Open IE systems process dependency parses to obtain extractions. However, dependency parsers frequently make errors in resolving coordination ambiguity. Predicting the correct conjunct span is considered to be one of the biggest challenges for parsers (Ficler and Goldberg, 2016). The state of the art approach by Ficler and Goldberg (2016) trains an LSTM-based network for predicting the boundaries for the two conjuncts on either side of the coordinating conjunction, but does not handle cases where a conjunction coordinates more than two conjuncts. Contributions: We propose a novel coordination analyzer called C ALM (Coordination Analyzer using Language Model), which corrects the typical errors made by dependency parsers (specifically Clear parser, which is used in Open IE 4.2) using additional features and linguistic constraints. Under the intuition that one can split"
C18-1194,E12-1044,0,0.0149982,"ted into OpenIE 5.0, the latest software of OpenIE. It is publicly available at https://github. com/dair-iitd/OpenIE-standalone. 2289 Figure 1: Flow diagram of C ALM IE Table 1), thereby motivating the need to split conjunctive sentences. Conjunct Boundary Detection: Although co-ordination disambiguation has attracted attention of researchers over the years, it still remains one of the hardest problems to solve. Prior work has used two main principles for resolving ambiguities – (1) coordinated conjuncts have similar syntactic structures (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) and (2) replacing a full coordinated phrase with just one conjunct produces coherent (simple) sentences (Ficler and Goldberg, 2016). The state-of-the-art coordination analyzer is by Ficler and Goldberg (2016), which uses LSTM-based components for operationalizing both these principles. It is a machine-learned model, which requires explicit annotation of coordination phrases for training (which isn’t available in original Penn TreeBank). Importantly, it only outputs spans for the two conjuncts on either side of the conjunctive word and ignores any other conjuncts coordinated by the same word."
C18-1194,P09-1109,0,0.0675332,"LM IE(O) is integrated into OpenIE 5.0, the latest software of OpenIE. It is publicly available at https://github. com/dair-iitd/OpenIE-standalone. 2289 Figure 1: Flow diagram of C ALM IE Table 1), thereby motivating the need to split conjunctive sentences. Conjunct Boundary Detection: Although co-ordination disambiguation has attracted attention of researchers over the years, it still remains one of the hardest problems to solve. Prior work has used two main principles for resolving ambiguities – (1) coordinated conjuncts have similar syntactic structures (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) and (2) replacing a full coordinated phrase with just one conjunct produces coherent (simple) sentences (Ficler and Goldberg, 2016). The state-of-the-art coordination analyzer is by Ficler and Goldberg (2016), which uses LSTM-based components for operationalizing both these principles. It is a machine-learned model, which requires explicit annotation of coordination phrases for training (which isn’t available in original Penn TreeBank). Importantly, it only outputs spans for the two conjuncts on either side of the conjunctive word and ignores any other conjuncts coordi"
C18-1194,P07-1086,0,0.035388,"IE (extraction #9, #10, #11 in 1 C ALM IE(O) is integrated into OpenIE 5.0, the latest software of OpenIE. It is publicly available at https://github. com/dair-iitd/OpenIE-standalone. 2289 Figure 1: Flow diagram of C ALM IE Table 1), thereby motivating the need to split conjunctive sentences. Conjunct Boundary Detection: Although co-ordination disambiguation has attracted attention of researchers over the years, it still remains one of the hardest problems to solve. Prior work has used two main principles for resolving ambiguities – (1) coordinated conjuncts have similar syntactic structures (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) and (2) replacing a full coordinated phrase with just one conjunct produces coherent (simple) sentences (Ficler and Goldberg, 2016). The state-of-the-art coordination analyzer is by Ficler and Goldberg (2016), which uses LSTM-based components for operationalizing both these principles. It is a machine-learned model, which requires explicit annotation of coordination phrases for training (which isn’t available in original Penn TreeBank). Importantly, it only outputs spans for the two conjuncts on either side of the conjunctive w"
C18-1194,N16-1011,1,0.80462,"Missing"
C18-1194,D12-1048,1,0.817787,"tion coordinates more than two conjuncts, our methods significantly outperform Ficler’s, on an Open IE evaluation. We release our implementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Gemulla, 2013) – these outperformed others in a recent large-scale evaluation (Stanovsky and Dagan, 2016"
C18-1194,C10-1089,0,0.0332763,"ranking of conjunct spans is based on the second principle (coherence of simple sentences). However, the search space is additionally restricted by various linguistic constraints for both single and nested coordination cases. Our system operates on top of Clear dependency parses (as opposed to constituency parses for Ficler’s) and does not need any task-specific training data. Sentence Simplification: C ALM-based sentence splitting can be seen as a form of sentence simplification. Existing works on sentence simplification (Zhu et al., 2010; Vickrey and Koller, 2008; Vanderwende et al., 2007; Miwa et al., 2010) operate on top of syntactic parses, assuming them to be correct. C ALM, on the other hand, corrects typical errors made by the parser to output corrected conjunction spans. These spans should naturally improve any sentence simplification task as well. 3 C ALM IE Figure 1 illustrates various steps of C ALM IE. First, C ALM identifies specific conjuncts to split the sentence into multiple simple sentences. Then an Open IE system acts on simple sentences to generate extractions. In this section we first focus on our key technical contribution, the coordination analyzer named C ALM and conclude w"
C18-1194,W16-1307,1,0.917014,"tion over the simple sentences identified by C ALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences. 1 Introduction Open Information Extraction (Open IE) (Etzioni et al., 2008) extracts relational tuples from text in an unsupervised domain-independent manner, by identifying relational phrases and arguments from the sentences themselves. Recent work (Saha et al., 2017) has highlighted the lack of proper conjunction processing as the most significant source of missed yield in Open IE. We found Open IE 4.2 (Christensen et al., 2011; Pal and Mausam, 2016) and ClausIE (Corro and Gemulla, 2013) to frequently miss important extractions due to conjunctive relation phrases (see Table 1), and occasionally output conjunctive arguments, which are not ideal for readability or downstream applications (Angeli et al., 2015; Stanovsky et al., 2016a). Most modern Open IE systems process dependency parses to obtain extractions. However, dependency parsers frequently make errors in resolving coordination ambiguity. Predicting the correct conjunct span is considered to be one of the biggest challenges for parsers (Ficler and Goldberg, 2016). The state of the a"
C18-1194,P17-2050,1,0.897604,"tion Extraction (Open IE). State-of-the-art Open IE systems lose substantial yield due to ineffective processing of conjunctive sentences. Our Open IE system, C ALM IE, performs extraction over the simple sentences identified by C ALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences. 1 Introduction Open Information Extraction (Open IE) (Etzioni et al., 2008) extracts relational tuples from text in an unsupervised domain-independent manner, by identifying relational phrases and arguments from the sentences themselves. Recent work (Saha et al., 2017) has highlighted the lack of proper conjunction processing as the most significant source of missed yield in Open IE. We found Open IE 4.2 (Christensen et al., 2011; Pal and Mausam, 2016) and ClausIE (Corro and Gemulla, 2013) to frequently miss important extractions due to conjunctive relation phrases (see Table 1), and occasionally output conjunctive arguments, which are not ideal for readability or downstream applications (Angeli et al., 2015; Stanovsky et al., 2016a). Most modern Open IE systems process dependency parses to obtain extractions. However, dependency parsers frequently make err"
C18-1194,schmidek-barbosa-2014-improving,0,0.0205981,"n an Open IE evaluation. We release our implementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Gemulla, 2013) – these outperformed others in a recent large-scale evaluation (Stanovsky and Dagan, 2016). ClausIE can also split some conjunctive clauses, thereby obtaining a higher yield. There hasn’"
C18-1194,D07-1064,0,0.0239291,"n #9, #10, #11 in 1 C ALM IE(O) is integrated into OpenIE 5.0, the latest software of OpenIE. It is publicly available at https://github. com/dair-iitd/OpenIE-standalone. 2289 Figure 1: Flow diagram of C ALM IE Table 1), thereby motivating the need to split conjunctive sentences. Conjunct Boundary Detection: Although co-ordination disambiguation has attracted attention of researchers over the years, it still remains one of the hardest problems to solve. Prior work has used two main principles for resolving ambiguities – (1) coordinated conjuncts have similar syntactic structures (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) and (2) replacing a full coordinated phrase with just one conjunct produces coherent (simple) sentences (Ficler and Goldberg, 2016). The state-of-the-art coordination analyzer is by Ficler and Goldberg (2016), which uses LSTM-based components for operationalizing both these principles. It is a machine-learned model, which requires explicit annotation of coordination phrases for training (which isn’t available in original Penn TreeBank). Importantly, it only outputs spans for the two conjuncts on either side of the conjunctive word and ignores any oth"
C18-1194,D16-1252,0,0.233787,"ata (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Gemulla, 2013) – these outperformed others in a recent large-scale evaluation (Stanovsky and Dagan, 2016). ClausIE can also split some conjunctive clauses, thereby obtaining a higher yield. There hasn’t been any system with a specific focus on conjunctive sentences, which are recently found to be a reason for significant missed recall (Saha et al., 2017). While some of the missed recall is because of conjunctions appearing in arguments of extractions (extraction #6 in Table 1), which can be separated by reducing the argument spans (Stanovsky et al., 2016b), there are also important parts of the sentence which do not produce any extraction from either Open IE 4.2 or ClausIE (extraction #9, #10, #1"
C18-1194,P15-2050,1,0.754877,"rly achieves 1.8x the yield with a 5 pt precision gain against Open IE 4.2 on a random sample of 100 conjunctive sentences from ClueWeb. Finally, we compare C ALM with Ficler’s system. On the subset of cases where a conjunction coordinates more than two conjuncts, our methods significantly outperform Ficler’s, on an Open IE evaluation. We release our implementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 ("
C18-1194,P16-2077,0,0.13952,"unsupervised domain-independent manner, by identifying relational phrases and arguments from the sentences themselves. Recent work (Saha et al., 2017) has highlighted the lack of proper conjunction processing as the most significant source of missed yield in Open IE. We found Open IE 4.2 (Christensen et al., 2011; Pal and Mausam, 2016) and ClausIE (Corro and Gemulla, 2013) to frequently miss important extractions due to conjunctive relation phrases (see Table 1), and occasionally output conjunctive arguments, which are not ideal for readability or downstream applications (Angeli et al., 2015; Stanovsky et al., 2016a). Most modern Open IE systems process dependency parses to obtain extractions. However, dependency parsers frequently make errors in resolving coordination ambiguity. Predicting the correct conjunct span is considered to be one of the biggest challenges for parsers (Ficler and Goldberg, 2016). The state of the art approach by Ficler and Goldberg (2016) trains an LSTM-based network for predicting the boundaries for the two conjuncts on either side of the coordinating conjunction, but does not handle cases where a conjunction coordinates more than two conjuncts. Contributions: We propose a nov"
C18-1194,P08-1040,0,0.0604241,"that it is not true often enough to be helpful. Our ranking of conjunct spans is based on the second principle (coherence of simple sentences). However, the search space is additionally restricted by various linguistic constraints for both single and nested coordination cases. Our system operates on top of Clear dependency parses (as opposed to constituency parses for Ficler’s) and does not need any task-specific training data. Sentence Simplification: C ALM-based sentence splitting can be seen as a form of sentence simplification. Existing works on sentence simplification (Zhu et al., 2010; Vickrey and Koller, 2008; Vanderwende et al., 2007; Miwa et al., 2010) operate on top of syntactic parses, assuming them to be correct. C ALM, on the other hand, corrects typical errors made by the parser to output corrected conjunction spans. These spans should naturally improve any sentence simplification task as well. 3 C ALM IE Figure 1 illustrates various steps of C ALM IE. First, C ALM identifies specific conjuncts to split the sentence into multiple simple sentences. Then an Open IE system acts on simple sentences to generate extractions. In this section we first focus on our key technical contribution, the co"
C18-1194,N13-1107,0,0.0561985,"lementations of C ALM, sentence splitter and C ALM IE(O)1 for further research. 2 Related Work Open Information Extraction: Mausam (2016) surveys the progress in Open IE systems and its downstream applications (e.g., (Christensen et al., 2014; Stanovsky et al., 2015)). Existing Open IE systems are based on manually written patterns (Etzioni et al., 2011; de S´a Mesquita et al., 2013; Angeli et al., 2015), machine-learned patterns over bootstrapped training data (Mausam et al., 2012), sentence restructuring and decomposition (Bast and Haussmann, 2013; Schmidek and Barbosa, 2014), tree kernels (Xu et al., 2013) and simple inference (Bast and Haussmann, 2014). Some systems focus on specific kinds of extractions, e.g., noun-mediated (Pal and Mausam, 2016), numerical (Saha et al., 2017) and nested (Bhutani et al., 2016). We compare our methods to Open IE 4.2 (a combination of SRL-based IE (Christensen et al., 2011) and Relnoun (Pal and Mausam, 2016)) and ClausIE (Corro and Gemulla, 2013) – these outperformed others in a recent large-scale evaluation (Stanovsky and Dagan, 2016). ClausIE can also split some conjunctive clauses, thereby obtaining a higher yield. There hasn’t been any system with a specifi"
C18-1194,C10-1152,0,0.0296241,"nciple, as we find that it is not true often enough to be helpful. Our ranking of conjunct spans is based on the second principle (coherence of simple sentences). However, the search space is additionally restricted by various linguistic constraints for both single and nested coordination cases. Our system operates on top of Clear dependency parses (as opposed to constituency parses for Ficler’s) and does not need any task-specific training data. Sentence Simplification: C ALM-based sentence splitting can be seen as a form of sentence simplification. Existing works on sentence simplification (Zhu et al., 2010; Vickrey and Koller, 2008; Vanderwende et al., 2007; Miwa et al., 2010) operate on top of syntactic parses, assuming them to be correct. C ALM, on the other hand, corrects typical errors made by the parser to output corrected conjunction spans. These spans should naturally improve any sentence simplification task as well. 3 C ALM IE Figure 1 illustrates various steps of C ALM IE. First, C ALM identifies specific conjuncts to split the sentence into multiple simple sentences. Then an Open IE system acts on simple sentences to generate extractions. In this section we first focus on our key tech"
D10-1123,P08-1004,1,0.606395,"and freelyavailable knowledge resources such as Freebase. It first computes multiple typed functionality scores, representing functionality of the relation phrase when its arguments are constrained to specific types. It then aggregates these scores to predict the global functionality for the phrase. L EIBNIZ outperforms previous work, increasing area under the precisionrecall curve from 0.61 to 0.88. We utilize L EIBNIZ to generate the first public repository of automatically-identified functional relations. 1 Introduction The paradigm of Open Information Extraction (IE) (Banko et al., 2007; Banko and Etzioni, 2008) has scaled extraction technology to the massive set of relations expressed in Web text. However, additional work is needed to better understand these relations, and to place them in richer semantic structures. A step in that direction is identifying the properties of these relations, e.g., symmetry, transitivity and our focus in this paper – functionality. We refer to this problem as functionality identification. A binary relation is functional if, for a given arg1, there is exactly one unique value for arg2. Examples of functional relations are father, death date, birth city, etc. We define"
D10-1123,N06-1038,0,0.0162819,"l-suited for identifying whether a verb phrase can take multiple objects or not. This can be understood as a functionality property of the verb phrase within a sentence, as opposed to functionality of the semantic relation the phrase represents. WIE: In a preliminary study, Popescu (2007) applies an instance based counting approach, but her relations require manually annotated type restrictions, which makes the approach less scalable. Finally, functionality is just one property of relations that can be learned from text. A number of other studies (Guarino and Welty, 2004; Volker et al., 2005; Culotta et al., 2006) have examined detecting other relation properties from text and applying them to tasks such as ontology cleaning. 3 Challenges for Functionality Identification A functional binary relation r is formally defined as one such that ∀x, y1 , y2 : r(x, y1 )∧r(x, y2 ) ⇒ y1 = y2 . We define a relation string to be functional if all semantic relations commonly expressed by the relation string are individually functional. Thus, under our definition, ‘was born in’ and ‘died in’ are functional, even though they can take different arg2s for the same arg1, e.g., year, city, state, country, etc. The definit"
D10-1123,J93-1003,0,0.0250864,"enables L EIBNIZ to identify far more functional relations than T EXT RUNNER. 6.1 Addressing Evidence Sparsity Scaling up to a large collection of typed relations requires us to consider the size of our data sets. For example, consider which relation is more likely to be functional—a relation with 10 instances all of which indicate functionality versus a relation with 100 instances where 95 behave functionally. To address this problem, we adapt the likelihood ratio approach from Schoenmackers et al. (2010). For a typed relation with n instances, f of which indicate functionality, the G-test (Dunning, 1993), G = 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a measure for the likelihood that the relation is not functional. Here k denotes the evidence indicating functionality for the case where the relation is not functional. Setting k = n*0.25 worked well for us. This G-score replaces our previous metric for scoring functional relations. 6.2 Evaluation of the Repository In C LEAN L ISTS a factor that affects the quality of the results is the exact set of lists that is used. If the lists are not clean, results get noisy. For example, Freebase’s list of films contains 73,000 entries, many of which"
D10-1123,P10-1150,0,0.00617076,"Missing"
D10-1123,D08-1002,1,0.823808,"e is exactly one unique value for arg2. Examples of functional relations are father, death date, birth city, etc. We define a relation phrase to be functional if all semantic relations commonly expressed by that phrase are functional. For example, we say that the phrase ‘was born in’ denotes a functional relation, because the different semantic relations expressed by the phrase (e.g., birth city, birth year, etc.) are all functional. Knowing that a relation is functional is helpful for numerous NLP inference tasks. Previous work has used functionality for the tasks of contradiction detection (Ritter et al., 2008), quantifier scope disambiguation (Srinivasan and Yates, 2009), and synonym resolution (Yates and Etzioni, 2009). It could also aid in other tasks such as ontology generation and information extraction. For example, consider two sentences from a contradiction detection task: (1) “George Washington was born in Virginia.” and (2) “George Washington was born in Texas.” As Ritter et al. (2008) points out, we can only determine that the two sentences are contradictory if we know that the semantic relation referred to by the phrase ‘was born in’ is functional, and that both Virginia and Texas are di"
D10-1123,P10-1044,1,0.0728578,"Missing"
D10-1123,D08-1009,1,0.19266,"Missing"
D10-1123,D10-1106,1,0.069093,"erspecified tuple, <Gold, has, an atomic number of 79>, O CCAM extracts <Gold, has an atomic number of, 79>. O CCAM enables L EIBNIZ to identify far more functional relations than T EXT RUNNER. 6.1 Addressing Evidence Sparsity Scaling up to a large collection of typed relations requires us to consider the size of our data sets. For example, consider which relation is more likely to be functional—a relation with 10 instances all of which indicate functionality versus a relation with 100 instances where 95 behave functionally. To address this problem, we adapt the likelihood ratio approach from Schoenmackers et al. (2010). For a typed relation with n instances, f of which indicate functionality, the G-test (Dunning, 1993), G = 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a measure for the likelihood that the relation is not functional. Here k denotes the evidence indicating functionality for the case where the relation is not functional. Setting k = n*0.25 worked well for us. This G-score replaces our previous metric for scoring functional relations. 6.2 Evaluation of the Repository In C LEAN L ISTS a factor that affects the quality of the results is the exact set of lists that is used. If the lists are not c"
D10-1123,D09-1152,0,0.121581,"ctional relations are father, death date, birth city, etc. We define a relation phrase to be functional if all semantic relations commonly expressed by that phrase are functional. For example, we say that the phrase ‘was born in’ denotes a functional relation, because the different semantic relations expressed by the phrase (e.g., birth city, birth year, etc.) are all functional. Knowing that a relation is functional is helpful for numerous NLP inference tasks. Previous work has used functionality for the tasks of contradiction detection (Ritter et al., 2008), quantifier scope disambiguation (Srinivasan and Yates, 2009), and synonym resolution (Yates and Etzioni, 2009). It could also aid in other tasks such as ontology generation and information extraction. For example, consider two sentences from a contradiction detection task: (1) “George Washington was born in Virginia.” and (2) “George Washington was born in Texas.” As Ritter et al. (2008) points out, we can only determine that the two sentences are contradictory if we know that the semantic relation referred to by the phrase ‘was born in’ is functional, and that both Virginia and Texas are distinct states. Automatic functionality identification is essen"
D10-1123,D11-1142,1,\N,Missing
D11-1141,P11-1040,0,0.697784,"Missing"
D11-1141,J92-4003,0,0.0910564,"Missing"
D11-1141,W99-0613,0,0.0450456,"l release the [Nintendo]ORG 3DS in north [America]LOC march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most"
D11-1141,N09-1019,0,0.0205718,"C march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entit"
D11-1141,W10-0713,0,0.605816,"Missing"
D11-1141,P05-1045,0,0.09483,"ue to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared across its mentions. 5 Conclusions We have demonstrated that existing tools for POS tagging, Chunking and Named Entity Recognition perform quite poorly when applied to Tweets. To address this challenge we have annotated tweets and built tools trained on unlabeled, in-domain and outof-domain data, showing substantial improvement"
D11-1141,W02-2010,0,0.0537015,"Missing"
D11-1141,P11-2008,0,0.750872,"Missing"
D11-1141,W11-0704,0,0.045433,"yles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in segmenting Named Entiti"
D11-1141,P11-1038,0,0.693265,"tter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in se"
D11-1141,C08-1056,0,0.0175354,"Missing"
D11-1141,N10-1087,0,0.0117909,"y annotated data to be very beneficial for named entity segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’"
D11-1141,P11-1037,0,0.762242,"End to End System: Finally we present the end to end performance on segmentation and classification (T- NER) in Table 12. We observe that T- NER again outperforms co-training. Moreover, comparing against the Stanford Named Entity Recognizer on the 3 MUC types, T- NER doubles F1 score. 4 Related Work There has been relatively little previous work on building NLP tools for Twitter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Ama"
D11-1141,D10-1035,0,0.0451038,"ty segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared ac"
D11-1141,H05-1056,0,0.0177837,"Missing"
D11-1141,P09-1113,0,0.173591,"Missing"
D11-1141,D09-1026,0,0.642196,"Missing"
D11-1141,D10-1037,0,0.0260391,"ich we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entity, we need a larger annotated dataset to effectively learn a model of named entities. We therefore use a randomly sampled set of 2,400 tweets for NER. All experiments (Tables 6, 8-10) report results using 4-fold cross validation. 3.1 Segmenting Named Entities Because capitalization in Twitter is less informative than news, in-domain data is needed to train models which rely less heavily on capitalization, and also are able to utilize features provided by T- CAP. We exhaustively annotated our s"
D11-1141,N03-1028,0,0.0319403,"Missing"
D11-1141,N10-1009,0,0.0418606,"Missing"
D11-1141,P10-1149,0,0.0787574,"Missing"
D11-1141,W00-0726,0,0.037791,"Missing"
D11-1141,N03-1033,0,0.0846633,"Missing"
D11-1141,P10-1040,0,0.504898,"Missing"
D11-1141,P95-1026,0,0.0853228,"Missing"
D11-1141,J93-2004,0,\N,Missing
D12-1048,H05-1091,0,0.283064,"ng approaches in traditional information extraction, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First, and most importantly, these previous systems learn an extractor for each relation of interest, whereas OLLIE is an open extractor. OLLIE’s strength is its ability to generalize from one relation to many other relations that are expressed in similar forms. This happens both via syntactic generalization and type generalization of relation words (sections 3.2.1 and 3.2.2). This capability is essential as many relations in the test set are not even seen in the tra"
D12-1048,de-marneffe-etal-2006-generating,0,0.0480838,"Missing"
D12-1048,D11-1142,1,0.841584,") has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOEparse (Wu and Weld, 2010) suffer from two k"
D12-1048,P10-1030,0,0.0132882,"E V ERB and 1.9 times the AUC of WOEparse . 1 Figure 1: OLLIE (O) has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni e"
D12-1048,P11-1055,0,0.356847,"find this quality to be satisfactory for our needs of learning general patterns. Bootstrapped data has been previously used to generate positive training data for IE (Hoffmann et al., 2010; Mintz et al., 2009). However, previous systems retrieved sentences that only matched the two arguments, which is error-prone, since multiple relations can hold between a pair of entities (e.g., Bill Gates is the CEO of, a co-founder of, and has a high stake in Microsoft). Alternatively, researchers have developed sophisticated probabilistic models to alleviate the effect of noisy data (Riedel et al., 2010; Hoffmann et al., 2011). In our case, by enforcing that a sentence additionally contains some syntactic form of the relation content words, our bootstrapping set is naturally much cleaner. Moreover, this form of bootstrapping is better suited for Open IE’s needs, as we will use this data to generalize to other unseen relations. Since the relation words in the sentence and seed match, we can learn general pattern templates that may apply to other relations too. We discuss this process next. 3.2 Open Pattern Learning OLLIE’s next step is to learn general patterns that encode various ways of expressing relations. OL -"
D12-1048,C08-1050,0,0.0165868,"apped training set. Section 4 discusses the context analysis component, which is based on supervised training with linguistic and lexical features. Section 5 compares OLLIE with R E V ERB and WOE parse on a dataset from three domains: News, Wikipedia, and a Biology textbook. We find that OLLIE obtains 2.7 times the area in precision-yield curves (AUC) as R E V ERB and 1.9 times the AUC as WOEparse . Moreover, for specific relations commonly mediated by nouns (e.g., ‘is the president of’) OLLIE obtains two order of magnitude higher yield. We also compare OLLIE to a state-of-the-art SRL system (Johansson and Nugues, 2008) on an IE-related end task and find that they both have comparable performance at argument identification and have complimentary strengths in sentence analysis. In Section 6 we discuss related work on patternbased relation extraction. 2 Background Open IE systems extract tuples consisting of argument phrases from the input sentence and a phrase 1 Available for download at http://openie.cs.washington.edu 524 from the sentence that expresses a relation between the arguments, in the format (arg1; rel; arg2). This is done without a pre-specified set of relations and with no domain-specific knowled"
D12-1048,kingsbury-palmer-2002-treebank,0,0.0863481,"g1: Macromolecules Macromolecules Macromolecules types of RNA types of RNA arg2: phloem proteins types of RNA sieve tubes plasmodesmata relation term translocated include include enter enter We find an average of 4.0 verb-mediated relations and 0.3 noun-mediated relations per sentence. Evaluating OLLIE against this gold standard helps to answer the question of absolute recall: what percentage of binary relations expressed in a sentence can our systems identify. For comparison, we use a state-of-the-art SRL system from Lund University (Johansson and Nugues, 2008), which is trained on PropBank (Martha and Palmer, 2002) for its verb-frames and NomBank (Meyers et al., 2004) for noun-frames. The PropBank version of the system won the very competitive 2008 CONLL SRL evaluation. We conduct this experiment by manually compar531 Verb relations Noun relations All relations LUND OLLIE 0.58 (0.69) 0.07 (0.33) 0.54 (0.67) 0.49 (0.55) 0.13 (0.13) 0.47 (0.52) union 0.71 (0.83) 0.20 (0.33) 0.67 (0.80) Figure 9: Recall of LUND and OLLIE on binary relations. In parentheses is recall with oracle co-reference. Both systems identify approximately half of all argument pairs, but have lower recall on noun-mediated relations. in"
D12-1048,meyers-etal-2004-annotating,0,0.00488954,"RNA types of RNA arg2: phloem proteins types of RNA sieve tubes plasmodesmata relation term translocated include include enter enter We find an average of 4.0 verb-mediated relations and 0.3 noun-mediated relations per sentence. Evaluating OLLIE against this gold standard helps to answer the question of absolute recall: what percentage of binary relations expressed in a sentence can our systems identify. For comparison, we use a state-of-the-art SRL system from Lund University (Johansson and Nugues, 2008), which is trained on PropBank (Martha and Palmer, 2002) for its verb-frames and NomBank (Meyers et al., 2004) for noun-frames. The PropBank version of the system won the very competitive 2008 CONLL SRL evaluation. We conduct this experiment by manually compar531 Verb relations Noun relations All relations LUND OLLIE 0.58 (0.69) 0.07 (0.33) 0.54 (0.67) 0.49 (0.55) 0.13 (0.13) 0.47 (0.52) union 0.71 (0.83) 0.20 (0.33) 0.67 (0.80) Figure 9: Recall of LUND and OLLIE on binary relations. In parentheses is recall with oracle co-reference. Both systems identify approximately half of all argument pairs, but have lower recall on noun-mediated relations. ing the outputs of LUND and OLLIE against the gold stand"
D12-1048,P09-1113,0,0.325368,"the AUC of WOEparse . 1 Figure 1: OLLIE (O) has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOE"
D12-1048,W04-2407,0,0.0114463,"rce additional dependency restrictions on the sentences. We only allow sentences where the content words from arguments and relation can be linked to each other via a linear path of size four in the dependency parse. To implement this restriction, we only use the subset of content words that are headwords in the parse tree. In the above sentence ‘Ireland’, ‘Boyle’ and ‘born’ connect via a dependency path of length six, and hence this sentence is rejected from the training set. This reduces our set to 4 million (seed tuple, sentence) pairs. In our implementation, we use Malt Dependency Parser (Nivre and Nilsson, 2004) for dependency parsing, since it is fast and hence, easily applicable to a large corpus of sentences. We post-process the parses using Stanford’s CCprocessed algorithm, which compacts the parse structure for easier extraction (de Marneffe et al., 2006). We randomly sampled 100 sentences from our bootstrapping set and found that 90 of them satisfy our bootstrapping hypothesis (64 without dependency constraints). We find this quality to be satisfactory for our needs of learning general patterns. Bootstrapped data has been previously used to generate positive training data for IE (Hoffmann et al"
D12-1048,P06-1015,0,0.0131284,"First, nouns, although less frequently mediating relations, are much harder and both systems are failing significantly on those – OLLIE is somewhat better. Two, neither systems dominates the other; in fact, recall is increased significantly by a union of the two. Three, and probably most importantly, significant information is still being missed by both systems, and more research is warranted. 6 Related Work There is a long history of bootstrapping and pattern learning approaches in traditional information extraction, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First,"
D12-1048,P10-1044,1,0.11327,"7 shows the results of this experiment on our dataset from three domains. As the curves show, OLLIE was correct to add lexical/semantic constraints to these patterns – precision is quite low without the restrictions. This matches our intuition, since these are not completely general patterns and generalizing to all unseen relations results in a large number of errors. OLLIE[lex] performs well though at lower yield. The type generalization helps the yield somewhat, without hurting the precision. We believe that a more data-driven type generalization that uses distributional similarity (e.g., (Ritter et al., 2010)) may help much more. Also, notice that overall precision numbers are lower, since these are the more difficult relations to extract reliably. We conclude that lexical/semantic restrictions are valuable for good performance of OLLIE. We also compare our full system to a version that does not use the context analysis of Section 4. Figure 8 compares OLLIE to a version (OLLIE[pat]) that does not add the AttributedTo and ClausalModifier fields, and, instead of context-sensitive confidence function, uses the pattern frequency in the training 530 Figure 8: Context analysis increases precision, raisi"
D12-1048,N06-1039,0,0.0103107,"ng set and high rule precision while still allowing morphological variants that cover noun-mediated relations. A third difference is in the scale of the training – R E V ERB yields millions of training seeds, where previous systems had orders of magnitude less. This enables OLLIE to learn patterns with greater coverage. The closest to our work is the pattern learning based open extractor WOEparse . Section 3.4 details the differences between the two extractors. Another extractor, StatSnowBall (Zhu et al., 2009), has an Open IE version, which learns general but shallow patterns. Preemptive IE (Shinyama and Sekine, 2006) is a paradigm related to Open IE that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters. The clustering steps make it difficult for it to scale to large corpora. 7 Conclusions Our work describes OLLIE, a novel Open IE extractor that makes two significant advances over the existing Open IE systems. First, it expands the syntactic scope of Open IE systems by identifying relationships mediated by nouns and adjectives. Our experiments found that for some relations this increases the number of correct extract"
D12-1048,P10-1013,0,0.931227,"on et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOEparse (Wu and Weld, 2010) suffer from two key drawbacks. Firstly, they handle a limited subset of sentence constructions for expressing relationships. Both extract only relations that are mediated by verbs, and R E V ERB further restricts this to a subset of verbal patterns. This misses important information mediated via other syntactic entities such as nouns and adjectives, as well as a wider range of verbal structures (examples #1-3 in Figure 1). 523 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 523–534, Jeju Island,"
D12-1048,P05-1052,0,0.00403611,"on, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First, and most importantly, these previous systems learn an extractor for each relation of interest, whereas OLLIE is an open extractor. OLLIE’s strength is its ability to generalize from one relation to many other relations that are expressed in similar forms. This happens both via syntactic generalization and type generalization of relation words (sections 3.2.1 and 3.2.2). This capability is essential as many relations in the test set are not even seen in the training set – in early exper532 iments we found tha"
D12-1082,E06-1002,0,0.27264,"Missing"
D12-1082,D07-1074,0,0.121924,"“EMNLP” “ICAPS” present absent Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Introduction A key challenge in machine reading (Etzioni et al., 2006) is to identify the entities mentioned in text, and associate them with appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1"
D12-1082,C10-1032,0,0.0239483,"not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by R E V ERB (Fader et al., 2011).2 R E V ERB already filters out relative pronouns, WHO-adverbs, and existential “there” nou"
D12-1082,D11-1142,1,0.0433304,"s, but general purpose linking systems all use Wikipedia because of its broad 894 general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed when using entity linking systems is that despite containing over 3 million entities, Wikipedia does not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic"
D12-1082,D09-1099,0,0.00833072,"instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011)"
D12-1082,D11-1011,0,0.342843,"a article, and also that it occurs with textual relations such as “has already announced” and “has released updates for.” For each Wikipedialinked entity in L, we further look up its exact set of Freebase types.4 From U we obtain the set of textual relations that each e ∈ E is in the domain of. We now have a large set of class-labeled instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of o"
D12-1082,N04-1041,0,0.0146658,"y shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding th"
D12-1082,P11-3004,0,0.0115974,"nt number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by R E V ERB (Fader et al., 2011).2 R E V ERB already filters out relative pronouns, WHO-adverbs, and existential “there” noun phrases that"
D12-1082,C10-1105,0,0.0149933,"approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding the textual relations that e is in the domain of, (2) 4 data available at http://download.freebase.com/wex Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase “Sun Microsystems.” We predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) find"
D12-1082,W09-1119,0,0.0343828,"Missing"
D12-1082,P11-1138,0,0.0209962,"h appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1 (Wang et al., 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: G"
D12-1082,sekine-nobata-2004-definition,0,0.00846455,"ion Named Entity Recognition (NER) is the task of identifying named entities in text. A key difference between our final goals and NER is that in the con2 available at http://reverb.cs.washington.edu text of entity linking and Wikipedia, there are many more entities than just the named entities. For example, “apple juice” and “television” are Wikipedia entities (with Wikipedia articles), but are not traditional named entities. Still, as named entities do comprise a sizable portion of our unlinkable noun phrases, we compare against a NER baseline in our entity detection step. Fine-grained NER (Sekine and Nobata, 2004; Lee et al., 2007) has studied scaling NER to up to 200 semantic types. This differs from our semantic typing of unlinked entities because our approach assumes access to corpora-level relationships between a large set of linked entities (with semantic types) and the unlinked entities. As a result we are able to propagate 1,339 Freebase semantic types from the linked entities to the unlinked entities, which is substantially more types than fine-grained NER. 2.3 Extracting Entity Sets There is a line of research in using Web extraction (Etzioni et al., 2005) and entity set expansion (Pantel et"
D12-1082,P10-1149,0,0.0143317,"on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER"
D12-1082,D08-1061,0,0.0653835,"Missing"
D12-1082,buscaldi-rosso-2006-mining,0,\N,Missing
D12-1082,D09-1098,0,\N,Missing
D13-1178,W12-3019,1,0.843852,"ect. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sparisty and to improve coherence. We developed a page rank based schema induction algorithm which results in more coherent schemas with several actors. Unlike Chambers’ approach this method does not require explicit parameter tuning for controlling the number of actors. While our event schemas are close to being templates (because of associated types, and actor clustering), they do not have associated extractors. Our future work will focus on buildin"
D13-1178,P08-1090,0,0.659089,"Missing"
D13-1178,P09-1068,0,0.816405,"oni}@cs.washington.edu Actor Rel Actor A1:<person> failed A2:test A1:<person> was suspended for A3:<time period> A1:<person> used A4:<substance, drug> A1:<person> was suspended for A5:<game, activity> A1:<person> was in A6:<location> A1:<person> was suspended by A7:<org, person> Actor Instances: A1: {Murray, Morgan, Governor Bush, Martin, Nelson} A2: {test} A3: {season, year, week, month, night} A4: {cocaine, drug, gasoline, vodka, sedative} A5: {violation, game, abuse, misfeasance, riding} A6: {desert, Simsbury, Albany, Damascus, Akron} A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush} Abstract Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. Table 1: An event schema produced by our system, represented as a set of (Actor, Rel, Actor) triples, and a set of instances for each actor A1, A2, e"
D13-1178,chambers-jurafsky-2010-database,0,0.0326584,"Missing"
D13-1178,P11-1098,0,0.0260073,"pend than, commissioner). 6 Related Work Prior work by Chambers and Jurafsky (2008; 2009; 2010) showed that event sequences (narrative chains) mined from text can be used to induce event schemas in a domain-independent fashion. However, our manual evaluation of their output showed key limitations which may limit applicability. As pointed out earlier, a major weakness in Chambers’ approach is the pair-wise representation of subject-verb and verb-object. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sp"
D13-1178,N13-1104,0,0.0680972,"rom text can be used to induce event schemas in a domain-independent fashion. However, our manual evaluation of their output showed key limitations which may limit applicability. As pointed out earlier, a major weakness in Chambers’ approach is the pair-wise representation of subject-verb and verb-object. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sparisty and to improve coherence. We developed a page rank based schema induction algorithm which results in more coherent schemas with several"
D13-1178,doddington-etal-2004-automatic,0,0.0167408,"event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead of labeled roles these schemas have a set of relations and actors that serve as arguments.1 Their system is fully automatic, domain-independent, and scales to large text corpora. However, we identify several limitations in the schemas produced by their system.2 Their schemas 1 In the rest of this paper we use event schemas to refer to these"
D13-1178,P05-1045,0,0.0057794,"resented as stemmed head nouns, and we also record semantic types of the arguments. We selected 29 semantic types from WordNet, examining the set of instances on a small development set to ensure that the types are useful, but not overly specific. The set of types are: person, organization, location, time unit, number, amount, group, business, executive, leader, effect, activity, game, sport, device, equipment, structure, building, substance, nutrient, drug, illness, organ, animal, bird, fish, art, book, and publication. To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005)5 , and also look up the argument in WordNet 2.1 and record 4 Available at: http://knowitall.github.io/ ollie/ 5 We used the system downloaded from: http://nlp. stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it. 1723 the first three senses if they map to our target semantic types. We use regular expressions to recognize dates and numeric expressions, and map personal pronouns to <person>. We associate all types found by this mechanism with each argument. The tuples in the example above are normalized to the following: 1. 2. 3. 4. 5. 6. 7. 8. 9. ... (He,"
D13-1178,W13-1202,0,0.0220333,"Missing"
D13-1178,W11-1913,0,0.0910754,"rs’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community. 1 Introduction Event schemas (also known as templates or frames) have been widely used in information extraction. An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead of labeled roles these schemas have a set of relations and actors that serve as"
D13-1178,J13-4004,0,0.0138012,"er relational tuples in a document. For clarity, we show the unstemmed version. Top tuples related to (<person>, convicted of, murder) 1. (<person>, convicted in, <time unit>) 2. (<person>, sentenced to, death) 3. (<person>, sentenced to, year) 4. (<person>, convicted in, <location>) 5. (<person>, sentenced to, life) 6. (<person>, convicted in, <person>) 7. (<person>, convicted after, trial) 8. (<person>, sent to, prison) pair is equal if they are from the same token sequence in the source sentence or one argument is a co-referent mention of the other. We use the Stanford Co-reference system (Lee et al., 2013)6 to detect co-referring mentions. There are four possible equalities depending on the specific pair of arguments in the tuples are the same, shown as E11, E12, E21 and E22 in Figure 1. For example, the E21 column has counts for the number of times the Arg2 of T1 was determined to be the same as the Arg1 of T2. Implementation and Query Language: We populated the Rel-grams database using OLLIE extractions from a set of 1.8 Million New York Times articles drawn from the Gigaword corpus. The database consisted of approximately 320K tuples that have frequency ≥ 3 and 1.1M entries in the bigram tab"
D13-1178,D12-1048,1,0.27349,"at do not make sense in the real world. For example, the assertions “fire caused virus” and “bacteria burned AIDS” are implicit in Table 2. Another limitation in schemas Chambers released is that they restrict schemas to two actors, which can result in combining different actors. Table 4 shows an example of combining perpeterators and victims into a single actor. 1.1 Contributions We present an event schema induction algorithm that overcomes these weaknesses. Our basic representation is triples of the form (Arg1, Relation, Arg2), extracted from a text corpus using Open Information Extraction (Mausam et al., 2012). The use of triples aids in agreement between subject and object of a relation. The use of Open IE leads to more expressive relation phrases (e.g., with prepositions). We also assign semantic types to arguments, both to alleviate data sparsity and to produce coherent actors for our schemas. Table 1 shows an event schema generated by our system. It has six relations and seven actors. The schema makes several related assertions about a person using a drug, failing a test, and getting suspended. The main actors in the schema include the person who failed the test, the drug used, and the agent th"
D13-1178,D09-1016,0,0.0611554,"all Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community. 1 Introduction Event schemas (also known as templates or frames) have been widely used in information extraction. An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead"
D13-1178,D08-1027,0,0.0407593,"Missing"
D13-1178,E09-1005,0,\N,Missing
D13-1178,P08-1000,0,\N,Missing
N13-1136,P11-1051,0,0.0250394,"ght CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while our approach performs joint selection and ordering. Discourse models have also been used for evaluating summary quality (Barzilay and Lapata, 2008; Louis and Nenkova, 2009; Pitler et al., 2010). Finally, there is work on generating coherent summaries in specific domains, such as scientific articles (Saggion and Lapalme, 2002; Abu-Jbara and Radev, 2011) using domain-specific cues like citations. In contrast, our work generates summaries without any domain-specific knowledge. Other research has focused on identifying coherent threads of documents rather than sentences (Shahaf and Guestrin, 2010). 3 Discourse Graph As described in Section 1, our goal is to identify pairwise ordering constraints over a set of input sentences. These constraints specify a multi-document discourse graph, which is used by G-F LOW to evaluate the coherence of a candidate summary. In this graph G, each vertex is a sentence and an edge from si to sj indicates that sj"
N13-1136,D10-1047,0,0.020387,"Missing"
N13-1136,W97-0703,0,0.0626535,"reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an"
N13-1136,J08-1001,0,0.657666,"doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joint model. 1 doc2: Hamas clai"
N13-1136,H01-1065,0,0.0217437,"nce as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespec"
N13-1136,P10-1084,0,0.0265154,"Missing"
N13-1136,chang-manning-2012-sutime,0,0.00654009,"m the New York Times and Reuters, and filter out candidate pairs scoring below a threshold identified over a small training set. We construct edges in the ADG between pairs of sentences containing these verb to noun mappings. To our knowledge, we are the first to use deverbal nouns for summarization. 3.2 Event/Entity Continuation Our second indicator is related to lexical chains (Barzilay and Lapata, 2008). We add an edge in the ADG from a sentence si to sj if they contain the same event or entity and the timestamp of si is less than or equal to the timestamp of sj (timestamps generated with (Chang and Manning, 2012)). 3.3 Discourse Markers We use 36 explicit discourse markers (e.g., ‘but’, ‘however’, ‘moreover’) to identify edges between two adjacent sentences of a document (Marcu and Echihabi, 2002). This indicator lets us learn an edge from s4 to s5 below: s4 Arafat condemned the bombing. s5 However, Netanyahu suspended peace talks. 3.4 Inferred Edges We exploit the redundancy of information in MDS documents to infer edges to related sentences. An edge (s, s00 ) can be inferred if there is an existing edge (s, s0 ) and s0 and s00 express similar information. As an example, the edge (s6 , s7 ) can be in"
N13-1136,J86-3001,0,0.646458,"out regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure The"
N13-1136,N09-1041,0,0.376363,": Suspension of peace accord due to bombing doc4: Mubarak urges peace accord doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering c"
N13-1136,P03-1069,0,0.0209978,"peace accord doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joi"
N13-1136,W11-1902,0,0.0178603,"Missing"
N13-1136,I11-1118,0,0.126842,"Missing"
N13-1136,D11-1105,0,0.0661184,"Missing"
N13-1136,P11-1052,0,0.215437,"se colony occupied by Indonesia since 1975. • Indonesia invaded East Timor in 1975 and annexed it the following year. • Thailand won host rights for the quadrennial games in 1995, but setbacks in preparations led officials of the Olympic Council of Asia late last year to threaten to move the games to another country. • Thailand showed its nearly complete facilities for the Asian Games to a tough jury Thursday - the heads of the organizing committees from the 43 nations competing in the December event. Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and sentence orderer (Li et al., 2011a), and by G-F LOW. graph to estimate coherence of a candidate summary. Second, G-F LOW introduces a novel methodology for joint sentence selection and ordering (Section 4). It casts MDS as a constraint optimization problem where salience and coherence are soft constraints, and redundancy and summary length are hard constraints. Because this optimization problem is NPhard, G-F LOW uses local search to approximate it. We report on a Mechanical Turk evaluation that directly compares G-F LOW to state-of-the-art MDS systems. Using DUC’04 as our test set, we co"
N13-1136,W04-1013,0,0.0207662,"d Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et"
N13-1136,W10-4310,0,0.0120046,"tion, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an instance of summariza"
N13-1136,W01-0100,0,0.335287,"ive of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation"
N13-1136,P02-1047,0,0.0191987,"ontaining these verb to noun mappings. To our knowledge, we are the first to use deverbal nouns for summarization. 3.2 Event/Entity Continuation Our second indicator is related to lexical chains (Barzilay and Lapata, 2008). We add an edge in the ADG from a sentence si to sj if they contain the same event or entity and the timestamp of si is less than or equal to the timestamp of sj (timestamps generated with (Chang and Manning, 2012)). 3.3 Discourse Markers We use 36 explicit discourse markers (e.g., ‘but’, ‘however’, ‘moreover’) to identify edges between two adjacent sentences of a document (Marcu and Echihabi, 2002). This indicator lets us learn an edge from s4 to s5 below: s4 Arafat condemned the bombing. s5 However, Netanyahu suspended peace talks. 3.4 Inferred Edges We exploit the redundancy of information in MDS documents to infer edges to related sentences. An edge (s, s00 ) can be inferred if there is an existing edge (s, s0 ) and s0 and s00 express similar information. As an example, the edge (s6 , s7 ) can be inferred based on edge (s4 , s5 ): s6 Arafat condemned the attack. s7 Netanyahu has suspended the talks. To infer edges we need an algorithm to identify sentences expressing similar informat"
N13-1136,W97-0713,0,0.102958,"ry, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST"
N13-1136,D12-1048,1,0.239931,"he complete list of features and learned weights is in Table 2. The classifier finds a sentence more salient if it mentions nouns or verbs that are present in more sentences across the documents. The highest ranked features are the last three – number of other sentences that mention a noun or a verb in the given sentence. We use the same procedure as in deverbal nouns for detecting verb mentions that appear as nouns in other sentences (Section 3.1). 4.3 Redundancy We also wish to avoid redundancy. G-F LOW first processes each sentence with a state-of-the-art Open Information extractor O LLIE (Mausam et al., 2012), which converts a sentence into its component relational tuples of the form (arg1, relational phrase, arg2).3 For example, it finds (Militants, bombed, a marketplace) as a tuple from sentence s12 . Two sentences will express redundant information if they both contain the same or synonymous component fact(s). Unfortunately, detecting synonymy even at relational tuple level is very hard. G-F LOW approximates this synonymy by considering two relational tuples synonymous if the relation phrases contain verbs that are synonyms of each other, have at least one synonymous argument, and are timestamp"
N13-1136,C04-1108,0,0.0113353,"erata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succee"
N13-1136,P10-1056,0,0.0289057,"Missing"
N13-1136,prasad-etal-2008-penn,0,0.31003,"selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge an"
N13-1136,C10-1101,0,0.0448864,"Missing"
N13-1136,J98-3005,0,0.0243244,"ry, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an instance of summarization based on lightweight CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while"
N13-1136,J02-4005,0,0.0424646,"arization based on lightweight CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while our approach performs joint selection and ordering. Discourse models have also been used for evaluating summary quality (Barzilay and Lapata, 2008; Louis and Nenkova, 2009; Pitler et al., 2010). Finally, there is work on generating coherent summaries in specific domains, such as scientific articles (Saggion and Lapalme, 2002; Abu-Jbara and Radev, 2011) using domain-specific cues like citations. In contrast, our work generates summaries without any domain-specific knowledge. Other research has focused on identifying coherent threads of documents rather than sentences (Shahaf and Guestrin, 2010). 3 Discourse Graph As described in Section 1, our goal is to identify pairwise ordering constraints over a set of input sentences. These constraints specify a multi-document discourse graph, which is used by G-F LOW to evaluate the coherence of a candidate summary. In this graph G, each vertex is a sentence and an edge from"
N13-1136,C10-1111,0,0.0779683,"of single documents. There is some preliminary work on the use of manually-created discourse models in MDS. Our approach is fully automated. 2.1 Subset Selection in MDS Most extractive summarization research aims to increase the coverage of concepts and entities while reducing redundancy. Approaches include the use of maximum marginal relevance (Carbonell and Goldstein, 1998), centroid-based summarization (Saggion and Gaizauskas, 2004; Radev et al., 2004), covering weighted scores of concepts (Takamura and Okumura, 2009; Qazvinian et al., 2010), formulation as minimum dominating set problem (Shen and Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004),"
N13-1136,E09-1089,0,0.0316897,"riven summarization (Section 2.3). There is prior work in this area, but primarily for summarization of single documents. There is some preliminary work on the use of manually-created discourse models in MDS. Our approach is fully automated. 2.1 Subset Selection in MDS Most extractive summarization research aims to increase the coverage of concepts and entities while reducing redundancy. Approaches include the use of maximum marginal relevance (Carbonell and Goldstein, 1998), centroid-based summarization (Saggion and Gaizauskas, 2004; Radev et al., 2004), covering weighted scores of concepts (Takamura and Okumura, 2009; Qazvinian et al., 2010), formulation as minimum dominating set problem (Shen and Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the se"
N13-1136,J05-2005,0,0.0102645,"eads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang"
N13-1136,C10-2170,0,0.0148097,"ry. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coh"
N13-1136,D09-1032,0,\N,Missing
N16-1009,P14-1085,1,0.829479,"(i.e., comparison tables) against reading the original WikiTravel articles. For fairness we only use the ‘places to see’ sections from WikiTravel, since that was the raw text used in generating comparison tables in the first place. Since the comparisons are generated automatically, people may not find them understandable, or there may be missing valuable information. We test this in a human subject evaluation. We adapt the 76 evaluation methodology developed recently for contrasting multiple ways of presenting information and testing the overall learning of the subjects (Shahaf et al., 2012; Christensen et al., 2014). The evaluation is divided into two parts. In the first part the workers are given a limited time to read the information provided (articles or comparison tables) for an entity-pair. They are then asked to write a short 150300 word summary contrasting different aspects of the two entities. Each user writes two summaries, one based on articles and the other based on our table. Our study pairs two users such that if user1 read the articles for city pair 1 and the table for city pair 2, their partner user will see the reverse. The workers were additionally asked which knowledge source they prefe"
N16-1009,P15-1077,0,0.0152559,"ichlet prior is equivalent to pLSA 73 In response, we exploit the availability of pretrained word vectors as a source of background semantic knowledge for every phrase, and generalize the pLSA model to Gaussian pLSA (G-pLSA). We construct a vector representation for each descriptive phrase by averaging the word-vectors of individual words in a phrase (Mikolov et al., 2013)4 . Thus, this model is pLSA with each topic-word distribution represented as a Gaussian distribution over descriptive phrases in the embedding space. This model is also similar to the recently introduced Gaussian LDA model (Das et al., 2015), but without LDA’s Dirichlet priors as discussed above. Gaussian pLSA has several advantages for our task. First, it can meaningfully learn topics only for the entities being compared, instead of needing to learn a global topic model over the whole corpus. Second, due to additional context from word vectors, the topics are expected to be much more coherent compared to traditional topic models for cases when the underlying corpus is small, as in our case. Finally, in our model the vectors are generated from a Gaussian distribution and that helps capture the theme of the cluster directly by ena"
N16-1009,N09-2029,0,0.0166829,"set of museums and gardens to visit, while palaces and courtyards are only in Granada. Granada’s art and architecture are more ornamental, whereas New York’s might be more contemporary. contributed comparisons that have been categorized based on the nature of the entities being compared. These are manually curated and therefore do not scale to the quadratic number of entity pairs. Perhaps the most closely related work to ours is the field of contrastive opinion mining and summarization (Kim et al., 2011; Liu and Zhang, 2012). Examples include extraction of contrastive sentiments on a product (Lerman and McDonald, 2009) and summarization of opinionated political articles (Paul et al., 2010). Contrastive opinion mining extracts contrasting view points about a single entity or event instead of comparing multiple ones. A recent preliminary study extends this for comparing reviews of two products (Sipos and Joachims, 2013). It uses a supervised method for learning sentence alignments per product-type, and does not organize various opinions for an entity via clustering. Other related work includes comparative text mining tasks where document collections are analyzed to extract shared topics or themes (Zhai et al."
N16-1009,D10-1007,0,0.0218413,"anada. Granada’s art and architecture are more ornamental, whereas New York’s might be more contemporary. contributed comparisons that have been categorized based on the nature of the entities being compared. These are manually curated and therefore do not scale to the quadratic number of entity pairs. Perhaps the most closely related work to ours is the field of contrastive opinion mining and summarization (Kim et al., 2011; Liu and Zhang, 2012). Examples include extraction of contrastive sentiments on a product (Lerman and McDonald, 2009) and summarization of opinionated political articles (Paul et al., 2010). Contrastive opinion mining extracts contrasting view points about a single entity or event instead of comparing multiple ones. A recent preliminary study extends this for comparing reviews of two products (Sipos and Joachims, 2013). It uses a supervised method for learning sentence alignments per product-type, and does not organize various opinions for an entity via clustering. Other related work includes comparative text mining tasks where document collections are analyzed to extract shared topics or themes (Zhai et al., 2004). Since such methods only identify latent topics for the full doc"
N16-1009,D09-1098,0,\N,Missing
N16-1011,D14-1059,0,0.0196912,"variety of inaccurate rules (see rules #4, and #5 in Figure 1). 1 http://u.cs.biu.ac.il/˜nlp/resources/downloads/predicativeentailment-rules-learned-using-local-and-global-algorithms 87 Our early experiments with C LEAN revealed its precision to be about 0.49, not enough to be useful in practice, especially for human-facing applications. Similar to our paper, some past works have used alternative sources of knowledge. Weisman et al. (2012) study inference between verbs (e.g., hstartle → surprisei), but they get low (0.4) precision. Wordnet corpus to generate inference rules for natural logic (Angeli and Manning, 2014) improved noun-based inference. But, they recognize relation entailments as a key missing piece. Recently, natural logic semantics is added to a paraphrase corpus (PPDB2.0). Many of their features, e.g., lexical/orthographic, multilingual translation based, are complimentary to our method. We test our K GLR algorithm on C LEAN and entailment/paraphrase subset of PPDB2.0 (which we call PPDBe ). 3 Knowledge-Guided Linguistic Rewrites (K GLR) Given a rule h(X, r1 , Y) → (X, r2 , Y)i or h(X, r1 , Y) → (Y, r2 , X)i we present K GLR, a series of rewrites of relation phrase r1 to prove r2 (examples i"
N16-1011,P12-1013,0,0.154573,"e background knowledge – the set of inference rules that enable the inference (Clark et al., 2014). The paper focuses on generating a high precision subset of inference rules over Open Information Extraction (OpenIE) (Etzioni et al., 2011) relation phrases (see Fig 1). OpenIE systems generate a schema-free KB where entities and relations are represented via normalized but not disambiguated textual strings. Such OpenIE KBs scale to the Web. Most existing large-scale corpora of inference rules are generated using distributional similarity, like argument-pair overlap (Schoenmackers et al., 2010; Berant et al., 2012), but often eschew any linguistic or compositional insights. Our early analysis revealed that such inference rules have very low precision, not enough to be useful for many real tasks. For human-facing applications (such as IE-based demos), high precision is critical. Inference rules have a multiplicative impact, since one poor rule could potentially generate many bad KB facts. Contributions: We investigate the hypothesis that “knowledge-guided linguistic rewrites can provide independent verification for statistically-generated Open IE inference rules.” Our system K GLR’s rewrites exploit the"
N16-1011,D15-1113,0,0.0195058,"hel et al., 2015a), natural logic inference (MacCartney and Manning, 2007) and graphical models (Schoenmackers et al., 2008; Raghavan et al., 2012). Most of these need (or benefit from) a background knowledge of inference rules, including matrix completion (Rockt¨aschel et al., 2015b). Inference rules are predominantly generated via extended distributional similarity – two phrases having a high degree of argument overlap are similar, and thus candidates for a unidirectional or a bidirectional inference rule. Methods vary on the base representation, e.g., KB relations (Gal´arraga et al., 2013; Grycner et al., 2015), Open IE relation phrases (Schoenmackers et al., 2010), syntacticontological-lexical (SOL) patterns (Nakashole et al., 2012), and dependency paths (Lin and Pantel, 2001). An enhancement is global transitivity (TNCF algorithm) for improving recall (Berant et al., 2012). The highest precision setting of TNCF (λ = 0.1) was released as a corpus (informally called C LEAN) of Open IE inference rules.1 Distributional similarity approaches have two fundamental limitations. First, they miss obvious commonsense facts, e.g., h(X, married, Y) → (X, knows, Y)i – text will rarely say that a couple know eac"
N16-1011,D11-1049,0,0.0305097,"ick et al., 2015) and can be seen as additional annotation on existing inference rules. The verified rules are 27 to 33 points more accurate than the original corpora and still retain a substantial recall. The precision of inferred knowledge also has a precision boost of over 29 points. We release our K GLR implementation, its annotations on two popular rule corpora along with gold set used for evaluation and the annotation guidelines for further use (available at https://github.com/dair-iitd/kglr.git). 2 Related work Methods for inference over text include random walks over knowledge graphs (Lao et al., 2011), matrix completion (Riedel et al., 2013), deep neural networks (Socher et al., 2013; Rockt¨aschel et al., 2015a), natural logic inference (MacCartney and Manning, 2007) and graphical models (Schoenmackers et al., 2008; Raghavan et al., 2012). Most of these need (or benefit from) a background knowledge of inference rules, including matrix completion (Rockt¨aschel et al., 2015b). Inference rules are predominantly generated via extended distributional similarity – two phrases having a high degree of argument overlap are similar, and thus candidates for a unidirectional or a bidirectional inferen"
N16-1011,W07-1431,0,0.0421192,"l corpora and still retain a substantial recall. The precision of inferred knowledge also has a precision boost of over 29 points. We release our K GLR implementation, its annotations on two popular rule corpora along with gold set used for evaluation and the annotation guidelines for further use (available at https://github.com/dair-iitd/kglr.git). 2 Related work Methods for inference over text include random walks over knowledge graphs (Lao et al., 2011), matrix completion (Riedel et al., 2013), deep neural networks (Socher et al., 2013; Rockt¨aschel et al., 2015a), natural logic inference (MacCartney and Manning, 2007) and graphical models (Schoenmackers et al., 2008; Raghavan et al., 2012). Most of these need (or benefit from) a background knowledge of inference rules, including matrix completion (Rockt¨aschel et al., 2015b). Inference rules are predominantly generated via extended distributional similarity – two phrases having a high degree of argument overlap are similar, and thus candidates for a unidirectional or a bidirectional inference rule. Methods vary on the base representation, e.g., KB relations (Gal´arraga et al., 2013; Grycner et al., 2015), Open IE relation phrases (Schoenmackers et al., 201"
N16-1011,D12-1104,0,0.0354821,"; Raghavan et al., 2012). Most of these need (or benefit from) a background knowledge of inference rules, including matrix completion (Rockt¨aschel et al., 2015b). Inference rules are predominantly generated via extended distributional similarity – two phrases having a high degree of argument overlap are similar, and thus candidates for a unidirectional or a bidirectional inference rule. Methods vary on the base representation, e.g., KB relations (Gal´arraga et al., 2013; Grycner et al., 2015), Open IE relation phrases (Schoenmackers et al., 2010), syntacticontological-lexical (SOL) patterns (Nakashole et al., 2012), and dependency paths (Lin and Pantel, 2001). An enhancement is global transitivity (TNCF algorithm) for improving recall (Berant et al., 2012). The highest precision setting of TNCF (λ = 0.1) was released as a corpus (informally called C LEAN) of Open IE inference rules.1 Distributional similarity approaches have two fundamental limitations. First, they miss obvious commonsense facts, e.g., h(X, married, Y) → (X, knows, Y)i – text will rarely say that a couple know each other. Second, they are consistently affected by statistical noise and end up generating a wide variety of inaccurate rules"
N16-1011,P15-1146,0,0.0400672,"Missing"
N16-1011,P12-1037,0,0.0958516,"d. Introduction The visions of machine reading (Etzioni, 2007) and deep language understanding (Dorr, 2012) emphasize the ability to draw inferences from text to discover implicit information that may not be explicitly stated (Schubert, 2002). This has natural applications to textual entailment (Dagan et al., 2013), KB completion (Socher et al., 2013), and effective querying over Knowledge Bases (KBs). One popular approach for fact inference is to use a set of inference rules along with probabilistic models such as Markov Logic Networks (Schoenmackers et al., 2008) or Bayesian Logic Programs (Raghavan et al., 2012) to produce humaninterpretable proof chains. While scalable (Niu et al., 2011; Domingos and Webb, 2012), this is bound by the coverage and quality of the background knowledge – the set of inference rules that enable the inference (Clark et al., 2014). The paper focuses on generating a high precision subset of inference rules over Open Information Extraction (OpenIE) (Etzioni et al., 2011) relation phrases (see Fig 1). OpenIE systems generate a schema-free KB where entities and relations are represented via normalized but not disambiguated textual strings. Such OpenIE KBs scale to the Web. Most"
N16-1011,N13-1008,0,0.0163924,"dditional annotation on existing inference rules. The verified rules are 27 to 33 points more accurate than the original corpora and still retain a substantial recall. The precision of inferred knowledge also has a precision boost of over 29 points. We release our K GLR implementation, its annotations on two popular rule corpora along with gold set used for evaluation and the annotation guidelines for further use (available at https://github.com/dair-iitd/kglr.git). 2 Related work Methods for inference over text include random walks over knowledge graphs (Lao et al., 2011), matrix completion (Riedel et al., 2013), deep neural networks (Socher et al., 2013; Rockt¨aschel et al., 2015a), natural logic inference (MacCartney and Manning, 2007) and graphical models (Schoenmackers et al., 2008; Raghavan et al., 2012). Most of these need (or benefit from) a background knowledge of inference rules, including matrix completion (Rockt¨aschel et al., 2015b). Inference rules are predominantly generated via extended distributional similarity – two phrases having a high degree of argument overlap are similar, and thus candidates for a unidirectional or a bidirectional inference rule. Methods vary on the base represe"
N16-1011,N15-1118,0,0.043212,"Missing"
N16-1011,D08-1009,0,0.178546,"thod. Rules #4, #5 were correctly and #6 wrongly filtered. Introduction The visions of machine reading (Etzioni, 2007) and deep language understanding (Dorr, 2012) emphasize the ability to draw inferences from text to discover implicit information that may not be explicitly stated (Schubert, 2002). This has natural applications to textual entailment (Dagan et al., 2013), KB completion (Socher et al., 2013), and effective querying over Knowledge Bases (KBs). One popular approach for fact inference is to use a set of inference rules along with probabilistic models such as Markov Logic Networks (Schoenmackers et al., 2008) or Bayesian Logic Programs (Raghavan et al., 2012) to produce humaninterpretable proof chains. While scalable (Niu et al., 2011; Domingos and Webb, 2012), this is bound by the coverage and quality of the background knowledge – the set of inference rules that enable the inference (Clark et al., 2014). The paper focuses on generating a high precision subset of inference rules over Open Information Extraction (OpenIE) (Etzioni et al., 2011) relation phrases (see Fig 1). OpenIE systems generate a schema-free KB where entities and relations are represented via normalized but not disambiguated text"
N16-1011,D10-1106,0,0.148678,"e coverage and quality of the background knowledge – the set of inference rules that enable the inference (Clark et al., 2014). The paper focuses on generating a high precision subset of inference rules over Open Information Extraction (OpenIE) (Etzioni et al., 2011) relation phrases (see Fig 1). OpenIE systems generate a schema-free KB where entities and relations are represented via normalized but not disambiguated textual strings. Such OpenIE KBs scale to the Web. Most existing large-scale corpora of inference rules are generated using distributional similarity, like argument-pair overlap (Schoenmackers et al., 2010; Berant et al., 2012), but often eschew any linguistic or compositional insights. Our early analysis revealed that such inference rules have very low precision, not enough to be useful for many real tasks. For human-facing applications (such as IE-based demos), high precision is critical. Inference rules have a multiplicative impact, since one poor rule could potentially generate many bad KB facts. Contributions: We investigate the hypothesis that “knowledge-guided linguistic rewrites can provide independent verification for statistically-generated Open IE inference rules.” Our system K GLR’s"
N16-1011,D12-1018,0,0.0258764,"h(X, married, Y) → (X, knows, Y)i – text will rarely say that a couple know each other. Second, they are consistently affected by statistical noise and end up generating a wide variety of inaccurate rules (see rules #4, and #5 in Figure 1). 1 http://u.cs.biu.ac.il/˜nlp/resources/downloads/predicativeentailment-rules-learned-using-local-and-global-algorithms 87 Our early experiments with C LEAN revealed its precision to be about 0.49, not enough to be useful in practice, especially for human-facing applications. Similar to our paper, some past works have used alternative sources of knowledge. Weisman et al. (2012) study inference between verbs (e.g., hstartle → surprisei), but they get low (0.4) precision. Wordnet corpus to generate inference rules for natural logic (Angeli and Manning, 2014) improved noun-based inference. But, they recognize relation entailments as a key missing piece. Recently, natural logic semantics is added to a paraphrase corpus (PPDB2.0). Many of their features, e.g., lexical/orthographic, multilingual translation based, are complimentary to our method. We test our K GLR algorithm on C LEAN and entailment/paraphrase subset of PPDB2.0 (which we call PPDBe ). 3 Knowledge-Guided Li"
N19-1126,W17-5506,0,0.68436,"ntial context surrounding each token, aiding in better interpretation of unseen tokens at test time. Second, we augment the standard cross-entropy loss used in dialog systems with an additional loss term to encourage the model to only copy KB tokens in a response, instead of generating them via the language model. This combination of sequence encoding and additional loss (along with dropout) helps in effective disentangling between language and knowledge. We perform evaluations over three datasets – bAbI (Bordes and Weston, 2017), CamRest (Wen et al., 2016), and Stanford Multi-Domain Dataset (Eric et al., 2017). Of these, the last two are realworld datasets. We find that B O S S N ET is competitive or significantly better on standard metrics in all datasets as compared to state-of-the-art baselines. We also introduce a knowledge adaptability (KA) evaluation, in which we systematically increase the percentage of previously unseen entities in the KB. We find that B O S S N ET is highly robust across all percentage levels. Finally, we also report a humanbased evaluation and find that B O S S N ET responses are frequently rated higher than other baselines. Overall, our contributions are: Encoder Decoder"
N19-1126,E17-2075,0,0.492622,"q (Madotto et al., 2018) exhibits satisfactory performance when tested on the training KB. It represents the dialog history and the KB knowledge as a bag of words in a flat memory arrangement. This enables Mem2Seq to revisit each word several times, as needed, obtaining good performance. But at the same time, flat memory prevents it from capturing any surrounding context – this deteriorates its performance rapidly when the amount of new unseen information in the KB increases, as shown in Figure 1. On the other hand, the performance of copy augmented sequence-tosequence network (Seq2Seq+Copy) (Eric and Manning, 2017), is robust to changes in the KB, but fails to achieve acceptable task-oriented performance. It captures context by representing the entire dialog history as one continuous sequence. However, it can be difficult for a sequence encoder to reason 1239 Proceedings of NAACL-HLT 2019, pages 1239–1255 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. We propose B O S S N ET, a novel architecture to disentangle the language model from knowledge incorporation in task-oriented dialogs. 2. We introduce a knowledge adaptability evaluation to measure the ab"
N19-1126,P16-1154,0,0.0261447,"T Most of the existing end-to-end approaches retrieve a response from a pre-defined set (Bordes and Weston, 2017; Liu and Perez, 2017; Seo et al., 2017). These methods are generally successful when they have to provide boilerplate responses – they cannot construct responses by using words in KB not seen during training. Alternatively, generative approaches are used where the response is generated one word at a time (Eric and Manning, 2017; Madotto et al., 2018). These approaches mitigate the unseen entity problem by incorporating the ability to copy words from the input (Vinyals et al., 2015; Gu et al., 2016). The copy mechanism has also found success in summarization (Nallapati et al., 2016; See et al., 2017) and machine translation (Gulcehre et al., 2016). B O S S N ET is also a copy incorporated generative approach. dialog history as a sequence (Eric and Manning, 2017; Gulcehre et al., 2016). Unfortunately, using a single long sequence for encoding also enforces an order over the set of KB tuples making it harder to perform inferencing over them. Other approaches represent the dialog context as a bag. Original Memory Networks (Bordes and Weston, 2017) and its extensions encode each memory eleme"
N19-1126,P16-1014,0,0.408042,"B entities and replaced all their occurrences in the dialog with new entity names. We will refer to these generated dialogs as the Knowledge Adaptability (KA) test sets. 3.2 Baselines We compare B O S S N ET against several existing end-to-end task-oriented dialog systems. These include retrieval models, such as the query reduction network (QRN) (Seo et al., 2017), memory network (MN) (Bordes and Weston, 2017), and gated memory network (GMN) (Liu and Perez, 2017). We also compare against generative models such as a sequence-to-sequence model (Seq2Seq), a copy augmented Seq2Seq (Seq2Seq+Copy) (Gulcehre et al., 2016), and Mem2Seq (Madotto et al., 2018).2 For fairness across models, we do not compare against key-value retrieval networks (Eric et al., 2017) as they simplify the dataset by canonicalizing all KB words in dialogs. We noticed that the reported results in the Mem2Seq paper are not directly comparable, as they pre-processed3 training data in SMD and bAbI datasets. For fair comparisons, we re-run Mem2Seq on the original training datasets. For completeness we mention their reported results (with pre-processing) as Mem2Seq*. 3.3 Evaluation Metrics We evaluate B O S S N ET and other models based on t"
N19-1126,W14-4340,0,0.0256137,"dify existing datasets to measure disentanglement and show B O S S N ET to be robust to KB modifications. 1 Figure 1: Performance of various task-oriented dialog systems on the CamRest dataset as the percentage of unseen information in the KB changes. model from the knowledge interface model. This separation will enable the system to generalize to KB modifications, without a loss in performance. Introduction Task-oriented dialog agents converse with a user with the goal of accomplishing a specific task and often interact with a knowledge-base (KB). For example, a restaurant reservation agent (Henderson et al., 2014) will be grounded to a KB that contains the names of restaurants, and their details. In real-world applications, the KB information could change over time. For example, (1) a KB associated with a movie ticket booking system gets updated every week based on new film releases, and (2) a restaurant reservation agent, trained with the knowledge of eateries in one city, may be deployed in other cities with an entirely different range of establishments. In such situations, the system should have the ability to conform to new-found knowledge unseen during its training. Ideally, the training algorithm"
N19-1126,E17-1001,0,0.216586,"ional test sets for each real-world corpus by varying the percentage (in multiples of 10) of unseen entities in the KB. We systematically picked random KB entities and replaced all their occurrences in the dialog with new entity names. We will refer to these generated dialogs as the Knowledge Adaptability (KA) test sets. 3.2 Baselines We compare B O S S N ET against several existing end-to-end task-oriented dialog systems. These include retrieval models, such as the query reduction network (QRN) (Seo et al., 2017), memory network (MN) (Bordes and Weston, 2017), and gated memory network (GMN) (Liu and Perez, 2017). We also compare against generative models such as a sequence-to-sequence model (Seq2Seq), a copy augmented Seq2Seq (Seq2Seq+Copy) (Gulcehre et al., 2016), and Mem2Seq (Madotto et al., 2018).2 For fairness across models, we do not compare against key-value retrieval networks (Eric et al., 2017) as they simplify the dataset by canonicalizing all KB words in dialogs. We noticed that the reported results in the Mem2Seq paper are not directly comparable, as they pre-processed3 training data in SMD and bAbI datasets. For fair comparisons, we re-run Mem2Seq on the original training datasets. For co"
N19-1126,D15-1166,0,0.0365696,"s, qrk , is assigned as the initial state of the B O S S N ET decoder. 2.3 The B O S S N ET Decoder B O S S N ET models a copy-augmented sequence decoder, which generates the response one word at a time. At any decode time step t, the decoder can either generate a word from the decode vocabulary or copy a word from the memory. Consequently, the decoder computes: (1) generate distribution Pg (yt ) over the decode vocabulary, and (2) copy distribution Pc (yt ) over words in the memory. The generate distribution is computed using a standard sequence decoder (Sutskever et al., 2014) by attending (Luong et al., 2015) over the memory cell representations ψ. The copy distribution is generated by using a two-level attention. Given the decoder state st , it first computes attention αt over the memory cells. Then it computes attention over the tokens in each memory cell mi . Finally it multiplies both these attentions to compute Pc (yt ) as follows: αit = softmax(st ψ(mi )) etij = st φ(wij ) exp(etij ) t t P βij = αi ∗ t k exp(eik ) X t βij Pc (yt = w) = (6) (7) (8) (9) ij:wij =w The copy and generate distributions are combined using a soft gate gst ∈ [0, 1] as in See et al. (2017). gst is a function of the de"
N19-1126,P18-1136,0,0.383435,"Missing"
N19-1126,K16-1028,0,0.0316791,"fined set (Bordes and Weston, 2017; Liu and Perez, 2017; Seo et al., 2017). These methods are generally successful when they have to provide boilerplate responses – they cannot construct responses by using words in KB not seen during training. Alternatively, generative approaches are used where the response is generated one word at a time (Eric and Manning, 2017; Madotto et al., 2018). These approaches mitigate the unseen entity problem by incorporating the ability to copy words from the input (Vinyals et al., 2015; Gu et al., 2016). The copy mechanism has also found success in summarization (Nallapati et al., 2016; See et al., 2017) and machine translation (Gulcehre et al., 2016). B O S S N ET is also a copy incorporated generative approach. dialog history as a sequence (Eric and Manning, 2017; Gulcehre et al., 2016). Unfortunately, using a single long sequence for encoding also enforces an order over the set of KB tuples making it harder to perform inferencing over them. Other approaches represent the dialog context as a bag. Original Memory Networks (Bordes and Weston, 2017) and its extensions encode each memory element (utterance) as an average of all constituent words – this cannot point to individ"
N19-1126,P02-1040,0,0.1082,"completeness we mention their reported results (with pre-processing) as Mem2Seq*. 3.3 Evaluation Metrics We evaluate B O S S N ET and other models based on their ability to generate valid responses. The per-response accuracy (Bordes and Weston, 2017) is the percentage of generated responses that exactly match their respective gold response. The per-dialog accuracy is the percentage of dialogs with all correctly generated responses. These accuracy metrics are a good measure for evaluating datasets with boilerplate responses such as bAbI. To quantify performance on other datasets, we use BLEU (Papineni et al., 2002) and Entity F1 (Eric and Manning, 2017) scores. BLEU measures the overlap of n-grams between the generated response and its gold response and has become a popular measure to compare task-oriented dialog systems. Entity F1 is computed by micro-F1 over KB entities in the entire set of gold responses. 2 We thank the authors for releasing a working code at https://github.com/HLTCHKUST/Mem2Seq 3 Mem2Seq used the following pre-processing on the data: 1) The subject (restaurant name) and object (rating) positions of the rating KB tuples in bAbI dialogs are flipped 2) An extra fact was added to the na"
N19-1126,P17-1099,0,0.263853,"t al., 2014) by attending (Luong et al., 2015) over the memory cell representations ψ. The copy distribution is generated by using a two-level attention. Given the decoder state st , it first computes attention αt over the memory cells. Then it computes attention over the tokens in each memory cell mi . Finally it multiplies both these attentions to compute Pc (yt ) as follows: αit = softmax(st ψ(mi )) etij = st φ(wij ) exp(etij ) t t P βij = αi ∗ t k exp(eik ) X t βij Pc (yt = w) = (6) (7) (8) (9) ij:wij =w The copy and generate distributions are combined using a soft gate gst ∈ [0, 1] as in See et al. (2017). gst is a function of the decoder state at time t and the word decoded in the previous time step. 2.4 Loss The decoder is trained using cross-entropy loss. The loss per response is defined as: Lce = − T X   log gst Pg (yt ) + (1 − gst )Pc (yt ) t=1 (10) where T is the number of words in the sequence to be generated and yt is the word to be generated at time step t. The decision to generate or copy is learnt implicitly by the network. However, to attain perfect disentanglement, the KB words should be copied, while the language should be generated. In other words, any word in the response tha"
P09-1030,P08-1088,0,0.0355916,"l in English. For each suggested translation they discussed the various senses of words in their respective languages and tag a translation correct if they found some sense that is shared by both words. For this study we tagged 7 language pairs: Hindi-Hebrew, English Wiktionary PanDictionary (0.90) PanDictionary (0.85) PanDictionary (0.70) # languages with distinct words ≥ 1000 ≥ 100 ≥1 49 107 505 67 146 608 75 175 794 107 607 1066 Table 1: PAN D ICTIONARY covers substantially more languages than the English Wiktionary. 268 lingual corpora, which may scale to several language pairs in future (Haghighi et al., 2008). Little work has been done in combining multiple dictionaries in a way that maintains word senses across dictionaries. Gollins and Sanderson (2001) explored using triangulation between alternate pivot languages in cross-lingual information retrieval. Their triangulation essentially mixes together circuits for all word senses, hence, is unable to achieve high precision. Dyvik’s “semantic mirrors” uses translation paths to tease apart distinct word senses from inputs that are not sense-distinguished (Dyvik, 2004). However, its expensive processing and reliance on parallel corpora would not scal"
P09-1030,2005.mtsummit-osmtw.3,0,0.0226508,". Because lexical translation does not require aligned corpora as input, it is feasible for a much broader set of languages than statistical Machine Translation (SMT). Of course, lexical translation cannot replace SMT, but it is useful for several applications including translating search-engine queries, library classifications, meta-data tags,2 and recent applications like cross-lingual image search (Etzioni et al., 2007), and enhancing multi-lingual Wikipedias (Adar et al., 2009). Furthermore, lexical translation is a valuable component in knowledge-based Machine Translation systems, e.g., (Bond et al., 2005; Carbonell et al., 2006). PAN D ICTIONARY currently contains over 200 million pairwise translations in over 200,000 language pairs at precision 0.8. It is constructed from information harvested from 631 online dictionaries and Wiktionaries. This necessitates matchIntroduction and Motivation In the era of globalization, inter-lingual communication is becoming increasingly important. Although nearly 7,000 languages are in use today (Gordon, 2005), most language resources are mono-lingual, or bi-lingual.1 This paper investigates whether Wiktionaries and other translation dictionaries available o"
P09-1030,P97-1063,0,0.129933,"Missing"
P09-1030,2006.amta-papers.3,0,0.0405188,"ranslation does not require aligned corpora as input, it is feasible for a much broader set of languages than statistical Machine Translation (SMT). Of course, lexical translation cannot replace SMT, but it is useful for several applications including translating search-engine queries, library classifications, meta-data tags,2 and recent applications like cross-lingual image search (Etzioni et al., 2007), and enhancing multi-lingual Wikipedias (Adar et al., 2009). Furthermore, lexical translation is a valuable component in knowledge-based Machine Translation systems, e.g., (Bond et al., 2005; Carbonell et al., 2006). PAN D ICTIONARY currently contains over 200 million pairwise translations in over 200,000 language pairs at precision 0.8. It is constructed from information harvested from 631 online dictionaries and Wiktionaries. This necessitates matchIntroduction and Motivation In the era of globalization, inter-lingual communication is becoming increasingly important. Although nearly 7,000 languages are in use today (Gordon, 2005), most language resources are mono-lingual, or bi-lingual.1 This paper investigates whether Wiktionaries and other translation dictionaries available over the Web can be automa"
P09-1030,P95-1032,0,0.211724,"Missing"
P09-1030,P91-1023,0,0.168357,"Missing"
P09-1030,J93-1004,0,\N,Missing
P09-2049,P09-1030,1,0.781508,"Missing"
P10-1044,J03-3005,0,0.292451,"Missing"
P10-1044,P08-1004,1,0.0945607,"2 in LinkLDA) are sampled sequentially conditioned on a fullassignment to all others, integrating out the parameters (Griffiths and Steyvers, 2004). This produces robust parameter estimates, as it allows computation of expectations over the posterior distribution LinkLDA Figure 2 illustrates the LinkLDA model in the plate notation, which is analogous to the model in (Erosheva et al., 2004). In particular note that each ai is drawn from a different hidden topic zi , however the zi ’s are drawn from the same distribution θr for a given relation r. To facilitate learn427 tracted by T EXT RUNNER (Banko and Etzioni, 2008) from 500 million Web pages. To create a generalization corpus from this large dataset. We first selected 3,000 relations from the middle of the tail (we used the 2,0005,000 most frequent ones)3 and collected all instances. To reduce sparsity, we discarded all tuples containing an NP that occurred fewer than 50 times in the data. This resulted in a vocabulary of about 32,000 noun phrases, and a set of about 2.4 million tuples in our generalization corpus. We inferred topic-argument and relation-topic multinomials (β, γ, and θ) on the generalization corpus by taking 5 samples at a lag of 50 aft"
P10-1044,P08-1119,0,0.0281168,"Missing"
P10-1044,D08-1007,0,0.732714,"ing multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical co"
P10-1044,J98-2002,0,0.720098,"s used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce"
P10-1044,E09-1013,0,0.0251596,"e. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in Section 4, it is the best model for our relation da"
P10-1044,N09-1042,0,0.010883,".washington.edu/research/ ldasp. 425 these relations. 2 Our task is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is"
P10-1044,D09-1092,0,0.0188142,"vor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in"
P10-1044,J02-2003,0,0.533616,"e parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable"
P10-1044,N10-1012,0,0.0218463,"Missing"
P10-1044,P06-1039,0,0.0314514,"Missing"
P10-1044,P10-1045,0,0.631386,"Missing"
P10-1044,N07-1071,0,0.755472,"del. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suf"
P10-1044,P07-1028,0,0.746694,"rd pseudodisambiguation task. Additionally, because L DA - SP is based on a formal probabilistic model, it has the advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Erk (2007) showed the advantages of this approach over Resnik’s information-theoretic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbase"
P10-1044,P09-1070,0,0.0561091,"k is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generate"
P10-1044,J02-3001,0,0.173292,"Missing"
P10-1044,W97-0209,0,0.907626,"Missing"
P10-1044,P99-1014,0,0.952895,"retic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbased approaches. Probably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectiona"
P10-1044,W03-0902,0,0.0148719,"dels to similar tasks. O poses a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system. Van Durme and Gildea (2009) proposed applying LDA to general knowledge templates extracted using the K NEXT system (Schubert and Tong, 2003). In contrast, our work uses LinkLDA and focuses on modeling multiple arguments of a relation (e.g., the subject and direct object of a verb). 3 Topic Models for Selectional Prefs. 426 3.2 JointLDA α As a more tightly coupled alternative, we first propose JointLDA, whose graphical model is depicted in Figure 1. The key difference in JointLDA (versus LDA) is that instead of one, it maintains two sets of topics (latent distributions over words) denoted by β and γ, one for classes of each argument. A topic id k represents a pair of topics, βk and γk , that co-occur in the arguments of extracted r"
P10-1044,N09-1054,0,0.00743456,"companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z"
P14-1085,P10-1066,0,0.00988716,"lexical connections or lack of a theme or story. While the facts of the sentences made sense together, the summaries sometimes did not read as if they were written by a human, but as a series of disparate sentences. For the second level, the problems were more basic. The parent sentence occasionally expressed a less important fact that the child summary did 6 Related Work http://duc.nist.gov/duc2004/quality.questions.txt 909 drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen,"
P14-1085,P11-1052,0,0.0421957,"stions: • Do users prefer hierarchical summaries for topic exploration? (Section 5.1) • Are hierarchical summaries more effective than other methods for learning about complex events? (Section 5.2) • How informative are the hierarchical summaries compared to the other methods? (Section 5.3) • How coherent is the hierarchical structure in the summaries? (Section 5.4) We compared S UMMA against two baseline systems which represent the main NLP methods for large-scale summarization: an algorithm for creating timelines over sentences (Chieu and Lee, 2004),3 and a state-of-the-art flat MDS system (Lin and Bilmes, 2011).4 Each system was given the same budget (over 10 times the traditional MDS budget, which is 665 bytes). We evaluated the questions on ten news topics, representing a range of tasks: (1) Pope John Paul II’s death and the 2005 Papal Conclave, (2) Bush v. Gore, (3) the Tulip Revolution, (4) Daniel Pearl’s kidnapping, (5) the Lockerbie bombing handover of suspects, (6) the Kargil War, (7) NATO’s bombing of Yugoslavia in 1999, (8) Pinochet’s arrest in London, (9) the 2005 London bombings, and (10) the crash and investigation of SwissAir Flight 111. We chose topics containing a set of related event"
P14-1085,P10-1084,0,0.00993645,"ined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present S UMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares S UMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with S UMMA in twice as many cases, and S UMMA was preferred more than three times as often. When compared to flat summaries, users overwhelming prefer"
P14-1085,W04-1013,0,0.0127871,"0 20 0 6 8 10 12 14 16 18 20 22 24 Day of Month Figure 3: News coverage by date for the embassy bombings in Tanzania and Kenya. There are spikes in the number of articles published at the two major events. 3.2 4.1 Choosing the number of clusters Salience is the value of each sentence to the topic from which the documents are drawn. We measure salience of a summary (Sal(X)) as the P sum of the saliences of individual sentences ( i Sal(xi )). Following previous research in MDS, we computed individual saliences using a linear regression classifier trained on ROUGE scores over the DUC’03 dataset (Lin, 2004; Christensen et al., 2013). This method finds those sentences more salient that mention nouns or verbs that occur frequently in the cluster. In preliminary experiments, we noticed that many sentences that were reaction sentences were given a higher salience than action sentences. For example, the reaction sentence, “President Clinton vowed to track down the perpetrators behind the bombs that exploded outside the embassies in Tanzania and Kenya on Friday,” would have a higher score than the action sentence, “Bombs exploded outside the embassies in Tanzania and Kenya on Friday.” This problem oc"
P14-1085,chang-manning-2012-sutime,0,0.0428989,"he output is the hierarchical summary H, which we define formally as follows. 2 Hierarchical Clustering http://knowitall.cs.washington.edu/summa/ 903 Hierarchical Clustering H s1 s1 Hierarchical Summary X s1 ... sk ... si ... sk+1 ... si si+1 ... sj x2,1 x2,2 x5,1 x5,2 x5,3 x6,1 x6,2 x1,1 x1,2 x1,3 x3,1 x3,2 x3,3 x7,1 x7,2 x7,3 algorithm that automatically the approprito determine if the chooses timestamps in the sentence reate number of clusters at each split. fer to the root verb. If no timestamp is given, we Before clustering, we timestamp all sentences. useSUTime the article date. We use (Chang and Manning, 2012) to normalize temporal references, and we parse the 3.1 with Temporal Clustering sentences the Stanford parser (Klein and Manning, 2003) and use athe set of simple heuristics After acquiring timestamps, we must hierarto determine if the timestamps in the sentence chically cluster the sentences intore-sets that make fer to the root verb. If no timestamp is given, we sense to summarize together. Since we wish to use the article date. partition along the temporal dimension, our probthe best dates at which ... After to acquiring the timestamps, we must hierars split a cluster into subclusters. We"
P14-1085,D12-1048,1,0.216012,"tion. Ideally, the maximum depth of the clustering would be a function of the number of sentences in each cluster, but in our implementation, we set the maximum depth to three, which works well for the size of the datasets we use (300 articles). 4 Salience 4.2 Redundancy We identify redundant sentences using a linear regression classifier trained on a manually labeled subset of the DUC’03 sentences. The features include shared noun counts, sentence length, TF*IDF cosine similarity, timestamp difference, and features drawn from information extraction such as number of shared tuples in Open IE (Mausam et al., 2012). Summarizing within the Hierarchy After the sentences are clustered, we have a structure for the hierarchical summary that dictates the number of summaries and the number of sentences 905 4.3 4.4 Summary Coherence Having estimated salience, redundancy, and two forms of coherence, we can now put this information together into a single objective function that measures the quality of a candidate hierarchical summary. Intuitively, the objective function should balance salience and coherence. Furthermore, the summary should not contain redundant information and each cluster summary should honor th"
P14-1085,N13-1136,1,0.766474,"10 12 14 16 18 20 22 24 Day of Month Figure 3: News coverage by date for the embassy bombings in Tanzania and Kenya. There are spikes in the number of articles published at the two major events. 3.2 4.1 Choosing the number of clusters Salience is the value of each sentence to the topic from which the documents are drawn. We measure salience of a summary (Sal(X)) as the P sum of the saliences of individual sentences ( i Sal(xi )). Following previous research in MDS, we computed individual saliences using a linear regression classifier trained on ROUGE scores over the DUC’03 dataset (Lin, 2004; Christensen et al., 2013). This method finds those sentences more salient that mention nouns or verbs that occur frequently in the cluster. In preliminary experiments, we noticed that many sentences that were reaction sentences were given a higher salience than action sentences. For example, the reaction sentence, “President Clinton vowed to track down the perpetrators behind the bombs that exploded outside the embassies in Tanzania and Kenya on Friday,” would have a higher score than the action sentence, “Bombs exploded outside the embassies in Tanzania and Kenya on Friday.” This problem occurs because the first sent"
P14-1085,P09-2029,0,0.104041,"few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present S UMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares S UMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with S UMMA in twice as many cases, and S UMMA was preferred more than three times as often. When compared to flat"
P14-1085,D12-1065,0,0.020432,"ains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Guestrin (2010) formalized the characteristics of a good chain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated"
P14-1085,N09-1041,0,0.0565763,"xtended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present S UMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares S UMMA with a timeline system"
P14-1085,P09-1024,0,0.0261742,"The lower coherence scores were often the result of too few lexical connections or lack of a theme or story. While the facts of the sentences made sense together, the summaries sometimes did not read as if they were written by a human, but as a series of disparate sentences. For the second level, the problems were more basic. The parent sentence occasionally expressed a less important fact that the child summary did 6 Related Work http://duc.nist.gov/duc2004/quality.questions.txt 909 drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important da"
P14-1085,P12-1077,0,0.0555171,"Missing"
P14-1085,P03-1054,0,0.0386984,"Missing"
P14-1085,D11-1040,0,0.0192353,"rtant fact that the child summary did 6 Related Work http://duc.nist.gov/duc2004/quality.questions.txt 909 drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document"
P15-2050,P12-1015,0,0.0345862,"Missing"
P15-2050,N13-1136,1,0.584267,"ook advantage) are grouped in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structures – lexical, dependency and SRL, for three downstream NLP tasks. 3 (a) Lexical matching of a 5 words window (marked with a box). Current window yields a score of 4 - words contributing to the score are marked in bold. (b) Dependency matching yields a score of 3. Contributing triplets are marked in bold. S: refused0.1 : A0 : John A1 : to visit a Vegas casino"
P15-2050,N13-1090,0,0.0553311,"sk Text comprehension tasks extrinsically test natural language understanding through question answer304 Target refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts a"
P15-2050,W08-1301,0,0.0428588,"Missing"
P15-2050,D11-1142,0,0.780242,"Missing"
P15-2050,D13-1020,0,0.00596479,"ch as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of this sentence. ing. We use the MCTest corpus (Richardson et al., 2013), which is composed of short stories followed by multiple choice questions. The MCTest task does not require extensive world knowledge, which makes it ideal for testing underlying sentence representations, as performance will mostly depend on accuracy and informativeness of the extracted structures. We adapt the unsupervised lexical matching algorithm from the original MCTest paper. It counts lexical matches between an assertion obtained from a candidate answer (CA) and a sliding window over the story. The selected answer is the one for which the maximum number of matches are found. Our adapta"
P15-2050,D07-1002,0,0.0221474,"equence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, and highlight its unique properties. As we show in Section 4, these differences have an im"
P15-2050,J15-4004,0,0.0052185,"Missing"
P15-2050,P14-2050,0,0.0651708,"et refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of thi"
P15-2050,P10-1040,0,0.0288292,"). lar to the word similarity tasks. To our knowledge, Open IE results on both analogy datasets surpass the state of the art. An example (from the Microsoft test set) which supports the observation regarding Open IE embeddings space is (gentlest:gentler, loudest:?), for which only Open IE answers correctly as louder, while lexical respond with higher-pitched (domain similar to loudest), and dependency with thinnest (functionally similar to loudest). Our Open-IE embeddings are freely available6 and we note that these can serve as plug-in features for other NLP applications, as demonstrated in (Turian et al., 2010). 5 References Steven P Abney. 1992. Parsing by chunks. Principlebased parsing, pages 257–278. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics. Conclusions Niranjan Balasubramanian, Stephen Mausam, and Oren Etzioni. 2013. coherent event schemas at sca"
P15-2050,J13-2005,0,0.00683734,"(1) Lexical representations, in which features are extracted from the original word sequence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, an"
P15-2050,W13-3512,0,0.00940111,"Missing"
P15-2050,D12-1048,1,0.598339,"Missing"
P15-2050,W11-0906,0,\N,Missing
P15-2050,W05-0620,0,\N,Missing
P15-2050,P11-1060,0,\N,Missing
P15-2050,N09-1003,0,\N,Missing
P15-2050,D13-1178,1,\N,Missing
P17-2050,P16-1055,0,0.0140595,"and Haussmann, 2013). Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information from text. Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010). More recently, the focus has been on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016), or on numbers that represent cardinalities of relations (Mirza et al., 2017). Figure 1: BONIE flow diagram 3 Open Numerical Relation Extraction The goal of Open Numerical Relation Extraction is to process a sentence that has a quantity mention in it, and extract any tuple of the form (Arg1, relation phrase, Arg2) where Arg2 (or Arg1) is a quantity. As a first step, BONIE learns patterns where Arg2 is a quantity, as most English sentences tend to express numerical facts in active voice. Figure 1 outlines BONIE’s alg"
P17-2050,D13-1043,0,0.0517684,"ia the adjective ‘long’. And, sentence #5 expresses the relation ‘area’ via the units. (3) BONIE identifies implicit relations using additional processing of units and adjectives. (4) Finally, BONIE can tag a quantity as count and prepends “number of” in the relation phrase (sentence #2). 2 Related Work One of the first Open IE systems to obtain substantial recall is OLLIE (Mausam et al., 2012), which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions. Other methods augment the linguistic knowledge in the systems – Exemplar (de S´a Mesquita et al., 2013) adds new rules over dependency parses, SRLIE develops extraction logic over SRL frames (Christensen et al., 2011). Several works identify clauses and operate over restructured sentences (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information f"
P17-2050,D12-1048,1,0.883403,"threshold. Additionally, BONIE uses a quantity extractor (Roy et al., 2015), which provides Introduction Open Information Extraction (Open IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary (Etzioni et al., 2008; Mausam, 2016), by constructing the relation phrases and arguments from within the sentences themselves. Early works on Open IE such as R E V ERB (Etzioni et al., 2011) extract verbmediated relations via a handful of human-defined patterns. OLLIE improves recall by learning dependency patterns, using bootstrapping over R E V ERB extractions (Mausam et al., 2012). Open IE 4.2, a state-of-the-art open information extractor, is based on a combination of SRLIE, a verbmediated extractor over SRL frames (Christensen et al., 2011), and R EL N OUN 2.0, which performs special linguistic processing for extraction from complex noun phrases (Pal and Mausam, 2016). 1 ∗ Most work was done when the author was a graduate student at IIT Delhi. 1 https://github.com/knowitall/openie 2 Available at https://github.com/Open-NRE 317 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323 c Vancouver, Canada, Jul"
P17-2050,P17-2055,0,0.0664549,"and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information from text. Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010). More recently, the focus has been on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016), or on numbers that represent cardinalities of relations (Mirza et al., 2017). Figure 1: BONIE flow diagram 3 Open Numerical Relation Extraction The goal of Open Numerical Relation Extraction is to process a sentence that has a quantity mention in it, and extract any tuple of the form (Arg1, relation phrase, Arg2) where Arg2 (or Arg1) is a quantity. As a first step, BONIE learns patterns where Arg2 is a quantity, as most English sentences tend to express numerical facts in active voice. Figure 1 outlines BONIE’s algorithm, which operates in two phases: training and extraction. BONIE’s training includes creation of seed facts, generation of training data via bootstrappi"
P17-2050,P10-1030,0,0.0278076,"Several works identify clauses and operate over restructured sentences (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information from text. Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010). More recently, the focus has been on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016), or on numbers that represent cardinalities of relations (Mirza et al., 2017). Figure 1: BONIE flow diagram 3 Open Numerical Relation Extraction The goal of Open Numerical Relation Extraction is to process a sentence that has a quantity mention in it, and extract any tuple of the form (Arg1, relation phrase, Arg2) where Arg2 (or Arg1) is a quantity. As a first step, BONIE le"
P17-2050,W16-1307,1,0.450499,"lation phrases and arguments from within the sentences themselves. Early works on Open IE such as R E V ERB (Etzioni et al., 2011) extract verbmediated relations via a handful of human-defined patterns. OLLIE improves recall by learning dependency patterns, using bootstrapping over R E V ERB extractions (Mausam et al., 2012). Open IE 4.2, a state-of-the-art open information extractor, is based on a combination of SRLIE, a verbmediated extractor over SRL frames (Christensen et al., 2011), and R EL N OUN 2.0, which performs special linguistic processing for extraction from complex noun phrases (Pal and Mausam, 2016). 1 ∗ Most work was done when the author was a graduate student at IIT Delhi. 1 https://github.com/knowitall/openie 2 Available at https://github.com/Open-NRE 317 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2050 One of the prior works that applies to generic numerical relations is LUCHS (Hoffmann et al., 2010), where the system uses distant supervision to create 5,000 relation extractors, which in"
P17-2050,Q15-1001,0,0.0147186,"es. At a high level BONIE follows OLLIE’s design of identifying seed facts, constructing training data by bootstrapping sentences that may mention a seed fact, pattern learning and ranking. Madaan et al (2016) note that bootstrapping for numerical IE is challenging; it can lead to high noise and missed recall, since numbers can easily match out of context, and numbers may not match due to approximations. In response, similar to most previous works (e.g., LUCHS (Hoffmann et al., 2010)) BONIE matches a number if it is within a percentage threshold. Additionally, BONIE uses a quantity extractor (Roy et al., 2015), which provides Introduction Open Information Extraction (Open IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary (Etzioni et al., 2008; Mausam, 2016), by constructing the relation phrases and arguments from within the sentences themselves. Early works on Open IE such as R E V ERB (Etzioni et al., 2011) extract verbmediated relations via a handful of human-defined patterns. OLLIE improves recall by learning dependency patterns, using bootstrapping over R E V ERB extractions (Mausam et al., 2012). Open IE 4.2, a state-of-the-art open information extra"
P17-2050,schmidek-barbosa-2014-improving,0,0.166087,"uantity as count and prepends “number of” in the relation phrase (sentence #2). 2 Related Work One of the first Open IE systems to obtain substantial recall is OLLIE (Mausam et al., 2012), which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions. Other methods augment the linguistic knowledge in the systems – Exemplar (de S´a Mesquita et al., 2013) adds new rules over dependency parses, SRLIE develops extraction logic over SRL frames (Christensen et al., 2011). Several works identify clauses and operate over restructured sentences (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information from text. Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010). More recently, the focus has been on numbers that express physical quantities or meas"
P17-2050,D15-1312,0,0.0135842,"e 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2050 One of the prior works that applies to generic numerical relations is LUCHS (Hoffmann et al., 2010), where the system uses distant supervision to create 5,000 relation extractors, which included numerical relations as well. Researchers have also specifically developed numerical relation extractors to extract those relations where one of the arguments is a quantity (Vlachos and Riedel, 2015; Intxaurrondo et al., 2015; Madaan et al., 2016). However, all of them extract only an ontology relation, and hence are not directly applicable to Open IE. the units mentioned in the sentence – BONIE bootstraps a sentence only when the units match. When compared to OLLIE, BONIE contributes several numerical IE specific customizations. (1) Since no open facts are available for this task, we first manually define a set of high-precision seed patterns, which are run over a large corpus to generate seed facts. (2) Not all seeds are fit for bootstrapping – many don’t even have an entity as first a"
P17-2050,P10-1013,0,0.0615114,"Missing"
P17-2050,N13-1107,0,0.0577209,"n IE systems to obtain substantial recall is OLLIE (Mausam et al., 2012), which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions. Other methods augment the linguistic knowledge in the systems – Exemplar (de S´a Mesquita et al., 2013) adds new rules over dependency parses, SRLIE develops extraction logic over SRL frames (Christensen et al., 2011). Several works identify clauses and operate over restructured sentences (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014). However, none of them handle numbers specifically, and hence do not work for our problem. Numerical Relations: Numbers play an important role in extracting information from text. Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010). More recently, the focus has been on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al."
P18-2013,D07-1043,0,0.227786,"Missing"
P18-2013,N16-1011,1,0.895768,"Missing"
P18-2013,W15-4007,0,0.140233,"Missing"
P18-2013,D15-1174,0,0.47075,"that depends purely on type compatibility checking. as , b r , a o i, b r mediates a diObserve that, in ha rect compatibility between s and o for relation r, ao ·w w r , we are scoring how well whereas, in a s ·vv r +a s can serve as subject and o as object of the relation r. Thus, in the second case, a e is expected to encode the type(s) of entity e, where, by ‘type’, we loosely mean “information that helps decide if e can participate in a relation r, as subject or object.” Heuristic filtering of the entities that do not match the desired type at test time has been known to improve accuracy (Toutanova et al., 2015; Krompaß et al., 2015). Our typed models formalize this within the embeddings and allow for discovery of latent types without additional data. Krompaß et al. (2015) also use heuristic typing of entities for generating negative samples while training the model. Our experiment finds that this approach is not very competitive against our typed models. 3 Cv vr us as f br uo f0 ao wr Cw Figure 1: TypeDM and TypeComplex. Prediction: DM’s base prediction score for tuas , b r , a o i. We apply a (sigmoid) ple (s, r, o) is ha nonlinearity: as , br , ao i), f (s, r, o) = σ(ha (1) and then combine with"
P18-2013,P18-1010,0,0.0237448,"l Linguistics (Short Papers), pages 75–80 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics information. Typically, K  D0 . The second, concomitant modification is that each relation r is 0 now associated with three vectors: b r ∈ RD as before, and also v r , w r ∈ RK . v r and w r encode the expected types for subject and object entities. An ideal way to train type embeddings would be to provide canonical type signatures for each relation and entity. Unfortunately, these aspects of realistic KBs are themselves incomplete (Neelakantan and Chang, 2015; Murty et al., 2018). Our models train all embeddings using T only and don’t rely on any explicit type supervision. DM uses (E + R)D model weights for a KB with R relations and E entities, whereas TypeDM uses E(D0 +K)+R(D0 +2K). To make comparisons fair, we set D0 and K so that the total number of model weights (real or complex) are about the same for base and typed models. for further research. 2 Background and Related Work We are given an incomplete KB with entities E and relations R. The KB also contains T = {hs, r, oi}, a set of known valid tuples, each with subject and object entities s, o ∈ E, and relation"
P18-2013,P17-1088,0,0.0389466,"Missing"
P18-2013,N15-1054,0,0.0331795,"Association for Computational Linguistics (Short Papers), pages 75–80 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics information. Typically, K  D0 . The second, concomitant modification is that each relation r is 0 now associated with three vectors: b r ∈ RD as before, and also v r , w r ∈ RK . v r and w r encode the expected types for subject and object entities. An ideal way to train type embeddings would be to provide canonical type signatures for each relation and entity. Unfortunately, these aspects of realistic KBs are themselves incomplete (Neelakantan and Chang, 2015; Murty et al., 2018). Our models train all embeddings using T only and don’t rely on any explicit type supervision. DM uses (E + R)D model weights for a KB with R relations and E entities, whereas TypeDM uses E(D0 +K)+R(D0 +2K). To make comparisons fair, we set D0 and K so that the total number of model weights (real or complex) are about the same for base and typed models. for further research. 2 Background and Related Work We are given an incomplete KB with entities E and relations R. The KB also contains T = {hs, r, oi}, a set of known valid tuples, each with subject and object entities s,"
Q13-1030,P11-1040,0,0.046149,"Missing"
Q13-1030,P07-1073,0,0.0926997,"Missing"
Q13-1030,W99-0613,0,0.16261,"all curve (from 0.16 to 0.34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, s"
Q13-1030,W02-1001,0,0.0880796,"the sentences in our text corpus: θ∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this app"
Q13-1030,P11-1055,1,0.951987,"e. To address this challenge, we propose a joint model of extraction from text and the process by which propositions are observed or missing in both the database and text. Our approach provides a natural way to incorporate side information in the form of a missing data model. For instance, popular entities such as Barack Obama already have good coverage in Freebase, so new extractions are more likely to be errors than those involving rare entities with poor coverage. Our approach to missing data is general and can be combined with various IE solutions. As a proof of concept, we extend MultiR (Hoffmann et al., 2011), a recent model for distantly supervised information extraction, to explicitly model missing data. These extensions complicate the MAP inference problem which is used as a subroutine in learning. This motivated us to explore a variety of approaches to inference in the joint extraction and missing data model. We explore both exact inference based on A* search and efficient approximate inference using local search. Our experiments demonstrate that with a carefully designed set of search operators, local search produces optimal solutions in most cases. Experimental results demonstrate large perf"
Q13-1030,P06-1096,0,0.0137234,"Missing"
Q13-1030,N10-1082,0,0.0108074,"penalty for extracting a fact not in Freebase, and produce an overall higher score. To avoid the problem of getting stuck in local optima, we propose an additional search operator which considers changing all variables, zi , which are currently assigned to a specific relation r, to a new relation r0 , resulting in an additional (k − 1)2 possible neighbors, in addition to the n × (k − 1) neighbors which come from the standard search operator. This aggregate-level search operator allows for more global moves which help to avoid local optima, similar to the type-level sampling approach for MCMC (Liang et al., 2010). At each iteration, we consider all n × (k − 1) + (k−1)2 possible neighboring solutions generated by both search operators, and pick the one with biggest overall improvement, or terminate the algorithm if no improvements can be made over the current solution. 20 random restarts were used for each infer372 ence problem. We found this approach to almost always find an optimal solution. In over 100,000 problems with 200 or fewer variables from the New York Times dataset used in Section 7, an optimal solution was missed in only 3 cases which was verified by comparing against optimal solutions fou"
Q13-1030,N13-1095,0,0.0522778,"on, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approach assuming a fixed proportion of true positives for each entity pair. The Min et al. (2013) approach is perhaps the most closely related of the recent approaches for distant supervision. However, there are a number of key differences: (1) They impose a hard constraint on the proportion of true positive examples for each entity pair, whereas we jointly model relation extraction and missing data in the text and KB. (2) They only handle the case of missing information in the database and not in the text. (3) Their mo"
Q13-1030,P09-1113,0,0.938168,"Missing"
Q13-1030,W04-2407,0,0.0215227,"Times text, features and Freebase relations developed by Riedel et. al. (2010) which was also used by Hoffmann et. al. (2011). This dataset is constructed by extracting named entities from 1.8 million New York Times articles, which are then match against entities in Freebase. Sentences which contain pairs of entities participating in one or more relations are then used as training examples for those relations. The sentencelevel features include word sequences appearing in context with the pair of entities, in addition to part 373 of speech sequences, and dependency paths from the Malt parser (Nivre et al., 2004). 7.1.1 Baseline To evaluate the effect of modeling missing data in distant supervision, we compare against the MultiR model for distant supervision (Hoffmann et al., 2011), a state of the art approach for binary relation extraction which is the most similar previous work, and models facts in Freebase as hard constraints disallowing the possibility of missing information in either the text or the database. To make our experiment as controlled as possible and ruleout the possibility of differences in performance due to implementation details, we compare against our own re-implementation of Mult"
Q13-1030,N13-1008,0,0.0576422,"Missing"
Q13-1030,D11-1141,1,0.107054,"Missing"
Q13-1030,D12-1042,0,0.677887,"Missing"
Q13-1030,P12-1076,0,0.336988,"distant supervision to train event extractors from Twitter. Mintz et. al. (2009) used a set of relations from Freebase as a distant source of supervision to learn to extract information from Wikipedia. Ridel et. al. (2010), Hoffmann et. al. (2011), and Surdeanu et. al. (2012) presented a series of models casting distant supervision as a multiple-instance learning problem (Dietterich et al., 1997). Recent work has begun to address the challenge of noise in heuristically labeled training data generated by distant supervision, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approa"
Q13-1030,P10-1149,0,0.0213479,"34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, so we presented an approach ba"
Q13-1030,P13-2117,0,0.593462,"Missing"
Q13-1030,D07-1071,1,0.364681,"∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this approximation to the gradient requires solving two infe"
W10-0907,P98-1013,0,0.0227497,"ible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate SRL-IE’s effectiveness"
W10-0907,P08-1004,1,0.816759,"eneous, unstructured text. The traditional approaches to information extraction (e.g., (Soderland, 1999; Agichtein and Gravano, 2000)) do not operate at these scales, since they focus attention on a well-defined small set of relations and require large amounts of training data for each relation. The recent Open Information Extraction paradigm (Banko et al., 2007) attempts to overcome the knowledge acquisition bottleneck with its relation-independent nature and no manually annotated training data. 52 We are interested in the best possible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain"
W10-0907,W09-2201,0,0.0118398,"le for extracting common sense knowledge as opposed to factual information. Another Open IE system, Kylin (Weld et al., 2008), suggests automatically building an extractor for each relation using self-supervised training, with training data generated using Wikipedia infoboxes. This work has the limitation that it can only extract relations expressed in Wikipedia infoboxes. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). While one goal of Preemptive IE is to avoid relation-specificity, Preemptive IE does not emphasize Web scalability, which is essential to Open IE. (Carlson et al., 2009) presents a semi-supervised approach to information extraction on the Web. It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar, resulting in 46 general linkpaths as patterns for relation extraction. With these patterns WANDERLUST extracts binary relations from link grammar linkages. In contrast to"
W10-0907,N09-2022,0,0.0115482,"actor. Because not all roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constituents that are potentia"
W10-0907,C08-1050,0,0.012304,"tient and ‘son’ is the benefactor. Because not all roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constitu"
W10-0907,kingsbury-palmer-2002-treebank,0,0.731996,"terested in the best possible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate S"
W10-0907,J08-2003,0,0.0108118,"roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constituents that are potential arguments. In argument"
W10-0907,D09-1001,0,0.0269047,"It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar, resulting in 46 general linkpaths as patterns for relation extraction. With these patterns WANDERLUST extracts binary relations from link grammar linkages. In contrast to our approaches, this requires a large set of hand-labeled examples. USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion. They evaluate their work on biomedical text, so its applicability to general Web text is not yet clear. 8 Discussion and Future Work The Heavy Tail: It is well accepted that information on the Web is distributed according to Zipf’s 0 10 20 30 40 Time (hours) 0.0 50 0 10 20 30 40 Time (hours) F−measure 0.4 0.8 TextRunner SRL−IE RecallHybrid PrecHybrid TextRunner SRL−IE RecallHybrid PrecHybrid 0.0 Recall 0.4 0.8 Precision 0.2 0.4 0.6 0.0 TextRunner SRL−IE RecallHybrid PrecHybrid 50 0 10 20"
W10-0907,J08-2005,0,0.120937,"lar NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate SRL-IE’s effectiveness in the context of large scale and heterogeneous input data as found on the Web: because SRL uses deeper analysis we expect SRL-IE to be much slower. Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web. In this paper we address several empirical quesProceedings of the NAACL HLT 2"
W10-0907,N06-1039,0,0.0123911,"008). A version of K NEXT uses heuristic rules and syntactic parses to convert a sentence into an unscoped logical form (Van Durme and Schubert, 2008). This work is more suitable for extracting common sense knowledge as opposed to factual information. Another Open IE system, Kylin (Weld et al., 2008), suggests automatically building an extractor for each relation using self-supervised training, with training data generated using Wikipedia infoboxes. This work has the limitation that it can only extract relations expressed in Wikipedia infoboxes. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). While one goal of Preemptive IE is to avoid relation-specificity, Preemptive IE does not emphasize Web scalability, which is essential to Open IE. (Carlson et al., 2009) presents a semi-supervised approach to information extraction on the Web. It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar,"
W10-0907,J08-2002,0,0.0441675,"Missing"
W10-0907,W08-2219,0,0.0204424,"Missing"
W10-0907,C98-1013,0,\N,Missing
W10-0911,W10-0907,1,0.72391,"wledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage b"
W10-0911,P07-1088,1,0.752966,"leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KO"
W10-0911,P10-1030,1,0.76682,"omingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned inference patterns (Holmes, Sherlock), and cotraining using relation-specific and relation-independent (open) extraction to reinforce semantic coherence (Wu et al., 2008)"
W10-0911,D08-1068,1,0.90344,"radictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausi"
W10-0911,D09-1001,1,0.413926,"edge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Scho"
W10-0911,H05-1043,1,0.0833929,"ic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContrair"
W10-0911,D08-1002,1,0.769247,"nd Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via rel"
W10-0911,P10-1044,1,0.578046,"ustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and Text"
W10-0911,D08-1009,1,0.92382,"usters (e.g., in USP, LOFT). Similarly, a variety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to 90 machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better than more expressive ones. A key direction"
W10-0911,D10-1106,1,0.437126,"009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and ofte"
W10-0911,P10-1013,1,0.458065,"n and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned"
W10-0911,N07-1016,1,0.79964,"ioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs s"
W10-0911,H05-2017,1,\N,Missing
W10-0911,P10-1031,1,\N,Missing
W12-3016,E06-1002,0,0.182439,"Missing"
W12-3016,D07-1074,0,0.0237753,"t strings to meaningful entities that hold properties, semantic types, and relationships with each other. Introduction Information Extraction techniques such as Open IE (Banko et al., 2007; Weld et al., 2008) operate at unprecedented scale. The R E V ERB extractor (Fader et al., 2011) was run on 500 million Web pages, and extracted 6 billion (Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on anothe"
W12-3016,C10-1032,0,0.118648,"(Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on another document. This relates to cross-document coreference (Singh et al., 2011), but is not the same because cross-document coreference does not offer all the benefits of linking to Wikipedia. Another opportunity is that after linking a million documents, we can discover systematic linking errors when particular entities are linked to many more times"
W12-3016,D11-1142,1,0.0869527,"ng at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities. 1 Figure 1: Entity Linking elevates textual argument strings to meaningful entities that hold properties, semantic types, and relationships with each other. Introduction Information Extraction techniques such as Open IE (Banko et al., 2007; Weld et al., 2008) operate at unprecedented scale. The R E V ERB extractor (Fader et al., 2011) was run on 500 million Web pages, and extracted 6 billion (Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into"
W12-3016,D11-1072,0,0.0431029,"Missing"
W12-3016,P11-1138,0,0.0641982,"ambiguity is high. ing at this scale. 2 Entity Linking Given a textual assertion, we aim to find the Wikipedia entity that corresponds to the argument. For example, the assertion (“New York”, “acquired”, “Pineda”) should link to the Wikipedia article for New York Yankees, rather than New York City. Speed is a practical concern when linking this many assertions, so instead of designing a system with sophisticated features that rely on the full Wikipedia graph structure, we instead start with a faster system leveraging linking features such as string matching, prominence, and context matching. (Ratinov et al., 2011) found that these “local” features already provide a baseline that is very difficult to beat with the more sophisticated “global” features that take more time to compute. For efficient highquality entity linking of Web scale corpora, we focus on the faster techniques, and then later incorporate corpus-level features to increase precision. 2.1 Our Basic Linker Given an entity string, we first obtain the most prominent Wikipedia entities that meet string matching criteria. As in (Fader et al., 2009), we measure prominence using inlink count, which is the number of Wikipedia pages that link to a"
W12-3016,D10-1106,1,0.617096,"at, school)” are returned first. When results are sorted by link score, the top hundred results are all specific instances of professors and the schools they teach at, and are noticeably more specific and generally correct than the top frequency-sorted instances. 3.3 Inference Disambiguated and typed entities are especially valuable for inference applications over extracted data. For example if we observe enough instances like “Orange Juice is rich in Vitamin C,” “Vitamin C helps prevent scurvy,” and “Orange Juice helps prevent scurvy,” then we can learn the inference rule shown in Figure 3. (Schoenmackers et al., 2010) explored this, but without entity linking they had to rely on heavy filtering against hypernym data, losing most of their extraction instances in the process. We plan to explore how much gain we get in inference rule learning when using entity linking instead of hypernym filtering. Linked instances would also be higher precision input than what is currently available for learning implicit common sense properties of textual relations (Lin et al., 2010). 4 Conclusions While numerous entity-linking systems have been developed in recent years, we believe that going forward, researchers will incre"
W12-3016,P11-1080,0,0.0289028,", integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on another document. This relates to cross-document coreference (Singh et al., 2011), but is not the same because cross-document coreference does not offer all the benefits of linking to Wikipedia. Another opportunity is that after linking a million documents, we can discover systematic linking errors when particular entities are linked to many more times than expected. In this paper we entity link millions of highprecision extractions from the Web, and present our initial methods for addressing some of the opportunities and practical challenges that arise when link84 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-W"
W12-3019,J92-4003,0,0.37258,"the future, we will automatically determine semantic types for the slots. We will also split slots that have a mixture of semantic types, as in the example of the arguments {percent, year} for the extraction (sale; increase; ?) in Table 2. Table 3: Both Rel-clusters and Chambers system discovered clusters that covered most of the extraction slots for MUC-4 terrorism events. Fraction of slots Chambers Rel-clusters Bombing 0.50 1.00 Attack 0.67 1.00 Kidnapping 0.50 0.50 Arson 0.60 0.60 Average 0.57 0.77 4 Related Work There has been extensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extrac"
W12-3019,P08-1090,0,0.113394,"xtensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram language models, but is computed over relations rather than over words. We construct the Rel-grams probabilistic graphical model based on statistics stored in the Rel-grams database and demonstrate the model’s use in identifying event templates from c"
W12-3019,P09-1068,0,0.18985,"del language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram language models, but is computed over relations rather than over words. We construct the Rel-grams probabilistic graphical model based on statistics stored in the Rel-grams database and demonstrate the model’s use in identifying event templates from clusters of co-occurring relati"
W12-3019,P11-1098,0,0.416297,"ntation is linear in the size of the corpus and easily scaled to far larger corpora. Rel-grams database facilitates several tasks including: Relational Language Models: We define a relational language model, which encodes the probability of relational tuple R, having observed R0 in the k previous tuples. This can be used for discourse coherence, sentence order in summarization, etc. Event Template Construction: We cluster commonly co-occuring relational tuples as in Figure 1 and use them as the basis for open event templates (see Table 2). Our work builds on and generalizes earlier efforts by Chambers and Jurafsky (2011). Expectation-driven Extraction: The probabilities output by the relational language model may be 101 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 101–105, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics used to inform an information extractor. As has been the case with n-gram models and resources such as DIRT (Lin and Pantel, 2001), we expect the community to suggest additional applications leveraging this large scale, public resource. An intriguing possibility is to use"
W12-3019,W04-3205,0,0.0896575,"discovered clusters that covered most of the extraction slots for MUC-4 terrorism events. Fraction of slots Chambers Rel-clusters Bombing 0.50 1.00 Attack 0.67 1.00 Kidnapping 0.50 0.50 Arson 0.60 0.60 Average 0.57 0.77 4 Related Work There has been extensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram"
W12-3019,D11-1142,1,0.617966,"LP tasks such as extraction. 102 probability given (treasury bond; fall; ?). Our model matches well with human intuition about semantic relatedness. Predicted tuples Argument values 1.(bond; fall; ?) point, percent 2.(yield; rise; ?) point, percent 3.(report; show; ?) economy, growth 4.(bond; yield; ?) percent, point 5.(index; rise; ?) percent, point 6.(federal reserve; raise; ?) rate, interest rate The Rel-grams Database As a first step towards building the relational language model, we extract relational tuples from each sentence in our corpus using ReVerb, a state-of-theart Open IE system (Fader et al., 2011). This extracts relational tuples in the format (arg1; rel; arg2) where each tuple element is a phrase from the sentence. We construct a relational database to hold cooccurrence statistics for pairs of tuples found in each document as shown in Figure 2. The database consists of three tables: a Tuples table maps each tuple to a unique identifier; BigramCounts stores the cooccurrence frequency, a count for a pair of tuples and a window k; UniCounts counts the number of times each tuple was observed in the corpus. We refer to this resource as the Rel-grams database. Query Language: The relational"
W12-3019,P07-2045,0,0.0025624,"s. We make this database freely available and hope it will prove a useful resource for a wide variety of NLP tasks. 1 0.38 (bomb; explode at; ?) 0.58 0.40 (bomb; explode in; ?) 0.54 0.72 0.53 (bomb; kill; ?) 0.48 0.60 0.51 (?; kill; people) (bomb; wound; ?) 0.43 0.57 (?; detonate; bomb) 0.54 0.40 (bomb; destroy; ?) Figure 1: Part of a sub-graph that Rel-grams discovers showing relational tuples strongly associated with (bomb; kill; ?) Introduction The Google N-grams corpus (Brants and Franz, 2006) has enjoyed immense popularity in NLP and has proven effective for a wide range of applications (Koehn et al., 2007; Bergsma et al., 2009; Lin et al., 2010). However, it is a lexical resource and provides only local, sentence-level information. It does not capture the flow of semantic content within a larger document or even in neighboring sentences. We introduce the novel Rel-grams database1 containing corpus statistics on frequently occurring sequences of open-domain relational tuples. Relgrams is analogous to n-grams except that instead of word sequences within a sentence, it tabulates relation sequences within a document. Thus, we expect Rel-grams to model semantic and discourselevel regularities in th"
W12-3019,lin-etal-2010-new,0,0.0630824,"and hope it will prove a useful resource for a wide variety of NLP tasks. 1 0.38 (bomb; explode at; ?) 0.58 0.40 (bomb; explode in; ?) 0.54 0.72 0.53 (bomb; kill; ?) 0.48 0.60 0.51 (?; kill; people) (bomb; wound; ?) 0.43 0.57 (?; detonate; bomb) 0.54 0.40 (bomb; destroy; ?) Figure 1: Part of a sub-graph that Rel-grams discovers showing relational tuples strongly associated with (bomb; kill; ?) Introduction The Google N-grams corpus (Brants and Franz, 2006) has enjoyed immense popularity in NLP and has proven effective for a wide range of applications (Koehn et al., 2007; Bergsma et al., 2009; Lin et al., 2010). However, it is a lexical resource and provides only local, sentence-level information. It does not capture the flow of semantic content within a larger document or even in neighboring sentences. We introduce the novel Rel-grams database1 containing corpus statistics on frequently occurring sequences of open-domain relational tuples. Relgrams is analogous to n-grams except that instead of word sequences within a sentence, it tabulates relation sequences within a document. Thus, we expect Rel-grams to model semantic and discourselevel regularities in the English language. 1 (?; claim; responsi"
W12-3019,P08-1000,0,\N,Missing
W16-1307,D13-1043,0,0.061796,"al., 2008) systems output relational tuples from text without a pre-specified relational vocabulary by identifying relation phrases present in text. Early work on Open IE (Etzioni et al., 2011) focused on verb-mediated relations that could be expressed using a handful of patterns and still covered substantial information in text. Subsequent research has focused on increasing recall – a noteworthy approach (O LLIE) uses bootstrapping for learning general language patterns (Mausam et al., 2012). Various extensions improve on the amount of linguistic knowledge in the systems – E XEMPLAR (de S´a Mesquita et al., 2013) improves the set of rules on top of dependency parses; Open IE 4.01 uses carefully designed rules over 1 Mausam Indian Institute of Technology New Delhi, India mausam@cse.iitd.ac.in semantic role labeling systems (Christensen et al., 2011); several works attempt clause identification or sentence restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches include use of lexicosyntactic qualia-based patterns (Xavier et al., 2015), simple sentence-spe"
W16-1307,D12-1048,1,0.633066,"red to R EL N OUN 1.1, a publicly available nominal Open IE system. 1 Introduction Open Information Extraction (Etzioni et al., 2008) systems output relational tuples from text without a pre-specified relational vocabulary by identifying relation phrases present in text. Early work on Open IE (Etzioni et al., 2011) focused on verb-mediated relations that could be expressed using a handful of patterns and still covered substantial information in text. Subsequent research has focused on increasing recall – a noteworthy approach (O LLIE) uses bootstrapping for learning general language patterns (Mausam et al., 2012). Various extensions improve on the amount of linguistic knowledge in the systems – E XEMPLAR (de S´a Mesquita et al., 2013) improves the set of rules on top of dependency parses; Open IE 4.01 uses carefully designed rules over 1 Mausam Indian Institute of Technology New Delhi, India mausam@cse.iitd.ac.in semantic role labeling systems (Christensen et al., 2011); several works attempt clause identification or sentence restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussm"
W16-1307,schmidek-barbosa-2014-improving,0,0.256965,"rapping for learning general language patterns (Mausam et al., 2012). Various extensions improve on the amount of linguistic knowledge in the systems – E XEMPLAR (de S´a Mesquita et al., 2013) improves the set of rules on top of dependency parses; Open IE 4.01 uses carefully designed rules over 1 Mausam Indian Institute of Technology New Delhi, India mausam@cse.iitd.ac.in semantic role labeling systems (Christensen et al., 2011); several works attempt clause identification or sentence restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches include use of lexicosyntactic qualia-based patterns (Xavier et al., 2015), simple sentence-specific inference (Bast and Haussmann, 2014), and a supervised approach using tree kernels (Xu et al., 2013). While the focus on verbs continues to be common in these Open IE systems, some works have directed attention on noun-mediated relations such as O L LIE (Mausam et al., 2012), R E N OUN (Yahya et al., 2014), and R EL N OUN.2 A common observation is that many relations (e.g, capital of, economist at) are more frequently expres"
W16-1307,N13-1107,0,0.0460962,"ed rules over 1 Mausam Indian Institute of Technology New Delhi, India mausam@cse.iitd.ac.in semantic role labeling systems (Christensen et al., 2011); several works attempt clause identification or sentence restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches include use of lexicosyntactic qualia-based patterns (Xavier et al., 2015), simple sentence-specific inference (Bast and Haussmann, 2014), and a supervised approach using tree kernels (Xu et al., 2013). While the focus on verbs continues to be common in these Open IE systems, some works have directed attention on noun-mediated relations such as O L LIE (Mausam et al., 2012), R E N OUN (Yahya et al., 2014), and R EL N OUN.2 A common observation is that many relations (e.g, capital of, economist at) are more frequently expressed using nouns, instead of verbs. Common noun-mediated patterns include appositive constructions, possessive constructions, and compound noun phrases (see Table 1 for examples). While most patterns give some syntactic cues for the existence of a relation (such as a comma"
W16-1307,D14-1038,0,0.175747,"nce restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013). Other approaches include use of lexicosyntactic qualia-based patterns (Xavier et al., 2015), simple sentence-specific inference (Bast and Haussmann, 2014), and a supervised approach using tree kernels (Xu et al., 2013). While the focus on verbs continues to be common in these Open IE systems, some works have directed attention on noun-mediated relations such as O L LIE (Mausam et al., 2012), R E N OUN (Yahya et al., 2014), and R EL N OUN.2 A common observation is that many relations (e.g, capital of, economist at) are more frequently expressed using nouns, instead of verbs. Common noun-mediated patterns include appositive constructions, possessive constructions, and compound noun phrases (see Table 1 for examples). While most patterns give some syntactic cues for the existence of a relation (such as a comma or a possessive ’s), interpreting and extracting tuples from compound NPs is specifically challenging, since they are just a continuous sequence of nouns and adjectives (e.g., “Google CEO Larry Page”). This"
