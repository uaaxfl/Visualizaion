2020.aacl-main.16,C14-1001,0,0.0121497,"Model World Model World Model Learning DDQ Human Conversational Data Supervised Learning Acting Direct RL User Real Experience User Estimator OPPA Imitation Learning ?? ( ?$? ?? # Policy Model Direct RL Acting User Real Experience Figure 1: A comparison of dialogue policy learning a) with real/simulated user, b) with real user via DDQ and c) with real user guided by active user estimation. more efficient but also improve the target agent’s performance. In agreement with the findings from cognitive science, humans often maintain models of other people they interact with to capture their goals (Harper, 2014; Premack and Woodruff, 1978). And humans manage to use their mental process to simulate others’ behavior (Gordon, 1986; Gallese and Goldman, 1998). Therefore, to carefully treat and model the behaviors of other agents would be full of potential. For example, in competitive tasks such as chess, the player often sees a number of moves ahead by considering the possible reaction of the other player. In goal-oriented dialogues for a hotel booking task, the agent can reduce interaction numbers and improve user experience by modeling users as business travellers with strict time limit or backpackers"
2020.aacl-main.16,2020.sigdial-1.36,0,0.0249627,"selines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage human conversation data to build a user simulator, and then to learn the di"
2020.aacl-main.16,W18-5007,0,0.0131494,"et al., 2016) is a widely applied rule-based method, which starts with a randomly generated user goal that is unknown to the system. During a dialogue session, it remains a stack data structure known as user agenda, which holds some pending user intentions to achieve. In the stack update process, machine learning or expert-defined methods can be applied. There are also some model-based methods that learn a simulator from real conversation data. The seq2seq framework has recently been introduced by encoding dialogue history and generates the next response or dialogue action (Asri et al., 2016; Kreyssig et al., 2018). By incorporating a variational step to the seq2seq network, it can introduce meaningful diversity into the simulator (G¨ur et al., 2018). Our work tackles the problem from a different point of view. We let the target agent approximate an opposite agent model to save user simulation efforts. 3 Model In this section, we introduce our proposed OPPA model. There are two agents in our framework, one is the system agent we want to optimize, and the other is the user agent. We refer to these two agents as target and opposite agents in the following sections. Note that the proposed model works at di"
2020.aacl-main.16,D17-1259,0,0.377378,"goal-oriented dialogue dataset, which contains 7 domains, 13 intents and 25 slot types. There are 10,483 sessions and 71,544 turns, which is at least one order of magnitude larger than previous annotated task-oriented dialogue dataset. Among all the dialogue sessions, we used 1,000 each for validation and test. Specifically, in the data collection stage, the user follows a specific goal to converse with the agent but is encouraged to change his/her goal dynamically during the session, which makes the dataset more challenging. For the competitive task, we used a bilateral negotiation dataset (Lewis et al., 2017), where there are 5,808 dialogues from 2,236 scenarios. In each session, there are two people negotiating to divide some items, such as books, hats and balls. Each kind of item is of different value to each person, thus they can give priority to valuable items in the negotiation. For example, a hat may worth 5 for person A and 3 for person B, so B can give up some hat in order to get other valuable items. To conduct our experiment, we further labeled the dataset with system dialogue actions. 126 4.2 We implemented the model using PyTorch (Paszke et al., 2017). The hyper-parameters were decided"
2020.aacl-main.16,P17-1045,0,0.0298629,"Missing"
2020.aacl-main.16,I17-1074,0,0.0698595,"Missing"
2020.aacl-main.16,W16-3613,0,0.0349863,"Missing"
2020.aacl-main.16,N18-1187,0,0.0128112,"ogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage"
2020.aacl-main.16,P18-1203,0,0.390173,"this problem is to leverage human conversation data to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about"
2020.aacl-main.16,D17-1237,0,0.0139645,"ied in dialogue policy learning, including DQN (Mnih et al., 2015) and Policy Gradient (Sutton et al., 2000) methods, which mitigate the problem of domain adaptation through function approximation and representation learning (Zhao and Eskenazi, 2016). Recently, there are some efforts focused on multi-domain dialogue policy. An intuitive way is to learn independent policies for each specific domain and aggregate them (Wang et al., 2014; Gaˇsi´c 123 et al., 2015; Cuay´ahuitl et al., 2016). There are also some works using hierarchical RL, which decomposes the complex task into several sub-tasks (Peng et al., 2017; Casanueva et al., 2018) according to pre-defined domain structure and cross-domain constraints. Nevertheless, most of the above works regard the opposite agent as part of the environment without explicitly modeling its behavior. Planning based RL methods are also introduced to make a trade-off between reducing human interaction cost and learning a more realistic simulator. Peng et al. (2018) proposed to use Deep Dynamic Q-network, in which a world model is co-trained with the target policy model. By training the world model with the real system-human interaction data, it consistently approac"
2020.aacl-main.16,D18-1416,0,0.335433,"leverage human conversation data to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about the simulated ag"
2020.aacl-main.16,2020.acl-main.59,1,0.806324,"Missing"
2020.aacl-main.16,D19-1010,1,0.81731,"ance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage human conversation data to build a user s"
2020.aacl-main.16,D14-1007,0,0.0261301,"2007). However, these methods require manual work to define features and state representation, which leads to poor domain adaptation. More recently, deep learning methods are applied in dialogue policy learning, including DQN (Mnih et al., 2015) and Policy Gradient (Sutton et al., 2000) methods, which mitigate the problem of domain adaptation through function approximation and representation learning (Zhao and Eskenazi, 2016). Recently, there are some efforts focused on multi-domain dialogue policy. An intuitive way is to learn independent policies for each specific domain and aggregate them (Wang et al., 2014; Gaˇsi´c 123 et al., 2015; Cuay´ahuitl et al., 2016). There are also some works using hierarchical RL, which decomposes the complex task into several sub-tasks (Peng et al., 2017; Casanueva et al., 2018) according to pre-defined domain structure and cross-domain constraints. Nevertheless, most of the above works regard the opposite agent as part of the environment without explicitly modeling its behavior. Planning based RL methods are also introduced to make a trade-off between reducing human interaction cost and learning a more realistic simulator. Peng et al. (2018) proposed to use Deep Dyn"
2020.aacl-main.16,N07-2038,0,0.345898,"interaction data, it consistently approaches the performance of real users, which provides better simulated experience for planning. Adversarial methods are applied to dynamically control the proportion of simulated and real experience during different stages of training (Su et al., 2018; Wu et al., 2018). Still, these methods work from the opposite agents’ angle. 2.2 Dialogue User Simulation In RL-based dialogue policy learning methods, a user simulator is often required to provide affordable training environments due to the high cost of collecting real human corpus. Agenda-based simulation (Schatzmann et al., 2007; Li et al., 2016) is a widely applied rule-based method, which starts with a randomly generated user goal that is unknown to the system. During a dialogue session, it remains a stack data structure known as user agenda, which holds some pending user intentions to achieve. In the stack update process, machine learning or expert-defined methods can be applied. There are also some model-based methods that learn a simulator from real conversation data. The seq2seq framework has recently been introduced by encoding dialogue history and generates the next response or dialogue action (Asri et al., 2"
2020.aacl-main.16,P17-1062,0,0.0173839,". We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One prin"
2020.aacl-main.16,P19-1426,0,0.0435272,"Missing"
2020.aacl-main.16,P19-1364,0,0.112982,"to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about the simulated agents is still passively obtained thro"
2020.aacl-main.16,W16-3601,0,0.162786,"timation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts u"
2020.aacl-main.16,D18-1547,0,\N,Missing
2020.acl-main.100,C18-1057,1,0.889971,"Missing"
2020.acl-main.100,D18-1021,1,0.877217,"Missing"
2020.acl-main.100,D19-1025,1,0.824252,"Missing"
2020.acl-main.100,P17-1149,1,0.869886,"Missing"
2020.acl-main.100,E99-1042,0,0.410389,"expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models a"
2020.acl-main.100,W11-1601,0,0.0153766,"ne a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologi"
2020.acl-main.100,P19-1601,0,0.0500939,"oder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved sentences with a target style. Xu et al. (2018) jointly train the two steps with a neutralization module and a stylization module based on reinforcement learning. For better stylization, Zhang et al. (2018b) introduce a learned sentiment memory network, while John et al. (2019) utilize hierarchical reinforcement learning. 2.2 Zweigenbaum (2008) detect paraphrases from comparable medical corpora of specialized a"
2020.acl-main.100,P15-2011,0,0.0195333,"ed targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance"
2020.acl-main.100,W17-4902,0,0.0470796,"techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. The first group is Disentanglement methods that learn disentangled representations of style and content, and then directly manipulating these latent representations t"
2020.acl-main.100,D19-1306,0,0.0944358,"Missing"
2020.acl-main.100,P19-1041,0,0.0278074,"gma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then re"
2020.acl-main.100,E17-2068,0,0.0261161,"incorporates a phrase table into OpenNMT (Klein et al., 2017), which provides guidance for replacing complex words with their simple synonym (Shardlow and Nawaz, 2019); and (2) Unsupervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the"
2020.acl-main.100,P17-4012,0,0.0742569,"professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning betwe"
2020.acl-main.100,N19-1423,0,0.0221769,"supervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ Neu"
2020.acl-main.100,W19-8604,0,0.171898,"rs to annotate the parallel sentences between the two versions (examples shown in Figure 1). Compared with both ST and TS datasets, MSD is more challenging from two aspects: Knowledge Gap. Domain knowledge is the key factor that influences the expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and lay"
2020.acl-main.100,D19-1325,0,0.0648545,"ontent as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved senten"
2020.acl-main.100,N18-1169,0,0.329339,"the other hand, it also aims to improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without c"
2020.acl-main.100,W16-4912,0,0.0454583,"but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-"
2020.acl-main.100,P02-1040,0,0.110222,"i et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ NeuralTextSimplification/ 4.1 Baselines 1066 E2L L2E Dataset Metrics OpenNMT+PT UNTS ControlledGen DeleteAndRetrieve StyleTransformer Gold ControlledGen DeleteAndRetrieve StyleTransforme"
2020.acl-main.100,D14-1162,0,0.0827572,"Missing"
2020.acl-main.100,P18-1080,0,0.179009,"experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. T"
2020.acl-main.100,N18-1012,0,0.40865,"improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. Howev"
2020.acl-main.100,P18-2031,0,0.0241445,"s: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More p"
2020.acl-main.100,N16-1005,0,0.030617,"pertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm f"
2020.acl-main.100,P19-1037,0,0.0531654,"8), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understanding, and extracting their explanations could be helpful for TS (Shardlow and Nawaz, 2019). Del´eger and Discussion To sum up, both tasks lack parallel data for training and evaluation. This prevents researcher"
2020.acl-main.100,D18-1081,0,0.279357,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P08-1040,0,0.027235,"es) and inadequate (instances having non-simplified targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Sh"
2020.acl-main.100,D11-1038,0,0.0297356,"usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burd"
2020.acl-main.100,P12-1107,0,0.0302466,"Missing"
2020.acl-main.100,Q15-1021,0,0.0221396,"ational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples"
2020.acl-main.100,Q16-1029,0,0.0438076,"Missing"
2020.acl-main.100,C12-1177,0,0.367398,"o facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods withou"
2020.acl-main.100,D17-1062,0,0.0260756,"words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understandi"
2020.acl-main.100,N18-1138,0,0.0276499,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,D18-1138,0,0.0168144,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,C10-1152,0,0.011789,"implification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particula"
2020.acl-main.100,P18-1016,0,0.139175,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P19-1198,0,0.306698,"cs ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples shown in Figure 1)."
2020.acl-main.100,P18-1090,0,\N,Missing
2020.acl-main.135,W07-0734,0,0.04282,"th the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn (Bahdanau et al., 2014): the basic"
2020.acl-main.135,D18-1362,0,0.0304405,"east two potential future directions. First, graph structure that can accurately represent the semantic meaning of the document is crucial for our model. Although DP-based and SRL-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van Noord et al., 2018; Liu et al., 2019b) and knowledge graph-enhanced text representations (Cao et al., 2017; Yang et al., 2019). Second, our method can be improved by explicitly modeling the reasoning chains in generation of deep questions, inspired by related methods (Lin et al., 2018; Jiang and Bansal, 2019) in multi-hop question answering. Acknowledgments This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. 1471 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Nicola De Cao, Wilker Aziz"
2020.acl-main.135,P19-1629,0,0.325959,"ling better language flexibility, compared against rule-based methods. A comprehensive survey of QG can be found in Pan et al. (2019). Many improvements have been proposed since the first Seq2Seq model of Du et al. (2017): applying various techniques to encode the answer information, thus allowing for better quality answerfocused questions (Zhou et al., 2017; Sun et al., 2018; Kim et al., 2019); improving the training via combining supervised and reinforcement learning to maximize question-specific rewards (Yuan et al., 2017); and incorporating various linguistic features into the QG process (Liu et al., 2019a). However, these approaches only consider sentence-level QG. In contrast, our work focus on the challenge of generating deep questions with multi-hop reasoning over document-level contexts. Recently, work has started to leverage paragraphlevel contexts to produce better questions. Du and Cardie (2018) incorporated coreference knowledge to better encode entity connections across documents. Zhao et al. (2018) applied a gated selfattention mechanism to encode contextual information. However, in practice, semantic structure is difficult to distil solely via self-attention over the entire documen"
2020.acl-main.135,J08-2001,0,0.053577,"Missing"
2020.acl-main.135,Q18-1043,0,0.0602566,"Missing"
2020.acl-main.135,P02-1040,0,0.107103,"he supporting documents along with the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn ("
2020.acl-main.135,D14-1162,0,0.0837784,"Missing"
2020.acl-main.135,P19-1487,0,0.0222493,", What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. Learning to ask such deep questions has intrinsic research value concerning how human intelligence embodies the skills of curiosity and integration, and will have broad application in future intelligent systems. Despite a clear push towards answering deep questions (exemplified by multi-hop reading comprehension (Cao et al., 2019) and commonsense QA (Rajani et al., 2019)), generating deep questions remains un-investigated. There is thus a clear need to push QG research towards generating deep questions that demand higher cognitive skills. In this paper, we propose the problem of Deep Question Generation (DQG), which aims to generate questions that require reasoning over multiple pieces of information in the passage. Figure 1 b) shows an example of deep question which requires a comparative reasoning over two disjoint pieces of evidences. DQG introduces three additional challenges that are not captured by traditional QG systems. First, unlike generating questi"
2020.acl-main.135,W17-2603,0,0.0409227,"Missing"
2020.acl-main.135,D18-1424,0,0.249997,"xample of Deep Question Generation Figure 1: Examples of shallow/deep QG. The evidence needed to generate the question are highlighted. Introduction Question Generation (QG) systems play a vital role in question answering (QA), dialogue system, and automated tutoring applications – by enriching the training QA corpora, helping chatbots start conversations with intriguing questions, and automatically generating assessment questions, respectively. Existing QG research has typically focused on generating factoid questions relevant to one fact obtainable from a single sentence (Duan et al., 2017; Zhao et al., 2018; Kim et al., 2019), as exemplified in Figure 1 a). However, less explored has been the comprehension and reasoning aspects of questioning, resulting in questions that are shallow and not reflective of the true creative human process. People have the ability to ask deep questions about events, evaluation, opinions, synthesis, or reasons, usually in the form of Why, Why-not, How, What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy no"
2020.acl-main.135,P17-1099,0,0.0242224,"2 as an example); therefore its constituting words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from th"
2020.acl-main.135,D18-1427,0,0.0727727,"Missing"
2020.acl-main.135,P16-1008,0,0.0231203,"ng words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from the context vector ct , the decoder state st"
2020.acl-main.135,D18-1259,0,0.200913,"ncorporates an attention mechanism into the Gated Graph Neural Network (GGNN) (Li et al., 2016), to dynamically model the interactions between different semantic relations; (2) enhancing the word-level passage embeddings and the node-level semantic graph representations to obtain an unified semantic-aware passage representations for question decoding; and (3) introducing an auxiliary content selection task that jointly trains with question decoding, which assists the model in selecting relevant contexts in the semantic graph to form a proper reasoning chain. We evaluate our model on HotpotQA (Yang et al., 2018), a challenging dataset in which the questions are generated by reasoning over text from separate Wikipedia pages. Experimental results show that our model — incorporating both the use of the semantic graph and the content selection task — improves performance by a large margin, in terms of both automated metrics (Section 4.3) and human evaluation (Section 4.5). Error analysis (Section 4.6) validates that our use of the semantic graph greatly reduces the amount of semantic errors in generated questions. In summary, our contributions are: (1) the very first work, to the best of our knowledge, t"
2020.acl-main.135,C12-1030,0,\N,Missing
2020.acl-main.135,P16-1154,0,\N,Missing
2020.acl-main.135,D16-1264,0,\N,Missing
2020.acl-main.135,P17-1149,0,\N,Missing
2020.acl-main.135,D17-1090,0,\N,Missing
2020.acl-main.135,P18-1177,0,\N,Missing
2020.acl-main.135,N19-1240,0,\N,Missing
2020.emnlp-main.515,D19-1609,0,0.0126103,"ment Graph Alignment unifies the two KGs’ representations of each channel into a unified vector space by reducing the distance between the seed equivalent entities. We separately train the four channels and ensemble their outputs afterward for final evaluation (see Section 3.5). Following Li et al. (2019), we generate negative samples of (e, e0 ) ∈ ψ s by searching the nearest entities of e (or e0 ) in the entity embedding space. We denote the final output k k hL e of the channel GC as the entity embedding e . k For each channel GC , we optimize the following objective function: 2 As shown by Andor et al. (2019), BERT embedding can be used for simple numerical computation. 6358 Lk = X ( X k k [d(ek , e0 ) − d(ek− , e0 ) + γ]+ (e,e0 )∈ψ s e− ∈NS(e) + X k k [d(ek , e0 ) − d(e, e0 − ) + γ]+ ) e0− ∈NS(e0 ) (3) where ψ s is the seed set of equivalent entities, NS(e) denotes the negative samples of e; [·]+ = max{·, 0}, d(·, ·) = 1 − cos(·, ·) is the cosine distance, and γ is a margin hyperparameter. 3.5 Channel Ensemble We use the entity embedding of each channel 0 to infer the similarity matrices Sk ∈ R|E|×|E | k (k ∈ {1, 2, 3, 4}), where Ske,e0 = cos(ek , e0 ) is the cosine similarity score between e ∈ E"
2020.emnlp-main.515,C18-1057,1,0.846295,"demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/ thunlp/explore-and-evaluate. 1 Introduction The prosperity of data mining has spawned Knowledge Graphs (KGs) in many domains that are often complementary to each other. Entity Alignment (EA) provides an effective way to integrate the complementary knowledge in these KGs into a unified KG by linking equivalent entities, thus benefiting knowledge-driven applications such as Question Answering (Yang et al., 2017, 2018), Recommendation (Cao et al., 2019b) and Information Extraction (Kumar, 2017; Cao et al., 2018). However, EA is a non-trivial task that it could be formulated as * Corresponding author. a quadratic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1("
2020.emnlp-main.515,P17-1149,1,0.908382,"Missing"
2020.emnlp-main.515,P19-1140,1,0.800608,"ent subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/ thunlp/explore-and-evaluate. 1 Introduction The prosperity of data mining has spawned Knowledge Graphs (KGs) in many domains that are often complementary to each other. Entity Alignment (EA) provides an effective way to integrate the complementary knowledge in these KGs into a unified KG by linking equivalent entities, thus benefiting knowledge-driven applications such as Question Answering (Yang et al., 2017, 2018), Recommendation (Cao et al., 2019b) and Information Extraction (Kumar, 2017; Cao et al., 2018). However, EA is a non-trivial task that it could be formulated as * Corresponding author. a quadratic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a val"
2020.emnlp-main.515,N19-1423,0,0.00910856,"two GNN layers. Next, we describe attributed value encoder and mean aggregator in details. 3.3.1 Attributed Value Encoder Attributed value encoder can selectively gather discriminative information from the initial feature of attributes and values to the central entity. As an example, we show how to obtain e’s first layer hidden state h1e . The same method applies to all the entities. We obtain the sequence of attribute features {a1 , · · · , an } and value features {v1 , · · · , vn } given the attribute triples {(e, a1 , v1 ), · · · , (e, an , vn )} of e as inputs. Specifically, we use BERT (Devlin et al., 2019) to obtain the features of both literal and digital values2 . BERT is a language model that is pre-trained on a more than 3000M words corpora. It is popularly used as a feature extractor in NLP tasks. By adding (1) oj = LeakyReLU(uT [h0e ; aj ]), where j ∈ {1, · · · , n}, W1 ∈ RDh1 ×(Da +Dv ) and u ∈ R(De +Da )×1 are learnable matrices, σ is the ELU(·) function, and h0e is the initial entity feature. 3.3.2 Mean Aggregator Mean aggregator layer utilizes the features of the target entity and its neighbors to generate the entity embedding. The neighbor entities of e are defined by relation triple"
2020.emnlp-main.515,2020.acl-main.89,0,0.0330725,"Missing"
2020.emnlp-main.515,D19-1274,1,0.887955,"2019) and MultiKE (Zhang et al., 2019) encode values as extra entity embeddings. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value enco"
2020.emnlp-main.515,D18-1259,0,0.067005,"Missing"
2020.emnlp-main.515,D18-1032,0,0.0846666,"ic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1(c)), and the predicate is often called attribute. Most of the previous EA models (Sun et al., 2017; Wang et al., 2018; Wu et al., 2019a) rely on the structure assumption that, the adjacencies of two equivalent entities in KGs usually contain equivalent entities (Wang et al., 2018) (see Figure 1(a)). These models mainly focus on modeling KG structure defined by the relation triples. However, we argue that attribute triples can also provide important clues for judging whether two entities are the same, based on the attribute assumption that: equivalent entities often share similar attributes and values in KGs. For example, in Figure 1(b), the equivalent entities e and e0 share the attribute Area with similar v"
2020.emnlp-main.515,D19-1023,0,0.144565,"em (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1(c)), and the predicate is often called attribute. Most of the previous EA models (Sun et al., 2017; Wang et al., 2018; Wu et al., 2019a) rely on the structure assumption that, the adjacencies of two equivalent entities in KGs usually contain equivalent entities (Wang et al., 2018) (see Figure 1(a)). These models mainly focus on modeling KG structure defined by the relation triples. However, we argue that attribute triples can also provide important clues for judging whether two entities are the same, based on the attribute assumption that: equivalent entities often share similar attributes and values in KGs. For example, in Figure 1(b), the equivalent entities e and e0 share the attribute Area with similar values of 153, 909"
2020.emnlp-main.515,2020.acl-main.578,0,0.297622,"gs. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value encoder to deal with various types of attribute triples. In this section, we firs"
2020.emnlp-main.515,P19-1304,0,0.133774,"a entity embeddings. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value encoder to deal with various types of attribute triples. In thi"
2020.emnlp-main.564,D14-1179,0,0.0128654,"Missing"
2020.emnlp-main.564,N19-1423,0,0.038154,"tion, we use M LP to mean multilayer perceptron, ⊕ to represent the concatenation operation, and bold symbols to denote dense representations. Base Model We now detail the base model which consists of 1 an encoder and a decoder. The encoder (part in Figure 2) processes the input (i.e., Q and E) into hidden representation (denoted as h) and the 2 in Figure 2) generates the SQL decoder (part query (i.e, S) accordingly. Encoder: Following (Hwang et al., 2019; Guo et al., 2019; Zhang et al., 2019), we concatenate the input query Q and database schema E to an integrated sequence as input for BERT (Devlin et al., 2019) to generate embeddings for each question token and element in the schema (namely Q = {qi }|Q| i=1 and E = {ei }|E| i=1 ) and the overall representation for the input as h. Here, E consists of embeddings of all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plau"
2020.emnlp-main.564,P16-1004,0,0.0162855,"es and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It works in two steps: in s"
2020.emnlp-main.564,P18-1068,0,0.0255061,"research on the areas of problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to cons"
2020.emnlp-main.564,2020.emnlp-main.521,0,0.0284301,"proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pr"
2020.emnlp-main.564,P19-1444,0,0.0722991,"linking for future developments of text-to-SQL tasks.1 1 Introduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the"
2020.emnlp-main.564,D18-1190,0,0.0119239,"e idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first step towards treating sch"
2020.emnlp-main.564,D19-1624,0,0.0124777,"plex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annota"
2020.emnlp-main.564,J82-2003,0,0.69635,"Missing"
2020.emnlp-main.564,P19-1335,0,0.0220677,"modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string mat"
2020.emnlp-main.564,D15-1166,0,0.059739,"Missing"
2020.emnlp-main.564,P14-5010,0,0.00440625,"Missing"
2020.emnlp-main.564,D18-1299,0,0.0236196,"Missing"
2020.emnlp-main.564,D17-1127,0,0.016122,"Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first"
2020.emnlp-main.564,P18-1034,0,0.0162153,"f problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output w"
2020.emnlp-main.564,2020.acl-main.677,0,0.116538,"ntroduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking"
2020.emnlp-main.564,P18-1134,0,0.0208586,"intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/t"
2020.emnlp-main.564,P17-1041,0,0.0399859,"f all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It w"
2020.emnlp-main.564,N18-2093,0,0.272729,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1193,0,0.254085,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1425,0,0.139882,"Missing"
2021.acl-long.254,D19-1609,0,0.0221019,"Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and"
2021.acl-long.254,D13-1160,0,0.0535475,"million 5 QA Datasets Currently, there are many datasets for QA tasks, focusing on text, or KB/table. Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et"
2021.acl-long.254,2020.emnlp-main.549,0,0.0472191,"Missing"
2021.acl-long.254,2020.findings-emnlp.91,0,0.0300115,"Missing"
2021.acl-long.254,P17-2057,0,0.0158771,"ss hybrid data. Our dataset is publicly available for noncommercial use at https://nextplusplus. github.io/TAT-QA/. 1 Introduction Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., ∗ Corresponding author 2018; Zhang and Balog, 2019; Zhang et al., 2020). Though receiving growing interests (Das et al., 2017; Sun et al., 2019; Chen et al., 2020b, 2021), works on hybrid data comprising of unstructured text and structured or semi-structured KB/tables are rare. Recently, Chen et al. (2020b) attempt to simulate a type of hybrid data through manually linking table cells to Wiki pages via hyperlinks. However, such connection between table and text is relatively loose. In the real world, a more common hybrid data form is, the table (that usually contains numbers) is more comprehensively linked to text, e.g., semantically related or complementary. Such hybrid data are very pervasive in various scenarios"
2021.acl-long.254,N19-1246,0,0.0177281,"TAT-QA. Concretely, each cell in the table of TAT-QA is regarded as “linked” with associated paragraphs of this table, like hyperlinks in the original paper, and we only use its cell matching mechanism to link the question with the table cells in its linking stage. The selected cells and paragraphs are fed into the RC model in the last stage to infer the answer. For ease of training on TAT-QA, we also omit the prediction of the scale, i.e. we regard the predicted scale by this model as always correct. 4.2 Evaluation Metrics We adopt the popular Exact Match (EM) and numeracy-focused F1 score (Dua et al., 2019) to measure model performance on TAT-QA. However, the original implementation of both metrics is insensitive to whether a value is positive or negative in the answer as the minus is omitted in evaluation. Since this issue is crucial for correctly interpreting numerical values, especially in the finance domain, we keep the plus-minus of a value when calculating them. In addition, the numeracy-focused F1 score is set to 0 unless the predicted number multiplied by predicted scale equals exactly the ground truth. 4.3 Results and Analysis In the following, we report our experimental results on dev"
2021.acl-long.254,2020.acl-main.398,0,0.0414543,"Missing"
2021.acl-long.254,D17-1084,0,0.0211097,"2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TAT-QA, the close relation between table and paragraphs and numerical reasoning are required. We also propose a baseline"
2021.acl-long.254,P14-1026,0,0.0188711,"ables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TAT-QA, the close relation between table and paragraphs and numerical reasoni"
2021.acl-long.254,P18-1133,1,0.823949,"ken t in the paragraph, the probability of the tag is computed as tag = softmax(FFN(ht )) (1) where FFN is a two-layer feed-forward network with GELU (Hendrycks and Gimpel, 2016) activation and ht is the representation of sub-token t. TAG O P Model 3.2 We introduce a novel QA model, named TAG O P, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020). This step is analogy to slot filling or schema linking, whose effectiveness has been demonstrated in dialogue systems (Lei et al., 2018; Jin et al., 2018) and semantic parsing (Lei et al., 2020). And then TAG O P performs symbolic reasoning over them with a set of aggregation OPerators to arrive at the final answer. The overall architecture is illustrated in Figure 2. 3.1 Text Table 2: Question distribution regarding different answer types and sources in TAT-QA pt Table 1: Basic statistics of each split in TAT-QA 3 Span Spans Counting Arithmetic Total Table Sequence Tagging Given a question, TAG O P first extracts supporting evidences from its hybrid context (i.e. the table and associated paragraphs) via sequence tagging with"
2021.acl-long.254,2020.emnlp-main.564,1,0.806868,"Missing"
2021.acl-long.254,2020.coling-main.238,1,0.889855,"Missing"
2021.acl-long.254,P17-1015,0,0.0152871,"18). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TAT-QA, the close relation between table and paragraphs and numerical reasoning are required. We also propose a baseline model TAG O P based"
2021.acl-long.254,2021.ccl-1.108,0,0.0678348,"Missing"
2021.acl-long.254,P16-1202,0,0.0247679,"ng, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TAT-QA, the close relation between table and paragraphs and numerical reasoning are required. We als"
2021.acl-long.254,P15-1142,0,0.0146305,"r KB/table. Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra an"
2021.acl-long.254,D16-1264,0,0.0444727,"hs should be describing, analysing or complementing the content in the table. If yes, then all the surrounding paragraphs will be associated to this table. Otherwise, the table will be skipped (discarded).3 Question-Answer Pair Creation Based on the valid hybrid contexts, the annotators are then asked to create question-answer pairs, where the questions need to be useful in real-world financial analyses. In addition, we encourage them to create questions that can be answered by people without much finance knowledge and use common words instead of the same words appeared in the hybrid context (Rajpurkar et al., 2016). Given one hybrid context, at least 6 questions are generated, including extracted and calculated questions. For extracted questions, the answers can be a single span or multiple spans from either the table or the associated paragraphs. For calculated questions, numerical reasoning is required to produce the answers, including addition, subtraction, multiplication, division, counting, comparison/sorting and their compositions. Furthermore, we particularly ask the annotators to annotate the right scale for the numerical answer when necessary. Answer Type and Derivation Annotation The answers i"
2021.acl-long.254,W95-0107,0,0.0679331,"ei et al., 2020). And then TAG O P performs symbolic reasoning over them with a set of aggregation OPerators to arrive at the final answer. The overall architecture is illustrated in Figure 2. 3.1 Text Table 2: Question distribution regarding different answer types and sources in TAT-QA pt Table 1: Basic statistics of each split in TAT-QA 3 Span Spans Counting Arithmetic Total Table Sequence Tagging Given a question, TAG O P first extracts supporting evidences from its hybrid context (i.e. the table and associated paragraphs) via sequence tagging with the Inside–Outside tagging (IO) approach (Ramshaw and Marcus, 1995). In particular, it assigns each token either I or O label and takes Aggregation Operator Next, we perform symbolic reasoning over obtained evidences to infer the final answer, for which we apply an aggregation operator. In our TAG O P, there are ten types of aggregation operators. For each input question, an operator classifier is applied to decide which operator the evidences would go through; for some operators sensitive to the order of input numbers, an auxiliary number order classifier is used. The aggregation operators are explained as below, covering most reasoning types as listed in Fi"
2021.acl-long.254,D19-1251,0,0.0873638,"n the hybrid context. Gscale uses the annotated scale of the answer; Gorder is needed when the groundtruth operator is one of Difference, Division and Change ratio, which is obtained by mapping the two operands extracted from their corresponding ground-truth deviation in the input sequence. If their order is the same as that in the input sequence, Gorder = 0; otherwise it is 1. 4 Experiments and Results 4.1 Baselines Textual QA Models We adopt two reading comprehension (RC) models as baselines over textual data: BERT-RC (Devlin et al., 2018), which is a SQuAD-style RC model; and NumNet+ V2 4 (Ran et al., 2019), which achieves promising performance on DROP that requires numerical reasoning over textual data. We adapt them to our TAT-QA as follows. We convert the table to a sequence by row, also as input to the models, followed by tokens from the paragraphs. Besides, we add a multi-class classifier, exactly as in our TAG O P, to enable the two models to predict the scale based on Eq. (4). Tabular QA Model We employ TaPas for WikiTableQuestion (WTQ) (Herzig et al., 2020) as a baseline over tabular data. TaPas is pretrained over large-scale tables and associated text from Wikipedia jointly for table pa"
2021.acl-long.254,2020.emnlp-main.248,0,0.0253544,"sitive are combined as a span. During testing, all positive cells and spans are taken as the supporting evidences. Formally, for each sub-token t in the paragraph, the probability of the tag is computed as tag = softmax(FFN(ht )) (1) where FFN is a two-layer feed-forward network with GELU (Hendrycks and Gimpel, 2016) activation and ht is the representation of sub-token t. TAG O P Model 3.2 We introduce a novel QA model, named TAG O P, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020). This step is analogy to slot filling or schema linking, whose effectiveness has been demonstrated in dialogue systems (Lei et al., 2018; Jin et al., 2018) and semantic parsing (Lei et al., 2020). And then TAG O P performs symbolic reasoning over them with a set of aggregation OPerators to arrive at the final answer. The overall architecture is illustrated in Figure 2. 3.1 Text Table 2: Question distribution regarding different answer types and sources in TAT-QA pt Table 1: Basic statistics of each split in TAT-QA 3 Span Spans Counting Arithmetic Total Table Sequence Tagging Given a question,"
2021.acl-long.254,D19-1242,0,0.0157433,"r dataset is publicly available for noncommercial use at https://nextplusplus. github.io/TAT-QA/. 1 Introduction Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., ∗ Corresponding author 2018; Zhang and Balog, 2019; Zhang et al., 2020). Though receiving growing interests (Das et al., 2017; Sun et al., 2019; Chen et al., 2020b, 2021), works on hybrid data comprising of unstructured text and structured or semi-structured KB/tables are rare. Recently, Chen et al. (2020b) attempt to simulate a type of hybrid data through manually linking table cells to Wiki pages via hyperlinks. However, such connection between table and text is relatively loose. In the real world, a more common hybrid data form is, the table (that usually contains numbers) is more comprehensively linked to text, e.g., semantically related or complementary. Such hybrid data are very pervasive in various scenarios like scientific re"
2021.acl-long.254,N18-1059,0,0.0202425,"s Currently, there are many datasets for QA tasks, focusing on text, or KB/table. Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Bal"
2021.acl-long.254,D18-1259,0,0.040651,"Missing"
2021.acl-long.254,P15-1128,0,0.0257878,"ny datasets for QA tasks, focusing on text, or KB/table. Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arith"
2021.acl-long.254,2020.acl-main.745,0,0.0169732,", 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts. 6 Table 6: Examples of error and corresponding percentage. Q, G, P denote question, ground truth, prediction. Related Work Conclusion We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TA"
2021.acl-long.254,D18-1425,0,0.0221973,"il (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Lin"
2021.acl-long.359,D18-1245,0,0.0749324,"ion (DSRE) (Mintz et al., 2009) is proposed and becomes increasingly popular as it can automatically generate large-scale labeled data. DSRE is based on a simple yet effective principle: if there is a relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 2020) leveraged attention to select informative instances from a bag. Furthermore, researchers introduced KG embeddings to enhance the attention mechanism (Hu et al., 2019; Han et al., 2018a). The basic idea is to utilize entity embeddings as the query to compute attention scores, so that the sentences with high attention weights are more likely to be valid annotations (Zhang et al., 2019). Previous studies have shown performance gain on DSRE with attention module and KG embeddings, however, it’s still not clear how these mechanisms work, and, are there any limitations to apply th"
2021.acl-long.359,D18-1514,0,0.197928,"relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 2020) leveraged attention to select informative instances from a bag. Furthermore, researchers introduced KG embeddings to enhance the attention mechanism (Hu et al., 2019; Han et al., 2018a). The basic idea is to utilize entity embeddings as the query to compute attention scores, so that the sentences with high attention weights are more likely to be valid annotations (Zhang et al., 2019). Previous studies have shown performance gain on DSRE with attention module and KG embeddings, however, it’s still not clear how these mechanisms work, and, are there any limitations to apply them? In this paper, we aim to provide a thorough and quantitative analysis about the impact of both attention mechanism and KG on DSRE. By analyzing several public benchmarks including NYT-FB60K (Han et"
2021.acl-long.359,P11-1055,0,0.0719403,"wikimedia.org/wikidatawiki/entities/20201109/ • We have conducted extensive experiments to inspire and support us with the above findings. • We demonstrate that a straightforward method based on the findings can achieve improvements on public datasets. 2 Related Work To address the issue of insufficient annotations, Mintz et al. (2009) proposed distant supervision to generate training data automatically, which also introduces much noise. From then, DSRE becomes a standard solution that relies on multi-instance learning from a bag of sentences instead of a single sentence (Riedel et al., 2010; Hoffmann et al., 2011). Attention mechanism (Lin et al., 2016) accelerates this trend via strong ability in handling noisy instances within a bag (Liu et al., 2017; Du et al., 2018). Aside from intra-bag attention, Ye and Ling (2019) also designed inter-bag attention simultaneously handling bags with the same relation. To deal with only-one-instance bags, Li et al. (2020) utilized a new selective gate (SeG) framework to independently assign weights to each sentence. External KG is also incorporated to enhance the attention module (Han et al., 2018a; Hu et al., 2019). However, due to the lack of sentencelevel ground"
2021.acl-long.359,D19-1395,0,0.151099,"e: if there is a relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 2020) leveraged attention to select informative instances from a bag. Furthermore, researchers introduced KG embeddings to enhance the attention mechanism (Hu et al., 2019; Han et al., 2018a). The basic idea is to utilize entity embeddings as the query to compute attention scores, so that the sentences with high attention weights are more likely to be valid annotations (Zhang et al., 2019). Previous studies have shown performance gain on DSRE with attention module and KG embeddings, however, it’s still not clear how these mechanisms work, and, are there any limitations to apply them? In this paper, we aim to provide a thorough and quantitative analysis about the impact of both attention mechanism and KG on DSRE. By analyzing several public benchmarks including"
2021.acl-long.359,2020.acl-main.579,0,0.025865,"relation types in G, and T = {(h, r, t)} ⊆ E × R × E denotes the set of triples. KG embedding models, e.g., RotatE (Sun et al., 2019), can preserve the structure information in the learned vectors eh , et and er . We adopt TransE (Bordes et al., 2013) in experiments. Bag-level relation extraction (RE) takes a bag of sentences B = {s1 , s2 , . . . , sm } as input. Each sentence si in the bag contains the same entity pair (h, t), where h, t ∈ E. The goal is to predict a relation y ∈ R between (h, t). Attention-based Bag-level RE uses attention to assign a weight to each sentence within a bag. 2 Shahbazi et al. (2020) claim to annotate each positive bag in NYT-FB60K, but haven’t published their code and dataset. 4663 Given a bag B from the dataset D, an encoder is first used to encode all sentences from B into vectors {s0 1 , s0 2 , . . . , s0 m } separately. Then, an attention module computes an attention weight αi for each sentence and outputs the weighted sum of {s0 i } as s to denote B: 4 Benchmark To quantitatively evaluate the effect of attention and KG on Bag-level RE, we first define two metrics to measure the noise pattern (Section 4.1). Then, we construct a KG and a Bag-level RE dataset (Section"
2021.acl-long.359,P19-1279,0,0.0164918,"indicates that a model makes better prediction on the whole bag, valid sentences and noisy sentences, respectively. 4666 5 Method 6.1 To evaluate the effects of attention and KG, we design two straightforward Bag-level RE models without the attention module, BRE and BRE+CE. By comparing their performance with BRE+ATT (BRE with attention module) and BRE+KA (BRE with KG-enhanced attention module), we can have a better understanding of the roles of ATT and Knowledge-enhanced ATT. BRE uses BERT (Devlin et al., 2018) as the encoder. Specifically, we follow the way described in (Peng et al., 2020; Soares et al., 2019): entity mentions in sentences are highlighted with special markers before and after mentions. Then the concatenation of head and tail entity representations are used as the representation s0 . Since BRE does not have attention mechanism, it breaks the bags and compute loss on each sentence: L=− |Bi | n X X log(P (yi |sij )) (15) P (yi |sij ) = softmax(Wb s0ij + bb ) (16) i=1 j=1 BRE can be viewed as a special case of BRE+ATT. Its attention module assigns all sentences in all bags with the same attention weight 1. During inference, given a bag, BRE uses the mean of each sentence’s prediction a"
2021.acl-long.359,N19-1288,0,0.0612252,"ieve improvements on public datasets. 2 Related Work To address the issue of insufficient annotations, Mintz et al. (2009) proposed distant supervision to generate training data automatically, which also introduces much noise. From then, DSRE becomes a standard solution that relies on multi-instance learning from a bag of sentences instead of a single sentence (Riedel et al., 2010; Hoffmann et al., 2011). Attention mechanism (Lin et al., 2016) accelerates this trend via strong ability in handling noisy instances within a bag (Liu et al., 2017; Du et al., 2018). Aside from intra-bag attention, Ye and Ling (2019) also designed inter-bag attention simultaneously handling bags with the same relation. To deal with only-one-instance bags, Li et al. (2020) utilized a new selective gate (SeG) framework to independently assign weights to each sentence. External KG is also incorporated to enhance the attention module (Han et al., 2018a; Hu et al., 2019). However, due to the lack of sentencelevel ground truth, it is difficult to quantitatively evaluate the performance of the attention module. Previous researchers tend to provide examples as case study.2 Therefore, we aim to fill in this research gap by constru"
2021.acl-long.359,D15-1203,0,0.0319915,"ationship between entities. To save the manual annotation cost and alleviate the issue of data scarcity, distant supervision relation extraction (DSRE) (Mintz et al., 2009) is proposed and becomes increasingly popular as it can automatically generate large-scale labeled data. DSRE is based on a simple yet effective principle: if there is a relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 2020) leveraged attention to select informative instances from a bag. Furthermore, researchers introduced KG embeddings to enhance the attention mechanism (Hu et al., 2019; Han et al., 2018a). The basic idea is to utilize entity embeddings as the query to compute attention scores, so that the sentences with high attention weights are more likely to be valid annotations (Zhang et al., 2019). Previous studies have shown performance gain on DSRE wi"
2021.acl-long.359,N19-1306,0,0.0304574,"Missing"
2021.acl-long.359,P16-1200,0,0.163929,"istant supervision relation extraction (DSRE) (Mintz et al., 2009) is proposed and becomes increasingly popular as it can automatically generate large-scale labeled data. DSRE is based on a simple yet effective principle: if there is a relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 2020) leveraged attention to select informative instances from a bag. Furthermore, researchers introduced KG embeddings to enhance the attention mechanism (Hu et al., 2019; Han et al., 2018a). The basic idea is to utilize entity embeddings as the query to compute attention scores, so that the sentences with high attention weights are more likely to be valid annotations (Zhang et al., 2019). Previous studies have shown performance gain on DSRE with attention module and KG embeddings, however, it’s still not clear how these mechanisms work, and, are"
2021.acl-long.359,D17-1189,0,0.01541,"onstrate that a straightforward method based on the findings can achieve improvements on public datasets. 2 Related Work To address the issue of insufficient annotations, Mintz et al. (2009) proposed distant supervision to generate training data automatically, which also introduces much noise. From then, DSRE becomes a standard solution that relies on multi-instance learning from a bag of sentences instead of a single sentence (Riedel et al., 2010; Hoffmann et al., 2011). Attention mechanism (Lin et al., 2016) accelerates this trend via strong ability in handling noisy instances within a bag (Liu et al., 2017; Du et al., 2018). Aside from intra-bag attention, Ye and Ling (2019) also designed inter-bag attention simultaneously handling bags with the same relation. To deal with only-one-instance bags, Li et al. (2020) utilized a new selective gate (SeG) framework to independently assign weights to each sentence. External KG is also incorporated to enhance the attention module (Han et al., 2018a; Hu et al., 2019). However, due to the lack of sentencelevel ground truth, it is difficult to quantitatively evaluate the performance of the attention module. Previous researchers tend to provide examples as"
2021.acl-long.359,P09-1113,0,0.0636161,"l-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at https://github.com/zigkwin-hu/how-KG-ATT-help. 1 Figure 1: Examples of disturbing bags in NYT-FB60K. Introduction Relation Extraction (RE) is crucial for Knowledge Graph (KG) construction and population. Most recent efforts rely on neural networks to learn efficient features from large-scale annotated data, thus correctly extract the relationship between entities. To save the manual annotation cost and alleviate the issue of data scarcity, distant supervision relation extraction (DSRE) (Mintz et al., 2009) is proposed and becomes increasingly popular as it can automatically generate large-scale labeled data. DSRE is based on a simple yet effective principle: if there is a relation between two entities in KG, then all sentences containing mentions of both entities are Although effective, distant supervision may introduce noise to a sentence bag when the assumption fails — some sentences are not describing the target relation (Zeng et al., 2015) (a.k.a. noisy annotation). To alleviate the negative impacts of noise, recent studies (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Li et al., 202"
2021.acl-long.359,2020.emnlp-main.298,0,0.0548888,"Missing"
2021.findings-acl.196,D15-1075,0,0.0462323,". In particular, we devise a generation module to generate representative counterfactual samples for each factual sample, and a retrospective module to retrospect the model prediction by comparing the counterfactual and factual samples. Extensive experiments on sentiment analysis (SA) and natural language inference (NLI) validate the effectiveness of our method. 1 Introduction Language understanding (Ke et al., 2020) is a central theme of artificial intelligence (Chomsky, 2002), which empowers a wide spectral of applications such as sentiment evaluation (Feldman, 2013), commonsense inference (Bowman et al., 2015). The models are trained on labeled data to recognize the textual patterns closely correlated to different labels. Owing to the extraordinary representational capacity of deep neural networks, the models can well recognize the pattern and make prediction accordingly (Devlin et al., 2019). However, the cognitive ability of these data-driven models is still far from human beings due to lacking counterfactual thinking (Pearl, 2019). Counterfactual thinking is a high-level cognitive ability beyond pattern recognition (Pearl, 2019). In addition to observing the patterns within factual ∗ Correspondi"
2021.findings-acl.196,N19-1423,0,0.597947,"d natural language inference (NLI) validate the effectiveness of our method. 1 Introduction Language understanding (Ke et al., 2020) is a central theme of artificial intelligence (Chomsky, 2002), which empowers a wide spectral of applications such as sentiment evaluation (Feldman, 2013), commonsense inference (Bowman et al., 2015). The models are trained on labeled data to recognize the textual patterns closely correlated to different labels. Owing to the extraordinary representational capacity of deep neural networks, the models can well recognize the pattern and make prediction accordingly (Devlin et al., 2019). However, the cognitive ability of these data-driven models is still far from human beings due to lacking counterfactual thinking (Pearl, 2019). Counterfactual thinking is a high-level cognitive ability beyond pattern recognition (Pearl, 2019). In addition to observing the patterns within factual ∗ Corresponding author. samples, counterfactual thinking calls for comparing the fact with imaginations, so as to make better decision. For instance, given a factual sample “What do lawyers do when they die? Lie still.”, the intuitive evaluation of its sentiment based on the textual patterns will rec"
2021.findings-acl.196,2020.emnlp-main.12,0,0.0697023,"Missing"
2021.findings-acl.196,P18-1029,0,0.0260193,"on the NLI task. RI represents the relative performance improvement over the +CF method. adversarial training (Khashabi et al., 2020) enhances the model robustness against perturbations and attacks, which are hard samples for normally trained models. Debiased training (Tu et al., 2020; Utama et al., 2020) eliminates the spurious correlation or bias in training data to enhance the generalization ability and deal with out-of-distribution samples. In addition to the training phase, a few inference techniques might improve the model performance on hard samples, including posterior regularization (Srivastava et al., 2018) and causal inference (Yu et al., 2020; Niu et al., 2021). However, both techniques require domain knowledge such as prior or causal graph tailored for specific applications. On the contrary, this work provides a general paradigm that can be used for most language understanding tasks. 6 Conclusion In this work, we pointed out the issue of standard inference of existing language understanding models. We proposed a Counterfactual Reasoning Model which empowers the trained model with a highlevel cognitive ability, counterfactual thinking. By applying the proposed CRM, we formed a new paradigm for"
2021.findings-acl.196,2020.acl-main.519,0,0.0373447,"by confidence, a feeling of being wrong or right (Boldt et al., 2019). From the perspective of model confidence, we investigate the performance of language understanding models across different testing samples. We estimate the model confidence on a sample as the widely used Maximum Class Probability (MCP) (Corbi`ere et al., 2019), which is the probability over the predicted class. A lower value of MCP means less confidence and “hard” sample. According to the value of MCP, we rank the testing (b) Natural language inference Problem Formulation As discussed in the previous work (Wu et al., 2020; Li et al., 2020, 2019), language understanding tasks can be abstracted as a classification problem where the input is a text and the target is to make decision across a set of candidates of interests. We follow the problem setting with consideration of counterfactual samples (Kaushik et al., 2019; Liang et al., 2020), where the training data are twofold: 1) factual samples T = {(x, y)} where y ∈ [1, C] denotes the class or the target decision of the text; x ∈ RD is the latent representation of the text, which encodes the textual contents1 . 2) counterfactual samples 1 The input is indeed the plain text which"
2021.findings-acl.196,P19-1129,0,0.0620118,"Missing"
2021.findings-acl.196,2020.emnlp-main.265,0,0.434169,"y (MCP) (Corbi`ere et al., 2019), which is the probability over the predicted class. A lower value of MCP means less confidence and “hard” sample. According to the value of MCP, we rank the testing (b) Natural language inference Problem Formulation As discussed in the previous work (Wu et al., 2020; Li et al., 2020, 2019), language understanding tasks can be abstracted as a classification problem where the input is a text and the target is to make decision across a set of candidates of interests. We follow the problem setting with consideration of counterfactual samples (Kaushik et al., 2019; Liang et al., 2020), where the training data are twofold: 1) factual samples T = {(x, y)} where y ∈ [1, C] denotes the class or the target decision of the text; x ∈ RD is the latent representation of the text, which encodes the textual contents1 . 2) counterfactual samples 1 The input is indeed the plain text which is projected to a latent representation by an encoder (e.g., a Transformer (Devlin et al., 2019)) in the cutting edge solutions. We omit the 2227 T ∗ = {(x∗c , c)|(x, y) ∈ T , c ∈ [1, C]&c 6= y} where (x∗c , c) is a counterfactual sample in class c corresponds to the factual sample (x, y)2 . We assume"
2021.findings-acl.196,2020.tacl-1.40,0,0.0700697,"Missing"
2021.findings-acl.196,2020.acl-main.770,0,0.0387431,"Missing"
2021.findings-acl.196,N18-1101,0,0.0338066,"ntational capacity of language model, fine-tuning pre-trained language model has become the emergent technique for solving language understanding tasks (Devlin et al., 2019). We select the widely used RoBERTa-base5 and RoBERTalarge6 for the consideration of the robustness of the RoBERTa (Liu et al., 2019) and our limited computation resources. For SA, we also test the classical Multi-Layer Perceptron (MLP) (Teney et al., 2020) with tf-idf text features (Sch¨utze et al., 2008) as inputs. For NLI, we further test RoBERTa-largenli7 , which has been fine-tuned on the large-scale MultiNLI dataset (Williams et al., 2018). Baselines. As the proposed CRM leverages counterfactual samples, we compare CRM with three representative methods using counterfactual samples in language understanding tasks: 1) +CF (Kaushik et al., 2019), which uses counterfactual samples as data augmentation for model training; 2) +GS (Teney et al., 2020), which compares the factual and counterfactual samples in model training through regularizing their gradients; and 3) +CL (Liang et al., 2020), which compares the factual and counterfactual samples through a contrastive loss. Moreover, we report the performance of the testing model under"
2021.findings-acl.196,2020.acl-main.622,0,0.0330048,"ally accompanied by confidence, a feeling of being wrong or right (Boldt et al., 2019). From the perspective of model confidence, we investigate the performance of language understanding models across different testing samples. We estimate the model confidence on a sample as the widely used Maximum Class Probability (MCP) (Corbi`ere et al., 2019), which is the probability over the predicted class. A lower value of MCP means less confidence and “hard” sample. According to the value of MCP, we rank the testing (b) Natural language inference Problem Formulation As discussed in the previous work (Wu et al., 2020; Li et al., 2020, 2019), language understanding tasks can be abstracted as a classification problem where the input is a text and the target is to make decision across a set of candidates of interests. We follow the problem setting with consideration of counterfactual samples (Kaushik et al., 2019; Liang et al., 2020), where the training data are twofold: 1) factual samples T = {(x, y)} where y ∈ [1, C] denotes the class or the target decision of the text; x ∈ RD is the latent representation of the text, which encodes the textual contents1 . 2) counterfactual samples 1 The input is indeed the"
2021.findings-acl.196,2020.coling-main.541,0,0.0431218,"acilitate the retrospection while the generation quality can be further improved. Moreover, on the testing samples at confidence level of 1, using the generated samples achieves an accuracy of 81.3 which is much better than +CF (70.8) (cf. Figure 3). The generated samples indeed benefit the decision making over hard testing samples. 5 Related Work Counterfactual sample. Constructing counterfactual samples has become an emergent data augmentation technique in natural language processing, which has been used in a wide spectral of language understanding tasks, including SA (Kaushik et al., 2019; Yang et al., 2020), NLI (Kaushik et al., 2019), named entity recognition (Zeng et al., 2020) question answering (Chen et al., 2020), dialogue system (Zhu et al., 2020), vision-language navigation (Fu et al., 2020). Beyond data augmentation under the standard supervised learning paradigm, a line of research explores to incorporate counterfactual samples into other learning paradigms such as adversarial training (Zhu et al., 2020; Fu et al., 2020; Teney et al., 2020) and contrastive learning (Liang et al., 2020). This work lies in an orthogonal direction that incorporates counterfactual samples into the decision"
2021.findings-acl.196,2020.emnlp-main.590,0,0.483277,"ual thinking calls for comparing the fact with imaginations, so as to make better decision. For instance, given a factual sample “What do lawyers do when they die? Lie still.”, the intuitive evaluation of its sentiment based on the textual patterns will recognize “Lie still” as an objective description of body posture which is neutral. By scrutinizing that the “still” could be intentionally postposed, we can imagine a counterfactual sample “What do lawyers do when they die? Still lie.” and uncover the negative sarcastic pun, whose sentiment is more accurate. Recent work (Kaushik et al., 2019; Zeng et al., 2020) shows that incorporating counterfactual samples into model training improves the generalization ability. However, these methods follow the standard machine learning paradigm that uses the same procedure (e.g., a forward propagation) to make prediction in the testing phase. That is, making decision for testing samples according to their relative positions to the model decision boundary. The indiscriminate procedure focuses on the textual patterns occurred in the testing sample and treats all testing samples equally, which easily fails on hard samples (cf. Figure 1). On the contrary, humans can"
2021.findings-acl.196,2020.emnlp-main.276,0,0.0426291,"generated samples achieves an accuracy of 81.3 which is much better than +CF (70.8) (cf. Figure 3). The generated samples indeed benefit the decision making over hard testing samples. 5 Related Work Counterfactual sample. Constructing counterfactual samples has become an emergent data augmentation technique in natural language processing, which has been used in a wide spectral of language understanding tasks, including SA (Kaushik et al., 2019; Yang et al., 2020), NLI (Kaushik et al., 2019), named entity recognition (Zeng et al., 2020) question answering (Chen et al., 2020), dialogue system (Zhu et al., 2020), vision-language navigation (Fu et al., 2020). Beyond data augmentation under the standard supervised learning paradigm, a line of research explores to incorporate counterfactual samples into other learning paradigms such as adversarial training (Zhu et al., 2020; Fu et al., 2020; Teney et al., 2020) and contrastive learning (Liang et al., 2020). This work lies in an orthogonal direction that incorporates counterfactual samples into the decision making procedure of model inference. Counterfactual inference. A line of research attempts to enable deep neural networks with counterfactual thinkin"
2021.naacl-main.429,D19-1435,0,0.0134896,"he copying mechels, even for these hypotheses ranked to the anism is also introduced for GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from source senmore grammatical than source sentences. tence to hypothesis sentence. To further accelerate the generation process, some work also comes up • Beam search candidates can provide valuable with non-autoregressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 7"
2021.naacl-main.429,P17-1055,0,0.0316776,"ed with the verification representation Vpk : Evidence Aggregation with Node P (y|wpk ) = softmaxy (Linear((Hpk ◦ Vpk ); Hpk ; Vpk )), (10) Selection Attention where ◦ is the element-wise multiplication and ; is The node selection attention measures node importhe concatenate operator. We average all probabiltance and is used to aggregate supporting evidence ity P (y = 1|wpk ) of token level quality estimation from the fine-grained node representation V l→k of as hypothesis quality estimation score f (s, ck ) for the l-th node. We leverage attention-over-attention the pair hs, ck i: mechanism (Cui et al., 2017) to conduct source hls m+n+2 X and hypotheses hlh representations to calculate the 1 f (s, ck ) = P (y = 1|wpk ). (11) l n + 1 p=m+2 l-th node selection attention score γ . Then we get 5444 3.4 End-to-end Training We conduct joint training with token-level supervision. The source labels and hypothesis labels are used, which denote the grammatical quality of source sentences and GEC accuracy of hypotheses. The cross entropy loss for the p-th token wpk in the k-th node is calculated: L(wpk ) = CrossEntropy(y ∗ , P (y|wpk )), Sentence using the ground truth token labels Then the training loss of"
2021.naacl-main.429,N12-1067,0,0.0931344,"Missing"
2021.naacl-main.429,W13-1703,0,0.168148,"attention mechanisms on the reasoning graph, node interaction attention and node selection attention, to summarize and aggregate necessary GEC evidence from other hypotheses to estimate the quality of tokens. Our experiments show that VERNet can pick up necessary GEC evidence from multi-hypotheses provided by GEC models and help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-trained GEC models with bea"
2021.naacl-main.429,N19-1423,0,0.383601,"mploy language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (Rei, 2017; Rei and Søgaard, 2019) estimate the hypothesis quality on both sentence level (Kaneko et al., 2019) and token level (Yannakoudakis et al., 2017). Chollampatt and Ng (2018b) further estimate GEC quality by considering correction accuracy. They establish sourcehypothesis interactions with the encoder-decoder architecture and learn to directly predict the official evaluation score F0.5 . The pre-trained language model BERT (Devlin et al., 2019) has proven its effectiveness in producing contextual token representations, achieving better quality estimation (Kaneko et al., 2019; Chollampatt et al., 2019) and improving GEC performance by fuse BERT representations (Kaneko et al., 2020). However, existing quality estimation models regard each hypothesis independently and neglect the interactions among multihypotheses, which can also benefit the quality estimation (Fomicheva et al., 2020). 3 Neural Verification Network Source Sentence: &quot;: Do one who suffered from this disease … Hypotheses from the beam search decoding of basic GEC : $1: Do"
2021.naacl-main.429,C16-1079,0,0.0314161,"Missing"
2021.naacl-main.429,2020.acl-main.113,0,0.201106,"du.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Association for Computational Linguistics (a) CoNLL2014 (ann. 1). (b) CoNLL2014 (ann. 2). Figure 2: The GEC Performance of Generated Hypotheses. The hypotheses generated by Kiyono et al. (2019) are evaluated on the CoNLL2014 dataset. The average scores of Precision and Recall are calculated according to the two annotations of CoNLL2014. guistic variation (Fomicheva et al., 2020), which can be used to improve machine translation performance (Wang et al., 2019b). Fomicheva et al. (2020) further leverage multi-hypotheses to make convinced machine translation evaluation, which is more correlated with human judgments. Their work further demonstrates that multi-hypotheses from well-trained neural models have the ability to provide more hints to estimate generation quality. For GEC, the hypotheses from the beam search decoding of well-trained GEC models can provide some valuable GEC evidence. We illustrate the reasons as follows. Existing quality estimation models (Chollamp"
2021.naacl-main.429,P18-1097,0,0.0181342,"esults of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to est"
2021.naacl-main.429,W19-4427,0,0.0173378,"l. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ"
2021.naacl-main.429,2020.lrec-1.835,0,0.0201361,"g GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic a"
2021.naacl-main.429,N18-1055,0,0.0683094,"Missing"
2021.naacl-main.429,W19-4422,0,0.181403,"ks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Associ"
2021.naacl-main.429,2020.acl-main.391,0,0.699224,"hypothesis is compared to the source sentence with a BERT based language model and classified into Win (the hypothesis is better), Tie (the hypothesis and source are same) and Loss (the source is better). The ratios of different classes are plotted with different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding"
2021.naacl-main.429,D19-1119,0,0.0748377,"tion mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/ thunlp/VERNet. 0.8 0.4 0.4 0.2 0.2 0 0 1 2 3 4 5 Ranks in Beam Search (a) BEA19. 1 2 3 4 5 Ranks in Beam Search (b) CoNLL2014. Figure 1: The Grammaticality of Generated Hypotheses. The hypotheses are generated by Kiyono et al. (2019) with beam search decoding. The hypothesis is compared to the source sentence with a BERT based language model and classified into Win (the hypothesis is better), Tie (the hypothesis and source are same) and Loss (the source is better). The ratios of different classes are plotted with different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest"
2021.naacl-main.429,N19-1333,0,0.0144583,"as a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some meth"
2021.naacl-main.429,D19-1510,0,0.0261163,"Missing"
2021.naacl-main.429,I11-1017,0,0.215929,"proposes two kinds of attention mechanisms on the reasoning graph, node interaction attention and node selection attention, to summarize and aggregate necessary GEC evidence from other hypotheses to estimate the quality of tokens. Our experiments show that VERNet can pick up necessary GEC evidence from multi-hypotheses provided by GEC models and help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-tra"
2021.naacl-main.429,P15-2097,0,0.0221129,"on model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one of the state-of-the-art autoreGLEU (Napoles et al., 2015) to evaluate model gressive GEC systems (Kiyono et al., 2019) as our performance on the JFLEG dataset. The official basic GEC model and keep the same setting. The tool ERRANT of the BEA19 shared task (Bryant beam size of our baseline model is set to 5 (Kiyono et al., 2019) is used to calculate Precision, Recall, et al., 2019), and all these beam search hypotheses and F0.5 scores for other datasets. For the CoNLLare reserved in our experiments. 2014 dataset, the M2 evaluation (Dahlmeier and We generate quality estimation labels for tokens Ng, 2012) is also adopted as our main evaluation. in bot"
2021.naacl-main.429,E17-2037,0,0.0146724,"as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one of the state-of-the-art autoreGLEU (Napoles et al., 2015) to evaluate model gressive GEC systems (Kiyono et al., 2019) as our perfo"
2021.naacl-main.429,W14-1701,0,0.0601161,"d with ERRANT according to the golden correction. The words in red color are labeled as incorrect (0) and others are labeled as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one o"
2021.naacl-main.429,2020.bea-1.16,0,0.0373692,"GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from source senmore grammatical than source sentences. tence to hypothesis sentence. To further accelerate the generation process, some work also comes up • Beam search candidates can provide valuable with non-autoregressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation probl"
2021.naacl-main.429,N16-1042,0,0.0593817,"nd help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-trained GEC models with beam convinced correction results (Grundkiewicz et al., search usually win the favor of language mod2019; Kiyono et al., 2019). The copying mechels, even for these hypotheses ranked to the anism is also introduced for GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from"
2021.naacl-main.429,N19-1014,0,0.0607315,"regressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and conf"
2021.naacl-main.429,P17-1194,0,0.0253143,"). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (Rei, 2017; Rei and Søgaard, 2019) estimate the hypothesis quality on both sentence level (Kaneko et al., 2019) and token level (Yannakoudakis et al., 2017). Chollampatt and Ng (2018b) further estimate GEC quality by considering correction accuracy. They establish sourcehypothesis interactions with the encoder-decoder architecture and learn to directly predict the official evaluation score F0.5 . The pre-trained language model BERT (Devlin et al., 2019) has proven its effectiveness in producing contextual token representations, achieving better quality estimation (Kaneko et al., 2019; Chollampatt et al."
2021.naacl-main.429,D19-1412,0,0.0693247,"erence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Association for Computational Linguistics (a) CoNLL2014 (ann. 1). (b) CoNLL2014 (ann. 2). Figure 2: The GEC Performance of Generated Hypotheses. The hypotheses generated by Kiyono et al. (2019) are evaluated on the CoNLL2014 dataset. The average scores of Precision and Recall are calculated according to the two annotations of CoNLL2014. guistic variation (Fomicheva et al., 2020), which can be used to improve machine translation performance (Wang et al., 2019b). Fomicheva et al. (2020) further leverage multi-hypotheses to make convinced machine translation evaluation, which is more correlated with human judgments. Their work further demonstrates that multi-hypotheses from well-trained neural models have the ability to provide more hints to estimate generation quality. For GEC, the hypotheses from the beam search decoding of well-trained GEC models can provide some valuable GEC evidence. We illustrate the reasons as follows. Existing quality estimation models (Chollampatt and Ng, 2018b) for GEC regard hypotheses independently and neglect the potent"
2021.naacl-main.429,D19-1073,1,0.879907,"Missing"
2021.naacl-main.429,N18-1057,0,0.01776,"3.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (R"
2021.naacl-main.429,P11-1019,0,0.111359,"NUCLE CoNLL-2014 JFLEG Total Correction The 1 a 2 Mobile phone is a marvelous invention to 9 charge 10 the world 12 [SEP] Operation Span Edit Delete 1,2 Replace 9,10 change Insert 12,12 . p=1 Experimental Methodology Table 2: An Example of Token Label Annotation. All sentences are annotated with ERRANT according to the golden correction. The words in red color are labeled as incorrect (0) and others are labeled as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Ta"
2021.naacl-main.429,D17-1297,0,0.617309,"th different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–1"
C02-1080,M98-1017,0,0.173605,"Missing"
C02-1080,W99-0612,0,0.0211435,"Missing"
C02-1080,P01-1041,0,0.0648069,"Missing"
C02-1080,M98-1006,0,0.0405533,"Missing"
C02-1080,P97-1041,0,0.180986,"Missing"
C02-1080,J96-3004,0,0.0480089,"Missing"
C02-1080,M98-1016,0,0.488979,"Missing"
C04-1078,W99-0613,0,0.0303617,"each slot are generalized by traditional rule induction techniques and test instances are matched to the rules by their cosine similarities. The learning of soft pattern rules in this paper augments the soft matching method advocated by Nahm and Mooney (2001) by combining lexical tokens alongside syntactic and semantic features and adopting a probabilistic framework that combines slot content and sequential fidelity in computing the degree of pattern match. The bootstrapping scheme using the co-training (Blum and Mitchell, 1998) technique has been widely explored for IE tasks in recent years. Collins and Singer (1999) presented several techniques using co-training schemes for Named Entity (NE) extraction seeded by a small set of manually crafted NE rules. Riloff and Jones (1999) presented a multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. Our framework of combining two pattern learners is close to Niu, et al. (2003) in which two successive learners are used to learn named entities classifiers starting from a small set of co"
C04-1078,P03-1043,0,0.0187609,"co-training (Blum and Mitchell, 1998) technique has been widely explored for IE tasks in recent years. Collins and Singer (1999) presented several techniques using co-training schemes for Named Entity (NE) extraction seeded by a small set of manually crafted NE rules. Riloff and Jones (1999) presented a multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. Our framework of combining two pattern learners is close to Niu, et al. (2003) in which two successive learners are used to learn named entities classifiers starting from a small set of conceptbased seed words. The bootstrapping procedure is implemented as training a decision list and an HMM classifier sequentially. The HMM classifier uses the training corpus automatically, tagged by the first learner, i.e., the decision list learner. Our work differs from Niu, et al. (2003) in two ways. First, we repeat the automatic annotation process until it satisfies the stopping criteria. Second, we apply different patterns (hard and soft pattern rules) in both the training and te"
C04-1078,P03-1044,0,0.0351262,"c features and adopting a probabilistic framework that combines slot content and sequential fidelity in computing the degree of pattern match. The bootstrapping scheme using the co-training (Blum and Mitchell, 1998) technique has been widely explored for IE tasks in recent years. Collins and Singer (1999) presented several techniques using co-training schemes for Named Entity (NE) extraction seeded by a small set of manually crafted NE rules. Riloff and Jones (1999) presented a multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. Our framework of combining two pattern learners is close to Niu, et al. (2003) in which two successive learners are used to learn named entities classifiers starting from a small set of conceptbased seed words. The bootstrapping procedure is implemented as training a decision list and an HMM classifier sequentially. The HMM classifier uses the training corpus automatically, tagged by the first learner, i.e., the decision list learner. Our work differs from Niu, et al. (2003) in two ways. First"
C04-1187,C02-1026,0,0.0459098,"Missing"
C04-1187,P02-1054,0,0.0775725,"Missing"
C04-1187,P02-1006,0,0.0773343,"Missing"
C10-1130,H05-1076,0,0.0229808,"rule induction method to detect interrogative questions in email conversations based on part-of-speech features. Yeh and Yuan (2003) used a statistical approach to extract a set of question-related words and derived some syntax and semantic rules to detect mandarin question sentences. Cong et al. (2008) extracted labeled sequential patterns and used them as features to learn a classifier for question detection in online forums. Question pattern mining is also closely related to the learning of answer patterns. Work on answer patterns includes the web based pattern mining (Zhang and Lee, 2002; Du et al., 2005) and a combination of syntactic and semantic elements (Soubbotin and Soubbotin, 2002) etc. In contrast to previous work, we do not only focus on standard language corpus, but extensively explore characteristics of online questions. Our approach exploits salient question patterns at both the lexical and syntactic levels for question detection. In particular, we employ the one-class SVM algorithm such that the learning process is weakly supervised and no human annotation is involved. 6 Conclusion This paper proposed a new approach to detecting question sentences in cQA. We mined both lexical and"
C10-1130,P08-1019,0,0.0235616,"Missing"
C10-1130,P07-1059,0,0.0511116,"Missing"
C10-1130,C04-1128,0,0.0331216,"nt features at both the lexical and syntactic levels is comprehensive enough to capture various forms of questions online, and hence improve the performance of question matching. 5 Related Work Research on detecting question sentences can generally be classified into two categories. The first category simply employs rule-based methods such as question mark, 5W1H words, or handcrafted regular expressions to detect questions. As discussed, these conventional methods are not adequate to cope with online questions. The second category uses machine learning approaches to detect question sentences. Shrestha and McKeown (2004) proposed a supervised rule induction method to detect interrogative questions in email conversations based on part-of-speech features. Yeh and Yuan (2003) used a statistical approach to extract a set of question-related words and derived some syntax and semantic rules to detect mandarin question sentences. Cong et al. (2008) extracted labeled sequential patterns and used them as features to learn a classifier for question detection in online forums. Question pattern mining is also closely related to the learning of answer patterns. Work on answer patterns includes the web based pattern mining"
C12-1035,P12-1056,0,0.0170258,"h are partitioned into two subsets: a labeled set M l = {m1 , m2 , . . . , m L } and an unlabeled set M u = {m L+1 , m L+2 , . . . , m L+N }. M l includes only the example messages provided through user interaction, where each instance is associated with a predefined category ci with belonging to C = {c1 , c2 . . . cK }; while M u includes all the other messages. We aim to predict the category label for each data point in M u . Here we assume that each tweet belongs to only one category. Similar idea of assigning a single topic or category to a short sequence of words has been used before in (Diao et al., 2012) (Gruber et al., 2007) (Zhao et al., 2011). 2.1 The General Framework We now introduce the overview of the whole processing that aims to classify microblogging messages by exploiting the internal and external resources. The workflow consists of three phrases, as shown in Figure 1. It includes the preprocessing of external resources, preprocessing of microblogging messages, and construction of Semi-Supervised Bayesian Network (SSBN) model. Phrase 1: Preprocessing of External Resources Due to their short length, microblogging messages do not provide sufficient word co-occurrence or context share"
C12-1035,D09-1026,0,0.692779,"tive similarity measure (Hu et al., 2009). The data sparseness hinders general machine learning methods to achieve desirable accuracy. Second, microblogging messages are not well conformed as standard structures of documents. Sometimes they do not even obey grammatical rules (Hu and Liu, 2012b). Third, microblogs lack label information. It is time and labor consuming to label the huge amounts of messages. Intensive efforts have been made on the classification of short texts utilizing machine learning techniques (Nie et al., 2011). Some representative research efforts are based on topic model (Ramage et al., 2009) (Zhao et al., 2011). As these approaches heavily rely on the term co-occurrence information, the sparsity of short and informal messages unduly influence the significant improvement of the performance. Some others explore some traditional supervised learning methods to classify microblogging messages (Lee et al., 2011) (Zubiaga et al., 2011) (Sriram et al., 2010) (Tang et al., 2012). The sparsity problem again hinders the similarity measurement. Moreover, it is laborious and time consuming to obtain labeled data from microblogging. Consequently, new approaches towards microblog classification"
C12-1190,de-marneffe-etal-2006-generating,0,0.0176657,"Missing"
C12-1190,P08-1019,0,0.125719,"Search results comparison between TLM and drTLM for query “How do you charge a farad capacitor?”.Questions in bold font are relevant ones. 5 Related Work The existing IR technologies are frequently based on Bag-of-Words models and regard both the query and documents in collections as composition of individual and independent words. For example, Ponte et al. (Ponte and Croft, 1998) utilized unigram language model for information retrieval. Jones et al. (Jones et al., 2000) proposed the binary independent retrieval (BIR) model to capture the relevance between queries and documents. Duan et al. (Duan et al., 2008) proposed a new language model to capture the relation between question topic and focus. They may not be directly applicable in the question retrieval domain due to at least two reasons. First, compared to the simple keywords based search, the querying questions are usually represented in natural language and depict some concepts linked by intrinsic semantic relationships. Second, the to be searched documents are also questions, which are far shorter than the verbose documents in traditional search approaches. Jeon et al. (Jeon et al., 2005a,b), moving forward one step, provided comparison of"
C12-1190,N03-1032,0,0.0464781,"citor”, hence, the dependency relation path length d r_path_l en equals to 1 as shown in Figure 1(b) (1). However, “charge” is a bit farther away from the term “farad” as the d r_path_l en between term “charge” and “farad” equals to 2 as shown in Figure 1(b) (3). This implies that “charge” should be weighted more closely with “capacitor” than with “farad”. 2.2 Dependency based Closeness Estimation for Pairwise Terms Several existing methods can be employed to compute the closeness between pairwise terms, such as pointwise mutual information (pmi), Chi and mutual information (Gao et al., 2004; Terra and Clarke, 2003). However, few of them take the syntactic dependency into consideration. Instead our approach estimates the dependency relevance of term pairs by linearly integrating multi-faceted cues, i.e., dependency relation path analysis as well as probabilistic analysis. First, from the perspective of dependency relation path, we denote d r_path_l en(t i , t j ) as the length of dependency relation path between term t i and t j . The dependency relevance can be defined as: 1 (1) Dep(t i , t j ) = d r_path_len(t ,t ) i j b where b is a constant larger than 1, which is selected based on a development set"
D11-1013,H05-1044,0,\N,Missing
D11-1013,D09-1159,0,\N,Missing
D11-1013,C92-2082,0,\N,Missing
D11-1013,H05-2017,0,\N,Missing
D11-1013,H05-1043,0,\N,Missing
D11-1013,P06-1093,0,\N,Missing
D11-1013,P09-2045,0,\N,Missing
D11-1013,C04-1200,0,\N,Missing
D11-1013,P10-1031,0,\N,Missing
D11-1013,P10-2060,0,\N,Missing
D11-1013,J06-1005,0,\N,Missing
D11-1013,P10-2023,0,\N,Missing
D11-1013,P06-1101,0,\N,Missing
D11-1013,P08-1119,0,\N,Missing
D11-1013,P08-1036,0,\N,Missing
D11-1013,P02-1053,0,\N,Missing
D11-1013,P09-1031,0,\N,Missing
D11-1013,P09-1028,0,\N,Missing
D11-1013,P05-1015,0,\N,Missing
D11-1013,W02-1011,0,\N,Missing
D11-1013,P09-1079,0,\N,Missing
D11-1013,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
D11-1013,E06-1039,0,\N,Missing
D11-1013,W06-3808,0,\N,Missing
D12-1036,E06-1039,0,0.0267899,"f positive and negative reviews. This helps user to quickly get an overview of public opinions. The summary of relevant review sentences is then presented in the answer. The answer diversely comments the asked aspect and all its avail399 able sub-aspects following the general-to-specific logic. Moreover, we feel that the answers are informative and readable. 5 Related Works In this section, we review existing works related to the four components of our approach, including organization of reviews, question analysis, answer fragment retrieval, and answer generation. For organization of reviews, Carenini et al. (2006) proposed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Simila"
D12-1036,O07-1013,0,0.08542,"reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The paper argued that the subjective types (i.e. sentiment and arguing) can improve the performance of opinion-QA. Later, Ku et al. (2007) proposed a two-layered classifier for question analysis, and retrieved the answer-fragments by keyword matching. In particular, they first identified the opinion questions, and classified them into six predefined question types, including holder, target, attitude, reason, majority, and yes/no. These question types and corresponding polarity on the questions were used to filter non-relevant sentences in the answer fragments. F1 -measure was employed as the evaluation metric. For the topic of answer generation in opinion-QA, Li et al. (2009) formulated it as a sentence ranking task. They argued"
D12-1036,P03-1069,0,0.07451,"Missing"
D12-1036,N03-1020,0,0.293887,"Missing"
D12-1036,W11-1722,0,0.107776,"esolution” would help users better understand that the public complaints on the aspect “camera” are due to the poor “lens” and/or low “resolution.” Moreover, the answer should be presented following the generalto-specific logic, i.e., from general aspects to specific sub-aspects. This makes the answer easier to understand by the users (Ouyang et al., 2009). Current Opinion-QA methods mainly include three components, including question analysis that identifies aspects and opinions asked in the questions, answer fragment retrieval, and answer generation which summarizes the retrieved fragments (Lloret et al., 2011). Although existing methods show encouraging performance, they are usually not able to generate satisfactory answers due to the following drawbacks. First, current methods often identify aspects as the noun phrases in the questions. However, noun phrases contain noises that are not aspects. This gives rise to imprecise aspect identification. For example, in the question “What reasons can I persuade my wife that people prefer the battery of Nokia N95?” noun phrases “wife” and “people” are not aspects. Moreover, current methods relied on noun phrases are not able to reveal the implicit aspects,"
D12-1036,C10-2105,0,0.0165366,", 2008). φ(i) is the post time of sentence i. We normalize it to [0, 1]. 396 Grammaticality: The grammatical sentence is often more readable. We employ the method in Agichtein et al. (2008) to calculate the grammar score. In particular, φ(i) is calculated by the KLdivergence between language models of sentence i to Wikipedia articles. Position: The first sentence in a review is usually informative (He et al., 2011). φ(i) is computed based on the position of the sentence in the review, i.e. φ(i) = 1/positioni . Aspect Frequency: The sentence that contains the frequent aspects is often salient (Nishikawa et al., 2010). Hence, φ(i) is computed as the sum of the frequency for aspects in sentence i. Centroid Distance: As aforementioned, review sentences are stored in the corresponding aspect nodes in the hierarchy. The sentence that is close to the centroid of the reviews stored in an aspect node is more likely to be salient (Erkan et al., 2004). φ(i) is computed as the Cosine similarity between sentence i to the corresponding review cluster centroid based on the unigram features. Local Density: The sentence would be informative when it is in the dense part of the aspect node in the feature space (Scott et al"
D12-1036,P09-2029,0,0.0329403,"Missing"
D12-1036,H05-1116,0,0.194376,"Missing"
D12-1036,H05-1044,0,0.017502,"duct hierarchy. The classification greedily searches a path in the hierarchy from top to down. The search begins at the root node, and stops at the leaf node or a specific node where the relevance score is lower than a pre-defined threshold. The relevance score on each node is determined by a SVM classifier. Multiple SVM classifiers are learned on the hierarchy, one distinct classifier for a node. The reviews that are stored in the node and its child-nodes are used as training samples. We employ the features of noun terms, and sentiment terms in the sentiment lexicon provided by MPQA project (Wilson et al., 2005). Classifying the opinions: Given a set of testing questions, we first distinguish the opinion questions from the factual ones (Yu et al., 2003). Since the opinion questions often contain one or more sentiment terms, we classify them by employing the sentiment terms in the sentiment lexicon provided from MPQA project (Wilson et al., 2005). Subsequently, we learn a SVM sentiment classifier to determine the opinion polarity of the opinion questions. In particular, the reviews and corresponding opinions stored in the hierarchy are used as training samples, which are represented by the unigram fea"
D12-1036,D11-1013,1,0.834929,"standing the answers. To overcome these problems, we can resort to 392 the hierarchical organization of consumer reviews on products. As illustrated in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. Such hierarchy can naturally facilitate to identify aspects asked in questions. While explicit aspects can be recognized by referring to the hierarchy, implicit aspects can be inferred based on the associations between sentiment terms and aspects in the hierarchy (Yu et al., 2011). The sentiment terms are discovered from the reviews on corresponding aspects. Moreover, by following the parent-child relations in the hierarchy, sub-aspects of the asked aspect can be directly acquired, and the answers can present aspects from general to specific. Motivated by the above observations, we propose to exploit the hierarchical organization of consumer reviews for product opinion-QA. As illustrated in Figure 1, our framework first organizes consumer reviews of a certain product into a hierarchical organization. The resulting hierarchy is in turn used to help question analysis and"
D12-1036,P11-1150,1,0.865663,"standing the answers. To overcome these problems, we can resort to 392 the hierarchical organization of consumer reviews on products. As illustrated in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. Such hierarchy can naturally facilitate to identify aspects asked in questions. While explicit aspects can be recognized by referring to the hierarchy, implicit aspects can be inferred based on the associations between sentiment terms and aspects in the hierarchy (Yu et al., 2011). The sentiment terms are discovered from the reviews on corresponding aspects. Moreover, by following the parent-child relations in the hierarchy, sub-aspects of the asked aspect can be directly acquired, and the answers can present aspects from general to specific. Motivated by the above observations, we propose to exploit the hierarchical organization of consumer reviews for product opinion-QA. As illustrated in Figure 1, our framework first organizes consumer reviews of a certain product into a hierarchical organization. The resulting hierarchy is in turn used to help question analysis and"
D12-1036,W03-1017,0,\N,Missing
D12-1036,J06-4002,0,\N,Missing
D12-1036,P09-1083,0,\N,Missing
D12-1036,N07-1056,0,\N,Missing
D12-1073,P11-2118,0,0.0276656,"Missing"
D12-1073,D09-1026,0,\N,Missing
D18-1015,D16-1044,0,0.0528433,"word hsn and the t-th video frame hvt is determined by not only the content of the video and sentence but also their interaction status. Thus, we design one network to compute the relevance score of one video frame with respect to each word: 3.2.2 Interaction LSTM (i-LSTM) In order to accurately ground the sentence in a video, the multimodal interation behaviors between the video and sentence need to be comprehensively modeled. Previous approaches on multimodal interactions were limited to concatenation (Zhu et al., 2016), element-wise product or sum (Gao et al., 2017), and bilinear pooling (Fukui et al., 2016). These methods are not expressive enough since they ignore the evolving fine-grained interactions across video and sentence, particularly the frame-by-word interactions. In this paper, we propose a novel multimodal interaction model, which is realized by LSTM. We term it interaction LSTM (i-LSTM), which sequentially processes the video sequence frame by frame, holding deep interactions with the words in the sentence. In order to well capture the complicated temporal interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th vi"
D18-1015,N16-1170,0,0.0198701,"word embeddings of the sentence S = {wn }N n=1 , we employ two long shortterm memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to sequentially process the two different modalities, i.e., video and sentence, independently. Specifically, one LSTM sequentially models the video V , yielding the hidden states {hvt }Tt=1 , while the other LSTM processes the sequential words in the sentence S, resulting in its corresponding hidden states {hsn }N n=1 . Owing to natural behaviors and characteristics of LSTMs, both {hvt }Tt=1 and {hsn }N n=1 can encode and aggregate the contextual evidences (Wang and Jiang, 2016b) from the sequential video frame representations and word embeddings of the sentence, respectively, meanwhile casting aside the irrelevant information. 3.2.1 Frame-Specific Sentence Feature Directly operating on the clip-level and sentencelevel features generated by the encoders cannot well exploit the frame-by-word relationships between video and sentence that evolve over time. Inspired by (Wang and Jiang, 2016a; Feng et al., 2018), we introduce one novel frame-specific sentence feature, which adaptively summarizes the hidden states of the sentence {hsn }N n=1 with respect to the t-th video"
D18-1015,P14-5010,0,0.00269515,"Performance comparisons of different methods on DiDeMo. The best performance for each metric entry is highlighted in boldface. R@5 IoU=1 mIoU MFP MCN-VGG16 MCN-Flow MCN-Fusion MCN-Fusion+TEF TGN-VGG16 TGN-Flow TGN-Fusion 19.40 13.10 18.35 19.88 28.10 24.28 27.52 28.23 66.38 44.82 56.25 62.39 78.21 71.43 76.94 79.26 26.65 25.13 31.46 33.51 41.08 38.62 42.84 42.97 comparing with specific baseline methods, we use the same features as baseline methods, specifically, VGG16 and optical flow for MCN and C3D for CTRL, VSA-RNN, and VSA-STV. For sentences, we tokenize each sentence by Stanford CoreNLP (Manning et al., 2014) and use the 300-D word embeddings from GloVe (Pennington et al., 2014) to initialize the models. The words not found in GloVe are initialized as zero vectors. The hidden state dimensions of all LSTMs (including the video, sentence, and interaction LSTMs) are set as 512. We use the Adam (Kingma and Ba, 2014) optimizer with β1 = 0.5 and β2 = 0.999. The initial learning rate is set to 0.001. We train the network for 200 iterations, and the learning rate is gradually decayed over time. The mini-batch size is set to 64. 4.2.2 Evaluation Metrics A grounding of one natural sentence in a video is con"
D18-1015,P13-1006,0,0.0299226,"r to exploit the evolving fine-grained frameby-word interactions. Then, TGN works on the yielded interaction status to simultaneously score a set of temporal candidates of multiple scales and finally localize the video segment that corresponds to the sentence. More importantly, our proposed TGN is able to analyze an untrimmed video frame by frame without resorting to handling overlapping temporal video segments. 2 2.1 2.2 Grounding Natural Language in Video Analogous to spatial grounding in image, this work studies a similar problem—temporal natural language grounding in video. Earlier works (Yu and Siskind, 2013; Lin et al., 2014) learn the semantics of sentences, which are then matched to visual concepts via exploiting object appearance, motion and spatial relationships. However, they are limited to a small set of objects. Recently, larger datasets (Gao et al., 2017; Hendricks et al., 2017) are constructed to support more flexible groundings. The methods proposed in (Gao et al., 2017; Hendricks et al., 2017) learn a common embedding space shared by video segment features and sentence representations, in which their similarities are measured. Specifically, moment context network (MCN) (Hendricks et a"
D18-1015,P17-1117,0,0.0238144,"nd the words in the sentence interact attentively to perform fine-grained frame-by-word matchings for grounding sentence in video. Introduction We examine the task of Natural Sentence Grounding in Video (NSGV). Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sentence, as shown in Figure 1 (a). Comparing with the other video researches, such as bidirectional video-sentence retrieval (Xu et al., 2015b), video attractiveness prediction (Chen et al., 2018, 2016), and video captioning (Pasunuru and Bansal, 2017; Wang et al., 2018a,b), NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging. Recently, several related works (Gao et al., 2017; Hendricks et al., 2017) leverage one temporal sliding window approach over video sequences to generate video segment candidates, which are then independently combined (Gao et al., 2017) or compared (Hendricks et al., 2017) with the given sentence to make the grounding prediction. Although the existing works have achieved promising performances, they ar"
D18-1015,D14-1162,0,0.0800836,"Missing"
D18-1015,Q13-1003,0,0.254091,"eters, and σ denotes the nonlinear sigmoid function. 3.4 T X X 4 Experiments In this section, we evaluate the effectiveness of our proposed TGN on the NSGV task. We begin by describing the datasets used for evaluation, followed by the introduction of the experimental settings including the baselines, configurations, as well as the evaluation metrics. Afterwards, we demonstrate the effectiveness of TGN by comparing with the state-of-the-art approaches and efficiency through a runtime test. 4.1 Datasets We experiment on three publicly accessible datasets: DiDeMo (Hendricks et al., 2017), TACoS (Regneri et al., 2013), and ActivityNet Captions (Fabian Caba Heilbron and Niebles, 2015). These datasets consist of videos as well as their associated temporally annotated sentences. DiDeMo2 consists of 10464 25-50 second long videos. The same split provided by (Hendricks et al., 2017) is used for a fair comparison, with 33008, 4180, and 4022 video-sentence pairs for training, validation, and testing, respectively. w0k ytk log ckt + w1k (1 − ytk ) log(1 − ckt ), (6) k=1 where the weights w0k and w1k are calculated according to the frequencies of positive and negative samples in the training set with length kδ. ytk"
D19-1025,C18-1057,1,0.811724,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,D18-1021,1,0.843847,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,P17-1149,1,0.905559,"Missing"
D19-1025,P19-1140,1,0.887209,"Missing"
D19-1025,Q16-1026,0,0.0350068,"mation, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2"
D19-1025,R11-1017,0,0.030182,"languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262 O B-ORG I-ORG … O … O B"
D19-1025,D14-1162,0,0.0817182,"Missing"
D19-1025,W03-0426,0,0.243404,"Missing"
D19-1025,D18-1230,0,0.367798,"tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challenging. To address these i"
D19-1025,Q13-1001,0,0.132177,"Missing"
D19-1025,K16-1022,0,0.0256046,"vy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without f"
D19-1025,N16-1030,0,0.0729178,"damental task of extracting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et"
D19-1025,P17-1004,1,0.804855,"hose entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tives of se"
D19-1025,D18-1034,0,0.17044,"I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since it is difficult for"
D19-1025,P18-1074,1,0.911104,"t> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since i"
D19-1025,P17-1114,0,0.0456129,"Missing"
D19-1025,P16-1101,0,0.396346,"acting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et"
D19-1025,D17-1269,0,0.130772,"1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data"
D19-1025,C18-1183,0,0.328348,"tional Linguistics tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challengin"
D19-1025,P17-1135,0,0.153128,"E8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of we"
D19-1025,U08-1016,0,0.0450237,"gging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262"
D19-1025,P17-1178,1,0.902871,"XwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical"
D19-1025,N16-1029,1,0.847236,"Fs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model"
D19-1025,P16-2025,0,0.167213,"3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words"
D19-1274,C18-1057,1,0.815118,"nd its neighbors in two heterogeneous KGs (Dashed circles in the same color means prior alignments). Recently, many Knowledge Graphs (KGs) (e.g., DBpedia (Lehmann et al., 2015), YAGO (Rebele et al., 2016) and BabelNet (Navigli and Ponzetto, 2012)) have emerged to provide structural knowledge for different applications. These separately constructed KGs contain heterogeneous but complementary contents; thus integrating KGs from different sources or languages into a unified KG becomes essential to better benefit knowledgedriven applications, ranging from information extraction (Han et al., 2018; Cao et al., 2018) to question answering (Cui et al., 2017). Corresponding author New York (state) adjo Introduction ∗ locatedIn New York City Entity alignment aims at integrating complementary knowledge graphs (KGs) from different sources or languages, which may benefit many knowledge-driven applications. It is challenging due to the heterogeneity of KGs and limited seed alignments. In this paper, we propose a semi-supervised entity alignment method by joint Knowledge Embedding model and Cross-Graph model (KECG). It can make better use of seed alignments to propagate over the entire graphs with KG-based constr"
D19-1274,P15-1067,0,0.0359519,"pair (ei , ej ) ∈ S, where ei ∈ E1 , ej ∈ E2 , K2 is the number of negative samples, we choose K2 entities that are nearest to ej in E2 as the negative samples of ei , and vice versa for ej . After that, The goal of our knowledge embedding model is to model inner-graph relationships, making entities more distinguishable. Here, we use TransE (Bordes et al., 2013), which is one of the most representative translation-based methods, as our knowledge embedding model. It is worth mentioning that other advanced KG learning methods can also be applied to our knowledge embedding model such as TransD (Ji et al., 2015), which is left for future work as our main idea is to joint cross-graph embeddings and knowledge embeddings for entity alignment. Objective OK . Formally, given a relational triplet (eh , r, et ), TransE wants eh +r ' et . So it defines the score function f (eh , r, et ) = ||eh +r−et ||2 to measure the plausibility of (eh , r, et ), where ||· ||2 means 2-norm. Following TransE, we utilize a margin-based ranking loss function as the training objective of the knowledge embedding model, defined as: OK = X X (eh ,r,et )∈T (e0h ,r 0 ,e0t )∈T 0 [f (eh , r, et ) + γ2 −f (e0h , r0 , e0t )]+ (5) where"
D19-1274,D18-1032,0,0.691601,"ed models (Hao et al., 2016; Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Sun et al., 2018; Chen et al., 2018) utilize existing KG representation learning methods to learn embeddings of entities and relations in different KGs, and then align them into a unified vector space. This type of method can not only preserve KG structures, but also implicitly complete KG with the missing links from existing knowledge. However, KG-based methods require a sufficient number of seed alignments, which is usually expensive to obtain. To alleviate the burden of seed alignments, graph-based methods (Wang et al., 2018) utilize Graph Convolutional Network (GCN) (Kipf and Welling, 2017) to enhance entity embeddings with their neighbors’ information, thus can make better use of seed alignments to propagate them over 2723 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2723–2732, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the entire graph. However, GCN-based models are sensitive to structural differences of KGs (Vaswani et al., 2017) and may not perf"
E03-1070,C02-1150,0,0.11242,"Missing"
E03-1070,P01-1049,1,0.776444,"Table 2), they are actually corresponding to one or more of the QA event elements we discussed in Section 3. One promising advantage of our approach is that we are able to answer any factual questions about the elements in this QA event other than just &quot;What is the name of the volcano that destroyed the ancient city of Pompeii?&quot;. For instance, we can easily handle questions like &quot;When was the ancient city of Pompeii destroyed?&quot; and &quot;Which two 367 Roman cities were destroyed by Mount Vesuvius?&quot; etc. with the same set of knowledge. Currently, we are exploring the use of Semantic Perceptron Net (Liu & Chua 2001) to derive semantic word groups in order to form a more structured utilization of external knowledge. 5.4 Document Retrieval & Answer Selection Given q (1) , QUALIFIER makes use of the MG tool to retrieve up to N (N=50) relevant documents from the QA corpus. We choose Boolean retrieval because of the short length of the queries, and to avoid returning too many irrelevant documents when using the similarity based retrieval. If q (1) does not return sufficient number of relevant documents, the extra terms added is reduced and the Boolean search is repeated. Therefore, we successively relax the c"
E03-1070,W02-1033,0,0.0343798,"tyle QA is to overcome the mismatch in the lexical representations between the query space and document space. This mismatch, also known as the QA gap, is caused by the differences in the set of terms used in the question formulation and answer strings in the corpus. Given a source, such as the QA corpus, that contains only a relatively small number of answers to a query, we are faced with the difficulty to map the questions to answers by way of uncovering the complex lexical, syntactic, or semantic relationships between the question and the answer strings. Recent redundancy-based approaches (Brill et al 2002, Clarke et al 2002, Kwok et al 2001, Radev et al 2001) proposed the use of data, in363 stead of methods, to do most of the work to bridge the QA gap. These methods suggest that the greater the answer redundancy in the source data collection, the more likely that we can find an answer that occurs in a simple relation to the question. With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing. However, this does not mean that NLP is now out of the picture. For some question/answer pairs, deep reasoning is still needed to relate the"
E03-1070,C02-1026,0,\N,Missing
E03-1070,P01-1052,0,\N,Missing
I08-1021,P06-2027,0,0.500206,"age had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier"
I08-1021,harabagiu-etal-2002-multidocument,0,0.0241081,"land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier, 1998; Harabagiu and Lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments. Working with tree structures, Sudo et al. and Filatova et al. instead require shared subtrees. Calculating t"
I08-1021,W04-2013,0,0.0325754,"Missing"
I08-1021,W02-1006,0,0.0482409,"Missing"
I08-1021,P98-2127,0,0.060898,"Missing"
I08-1021,N04-1030,0,0.065701,"Missing"
I08-1021,W06-1603,1,0.86546,"h T [i] do for each cluster ccand , any candidate cluster that contextually related with tcontxt .cluster do P (T [i] ∈ ccand ) = comb(Ps , Pc ) likelihood = log(P (T [i] ∈ ccand )) if likelihood &gt; aBestLikelihood then aBestLikelihood = likelihood T [i].cluster = ccand tupleReassigned = true until tupleReassigned == f alse /*alignment stable*/ return During initialization, tuples whose pairwise similarity higher than a threshold τ are merged to form highly cohesive seed clusters. To compute a continuous similarity Sim(ta , tb ) of tuples ta and tb , we use the similarity measure described in (Qiu et al., 2006), which linearly combines similarities between the semantic roles shared by the two tuples. Some other tuples are related to these seed clusters by argument-similarity. These related tuples are temporarily put into a special “other” cluster. The cluster membership of these related tuples, together with those currently in the seed clusters, are to be further adjusted. The “other” cluster is so called because a tuple will end up being assigned to it if it is not found to be similar to any other tuple. Tuples that are neither similar to nor contextually related by argument-similarity to another t"
I08-1021,W98-1106,0,0.0448113,"nes indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier, 1998; Harabagiu and Lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments. Working with tree structures, Sudo et al. and Filatov"
I08-1021,P03-1029,0,0.113039,"esday. .... But Tokage had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overla"
I08-1021,A00-1039,0,0.0242911,"er the storm hit on Wednesday. .... But Tokage had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion use"
I08-1021,C98-2122,0,\N,Missing
P01-1049,H92-1045,0,0.0456567,"ms co-occur within the topic, we also group antonyms together to identify a topic. Moreover, if a word had two senses of, say, sense-1 and sense-2. And if there are two separate words that are lexically related to this word by sense-1 and sense2 respectively, we simply group these words together and do not attempt to distinguish the two different senses. The reason is because if a word is so important to be chosen as the keyword of a topic, then it should only have one dominant meaning in that topic. The idea that a keyword should have only one dominant meaning in a topic is also suggested in Church & Yarowsky (1992). synset corn maize is_a part_of member_of antonym metal tree family import zinc leaf son export perso perExamples of lexicalper Figure 3: relationship Based on the above discussion, we compute the thesaurus-based correlation between the two terms t1 and t2, in topic Ti, as: 1 (t1 and t2 are in the same synset, or t1=t2) 0.8 (t1 and t2 have “antonym” relation) (1) 0..5 (t1 and t2 have relations of “is_a”, ( i ) RL (t1, t2 ) = “part_of”, or “member_of”) 0 (others) 3.2 Co-occurrence based correlation Co-occurrence relationship is like the global context of words. Using co-occurrence statistics,"
P01-1049,J98-1001,0,0.0107073,"st of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing & Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen & Chang, 1998; Leacock et al, 1998; Ide & Veronis, 1998) and context (Cohen & Singer, 1999; Jing & Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee & Dubin, 1999; Sarkas & Boyer, 1995; Wang & Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this di"
P01-1049,J98-1002,0,0.0122636,"word to be used to select the exact meaning of a word. From this representation, we observe that: a) Nodes “c” and “d” are closely related and may not be fully separable. In fact, it is sometimes difficult even for human experts to decide how to divide them into separate topics. b) The same word, such as “water ”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov & Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is"
P01-1049,J98-1006,0,0.104765,"& Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing & Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen & Chang, 1998; Leacock et al, 1998; Ide & Veronis, 1998) and context (Cohen & Singer, 1999; Jing & Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee & Dubin, 1999; Sarkas & Boyer, 1995; Wang & Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been f"
P01-1049,P98-2127,0,0.00775369,"the context word to be used to select the exact meaning of a word. From this representation, we observe that: a) Nodes “c” and “d” are closely related and may not be fully separable. In fact, it is sometimes difficult even for human experts to decide how to divide them into separate topics. b) The same word, such as “water ”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov & Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic n"
P01-1049,P93-1024,0,0.0143166,"(i ) (i ) (i ) ( i) ∑ Rco (wd1k , wd2m(k )) * ρ1k * ρ 2 m(k ) Rc(i )(t1, t2 ) = k =1 (5) [ ∑ ( ρ1(ki )) 2 ]1 / 2 * [∑ ( ρ (2ik) )2 ]1 / 2 k k ( i) where m ( k ) = arg max R co ( wd 1(ki ) , wd 2(is) ) s For example, in Reuters 21578 corpus, “company” and “corp” are context-related words within the topic “acq”. This is because they have very similar context of “say, header, acquire, contract”. 4. Semantic Groups & Topic Tree There are many methods that attempt to construct the conceptual representation of a topic from the original data set (Veling & van der Weerd, 1999; Baker & McCallum, 1998; Pereira et al, 1993). In this Section, we will describe our semantic -based approach to finding basic semantic groups and constructing the topic tree. Given a set of training documents, the stages involved in finding the semantic groups for each topic are given below. A) Extract all distinct terms {t1, t2, ..tn} from the training document set for topic Ti. For each term tj, compute its df(i) (tj) and cv(i)(t j), where df(i) (tj) is defined as the fraction of documents in T i that contain tj. In other words, df (i)(tj) gives the conditional probability of tj appearing in Ti. B) Derive the semantic group Gj using t"
P01-1049,J98-1003,0,\N,Missing
P01-1049,C98-2122,0,\N,Missing
P01-1049,P99-1004,0,\N,Missing
P03-1032,O96-2001,0,\N,Missing
P06-2074,P04-1054,0,0.211668,"Missing"
P06-2074,P97-1009,0,0.0750995,"sed in different ways. As an example, consider the excerpts “Terrorists attacked victims” and “Victims were attacked by unidentified terrorists”. These instances have very similar semantic meaning. However, context-based approaches such as Autoslog-TS by Riloff (1996) and Yangarber et al. (2002) may face difficulties in handling these instances effectively because the context of entity ‘victims’ is located on the left context in the first instance and on the right context in the second. For these cases, we found that we are able to verify the context by performing dependency relation parsing (Lin, 1997), which outputs the word ‘victims’ as an object in both instances, with ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is word alignment. Insertion or deletion of tokens prevents instances from being generalized effectively during learning. Therefore, the instances “Victims were attacked by terrorists” and “Victims were recently attacked by terrorists” are difficult to unify. The common approach adopted in GRID by Xiao et al. (2003) is to apply more stable chun"
P06-2074,C02-1151,0,0.0265415,"e-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and relations together. The joint learning improves the performance of NE recognition in cases such as “X killed Y”. It also prevents the propagation of mistakes in NE extraction to the extraction of relations. However, long distance relations between entities are likely to cause mistakes in relation extraction. A possible approach for modeling relations of different complexity is the use of dependency-based kernel trees in support vector machines by Culotta and Sorensen (2004). The authors reported that nonrelation instances are very heterogeneous, and 572 hence they sug"
P06-2074,C04-1078,1,0.493781,"labor for porting across domains. Moreover, the systems tend to rely on heuristics in order to match case frames. PALKA by Kim and Moldovan (1995) performs keyword-based matching of concepts, while CRYSTAL by Soderland et al. (1995) relied on additional domain-specific annotation and associated lexicon for matching. Rule-based IE models allow differentiation of rules according to their performance. Autoslog-TS by Riloff (1996) learns the context rules for extraction and ranks them according to their performance on the training corpus. Although this approach is suitable for automatic training, Xiao et al. (2004) stated that hard matching techniques tend to have low recall due to data sparseness problem. To overcome this problem, (LP)2 by Ciravegna (2002) utilizes rules with high precision in order to improve the precision of rules with average recall. However, (LP)2 is developed for semi-structured textual domain, where we can find consistent lexical patterns at surface text level. This is not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants”"
P06-2074,C02-1154,0,0.0137066,"d Post. We decided to choose both terrorism and management succession domains, from MUC4 and MUC6 respectively, in order to demonstrate that our idea is applicable to multiple domains. Paraphrasing of instances is one of the crucial problems in IE. This problem leads to data sparseness in situations when information is expressed in different ways. As an example, consider the excerpts “Terrorists attacked victims” and “Victims were attacked by unidentified terrorists”. These instances have very similar semantic meaning. However, context-based approaches such as Autoslog-TS by Riloff (1996) and Yangarber et al. (2002) may face difficulties in handling these instances effectively because the context of entity ‘victims’ is located on the left context in the first instance and on the right context in the second. For these cases, we found that we are able to verify the context by performing dependency relation parsing (Lin, 1997), which outputs the word ‘victims’ as an object in both instances, with ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is word alignment. Insertion or"
P06-2074,P02-1060,0,0.0189378,"not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants” and “peasants were attacked 2 months ago by terrorists”. The classification-based approaches such as by Chieu and Ng (2002) tend to outperform rule-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and relations together. The joint learning improves the performance of NE recognition in cases such as “X killed Y”. It also prevents the propagation of mistakes in NE extraction to the extraction of relations. Howev"
P07-1075,P06-2074,1,0.902989,"(IE) is the task of identifying information in texts and converting it into a predefined format. The possible types of information include entities, relations or events. In this paper, we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC: slotbased extraction, template filling and relation extraction, respectively. Previous approaches to IE relied on cooccurrence (Xiao et al., 2004) and dependency (Zhang et al., 2006) relations between entities. These relations enable us to make reliable extraction of correct entities/relations at the level of a single clause. However, Maslennikov et al. (2006) reported that the increase of relation path length will lead to considerable decrease in performance. In most cases, this decrease in performance occurs because entities may belong to different clauses. Since clauses in a sentence are connected by clausal relations (Halliday and Hasan, 1976), it is thus important to perform discourse analysis of a sentence. Discourse analysis may contribute to IE in several ways. First, Taboada and Mann (2005) reported that discourse analysis helps to decompose long sentences into clauses. Therefore, it helps to distinguish relevant clauses from non-relevant"
P07-1075,N04-1030,0,0.0270623,"Missing"
P07-1075,N03-1030,0,0.0395913,"Missing"
P07-1075,P03-1002,0,0.0581931,"Missing"
P07-1075,P04-1054,0,0.107997,"Missing"
P07-1075,C04-1078,1,0.880085,"Missing"
P07-1075,P06-1104,0,0.0090402,"Missing"
P07-1075,P06-1016,0,0.0260588,"Missing"
P07-1075,J03-4002,0,\N,Missing
P07-1075,J86-3001,0,\N,Missing
P09-1023,C00-1072,0,0.14794,"Missing"
P09-1023,P04-3018,0,0.0297528,"Missing"
P09-1023,P08-1093,0,0.0247921,"Missing"
P09-1023,P08-1094,0,0.0312919,"Missing"
P09-1023,J02-4004,0,0.0680839,"Missing"
P09-1023,J02-4002,0,0.133857,"Missing"
P09-1023,P07-1070,0,0.0361866,"Missing"
P09-2047,D07-1086,0,0.0586891,"ence of each matrix element mi,j , we normalize Pk λi Pi=1 ≥ T hreshold n i=1 λi 186 (5) 3 Experiments where n is the number of eigenvalues. When λk À λk+1 , Eqn.(5) is very effective. However, according to the Gerschgorin circle theorem, the nondiagonal values of M are so small that the eigenvalues cannot be distinguished easily. Under this circumstance, a prefixed threshold is too restrictive to be applied in complex situations. Therefore a function of n is introduced into the threshold as follows: Pk n−1 2 λi Pi=1 ≥( ) (6) n n i=1 λi 3.1 Data set We experiment on the data set published by (Bergsma and Wang, 2007). This data set comprises 500 queries which were randomly taken from the AOL search query database and each query. These queries are all segmented manually by three annotators (the results are referred as A, B and C). We evaluate our results on the five test data sets (Tan and Peng, 2008), i.e. we use A, B, C, the intersection of three annotator’s results (referred to as D) and the conjunction of three annotator’s results (referred to as E). Besides, three evaluation metrics are used in our experiments (Tan and Peng, 2008; Peng and Schuurmans, 2001), i.e. Precision (referred to as Prec), Recal"
P09-2047,C08-1128,0,0.0422659,"Missing"
P09-2047,J00-3004,0,\N,Missing
P11-1150,E06-1039,0,0.0143962,"Missing"
P11-1150,P10-2050,0,0.327719,"Missing"
P11-1150,P09-1079,0,0.0234322,"Missing"
P11-1150,esuli-sebastiani-2006-sentiwordnet,0,0.110056,"Missing"
P11-1150,W10-3505,0,0.0586323,"hat users can make wise purchase decisions conveniently. In the following, we apply the aspect ranking results to assist document-level review sentiment classification. Generally, a review document contains consumer’s positive/negative opinions on various aspects of the product. It is difficult to get the accurate overall opinion of the whole review without knowing the importance of these aspects. In addition, when we learn a document-level sentiment classifier, the features generated from unimportant aspects lack of discriminability and thus may deteriorate the performance of the classifier (Fang et al., 2010). While the important aspects and the sentiment terms on these aspects can greatly influence the overall opinions of the review, they are highly likely to be discriminative features for sentiment classification. These observations motivate us to utilize aspect ranking results to assist classifying the sentiment of review documents. Specifically, we randomly sampled 100 reviews of each product as the testing data and used the remaining reviews as the training data. We first utilized our approach to identify the importance aspects from the training data. We then explored the aspect terms and sen"
P11-1150,W06-3808,0,0.126933,"Missing"
P11-1150,C04-1200,0,0.247089,"Missing"
P11-1150,P09-1029,0,0.0483334,"Missing"
P11-1150,E09-1059,0,0.0693085,"Missing"
P11-1150,P10-1139,0,0.0120543,"Missing"
P11-1150,P09-1028,0,0.0246995,"Missing"
P11-1150,P07-1055,0,0.0195472,"Missing"
P11-1150,P09-2043,0,0.0234318,"Missing"
P11-1150,W04-3253,0,0.111227,"Missing"
P11-1150,P10-2060,0,0.0314043,"Missing"
P11-1150,W02-1011,0,0.0251063,"9 0.730 0.772 0.762 0.767 0.744 0.815 0.778 0.695 0.708 0.701 SV M + IA P R F1 0.704 0.721 0.713 0.731 0.724 0.727 0.696 0.713 0.705 0.790 0.717 0.752 0.732 0.765 0.748 0.749 0.726 0.737 0.732 0.684 0.707 0.782 0.758 0.770 0.820 0.788 0.804 0.805 0.821 0.813 0.768 0.732 0.750 Table 6: Evaluations on Term Weighting methods for Document-level Review Sentiment Classification. IA denotes the term weighing based on the important aspects. * significant t-test, p-values&lt;0.05. been extensively studied (Pang and Lee, 2008). Earlier research had been studied unsupervised (Kim et al., 2004), supervised (Pang et al., 2002; Pang et al., 2005) and semi-supervised approaches (Goldberg et al., 2006) for the classification. For example, Mullen et al. (2004) proposed an unsupervised classification method which exploited pointwise mutual information (PMI) with syntactic relations and other attributes. Pang et al. (2002) explored several machine learning classifiers, including Na¨ıve Bayes, Maximum Entropy, SVM, for sentiment classification. Goldberg et al. (2006) classified the sentiment of the review using the graph-based semi-supervised learning techniques, while Li el al. (2009) tackled the problem using matrix fa"
P11-1150,P05-1015,0,0.309812,"Missing"
P11-1150,H05-1043,0,0.893653,"Missing"
P11-1150,N07-1038,0,0.136765,"Missing"
P11-1150,W09-3210,0,0.0209012,"Missing"
P11-1150,P10-1059,0,0.0127549,"Missing"
P11-1150,P02-1053,0,0.00903603,"Missing"
P11-1150,P08-1036,0,0.460392,"Missing"
P11-1150,P10-2048,0,0.0191932,"Missing"
P11-1150,H05-1044,0,0.0392089,"obtain unique aspects based on unigram feature. 2.3 Aspect Sentiment Classification Since the Pros and Cons reviews explicitly express positive and negative opinions on the aspects, respectively, our task is to determine the opinions in free text reviews. To this end, we here utilize Pros and Cons reviews to train a SVM sentiment classifier. Specifically, we collect sentiment terms in the Pros and Cons reviews as features and represent each review into feature vector using Boolean weighting. Note that we select sentiment terms as those appear in the sentiment lexicon provided by MPQA project (Wilson et al., 2005). With these features, we then train a SVM classifier based on Pros and Cons reviews. Given a free text review, since it may cover various opinions on multiple aspects, we first locate the opinionated expression modifying each aspect, and determine the opinion on the aspect using the learned SVM classifier. In particular, since the opinionated expression on each aspect tends to contain sentiment terms and appear closely to the aspect (Hu and Liu, 2004), we select the expressions which contain sentiment terms and are at the distance of less than 5 from the aspect NP in the parsing tree. 2.4 Asp"
P11-1150,W05-0308,0,0.0192927,"Missing"
P11-1150,D09-1159,0,0.354405,"ybased solution is not able to identify the truly important aspects. Motivated by the above observations, in this paper, we propose an effective approach to automatically identify the important product aspects from consumer reviews. Our assumption is that the important aspects of a product should be the aspects that are frequently commented by consumers, and consumers’ opinions on the important aspects greatly influence their overall opinions on the product. Given the online consumer reviews of a specific product, we first identify the aspects in the reviews using a shallow dependency parser (Wu et al., 2009), and determine consumers’ opinions on these aspects via a sentiment classifier. We then design an aspect ranking algorithm to identify the important aspects by simultaneously taking into account the aspect frequency and the influence of consumers’ opinions given to each aspect on their overall opinions. Specifically, we assume that consumer’s overall opinion rating on a product is generated based on a weighted sum of his/her specific opinions on multiple aspects of the product, where the weights essentially measure the degree of importance of the aspects. A probabilistic regression algorithm"
P11-1150,H05-2017,0,\N,Missing
P11-1150,P04-1035,0,\N,Missing
P11-1150,P10-1141,0,\N,Missing
P12-1061,P08-1081,0,0.0220649,"n, the semantic interactions between different sentence sites are crucial, that is, some context co-occurrences should be encouraged and others should be penalized for requirements of information novelty and non-redundancy in the generated summary. Here we consider both local (sentences from the same answer) and global (sentences from different answers) settings. This give rise to four contextual factors that we will explore for modeling the pairwise semantic interactions based on question segmentation. In this paper, we utilize a simple but effective lightweight question segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps: Step 1. Question sentence detection: every sentence in the original multi-sentence question is classified into question sentence and non-question (context) sentence. The question mark and 5W1H features are applied. Step 2. Context assignment: every context sentence is assigned to the most relevant question sentence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub quesFigure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based models. tions a"
P12-1061,W06-1643,0,0.0707576,"Missing"
P12-1061,W04-1013,0,0.00952528,"Missing"
P12-1061,C08-1063,0,0.377495,"Missing"
P12-1061,P07-1059,0,0.0800987,"Missing"
P12-1061,P10-1078,0,0.435579,"Missing"
P12-1061,zhou-etal-2006-summarizing,0,0.032358,"Missing"
P12-1061,J00-4006,0,\N,Missing
P16-1063,Q15-1022,0,0.508179,"lobal (document-level) semantic information to help the learning of word embeddings. The global embedding is simply a weighted average of the embeddings of words in the document. Le and Mikolov (2014) proposed Paragraph Vector. It assumes each piece of text has a latent paragraph vector, which influences the distributions of all words in this text, in the same way as a latent word. It can be viewed as a special case of TopicVec, with the topic number set to 1. Typically, however, a document consists of multiple semantic centroids, and the limitation of only one topic may lead to underfitting. Nguyen et al. (2015) proposed Latent Feature Topic Modeling (LFTM), which extends LDA to incorporate word embeddings as latent features. The topic is modeled as a mixture of the conventional categorical distribution and an embedding link function. The coupling between these two components makes the inference difficult. They designed a Gibbs sampler for model inference. Their implementation1 is slow and infeasible when applied to a large corpous. Liu et al. (2015) proposed Topical Word Embedding (TWE), which combines word embedding with LDA in a simple and effective way. They train word embeddings and a topic mode"
P16-1063,P12-1092,0,0.0392711,"However, this simple representation suffers from being high-dimensional and highly sparse, and loses semantic relatedness across the vector dimensions. Word Embedding methods have been demonstrated to be an effective way to represent words 666 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 666–675, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ument through weighted connections. Larochelle and Lauly (2012) assigned each word a unique topic vector, which is a summarization of the context of the current word. Huang et al. (2012) proposed to incorporate global (document-level) semantic information to help the learning of word embeddings. The global embedding is simply a weighted average of the embeddings of words in the document. Le and Mikolov (2014) proposed Paragraph Vector. It assumes each piece of text has a latent paragraph vector, which influences the distributions of all words in this text, in the same way as a latent word. It can be viewed as a special case of TopicVec, with the topic number set to 1. Typically, however, a document consists of multiple semantic centroids, and the limitation of only one topic"
P16-1063,D14-1162,0,0.110481,"elongs to a topic is determined by the Euclidean distance between the word embedding and the topic embedding. This assumption might be improper as the Euclidean distance is not an optimal measure of semantic relatedness between two embeddings2 . 3 Link Function of Topic Embedding P (wc |w0 : wc−1 )   c−1 c−1 X X > ≈P (wc ) exp v wc v wl + awl wc . (1) l=0 Notations and Definitions l=0 Here awl wc is referred as the bigram residual, indicating the non-linear part not captured by v> wc v wl . It is essentially the logarithm of the normalizing constant of a softmax term. Some literature, e.g. (Pennington et al., 2014), refers to such a term as a bias term. (1) is based on the assumption that the conditional distribution P (wc |w0 : wc−1 ) can be factorized approximately into independent logbilinear terms, each corresponding to a context word. This approximation leads to an efficient and effective word embedding algorithm PSDVec (Li et al., 2015). We follow this assumption, and propose to incorporate the topic of wc in a way like a latent word. In particular, in addition to the context words, the corresponding embedding tik is included as a new log-bilinear term that influences the distribution of wc . Henc"
P16-1063,Q15-1016,0,0.0660772,"Missing"
P16-1063,D15-1183,1,0.924143,"atistical foundations; 2) the LDA module requires a large corpus to derive semantically coherent topics. Das et al. (2015) proposed Gaussian LDA. It uses pre-trained word embeddings. It assumes that words in a topic are random samples from a multivariate Gaussian distribution with the topic embedding as the mean. Hence the probability that a Semantic centroids have the same nature as topics in LDA, except that the former exist in the embedding space. This similarity drives us to seek the common semantic centroids with a model similar to LDA. We extend a generative word embedding model PSDVec (Li et al., 2015), by incorporating topics into it. The new model is named TopicVec. In TopicVec, an embedding link function models the word distribution in a topic, in place of the categorical distribution in LDA. The advantage of the link function is that the semantic relatedness is already encoded as the cosine distance in the embedding space. Similar to LDA, we regularize the topic distributions with Dirichlet priors. A variational inference algorithm is derived. The learning process derives topic embeddings in the same embedding space of words. These topic embeddings aim to approximate the underlying sema"
P16-1063,P15-1077,0,\N,Missing
P19-1128,P18-2014,0,0.0429986,"en and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·)"
P19-1128,N19-1240,0,0.0430003,"Missing"
P19-1128,P18-1148,0,0.0240849,"15; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. T"
P19-1128,D17-1188,0,0.624311,"y an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·) indicates an embedding function, and θen denotes the parameters of the encoding module of n-th layer. 3.2 Pr"
P19-1128,P16-1200,1,0.891879,"and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat re"
P19-1128,D17-1159,0,0.0382641,"ning as compared to those models which extract relationships separately. Moreover, we also present three datasets, which could help future researchers compare their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the"
P19-1128,P16-1105,0,0.0569637,"that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · ,"
P19-1128,W15-1506,0,0.261678,"bjects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. ("
P19-1128,D15-1203,0,0.647955,"7) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochre"
P19-1128,C14-1220,0,0.816506,"e generic backpropagation and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le"
P19-1128,D17-1186,1,0.883827,"entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pai"
P19-1128,D18-1244,0,0.158096,"e their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the relationship during the learning process. In sharp contrast, this paper focuses on extracting relations from real-world relation datasets. 2.2 Rel"
P19-1128,D14-1162,0,0.0823489,"e n denotes the index of layer 1 , [·] means reshaping a vector as a matrix, BiLSTM encodes a sequence by concatenating tail hidden states of the forward LSTM and head hidden states of the backward LSTM together and MLP denotes a multilayer perceptron with non-linear activation σ. Word Representations We first map each token xt of sentence {x0 , x1 , . . . , xl−1 } to a kdimensional embedding vector xt using a word embedding matrix We ∈ R|V |×dw , where |V |is the size of the vocabulary. Throughout this paper, we stick to 50-dimensional GloVe embeddings pre-trained on a 6-billion-word corpus (Pennington et al., 2014). 1 Adding index to neural models means their parameters are different among layers. Position Embedding In this work, we consider a simple entity marking scheme2 : we mark each token in the sentence as either belonging to the first entity vi , the second entity vj or to neither of those. Each position marker is also mapped to a dp -dimensional vector by a position embedding matrix P ∈ R3×dp . We use notation pi,j t to represent the position embedding for xt corresponding to entity pair (vi , vj ). 4.2 Propagation Module Next, we use Eq. (2) to propagate information among nodes where the initia"
P19-1140,C18-1057,1,0.816626,"re 1: Illustration of the structural differences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar st"
P19-1140,D18-1021,1,0.856141,"re 1: Illustration of the structural differences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar st"
P19-1140,P17-1149,1,0.846504,"rom YAGO3 to DBpedia are grounded to 92,923 new ground rule triples, which is shocking and not informative. Further investigation finds that the rule (a, team, b) ⇒ (a, affiliation, b) alone contributes 92,743 ground rule triples. Although the rule is logically correct, it is suspicious such a rule that establishes similar relations between entities would benefit entity alignment. We will deal with such noise in future. 6 Related Work Merging different KGs into a unified one has attracted much attention since it shall benefit many Knowledge-driven applications, such as information extraction (Cao et al., 2017a, 2018b), question answering (Zhang et al., 2015) and recommendation (Cao et al., 2019). Early approaches for entity alignment leverage various features to overcome the heterogeneity between KGs, such as machine translation and external lexicons (Suchanek et al., 2011; Wang et al., 2013). Following the success of KG representation learning, recent work embeds entities in different KGs into a low-dimensional vector space with the help of seed alignments (Chen et al., 2017). However, the limited seeds and structural differences take great negative impacts on the quality of KG embeddings, which"
P19-1140,D15-1077,1,0.879704,"Missing"
P19-1140,I17-1024,1,0.8572,"rom YAGO3 to DBpedia are grounded to 92,923 new ground rule triples, which is shocking and not informative. Further investigation finds that the rule (a, team, b) ⇒ (a, affiliation, b) alone contributes 92,743 ground rule triples. Although the rule is logically correct, it is suspicious such a rule that establishes similar relations between entities would benefit entity alignment. We will deal with such noise in future. 6 Related Work Merging different KGs into a unified one has attracted much attention since it shall benefit many Knowledge-driven applications, such as information extraction (Cao et al., 2017a, 2018b), question answering (Zhang et al., 2015) and recommendation (Cao et al., 2019). Early approaches for entity alignment leverage various features to overcome the heterogeneity between KGs, such as machine translation and external lexicons (Suchanek et al., 2011; Wang et al., 2013). Following the success of KG representation learning, recent work embeds entities in different KGs into a low-dimensional vector space with the help of seed alignments (Chen et al., 2017). However, the limited seeds and structural differences take great negative impacts on the quality of KG embeddings, which"
P19-1140,D18-1032,0,0.1473,"fferences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar structures and thus similar embeddings. Ho"
P19-1140,D16-1019,0,0.0312459,"ly, and γ1 &gt; 0 and γ2 &gt; 0 are margin hyper-parameters separating positive and negative entity and relation alignments. During the experiments, by calculating cosine similarity, we select 25 entities closest to the corresponding entity in the same KG as negative samples (Sun et al., 2018). Negative samples will be re-calculated every 5 epochs. 1456 Rule Knowledge Constraints Since we have changed the KG structure by adding new triplets (i.e., grounded rules), we also introduce the triplet loss to hold the grounded rules as valid in the unified vector space. Taking KG G as an example, following Guo et al. (2016), we define the loss function as follows: Lr = X X [γr − I(g + ) + I(g − )]+ g + ∈G(K)g − ∈G − (K) + X X are extracted from multilingual DBpedia and include 15,000 entity pairs as seed alignments. DWY100K consists of two large-scale crossresource datasets: DWY-WD (DBpedia to Wikidata) and DWY-YG (DBpedia to YAGO3). Each dataset includes 100,000 alignments of entities in advance. As for the seed alignments of relations, we employ the official relation alignment list published by DBpedia for DWY100K. As for DWY-YG, we manually align the relations because there are only a small set of relation ty"
qiu-etal-2004-public,A00-2018,0,\N,Missing
qiu-etal-2004-public,C96-1021,0,\N,Missing
qiu-etal-2004-public,J94-4002,0,\N,Missing
qiu-etal-2004-public,mitkov-2000-towards,0,\N,Missing
W02-1814,W99-0613,0,0.0168218,"le annotated data is difficult to obtain, un-annotated data is readily available and plentiful, especially on the Internet. To take advantage of that, we need to tackle two major problems. The first is how to gather sufficient distinct P-Names from the Internet, and the second is how to use the available resources to derive reliable statistical information to characterize the P-Names. The problem of gathering sufficient reliable information from a small initial set of seed resources has been tackled in bootstrapping research for information extraction (Agichtein and Gravano, 2000; Brin, 1998; Collins and Singer, 1999; Mihalcea and Moldovan, 2001; Riloff and Jones, 1999). Bootstrapping approach aims to perform unsupervised text processing to extract information from open resources such as the Internet using minimum manual labor. Given the lack of annotated training samples for P-Name extraction, this paper introduces a bootstrapping algorithm, called PN-Finder. It starts from a small set of seed samples, and iteratively locates, extracts and classifies the new and more P-Names. It works in conjunction with a general Chinese named entity recognizer (Chua and Liu, 2002) to extract general named entities. In"
W02-1814,P01-1049,1,0.817748,"Missing"
W06-1603,C04-1051,0,0.0917421,"es describing the same event, paraphrases are widely used, possibly with extraneous information. We equate PR with solving these two issues, presenting a natural two-phase architecture. In the first phase, the nuggets shared by the sentences are identified by a pairing process. In the second phase, any unpaired nuggets are classified as significant or not (leading to −pp and +pp classifications, respectively). If the sentences do not contain unpaired nuggets, or if all unpaired nuggets are insignificant, then the sentences are considered paraphrases. Experiments on the widely-used MSR corpus (Dolan et al., 2004) show favorable results. We first review related work in Section 2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distribu"
W06-1603,P02-1031,0,0.00955003,"wide set of features of unpaired tuples, including internal counts of numeric expressions, named entities, words, semantic roles, whether they are similar to other tuples in the same sentence, and contextual features like source/target sentence length and paired tuple count. Currently, only two features are correlated in improved classification, which we detail now. Syntactic Parse Tree Path: This is a series of features that reflect how the unpaired tuple connects with the context: the rest of the sentence. It models the syntactic connection between the constituents on both ends of the path (Gildea and Palmer, 2002; Pradhan et al., 2004). Here, we model the ends of the path as the unpaired tuple and the paired tuple with the closest shared ancestor, and model the path itself as a sequence of constituent category tags and directions to reach the destination (the paired target) from the source (the 2 Copular constructions are not handled by ASSERT. Such constructions account for a large proportion of the semantic meaning in sentences. Consider the pair “Microsoft rose 50 cents” and “Microsoft was up 50 cents”, in which the second is in copular form. Similarly, NPs can often be equivalent to predicate argu"
W06-1603,P98-2127,0,0.0127747,"ferences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found in practice that aligning arg2 and above arguments do not cause problems. 20 Sentence Modification 1: paraphrase Richard Miller was hurt by a young man. (Paired) Tuples target: arg0: arg1: hurt a young man Richard Miller Model Sentence Authorities said a young man injured Richard Miller. target: said arg0: Authorities arg1: a young man injured"
W06-1603,N04-1030,0,0.0930859,"for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found in practice that aligning arg2 and above arguments do not cause problems."
W06-1603,W05-1202,0,0.0119731,"red nuggets are insignificant, then the sentences are considered paraphrases. Experiments on the widely-used MSR corpus (Dolan et al., 2004) show favorable results. We first review related work in Section 2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster,"
W06-1603,W05-1205,0,0.0335034,"2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different reali"
W06-1603,N03-1003,0,0.110538,"owed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different realization of arguments. If sentences from different clusters have shared arguments, the associated lattices are claimed to be paraphrase. Likewise, Shinyama et al. (2002) extracted paraphrases f"
W06-1603,I05-5001,0,0.00823651,"araphrases rather than detecting them, and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature vector, including features ("
W06-1603,A00-2018,0,0.0226765,"is a better choice for the representation of a nugget as it accounts for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found"
W06-1603,W05-1203,0,0.0190787,"geared towards acquiring paraphrases rather than detecting them, and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature v"
W06-1603,C98-2122,0,\N,Missing
