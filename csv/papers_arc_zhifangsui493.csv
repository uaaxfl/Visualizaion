2021.repl4nlp-1.10,Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions,2021,-1,-1,6,1,2470,damai dai,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Conventional Knowledge Graph Completion (KGC) assumes that all test entities appear during training. However, in real-world scenarios, Knowledge Graphs (KG) evolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and we need to efficiently represent these entities. Most existing Knowledge Graph Embedding (KGE) methods cannot represent OOKG entities without costly retraining on the whole KG. To enhance efficiency, we propose a simple and effective method that inductively represents OOKG entities by their optimal estimation under translational assumptions. Moreover, given pretrained embeddings of the in-knowledge-graph (IKG) entities, our method even needs no additional learning. Experimental results on two KGC tasks with OOKG entities show that our method outperforms the previous methods by a large margin with higher efficiency."
2021.naacl-main.437,"Decompose, Fuse and Generate: A Formation-Informed Method for {C}hinese Definition Generation",2021,-1,-1,5,0,2471,hua zheng,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word ({``}æ¡è±{''}, peach-blossom) is formed by formation components ({``}æ¡{''}, peach; {``}è±{''}, flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust."
2020.lrec-1.846,{H}ypo{NLI}: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference,2020,2,0,4,1,2474,tianyu liu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Many recent studies have shown that for models trained on datasets for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such bias. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out {`}hard{'} and {`}easy{'} instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (BERT, RoBerta, XLNet) and competitive non-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias: down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in NLI debiasing tasks."
2020.emnlp-main.32,A Spectral Method for Unsupervised Multi-Document Summarization,2020,-1,-1,3,1,20100,kexiang wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems."
2020.emnlp-main.657,Discriminatively-{T}uned {G}enerative {C}lassifiers for {R}obust {N}atural {L}anguage {I}nference,2020,-1,-1,4,0,4022,xiaoan ding,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the {``}infinilog loss{''}. Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise."
2020.conll-1.48,An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference,2020,-1,-1,5,1,2474,tianyu liu,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models{'} generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it{'}s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models."
2020.coling-main.500,An Anchor-Based Automatic Evaluation Metric for Document Summarization,2020,-1,-1,4,1,20100,kexiang wang,Proceedings of the 28th International Conference on Computational Linguistics,0,"The widespread adoption of reference-based automatic evaluation metrics such as ROUGE has promoted the development of document summarization. In this paper, we consider a new protocol for designing reference-based metrics that require the endorsement of source document(s). Following protocol, we propose an anchored ROUGE metric fixing each summary particle on source document, which bases the computation on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here."
2020.ccl-1.52,é¢åå»å­¦ææ¬å¤ççå»å­¦å®ä½æ æ³¨è§è(Medical Entity Annotation Standard for Medical Text Processing),2020,-1,-1,4,0,4157,huan zhang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"éçæºæ
§å»ççæ®å,å©ç¨èªç¶è¯­è¨å¤çææ¯è¯å«å»å­¦ä¿¡æ¯çéæ±æ¥çå¢é¿ãç®å,éå¯¹å»å­¦å®ä½èè¨,å»å­¦å
±äº«è¯­æåºä»å¤äºç©ºç½ç¶æ,è¿å¯¹å»å­¦ææ¬ä¿¡æ¯å¤çåé¡¹ä»»å¡çè¿å±é æäºå·¨å¤§é»åãå¦ä½å¤æ­ä¸åçå»å­¦å®ä½ç±»å«?å¦ä½çå®ä¸åå®ä½é´çæ¶µçèå´?è¿äºé®é¢å¯¼è´ç¼ºä¹ç±»ä¼¼éç¨åºæ¯çå¤§è§æ¨¡è§èæ æ³¨çå»å­¦ææ¬æ°æ®ãéå¯¹ä¸è¿°é®é¢,è¯¥æåèäºUMLSä¸­å®ä¹çè¯­ä¹ç±»å,æåºé¢åå»å­¦ææ¬ä¿¡æ¯å¤ççå»å­¦å®ä½æ æ³¨è§è,æ¶µçäºç¾ç
ãä¸´åºè¡¨ç°ãå»çç¨åºãå»çè®¾å¤ç­9ç§å»å­¦å®ä½,ä»¥ååºäºè§èæå»ºå»å­¦å®ä½æ æ³¨è¯­æåºãè¯¥æç»¼è¿°äºæ æ³¨è§èçæè¿°ä½ç³»ãåç±»ååãæ··æ·å¤çãè¯­ææ æ³¨è¿ç¨ä»¥åå»å­¦å®ä½èªå¨æ æ³¨åºçº¿å®éªç­ç¸å
³é®é¢,å¸æè½ä¸ºå»å­¦å®ä½è¯­æåºçæå»ºæä¾å¯åèçæ æ³¨è§è,ä»¥åä¸ºå»å­¦å®ä½è¯å«æä¾è¯­ææ¯æã"
P19-1194,Towards Fine-grained Text Sentiment Transfer,2019,0,3,7,1,2472,fuli luo,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation."
P19-1600,Towards Comprehensive Description Generation from Factual Attribute-value Tables,2019,0,2,6,1,2474,tianyu liu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. However previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. To relieve these problems, we first propose force attention (FA) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. Furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. In our experiments, we utilize the widely used WIKIBIO dataset as a benchmark. Besides, we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation."
P19-1603,Learning to Control the Fine-grained Sentiment for Story Ending Generation,2019,0,3,6,1,2472,fuli luo,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better."
D19-1336,Pun-{GAN}: Generative Adversarial Network for Pun Generation,2019,0,1,6,1,2472,fuli luo,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation."
P18-1230,Incorporating Glosses into Neural Word Sense Disambiguation,2018,19,5,5,1,2472,fuli luo,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets."
L18-1079,{E}vent{W}iki: A Knowledge Base of Major Events,2018,-1,-1,4,1,3790,tao ge,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1566,Revisiting Distant Supervision for Relation Extraction,2018,0,0,4,1,30123,tingsong jiang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1170,Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention,2018,0,10,5,1,2472,fuli luo,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The goal of Word Sense Disambiguation (WSD) is to identify the correct meaning of a word in the particular context. Traditional supervised methods only use labeled data (context), while missing rich lexical knowledge such as the gloss which defines the meaning of a word sense. Recent studies have shown that incorporating glosses into neural networks for WSD has made significant improvement. However, the previous models usually build the context representation and gloss representation separately. In this paper, we find that the learning for the context and gloss representation can benefit from each other. Gloss can help to highlight the important words in the context, thus building a better context representation. Context can also help to locate the key words in the gloss of the correct word sense. Therefore, we introduce a co-attention mechanism to generate co-dependent representations for the context and gloss. Furthermore, in order to capture both word-level and sentence-level information, we extend the attention mechanism in a hierarchical fashion. Experimental results show that our model achieves the state-of-the-art results on several standard English all-words WSD test datasets."
D18-1271,Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition,2018,0,1,6,1,3790,tao ge,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes to study fine-grained coordinated cross-lingual text stream alignment through a novel information network decipherment paradigm. We use Burst Information Networks as media to represent text streams and present a simple yet effective network decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment. Experiments on Chinese-English news streams show our approach not only outperforms previous approaches on bilingual lexicon extraction from coordinated text streams but also can harvest high-quality alignments from large amounts of streaming data for endless language knowledge mining, which makes it promising to be a new paradigm for automatic language knowledge acquisition."
P17-1189,A Progressive Learning Approach to {C}hinese {SRL} Using Heterogeneous Data,2017,15,1,4,0,26846,qiaolin xia,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods."
D17-1020,Affinity-Preserving Random Walk for Multi-Document Summarization,2017,0,0,3,1,20100,kexiang wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods."
D17-1189,A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction,2017,9,39,4,1,2474,tianyu liu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don{'}t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems."
P16-1116,{RBPB}: Regularization-Based Pattern Balancing Method for Event Extraction,2016,32,5,6,1,7754,lei sha,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Event extraction is a particularly challenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents."
N16-1049,Joint Learning Templates and Slots for Event Schema Induction,2016,1,0,4,1,7754,lei sha,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work."
D16-1075,News Stream Summarization using Burst Information Networks,2016,33,5,6,1,3790,tao ge,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"This paper studies summarizing key information from news streams. We propose simple yet effective models to solve the problem based on a novel and promising representation of text streams xe2x80x90 Burst Information Networks (BINets). A BINet can be aware of redundant information, allows global analysis of a text stream, and can be efficiently built and dynamically updated, which perfectly fits the demands of text stream summarization. Extensive experiments show that the BINet-based approaches are not only efficient and can be used in a real-time online summarization setting, but also can generate high-quality summaries, outperforming the state-of-the-art approach."
D16-1212,Capturing Argument Relationship for {C}hinese Semantic Role Labeling,2016,8,5,4,1,7754,lei sha,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1260,Encoding Temporal Information for Time-Aware Link Prediction,2016,20,18,7,1,30123,tingsong jiang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1161,Towards Time-Aware Knowledge Graph Completion,2016,27,15,7,1,30123,tingsong jiang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Knowledge graph (KG) completion adds new facts to a KG by making inferences from existing facts. Most existing methods ignore the time information and only learn from time-unknown fact triples. In dynamic environments that evolve over time, it is important and challenging for knowledge graph completion models to take into account the temporal aspects of facts. In this paper, we present a novel time-aware knowledge graph completion model that is able to predict links in a KG using both the existing facts and the temporal information of the facts. To incorporate the happening time of facts, we propose a time-aware KG embedding model using temporal order information among facts. To incorporate the valid time of facts, we propose a joint time-aware inference model based on Integer Linear Programming (ILP) using temporal consistencyinformationasconstraints. Wefurtherintegratetwomodelstomakefulluseofglobal temporal information. We empirically evaluate our models on time-aware KG completion task. Experimental results show that our time-aware models achieve the state-of-the-art on temporal facts consistently."
C16-1270,Reading and Thinking: Re-read {LSTM} Unit for Textual Entailment Recognition,2016,16,32,3,1,7754,lei sha,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences (premise and hypothesis) and send them together into a multi-layer perceptron to get their entailment type, or use LSTM-RNN to link two sentences together while using attention mechanic to enhance the model{'}s ability. In this paper, we propose to use the re-read mechanic, which means to read the premise again and again while reading the hypothesis. After read the premise again, the model can get a better understanding of the premise, which can also affect the understanding of the hypothesis. On the contrary, a better understanding of the hypothesis can also affect the understanding of the premise. With the alternative re-read process, the model can {``}think{''} of a better decision of entailment type. We designed a new LSTM unit called re-read LSTM (rLSTM) to implement this {``}thinking{''} process. Experiments show that we achieve results better than current state-of-the-art equivalents."
C16-1309,Event Detection with Burst Information Networks,2016,19,6,4,1,3790,tao ge,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Retrospective event detection is an important task for discovering previously unidentified events in a text stream. In this paper, we propose two fast centroid-aware event detection models based on a novel text stream representation {--} Burst Information Networks (BINets) for addressing the challenge. The BINets are time-aware, efficient and can be easily analyzed for identifying key information (centroids). These advantages allow the BINet-based approaches to achieve the state-of-the-art performance on multiple datasets, demonstrating the efficacy of BINets for the task of event detection."
P15-2110,One Tense per Scene: Predicting Tense in {C}hinese Conversations,2015,11,0,4,1,3790,tao ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We study the problem of predicting tense in Chinese conversations. The unique challenges include: (1) Chinese verbs do not have explicit lexical or grammatical forms to indicate tense; (2) Tense information is often implicitly hidden outside of the target sentence. To tackle these challenges, we first propose a set of novel sentence-level (local) features using rich linguistic resources and then propose a new hypothesis of xe2x80x9cOne tense per scenexe2x80x9d to incorporate scene-level (global) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark."
P15-1056,Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles,2015,32,13,6,1,3790,tao ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the first novel approach to automatically generate a topically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach consists of two core components xe2x80x93 a timeaware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to construct the final chronicle. Experimental results demonstrate our approach is promising to tackle this new problem."
D15-1185,Recognizing Textual Entailment Using Probabilistic Inference,2015,18,4,4,1,7754,lei sha,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore xe2x80x9cdeepxe2x80x9d expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work."
D15-1186,{C}hinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks,2015,20,21,4,0,1699,zhen wang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering. Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods. In this paper, we introduce bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance."
D15-1289,{ERSOM}: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation,2015,39,9,4,0,37854,chuncheng xiang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"As a key representation model of knowledge, ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc. In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem. Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions. Besides, the structural information of ontology havenxe2x80x99t been utilized adequately for ontology matching. We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings. The experimental results on the datasets from Ontology Alignment Evaluation Initiative (OAEI1) show that ERSOM achieves a competitive performance compared to the state-of-the-art ontology matching systems. The OAEI is an international initiative organizing annual campaigns for evaluating ontology matching systems. All of the ontologies provided by OAEI are described in OWL-DL language, and like most of the other participates our ERSOM also manages the OWL ontology in its current version. OAEI: http://oaei.ontologymatching.org/"
W14-6802,The Construction of language Resource and Knowledge Base for {C}hinese Language Computing,2014,0,0,1,1,2475,zhifang sui,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"With the urgent needs of enhancing the intelligence of the internet, Knowledge engineering is attracting high attention from both industry and academia. Different from the knowledge engineering (such as Dbpedia, Knowledge Graph, YAGO, etc.) based on structured knowledge resources, the task of textual knowledge engineering is to mine knowledge from unstructured natural language texts. One of the critical problems is, there is gap between the shallow structures expressed by natural languages and the deep structures in conceptual knowledge. In this talk we will introduce the building of the multi-level annotated Chinese language resource, the ontology engineering based on encyclopedias and the Web, and the construction of the mapping resource between conceptual relations and their natural language expressions to link linguistic knowledge and the world knowledge together. The ultimate goal is to lay resource foundation for Chinese language computing in the Web scale. BIO: Zhifang Sui, Professor of Institute of Computational Linguistics, Peking University. Her research focuses on computational linguistics, text mining and knowledge engineering. She has won the National Prize for Progress in Science and Technology for the comprehensive language knowledge base in 2011. Her work is supported by several grants from NSFC and National Key Basic Research Program of China etc."
W14-6814,The {CIPS}-{SIGHAN} {CLP} 2014 {C}hinese Word Segmentation Bake-off,2014,2,1,2,1,38116,huiming duan,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper summarizes the SIGHAN 2014 Chinese Word Segmentation bakeoff in several aspects such as dataset, evaluation results. In addition, we analyze errors of segmentation by instance and make a suggestion for improving segmentation systems."
P13-2141,Towards Accurate Distant Supervision for Relational Facts Extraction,2013,15,22,6,0,19603,xingxing zhang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach."
D13-1001,Event-Based Time Label Propagation for Automatic Dating of News Articles,2013,12,9,4,1,3790,tao ge,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small."
W12-6307,The {CIPS}-{SIGHAN} {CLP} 2012 {C}hinese{W}ord Segmentation on{M}icro{B}log Corpora Bakeoff,2012,4,11,2,1,38116,huiming duan,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"The CIPS-SIGHAN CLP 2012 Chinese Word Segmentation on MicroBlog Cor- pora Bakeoff was held in the autumn of 2012. This bake-off task of Chinese word segmentation is focused on the perfor- mance of Chinese word segmentation al- gorithms on MicroBlog corpora. 17 groups submitted 20 results, among which the best system has all the P, R and F values near 95%, and the average values of the 17 systems are 0.8931, 0.8981 and 0.8953, respectively."
C12-2068,Fine-Grained Classification of Named Entities by Fusing Multi-Features,2012,12,1,4,0,1826,wenjie li,Proceedings of {COLING} 2012: Posters,0,"Due to the increase in the number of classes and the decrease in the semantic differences between classes, fine-grained classification of Named Entities is a more difficult task than classic classification of NEs. Using only simple local context features for this fine-grained task cannot yield a good classification performance. This paper proposes a method exploiting Multi-features for fine-grained classification of Named Entities. In addition to adopting the context features, we introduce three new features into our classification model: the cluster-based features, the entityrelated features and the class-specific features. We experiment on them separately and also fused with prior ones on the subcategorization of person names. Results show that our method achieves a significant improvement for the fine-grained classification task when the new features are fused with others."
Y09-2011,{C}hinese Function Tag Labeling,2009,16,6,2,1,4541,weiwei sun,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Function tag assignment has been studied for English and Spanish. In this paper, we address the question of assigning function tags to parsed sentences in Chinese. We show that good performance for Chinese function tagging can be achieved by using labeling method, extending the work of Blaheta (2004). In this method, the objects being modeled are syntax trees which require some mechanism to convert them into feature vectors. To encode structural information of the complex inputs, we propose a set of new features. Experimental results show that these new features lead to significant improvements."
P09-2064,Prediction of Thematic Rank for Structured Semantic Role Labeling,2009,10,1,2,1,4541,weiwei sun,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"In Semantic Role Labeling (SRL), it is reasonable to globally assign semantic roles due to strong dependencies among arguments. Some relations between arguments significantly characterize the structural information of argument structure. In this paper, we concentrate on thematic hierarchy that is a rank relation restricting syntactic realization of arguments. A loglinear model is proposed to accurately identify thematic rank between two arguments. To import structural information, we employ re-ranking technique to incorporate thematic rank relations into local semantic role classification results. Experimental results show that automatic prediction of thematic hierarchy can help semantic role classification."
D09-1153,{C}hinese Semantic Role Labeling with Shallow Parsing,2009,15,29,2,1,4541,weiwei sun,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Most existing systems for Chinese Semantic Role Labeling (SRL) make use of full syntactic parses. In this paper, we evaluate SRL methods that take partial parses as inputs. We first extend the study on Chinese shallow parsing presented in (Chen et al., 2006) by raising a set of additional features. On the basis of our shallow parser, we implement SRL systems which cast SRL as the classification of syntactic chunks with IOB2 representation for semantic roles (i.e. semantic chunks). Two labeling strategies are presented: 1) directly tagging semantic chunks in one-stage, and 2) identifying argument boundaries as a chunking task and labeling their semantic types as a classification task. Lor both methods, we present encouraging results, achieving significant improvements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL."
W08-2135,The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer Maximum Entropy {M}arkov Models,2008,14,4,3,1,4541,weiwei sun,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper describes a system to solve the joint learning of syntactic and semantic dependencies. An directed graphical model is put forward to integrate dependency relation classification and semantic role labeling. We present a bilayer directed graph to express probabilistic relationships between syntactic and semantic relations. Maximum Entropy Markov Models are implemented to estimate conditional probability distribution and to do inference. The submitted model yields 76.28% macro-average F1 performance, for the joint task, 85.75% syntactic dependencies LAS and 66.61% semantic dependencies F1."
C08-1105,Prediction of Maximal Projection for Semantic Role Labeling,2008,10,5,2,1,4541,weiwei sun,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree. It is reasonable to label arguments locally in such a sub-tree rather than a whole tree. To identify active region of arguments, this paper models Maximal Projection (MP), which is a concept in D-structure from the projection principle of the Principle and Parameters theory. This paper makes a new definition of MP in S-structure and proposes two methods to predict it: the anchor group approach and the single anchor approach. The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%. Experimental results also indicate that the prediction of MP improves semantic role labeling."
chen-etal-2006-study,A Study on Terminology Extraction Based on Classified Corpora,2006,10,7,4,0,48076,yirong chen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Algorithms for automatic term extraction in a specific domain should consider at least two issues, namely Unithood and Termhood (Kageura, 1996). Unithood refers to the degree of a string to occur as a word or a phrase. Termhood (Chen Yirong, 2005) refers to the degree of a word or a phrase to occur as a domain specific concept. Unlike unithood, study on termhood is not yet widely reported. In classified corpora, the class information provides the cue to the nature of data and can be used in termhood calculation. Three algorithms are provided and evaluated to investigate termhood based on classified corpora. The three algorithms are based on lexicon set computing, term frequency and document frequency, and the strength of the relation between a term and its document class respectively. Our objective is to investigate the effects of these different termhood measurement features. After evaluation, we can find which features are more effective and also, how we can improve these different features to achieve the best performance. Preliminary results show that the first measure can effectively filter out independent terms or terms of general use."
I05-4001,Domain Knowledge Engineering Based on Encyclopedias and the Web Text,2005,8,1,1,1,2475,zhifang sui,Proceedings of the Fifth Workshop on {A}sian Language Resources ({ALR}-05) and First Symposium on {A}sian Language Resources Network ({ALRN}),0,None
P00-1060,An Information-Theory-Based Feature Type Analysis for the Modeling of Statistical Parsing,2000,17,0,1,1,2475,zhifang sui,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types' power for syntactic structure prediction and draws a series of interesting conclusions.
W99-0618,An Information-Theoretic Empirical Analysis of Dependency-Based Feature Types for Word Prediction Models,1999,8,5,3,0,33578,dekai wu,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"Over the years, many proposals have been made to incorporate assorted types of feature in language models. However, discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult to compare the models objectively. In this paper, we take an information theoretic approach to select feature types in a systematic manner. We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object. The experiments yield several conclusions on the predictive value of several feature types and feature types combinations for word prediction, which are expected to provide guidelines for feature type selection in language modeling."
