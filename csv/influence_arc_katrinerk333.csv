2020.coling-main.268,S18-1116,0,0.0664394,"the best benchmark by 4.11 points in hit-at-one (H@1). 1 Introduction Hypernymy, or the IS-A relation, is one of the most important lexical relations. It is used to create taxonomies of terms and it is the main organizational criterion of nouns and verbs in WordNet (Fellbaum, 1998). Learning hypernymy is also important in practice, as knowing a word’s hypernyms gives an approximation of its meaning, and enables inferences in downstream tasks such as question answering and reading comprehension. Predicting hypernymy is still a challenging task for word embeddings (Pinter and Eisenstein, 2018; Bernier-Colborne and Barriere, 2018; Nickel and Kiela, 2018) and previous studies have shown that it is more difficult to predict hypernymy than other lexical relations (Balaˇzevi´c et al., 2019; Allen et al., 2019). Hypernymy prediction is often evaluated against a given taxonomy, typically WordNet (Fellbaum, 1998). The main hypothesis that we pursue in this paper is that knowledge of this taxonomy, in particular of taxonomy paths, will be helpful for hypernymy prediction. So we introduce two simple encoderdecoder based models for hypernym prediction that make use of information in the full taxonomy paths. There has been much"
2020.coling-main.268,Q17-1010,0,0.0901661,"ext2edges uses a sequence-to-sequence model with an attention mechanism. However, there are several important differences. First, it encodes the definition of an input hyponym, while our prediction conditions only on the vector representation of the synsets themselves without looking at their definitions. Obtaining a definition of an unknown word is not always feasible, especially for domain-specific jargon and neologisms that are frequently used but seldom defined in dictionaries. On the other hand, computing their embeddings is a less challenging task when using approaches such as fastText (Bojanowski et al., 2017), which also works for words seen only once because it interpolates from embeddings of each word piece.6 Another key difference is that it can only apply to rooted tree graphs with a single root. For this reason, it cannot be trained on the verb taxonomy in WordNet, which has more than one root. 3.4 Other Path Encoding Approaches We next discuss some related path encoding approaches which we do not compare in our experiments. First, Das et al. (2017) proposed a model for link prediction that is similar to Path Encoder. Given a multirelational knowledge base, their task is to assign the correct"
2020.coling-main.268,E17-2013,0,0.0497309,"Missing"
2020.coling-main.268,E17-1013,0,0.0174847,"defined in dictionaries. On the other hand, computing their embeddings is a less challenging task when using approaches such as fastText (Bojanowski et al., 2017), which also works for words seen only once because it interpolates from embeddings of each word piece.6 Another key difference is that it can only apply to rooted tree graphs with a single root. For this reason, it cannot be trained on the verb taxonomy in WordNet, which has more than one root. 3.4 Other Path Encoding Approaches We next discuss some related path encoding approaches which we do not compare in our experiments. First, Das et al. (2017) proposed a model for link prediction that is similar to Path Encoder. Given a multirelational knowledge base, their task is to assign the correct relation to those entity pairs linked by an entity-relation path but without a direct relation between them. Here, sequences of entities and relations are encoded with an LSTM and the relation whose vector is closest to the encoded path is returned. In our task, however, concepts are linked by only one relation (hypernymy or instance hypernymy). This prevents us from drawing additional information from other relation paths between synsets. Alsuhaiba"
2020.coling-main.268,N16-1155,0,0.0141529,"ch is taken as the initial state of the LSTM decoder. The decoder generates synsets sequentially, conditioned on previously generated synsets. While the attention mechanism assigns weights to the source tokens, here we only have a single source token (i.e., a query hyponym). In our task, the attention mechanism serves as a way to avoid “forgetting” the source hyponym while decoding long paths2 . Reversing the order of the source or target sequences can the improve performance of encoder-decoder models, since the encoded hidden state is closer to the first target token (Sutskever et al., 2014; Gillick et al., 2016). Motivated by this, we experiment with a model variant called hypo2path rev in which we reverse the target path to generate a sequence of hypernyms starting from the direct hypernym of the source. This frames every generation step as direct hypernym prediction, which the decoder may more easily learn. Examples of generated reversed paths are shown in Table 1. In order to determine whether generating an entire hypernym path as an auxiliary task helps to accurately predict a synset’s direct hypernym, or whether generating only the direct hypernym is enough, we also perform experiments with hypo"
2020.coling-main.268,C92-2082,0,0.574065,".01 self-discipline.n.01 person.n.01 Table 1: We frame hypernym prediction as a sequence generation problem. Given a query hyponym (e.g., pizza.n.01), the hypo2path rev model generates its taxonomy path, from its direct hypernym (dish.n.02) to the root node (entity.n.01). 3 and 7 indicate a correct and an incorrect prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, anoth"
2020.coling-main.268,D17-1030,0,0.0157398,"Synsets Is hypernym prediction more difficult for infrequent synsets (i.e., synsets whose lemmas rarely appear in the corpus from which the word vectors were derived)? We define a synset’s frequency as the average of the frequencies of its lemmas.18 We find that the 164 synsets with frequency under 2,000 have an H@1 score of 15.2 (an 8.4 point drop). To further quantify the effect of synset frequency on performance, we ranked and binned every prediction by synset frequency (Figure 1). There is a clear upward trend, suggesting that methods designed to learn better embeddings with sparse data (Herbelot and Baroni, 2017) could improve performance substantially. Figure 1: H@1 for hyponyms of different frequency within equal-sized bins of size 108. 7 Conclusion and Future Work In this paper we have considered the hypernym prediction task, the task of identifying the correct direct hypernym in a taxonomy of a synset given its embedding. In terms of evaluation, we have focused on both “exact match” (H@1) and “ballpark match” (WuP) metrics, and we have for the first time provided a comparison of existing models that had not previously been evaluated with the same metrics or on the same datasets. We have introduced"
2020.coling-main.268,S16-1169,0,0.0247806,"correct prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, another thread of recent research on hypernymy comes from the literature on knowledge base completion (Bordes et al., 2013; Nickel and Kiela, 2017; Pinter and Eisenstein, 2018; Dettmers et al., 2018). Here, hypernymy is considered as one of multiple different semantic relations between two nodes in a graph. Extending from this"
2020.coling-main.268,P19-2022,0,0.0300172,"Missing"
2020.coling-main.268,D15-1166,0,0.05232,"he direct hypernym. For example, flock.n.02 should be mapped to its hypernym path by generating entity.n.01  abstraction.n.06  group.n.01  biological group.n.01  animal group.n.01. So the model is tasked to translate source synsets to target synset sequences. We denote this model as hypo2path. Our intuition behind this model is that training with a more difficult objective (i.e., entire hypernym path prediction rather than direct hypernym prediction) may result in a stronger model. We use a standard LSTM-based sequence-to-sequence model (Sutskever et al., 2014) with Luong-style attention (Luong et al., 2015). This encodes a synset embedding of a hyponym into a hidden state, which is taken as the initial state of the LSTM decoder. The decoder generates synsets sequentially, conditioned on previously generated synsets. While the attention mechanism assigns weights to the source tokens, here we only have a single source token (i.e., a query hyponym). In our task, the attention mechanism serves as a way to avoid “forgetting” the source hyponym while decoding long paths2 . Reversing the order of the source or target sequences can the improve performance of encoder-decoder models, since the encoded hid"
2020.coling-main.268,N19-1226,0,0.0517915,"Missing"
2020.coling-main.268,D18-1201,0,0.158017,"rt performance, outperforming the best benchmark by 4.11 points in hit-at-one (H@1). 1 Introduction Hypernymy, or the IS-A relation, is one of the most important lexical relations. It is used to create taxonomies of terms and it is the main organizational criterion of nouns and verbs in WordNet (Fellbaum, 1998). Learning hypernymy is also important in practice, as knowing a word’s hypernyms gives an approximation of its meaning, and enables inferences in downstream tasks such as question answering and reading comprehension. Predicting hypernymy is still a challenging task for word embeddings (Pinter and Eisenstein, 2018; Bernier-Colborne and Barriere, 2018; Nickel and Kiela, 2018) and previous studies have shown that it is more difficult to predict hypernymy than other lexical relations (Balaˇzevi´c et al., 2019; Allen et al., 2019). Hypernymy prediction is often evaluated against a given taxonomy, typically WordNet (Fellbaum, 1998). The main hypothesis that we pursue in this paper is that knowledge of this taxonomy, in particular of taxonomy paths, will be helpful for hypernymy prediction. So we introduce two simple encoderdecoder based models for hypernym prediction that make use of information in the full"
2020.coling-main.268,N19-1196,0,0.0284774,"M3GM reranks the top N candidates predicted by a local distributional feature model such as TransE (Bordes et al., 2013). CRIM Bernier-Colborne and Barriere (2018) proposed a hybrid system which exploits both unsupervised pattern-based hypernym discovery and supervised projection learning (Ustalov et al., 2017). The core idea of the supervised algorithm is to learn multiple projection matrices which map a query embedding to a target hypernym. Their system ranked first on the three subtasks in SemEval-2018 Task 9 (Camacho-Collados et al., 2018). Text2edges The approach most similar to ours is (Prokhorov et al., 2019), which represents each hyponym using its textual definition from WordNet and maps it to its taxonomy path from the root to its parent node. Given the definition of a query hyponym, a bidirectional LSTM encoder-decoder with attention is used to generate the taxonomic path starting from the root node. For example, the definition of swift (“a small bird that resembles a swallow and is noted for its rapid flight”) is mapped to the sequence ‘animal, chordate, vertebrate, bird, apodiform bird’. Their best system, text2edges with pre-trained ConceptNet numberbatch embeddings (Speer et al., 2017), us"
2020.coling-main.268,C08-1092,0,0.0971021,"Missing"
2020.coling-main.268,P18-2057,0,0.0127324,"rom its direct hypernym (dish.n.02) to the root node (entity.n.01). 3 and 7 indicate a correct and an incorrect prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, another thread of recent research on hypernymy comes from the literature on knowledge base completion (Bordes et al., 2013; Nickel and Kiela, 2017; Pinter and Eisenstein, 2018; Dettmers et al., 2018). Here, hypernymy i"
2020.coling-main.268,P16-1226,0,0.0197747,"s its taxonomy path, from its direct hypernym (dish.n.02) to the root node (entity.n.01). 3 and 7 indicate a correct and an incorrect prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, another thread of recent research on hypernymy comes from the literature on knowledge base completion (Bordes et al., 2013; Nickel and Kiela, 2017; Pinter and Eisenstein, 2018; Dettmers et al., 20"
2020.coling-main.268,P06-1101,0,0.11377,"a correct and an incorrect prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, another thread of recent research on hypernymy comes from the literature on knowledge base completion (Bordes et al., 2013; Nickel and Kiela, 2017; Pinter and Eisenstein, 2018; Dettmers et al., 2018). Here, hypernymy is considered as one of multiple different semantic relations between two nodes in"
2020.coling-main.268,E17-2087,0,0.0121744,"5. We omit these results, since they were similar to encoding the full path. 3 3009 M3GM Max-Margin Markov Graph Models (M3GM) (Pinter and Eisenstein, 2018) exploit graph motif properties in WordNet (e.g., number of cycles of length 2 or 3) to predict different semantic relations. As a global feature model, M3GM reranks the top N candidates predicted by a local distributional feature model such as TransE (Bordes et al., 2013). CRIM Bernier-Colborne and Barriere (2018) proposed a hybrid system which exploits both unsupervised pattern-based hypernym discovery and supervised projection learning (Ustalov et al., 2017). The core idea of the supervised algorithm is to learn multiple projection matrices which map a query embedding to a target hypernym. Their system ranked first on the three subtasks in SemEval-2018 Task 9 (Camacho-Collados et al., 2018). Text2edges The approach most similar to ours is (Prokhorov et al., 2019), which represents each hyponym using its textual definition from WordNet and maps it to its taxonomy path from the root to its parent node. Given the definition of a query hyponym, a bidirectional LSTM encoder-decoder with attention is used to generate the taxonomic path starting from th"
2020.coling-main.268,C14-1212,0,0.0140257,"h rev model generates its taxonomy path, from its direct hypernym (dish.n.02) to the root node (entity.n.01). 3 and 7 indicate a correct and an incorrect prediction, respectively. In each example, an underlined synset corresponds to what the model predicted as a direct hypernym. 2 Hypernym Prediction Several tasks related to hypernymy have been proposed under different names: extracting is-a relations from text (hypernym discovery) (Hearst, 1992; Snow et al., 2005; Camacho-Collados et al., 2018), binary classification of whether two given words are in a hypernym relation (hypernym detection) (Weeds et al., 2014; Shwartz et al., 2016; Roller et al., 2018), and constructing or extending a taxonomy (taxonomy induction) (Snow et al., 2006; Jurgens and Pilehvar, 2016). Another recently introduced task is hierarchical path completion (Alsuhaibani et al., 2019), where, given a hypernym path of length 4 from WordNet, the task is to predict the correct hyponym(s). While hypernymy has long been studied in computational lexical semantics, another thread of recent research on hypernymy comes from the literature on knowledge base completion (Bordes et al., 2013; Nickel and Kiela, 2017; Pinter and Eisenstein, 201"
2020.conll-1.17,N09-1003,0,0.200549,"Missing"
2020.conll-1.17,K18-1028,0,0.0851662,"d token prediction and next sentence prediction. For bert-base, the representation of an input sequence consists in a 12-layer activation network with 768-dimensional vectors for each input token (where tokens correspond to sub-word WordPieces [Schuster and Nakajima, 2012]) at each of 12 layers. The preponderance of research surrounding analysis and interpretability of BERT has been dubbed BERTology, after the poster-child of the contextual revolution (cf. Rogers et al., 2020 for an excellent survey). Probing Tasks Most studies of word meaning in BERT follow an agenda of extrinsic evaluation (Artetxe et al., 2018). Tenney et al. (2019a) found that CLMs improve over non-contextual counterparts largely on syntactic tasks, with smaller gains on semantic tasks. Tenney et al. (2019b) introduced edge-probing tasks to analyze the layer-wise structure of BERT, and found that early layers perform syntactic tasks like part-of-speech and dependency tagging, while later layers encode information pertinent to semantic tasks like coreference resolution, relation labeling, and semantic proto-role labeling. We aim to advance this kind of structural analysis of BERT through the intrinsic evaluation of representations a"
2020.conll-1.17,Q17-1010,0,0.0448635,"7 does provide marginally better results, but the success of the unioned model demonstrates that post hoc selection of K is not necessary to achieve good performance. Table 1 compares the best unioned models for similarity and relatedness to state-of-the-art distributional approaches trained on running monolingual text (without the injection of structured knowledge), as well as to other CLM-based static embeddings. For distributional models we compared to Symmetric Pattern embeddings (SP-15, [Schwartz et al., 2015]), GloVe (Pennington et al., 2014), CBOW (Mikolov et al., 2013), and FastText (Bojanowski et al., 2017). For other CLMbased embeddings, we compared to Bommasani et al. (2020), who tested layer-wise token aggregations for numerous architectures: BERT, RoBERTa, GPT2, XLNet, and DistilBert. We also compared to Ethayarajh (2019), who examined the first PCA component of individual layers of several CLMs. Performance drastically improves over other CLMbased embeddings, and our generalized similarity estimation model surpasses the distributional stateof-the-art on all three datasets.6 6 Benchmark scores are self-reported by the authors or obtained from Lastra-D´ıaz et al. (2019)’s reproducible survey."
2020.conll-1.17,2020.acl-main.431,0,0.703693,". The contributions of this paper are as follows: 1. Application of BERT for type-level lexical modeling, and the testing of type-level lexicalsemantic hypotheses. 2. Clustering of contextualized representations into multi-prototype embeddings, which maintain the advantages of contextualization without the complexity burden of exemplar models, leading to improved performance on similarity and relatedness estimation. Figure 1: 2D t-SNE visualization of layer 8 vectors for tokens of asylum and madhouse sampled from the BNC. eraging over exemplar models to generate a single vector for each word (Bommasani et al., 2020). However, these models do not leverage the contextual knowledge resident in BERTs later layers. Our approach aims to capture the regularities in contextual variation while reducing the noise it introduces. A set of BERT token vectors for a single word naturally tends to separate spatially into groups of similar usages, or ‘usage types’ (Giulianelli et al., 2020). Usage types often correspond to polysemous or homonymous senses, idiomatic constructions, and affordances.1 Figure 1 shows a t-SNE visualization of tokens for madhouse and asylum. The dominant usage of asylum is as political refuge,"
2020.conll-1.17,D19-1006,0,0.544006,"gh the intrinsic evaluation of representations at different layers. 228 Contextual Word Embeddings Existing applications of CLMs to lexical tasks use exemplar models or single-prototype models. Wiedemann et al. (2019) successfully employed a K-nearestneighbor approach to BERT exemplar models for word sense disambiguation (WSD). Coenen et al. (2019) created a visualization tool that generates a ‘word cloud’ from BERT tokens, browsable by layer.2 They also achieved a state-of-the-art F1 score on a WSD task with the simple scheme involving sense-annotated training data from Peters et al. (2018). Ethayarajh (2019) generated static embeddings from CLM models, using the first PCA component sets of token representations. Bommasani et al. (2020) experimented with averaging tokens for context-agnostic word vectors. This approach performs well on similarity and relatedness tasks compared to traditional static embeddings. However, averaging evaporates much of BERT’s contextual variation, cutting off its potential to aid in similarity estimation. It’s no surprise that averages derived from earlier layers of BERT performed best at similarity estimation. Later layers demonstrate more contextual variation (Ethaya"
2020.conll-1.17,W16-2506,0,0.0190436,"wever, major critiques have been leveled at the standard similarity datasets, and even at the construct of similarity itself. Depending on which comparison a word enters into, there is variation in which senses (and/or features) of the word are considered for the calculation. Nelson Goodman’s (1972) dismissal of similarity argues that the selection of properties to consider varies so widely as to be essentially arbitrary. Thus, similarity datasets which ask raters to assess the similarity between two words out of context are argued to be premised on the flawed notion that similarity is fixed (Faruqui et al., 2016). Two words are never absolutely similar or dissimilar. Rather, in assessing the likeness between two words, one implicitly selects some grounds for assessing their likeness. In Tversky’s classic (1977) feature-matching model, the similarity of two items is computed from the number of shared properties they have as compared to the number of properties they hold distinct. Tversky notes that property selection is subject to variation. To give an extreme example involving polysemy, the relevant features for comparing bishop with rabbi are different from those for relevant for comparing bishop and"
2020.conll-1.17,D16-1235,0,0.0726713,"Missing"
2020.conll-1.17,2020.acl-main.365,0,0.035267,"Missing"
2020.conll-1.17,N19-1419,0,0.0256733,"individual outliers is not relevant to these tasks, which deal with stereotypical or prototypical class relationships rather than relationships between individuals. Similarity vs Relatedness The analysis also uncovered differences in the semantic relations approximated by different layers. The final layer of BERT approximate relatedness, while layer 7 is optimal for estimating similarity. This finding bears an interesting connection to recent insights in BERTology. The middle layers of bert-base (6-9) are consistently noted to be the most transferable, i.e., to perform the best across tasks (Hewitt and Manning, 2019; Goldberg, 2019; Jawahar et al., 2019; Liu et al., 2019a). The final layers, on the other hand, are the most specific to the next sentence prediction task. The connection suggests that successful representation of semantic similarity may be critical to many NLP tasks, moreso than relatedness. Our results support the thesis that static vectors cannot surfaces all aspects of lexical semantic meaning at once (Artetxe et al., 2018). But, perhaps when forced to compromise on one general purpose embedding for downstream applications, one which approximates similarity may be preferable over those wh"
2020.conll-1.17,W13-2609,0,0.0250296,"and Weber, 2006), and more resilient to brain damage (Katz and Goodglass, 1990) than abstract words. The dominant explanation for concreteness effects is called the Dual-Coding Hypothesis (Paivio, 1971, 2013; Crutch and Warrington, 2005). It holds that concrete and abstract concepts are organized differently in the brain: the former are grounded in experience, while the latter are based solely on other concepts. The sensory richness of concrete concepts explains their memory advantage. Significant distributional-semantic research has tested the Dual-Coding Hypothesis, with mixed results (cf. Hill et al., 2013, 2014a,b). Dual-Coding Theory demands discrete types of conceptual representations, and raises an uncomfortable metaphysical issue of whether a concept can be about anything other than the purely physical. Embodied views of abstract representation (Kousta et al., 2011) hold that both linguistic and experiential information contribute to rich representations for all concepts, and that the apparent distinction arises from statistical patterns in the proportion of sensorimotor to affective experiential information undergirding the concepts. The question then arises of how to account for concrete"
2020.conll-1.17,Q14-1023,0,0.0385248,"Missing"
2020.conll-1.17,J15-4004,0,0.0938472,"f K-means clusters (p < 1x10−100 ). of their two closest prototypes. We also experimented with their AVG S IM, defined as the mean of all pairwise similarities between w’s centroids against those of w0 . For AVG S IM, two words are close if many of their prototypes are close. In stark contrast to Reisinger and Mooney (2010), we found M AX S IM to universally outperform AVG S IM, and so report results on the former only. 4.2 4 4.1 Similarity and Relatedness Datasets The embeddings were evaluated against three similarity datasets and four relatedness datasets. For similarity, we used SimLex999 (Hill et al., 2015), SimVerb3500 (Gerz et al., 2016), and WordSim353sim (Agirre et al., 2009). The latter is a partition of WordSim353 (Finkelstein et al., 2001) devised to approximate similarity. SimLex999 contains 999 word pairs balanced for concreteness and annotated specifically for similarity. SimVerb3500, containing 3500 verb pairs, was also designed specifically to target similarity. For relatedness datasets we used WordSim353rel (Agirre et al. (2009)’s complementary relatedness subset); MEN, containing 3000 word pairs (Bruni et al., 2014), and YP-130, consisting of 130 verb pairs (Yang and Powers, 2006)."
2020.conll-1.17,P19-1356,0,0.0162926,"ese tasks, which deal with stereotypical or prototypical class relationships rather than relationships between individuals. Similarity vs Relatedness The analysis also uncovered differences in the semantic relations approximated by different layers. The final layer of BERT approximate relatedness, while layer 7 is optimal for estimating similarity. This finding bears an interesting connection to recent insights in BERTology. The middle layers of bert-base (6-9) are consistently noted to be the most transferable, i.e., to perform the best across tasks (Hewitt and Manning, 2019; Goldberg, 2019; Jawahar et al., 2019; Liu et al., 2019a). The final layers, on the other hand, are the most specific to the next sentence prediction task. The connection suggests that successful representation of semantic similarity may be critical to many NLP tasks, moreso than relatedness. Our results support the thesis that static vectors cannot surfaces all aspects of lexical semantic meaning at once (Artetxe et al., 2018). But, perhaps when forced to compromise on one general purpose embedding for downstream applications, one which approximates similarity may be preferable over those which approximate relatedness—these embe"
2020.conll-1.17,2021.ccl-1.108,0,0.160281,"Missing"
2020.conll-1.17,D14-1162,0,0.092714,"union of all clusters K ≤ 9. For both similarity and relatedness, K = 7 does provide marginally better results, but the success of the unioned model demonstrates that post hoc selection of K is not necessary to achieve good performance. Table 1 compares the best unioned models for similarity and relatedness to state-of-the-art distributional approaches trained on running monolingual text (without the injection of structured knowledge), as well as to other CLM-based static embeddings. For distributional models we compared to Symmetric Pattern embeddings (SP-15, [Schwartz et al., 2015]), GloVe (Pennington et al., 2014), CBOW (Mikolov et al., 2013), and FastText (Bojanowski et al., 2017). For other CLMbased embeddings, we compared to Bommasani et al. (2020), who tested layer-wise token aggregations for numerous architectures: BERT, RoBERTa, GPT2, XLNet, and DistilBert. We also compared to Ethayarajh (2019), who examined the first PCA component of individual layers of several CLMs. Performance drastically improves over other CLMbased embeddings, and our generalized similarity estimation model surpasses the distributional stateof-the-art on all three datasets.6 6 Benchmark scores are self-reported by the autho"
2020.conll-1.17,N19-1112,0,0.101671,"context-sensitivity of these processes. BERT’s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level behavior and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness. 1 Introduction The rampant success enjoyed by contextualized language models (CLMs) like CoVe (McCann et al., 2017), ElMo (Peters et al., 2018), BERT (Devlin et al., 2018), and RoBERTa (Liu et al., 2019b) has precipitated a deluge of research into analyzing and interpreting their functionality. But to date, there has been little work analyzing their lexical semantic knowledge. This paper seeks to answer two questions: 1) Is it possible to generate useful static word-type embeddings from BERT activations for word tokens? 2) What sort of semantic relations are represented in embeddings generated from BERT? ‘Useful’ word embeddings are those which successfully represent target semantic relations and enable the testing of linguistic and cognitive hypotheses. Many linguistic tasks and questions c"
2020.conll-1.17,N18-1202,0,0.3641,"tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT’s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level behavior and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness. 1 Introduction The rampant success enjoyed by contextualized language models (CLMs) like CoVe (McCann et al., 2017), ElMo (Peters et al., 2018), BERT (Devlin et al., 2018), and RoBERTa (Liu et al., 2019b) has precipitated a deluge of research into analyzing and interpreting their functionality. But to date, there has been little work analyzing their lexical semantic knowledge. This paper seeks to answer two questions: 1) Is it possible to generate useful static word-type embeddings from BERT activations for word tokens? 2) What sort of semantic relations are represented in embeddings generated from BERT? ‘Useful’ word embeddings are those which successfully represent target semantic relations and enable the testing of linguistic and"
2020.emnlp-main.427,N19-1052,0,0.0460966,"Missing"
2020.emnlp-main.427,C18-1248,1,0.869209,"Missing"
2020.emnlp-main.427,S19-2151,0,0.029602,"Missing"
2020.emnlp-main.427,N19-1423,0,0.544284,", finding the right solution to a problem is difficult, since advice may be spread over multiple posts and pages online. Even within the same post, not * Work done as an undergraduate student at UT Austin. † Work done at UT Austin while on the DREU undergraduate research program. Automatic identification of advice in text would thus be extremely useful. Yet, as we see above, it would also require a deep understanding of semantics and discourse pragmatics. In recent years, NLP systems based on large-scale pre-trained language models have shown impressive gains on several linguistic benchmarks (Devlin et al., 2019; Clark et al., 2020; Yang et al., 2019). However, these same models have been found to struggle at tasks that require higher-level processing (Ettinger, 2020), including giving advice (Zellers et al., 2020). This work aims to advance both our understanding of how people give advice, as well as to provide resources for learning to identify advice. First, we construct a dataset of annotations of advice in English from two advice-focused Reddit commu5295 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5295–5306, c November 16–20, 2020. 2020 Associati"
2020.emnlp-main.427,2020.tacl-1.3,0,0.0242687,"as an undergraduate student at UT Austin. † Work done at UT Austin while on the DREU undergraduate research program. Automatic identification of advice in text would thus be extremely useful. Yet, as we see above, it would also require a deep understanding of semantics and discourse pragmatics. In recent years, NLP systems based on large-scale pre-trained language models have shown impressive gains on several linguistic benchmarks (Devlin et al., 2019; Clark et al., 2020; Yang et al., 2019). However, these same models have been found to struggle at tasks that require higher-level processing (Ettinger, 2020), including giving advice (Zellers et al., 2020). This work aims to advance both our understanding of how people give advice, as well as to provide resources for learning to identify advice. First, we construct a dataset of annotations of advice in English from two advice-focused Reddit commu5295 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5295–5306, c November 16–20, 2020. 2020 Association for Computational Linguistics nities – r/AskParents and r/needadvice, totalling 18456 sentences across 684 posts (§3). These two subreddits are different in"
2020.emnlp-main.427,P18-1019,1,0.840469,"sts which contain the following highly frequent flairs: “Education”, “Career”, “Mental Health”, “Life Decisions”, and “Friendships”. Some flairs were not considered due to the lack of variety in responses. For example, in the “Medical” flair, replies often consisted of telling the original poster to see the doctor. 3.2 Annotation Task We crowdsource advice annotations from Amazon Mechanical Turk. Despite the inherent noise due to crowdsourcing (Parde and Nielsen, 2017), recent work showed that when designed carefully, aggregated crowdsourced annotations are trustworthy even for complex tasks (Nye et al., 2018). As (1) illustrates, not all sentences in a response to an advice-seeking question constitute advice. Thus, we want annotators to highlight which parts of the response to a question are advice, and which Dataset Sentences κmaj κDS AskParents 203 0.620 0.669 needadvice 110 0.680 0.681 Table 1: Gold annotator agreement on the internal task. are not. We also want to find instances of implicit advice, i.e., advice that is given indirectly, like in (2). To ensure that annotators can also identify advice that might be marked using contextual cues, we provide annotators with sufficient context. In o"
2020.emnlp-main.427,N15-1166,0,0.0116773,"drive mine he phone night adult stay set big game doctor fun bring less show love depend activity eat normal put teacher family etc minute teach allow home they area luck degree company college interview hobby student field mental course op sorry job dog anxiety hire eventually position path shit comment human online community shoe thanks note exercise depression slowly To motivate that the language of advice varies systematically from non-advice, we quantify how strongly individual lemmas are associated with advice versus non-advice text. We use the log-odds ratio as a metric of comparison (Nye and Nenkova, 2015). To counteract the tendency of log-odds scores to highlight infrequent lemmas (Monroe et al., 2017), we filter out lemmas that occurred less than 20 times in the train and validation set of our corpus. Table 5 shows the top 30 lemmas (excluding punctuation characters and numbers) from advice and non-advice sentences for each subreddit ranked by their log-odds ratio. We observe that there are fewer verbs among non-advice lemmas than advice lemmas, and that lemmas which are generally used in expressing sentiment (luck, sorry, thanks) are more likely to be found in non-advice sentences. Combined"
2020.emnlp-main.427,D17-1204,0,0.0171856,"for posting and replying to posts. We believe all of these factors contribute to two different “styles” of advice-giving. For r/needadvice, we study posts which contain the following highly frequent flairs: “Education”, “Career”, “Mental Health”, “Life Decisions”, and “Friendships”. Some flairs were not considered due to the lack of variety in responses. For example, in the “Medical” flair, replies often consisted of telling the original poster to see the doctor. 3.2 Annotation Task We crowdsource advice annotations from Amazon Mechanical Turk. Despite the inherent noise due to crowdsourcing (Parde and Nielsen, 2017), recent work showed that when designed carefully, aggregated crowdsourced annotations are trustworthy even for complex tasks (Nye et al., 2018). As (1) illustrates, not all sentences in a response to an advice-seeking question constitute advice. Thus, we want annotators to highlight which parts of the response to a question are advice, and which Dataset Sentences κmaj κDS AskParents 203 0.620 0.669 needadvice 110 0.680 0.681 Table 1: Gold annotator agreement on the internal task. are not. We also want to find instances of implicit advice, i.e., advice that is given indirectly, like in (2). To"
2020.emnlp-main.427,S19-2215,0,0.0852041,"constructed a dataset from the subreddits r/needadvice and r/AskParents as a general purpose resource for studying the breadth of advice-giving strategies. Our modelling experiments aim to establish baseline performance for rule-based models and language models at identifying advice, as well as explore how their performance varies with domain and provided context. We model advice identification as a binary classification task – given a sentence, predict whether the sentence is advice or not. Baselines We test the baseline rule-based model and the top performing rule-based submission (NTUA-IS; Potamias et al., 2019) from SEMEVAL Task 9 2019 on our dataset, and use the results of these rule based models as baselines against which to gauge the performance of more advanced ones based on pre-trained language models. The baseline model provided by Negi et al. (2019) uses search patterns to identify suggestions, including words (suggest, recommend), phrases (.*wouldslike.*if.*), and part-of-speech (POS) tags (modals, past tense verbs). However, some of these rules are naive and not intepretable – such as classifying a sentence as a suggestion if it contains a modal or the base form 5300 Utilizing pre-trained"
2020.emnlp-main.427,E12-2021,0,0.0242135,"Missing"
2020.emnlp-main.427,P19-3007,0,0.0535042,"Missing"
2020.emnlp-main.427,D18-1116,0,0.0469115,"Missing"
2021.naacl-main.129,2020.repl4nlp-1.10,0,0.0107468,"he classes. Training We split the data into 5 cross-validation folds, stratified by congressional hearing (to preserve the differing response distributions as seen in Figure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the com"
2021.naacl-main.129,J08-4004,0,0.105177,"Missing"
2021.naacl-main.129,Q19-1035,0,0.0287432,"Missing"
2021.naacl-main.129,N03-2011,0,0.336484,"Missing"
2021.naacl-main.129,N19-1173,0,0.0125943,"ure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the complete label (conversation act+intent). We train the classifiers independently, and condition the second classifier on the ground truth of the first classifier during"
2021.naacl-main.129,N19-1177,0,0.0258602,"speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting is argument persuasiveness, where Durmus and Cardie (2018) find prior beliefs of the audience play a strong role in their ability to be persuaded, which further motivates our focus on the annotator’s bias. 3 Dataset item question response #sents/ turn #toks/ total turn sents total toks total spkrs 4.1 2.6 81.5 47.0 82582 48831 91 20 4096 2634 Table 1: Statistics of our 20 U.S. congressional hearings. the response, and the data is plentiful.2 A dataset statement is in Appendix D. 3.1 Dataset creation Con"
2021.naacl-main.129,2020.emnlp-main.734,0,0.0351484,"bligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perception"
2021.naacl-main.129,P11-1032,0,0.0605211,"at the utterance level. Classification models typically combine representations of linguistic units (word, utterance, conversation-level) (Chen et al., 2018). In our work, we employ a hierarchical model to account for the levels in our label taxonomy. Intent detection is traditionally applied to human-computer scenarios for task-specific goals such as booking a flight. Our conversation data is not task-oriented, and we thus define our intents more closely aligned with beliefs in the sincerity of the speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting"
2021.naacl-main.129,Q19-1043,0,0.0269729,"ons of dialogue, or discourse obligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to un"
2021.naacl-main.129,C16-1181,0,0.029574,"Missing"
2021.naacl-main.129,W12-3205,0,0.0272315,"a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perceptions of discourse in a non-cooperative environment. More broadly, we show the need and value 1 for considering the subjectivity of NLP tasks. Our https://sites.google.com/view/ work i"
2021.naacl-main.129,W19-0411,0,0.0830517,". congressional hearings. In Figure 1, annotators give conflicting assessments of responses given by the witness Mark Zuckerberg (CEO of Facebook) who is being questioned by Congressman Eliot Engel. Discourse, like many uses of language, has inherent ambiguity, meaning it can have multiple, valid interpretations. Much work has focused on characTo make sense of our setting that has speakers terizing these “genuine disagreements” (Asher and (witness, politicians) and observers (annotators), Lascarides, 2003; Das et al., 2017; Poesio et al., we are inspired by the game-theoretic view of con2019; Webber et al., 2019) and incorporating their uncertainty through concurrent labels (Rohde et al., versation in Asher and Paul (2018). The players (witness, politicians) make certain discourse moves 2018) and underspecified structures (Hanneforth et al., 2003). However, prior work does not exam- in order to influence a third party, who is the judge of the game (the annotator). Importantly, the judge ine the subjectivity of discourse: how you resolve makes biased evaluations about the type of the an ambiguity by applying your personal beliefs and player (e.g., sincere vs. deceptive), which leads preferences. Our wo"
burchardt-etal-2006-salsa,erk-pado-2006-shalmaneser,1,\N,Missing
burchardt-etal-2006-salsa,burchardt-etal-2006-salto,1,\N,Missing
burchardt-etal-2006-salsa,fliedner-2006-towards,0,\N,Missing
burchardt-etal-2006-salsa,W04-2703,0,\N,Missing
burchardt-etal-2006-salsa,H05-1047,0,\N,Missing
burchardt-etal-2006-salsa,P98-1013,0,\N,Missing
burchardt-etal-2006-salsa,C98-1013,0,\N,Missing
burchardt-etal-2006-salsa,J02-3001,0,\N,Missing
burchardt-etal-2006-salsa,J05-1004,0,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2004-powerful,1,\N,Missing
burchardt-etal-2006-salsa,W04-1906,1,\N,Missing
burchardt-etal-2006-salto,brants-plaehn-2000-interactive,0,\N,Missing
burchardt-etal-2006-salto,P97-1003,0,\N,Missing
burchardt-etal-2006-salto,P98-1013,0,\N,Missing
burchardt-etal-2006-salto,C98-1013,0,\N,Missing
burchardt-etal-2006-salto,P03-1068,1,\N,Missing
burchardt-etal-2006-salto,erk-pado-2004-powerful,1,\N,Missing
C14-1097,W11-2501,0,0.672349,", proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by counting co-occurrences of these word types within a window of two words on the left and the right, using the top 20k most frequent content words as dimensions. The space was transformed using Positive Pointwise Mutual Information (PPMI). U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts instead of 2-word windows. TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM contains a list of weighted tuples, hhw1 , l, w2 i, σi, where w1 and w2 are content words, l is a corpus-derived syntagmatic relationship between the words, and σ is a weight estimating saliency of the relationship. We construct vectors for every unique w1 using the set of hl, w2 i pairs as dimensions and corresponding σ values as dimension weights. We select TypeDM for its excellent performance in previous comparisons of distributional hypernymy measures (Lenci and Benotto, 2012). Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three spaces to"
C14-1097,E12-1004,0,0.261153,"ibutional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy dete"
C14-1097,P99-1008,0,0.0266216,"imensions are context items (for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the D"
C14-1097,W04-3205,0,0.00966199,"uced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in whic"
C14-1097,W09-0215,0,0.0257716,"cision P (r) at every rank r among u’s dimensions – where precision is the fraction of dimensions shared with v –, and weighting by the rank of the same dimension in the broader term, rel0 (v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998). (We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).) ( 1 if x &gt; 0; 1(x) = 0 otherwise Pn u · 1(vi ) Pn i W eedsP rec(u, v) = i=1 (1) i=1 ui P|1(u)| P (r) · rel0 (v, r, u)) APinc(u, v) = r=1 (2) |1(u)| p balAPinc(u, v) = APinc(u, v) · LIN(u, v) The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the narrower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distributional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion Hypothesis as the degree to which a ⊆ relation holds, Lenci and Benotto test the degree to which proper inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are included in the contexts"
C14-1097,C04-1036,0,0.0220452,"to distinguish between semantic relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002) argues that distributional models cannot be a valid model of conceptual representation. Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog"
C14-1097,N03-1011,0,0.0130184,"(for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclus"
C14-1097,C92-2082,0,0.516587,"been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word is represented as a vector in a high-dimensional space in which the dimensions are context items (for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation indu"
C14-1097,P13-2078,0,0.0181,"relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appe"
C14-1097,S12-1012,0,0.387129,". Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which sta"
C14-1097,N13-1090,0,0.00751658,"ifferences features5 are analogous to a supervised distributional inclusion measure. The difference between two words on a particular dimension captures the degree of distributional inclusion on that dimension. The primary distinction between the difference features and the unsupervised measures is that the supervised classifier learns to weight the importance of different dimensions. The f features encode directional aspects of distributional inclusion: that the hyponym contexts should be included in 4 After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013). We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting the best. We also tried the Diff features with an SVM and other nonlinear classifiers, but they performed worse. 5 1030 Data set Baseline Classifier U+W2300 U+Sent300 TypeDM300 B LESS .46 Concat Diff .76 .84 .73 .80 .82 E NTAILMENT .50 Concat Diff .81 .85 .78 .82 .65 .85 Table 2: Average accuracy of Concat and Diff on B LESS and E NTAILMENT using different spaces for feature generation. those of the hypernym (the weight learned is positive), and the hypernym contexts s"
C14-1097,P06-1015,0,0.0335869,"Missing"
C14-1097,P06-1101,0,0.0594759,"hat NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy or the more loosely defined relation"
C14-1097,J06-3003,0,0.0391346,"nducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hypony"
C14-1097,W03-1011,0,0.0371289,"nymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy or the more loosely defined relation of lexical entailment. Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u for both a word and its associated vector hu1 , . . . , un i. Kotlerman et al. (2010) predict lexical entailment with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general notion is that scores should increase with the number of dimensions of v that u shares, and also give more weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This"
C14-1097,C04-1146,0,0.232308,"e a valid model of conceptual representation. Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 200"
C14-1097,P05-1014,0,0.961617,"rity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We show that the current best unsupervised approach is brittle in that their performance depends on"
C14-1097,J09-3004,0,0.0148978,"otlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We show that the current best unsupervised approach is brittle in that their performance depends on the space they are applied to. This r"
C14-1097,P06-1085,0,\N,Missing
C14-1097,J06-1005,0,\N,Missing
D07-1042,J02-2003,0,0.202254,"Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two problems: (a), limited coverage; and (b), the resource (at least partially) predetermines the generalisations that they can make. In this pa"
D07-1042,P07-1028,1,0.809619,"or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two problems: (a), limited coverage; and (b), the resource (at least partially) predetermines the generalisations that they can make. In this paper, we investigate whether it is possible to predict the plausibility of (v, r, a) triples in a completely corpus-driven way. We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. While that model relies on corpora with semantic role annotation, we show that it is possible to predict plausibility ratings solely on the basis of a parsed corpus, by using shallow cues and a suitable vector space specification. For evaluation, we use two balanced data sets of human plausibility judgements, i.e., datasets where each verb is paired both with a good agent and a good patient, and where both nouns are presented in either semantic relation (as in Table 1). Using balanced test data is a particularly difficult ta"
D07-1042,J02-3001,0,0.452368,"to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 20"
D07-1042,J93-1005,0,0.158514,"l preferences and argument, for such (verb, relation, argument), in short, (v, r, a), triples. Being able to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in"
D07-1042,P04-1061,0,0.0372102,"ctor space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. The goal of our exposition is thus to develop a model that can use more training data, and represent the corpus information optimally in order to obtain superior coverage. In fact, tasks (a) and (b) can be solved on the basis of unparsed corpora, but we would expect the results to be rather noisy. Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. We therefore describe two instantiations of our model: one based on an unprocessed corpus, and one based on a dependency-based parsed corpus. By comparing the models, we can gauge whether syntactic preprocessing improves model performance. In the following, we describe the strategies the two models adopt for (a) and (b). Example. Figure 1 shows an example vector space. Consider v = “shoot”, r = agent, and a = “hunter”. In order to judge whether a hunter is a plausible agent of “shoot”, the vector space re"
D07-1042,P99-1004,0,0.191718,"Seenr (v) by using all subjects and objects of v as agents and patients, respectively. We then construct a vector space for the experimental arguments and known headwords.2 We use 2,000 dimensions again, but adopt the most frequent (head , grammatical function) pairs in the BNC as basis elements. The context window is formed by subject and object dependencies. All counts are log-likelihood transformed. We experiment with two distance measures to compute vector similarity, namely the Jaccard Coefficient and Cosine Distance, both of which have been shown to yield good performance in NLP tasks (Lee, 1999; McDonald and Lowe, 1998). Evaluation Procedure. We evaluate our models by correlating the predicted plausibility values with the human judgements, which range between 1 and 7. Since the human judgement data is not normally distributed, we use Spearman’s ρ, a non-parametric rank-order test. We determine the statistical significance of differences in correlation strength using the method described in Raghunathan (2003). This method can deal with missing values and thus allows us to compare models with different coverage. It is difficult to specify a straightforward baseline for our correlation"
D07-1042,P93-1016,0,0.367173,"il which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. The goal of our exposition is thus to develop a model that can use more training data, and represent the corpus information optimally in order to obtain superior coverage. In fact, tasks (a) and (b) can be solved on the basis of unparsed corpora, but we would expect the results to be rather noisy. Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. We therefore describe two instantiations of our model: one based on an unprocessed corpus, and one based on a dependency-based parsed corpus. By comparing the models, we can gauge whether syntactic preprocessing improves model performance. In the following, we describe the strategies the two models adopt for (a) and (b). Example. Figure 1 shows an example vector space. Consider v = “shoot”, r = agent, and a = “hunter”. In order to judge whether a hunter is a plau"
D07-1042,J03-4004,0,0.311533,", argument), in short, (v, r, a), triples. Being able to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural"
D07-1042,P04-1036,0,0.146281,"alled basis elements). The underlying assumption is that words with similar meanings occur in similar contexts, and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), is a measure of their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton et al., 1975), automatic thesaurus extraction (Grefenstette, 1994), and predominant sense identification (McCarthy et al., 2004). In cognitive science, they have been used to account for the influence of context on human lexical processing (McDonald and Brew, 2004), and to model lexical priming (Lowe and McDonald, 2000). 402 A drawback of vector space models is the difficulty of interpreting what some degree of “generic semantic similarity” between two target words means in linguistic terms. In particular, this similarity is not sensitive to selectional preferences over specific semantic relations, and thus cannot model the plausibility data we are interested in. The next section demonstrates how the integration of ide"
D07-1042,P04-1003,0,0.17007,"milar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), is a measure of their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton et al., 1975), automatic thesaurus extraction (Grefenstette, 1994), and predominant sense identification (McCarthy et al., 2004). In cognitive science, they have been used to account for the influence of context on human lexical processing (McDonald and Brew, 2004), and to model lexical priming (Lowe and McDonald, 2000). 402 A drawback of vector space models is the difficulty of interpreting what some degree of “generic semantic similarity” between two target words means in linguistic terms. In particular, this similarity is not sensitive to selectional preferences over specific semantic relations, and thus cannot model the plausibility data we are interested in. The next section demonstrates how the integration of ideas from selectional preference induction makes this distinction possible. 3 The Vector Similarity Model: Corpus-Based Modelling of Plausi"
D07-1042,J07-2002,1,0.460937,"model, one using unparsed and one parsed data. Both are trained on the complete British National Corpus (Burnard, 1995, BNC) with more than six million sentences. The unparsed model (Unparsed) uses the BNC without any pre-processing. We first construct the set of known headwords, Seenr (v), as follows: All words up to 2 words to the left of instances of v are assumed to be subjects, and thus agents; vice versa for patients to the right. Then, we construct semantic space representations for the experimental arguments and known headwords, adopting optimal parameter settings from the literature (Padó and Lapata, 2007). This means a context window of 5 words to either side and 2,000 basis elements (dimensions), which are formed by the most frequent 1,000 words 1 We are grateful to Ken McRae for his dataset. in the BNC, combined with each of the relations agent and patient. All counts are log-likelihood transformed (Lowe, 2001). To construct the parsed model (Parsed), we dependency-parsed the BNC with Minipar (Lin, 1993). We first obtain the seen headwords Seenr (v) by using all subjects and objects of v as agents and patients, respectively. We then construct a vector space for the experimental arguments and"
D07-1042,N07-1071,0,0.0339889,"view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two proble"
D07-1042,P99-1014,0,0.826006,"n with the most strongly associated WordNet ancestor class of the argument. WordNet-based approaches however face two problems. One is a coverage problem due to the limited size of the resource (see the task-based evaluation in Gildea and Jurafsky (2002)). The other is that the shape of the WordNet hierarchy determines the generalisations that the models make. These are not always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives a high preference because “tragedy” in WordNet is a type of written communication, which is a preferred argument class of “answer”. Rooth et al. (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. We will argue in Section 6 that our model allows more control over the generalisations made. Modelling Selectional Preferences with Thematic Roles. Padó et al. (2006) present a deeper model for the plausibility of (v, r, a) triples that approximates the relations with thematic roles. It estimates the selectional preferences of a verb-role pair with a generative probability model that equates the pl"
D08-1094,E03-1034,0,0.0271593,"ectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pad´o et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch"
D08-1094,P07-1028,1,0.613356,"ng been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1: Structured meaning representations for noun ball and verb catch : lexical information plus expectations that integrates lexical information with selectional preferences. Then, we show how the SVS model provides a new way of computing meaning in context. Representing lemma meaning. We abandon the t"
D08-1094,J02-3001,0,0.0239697,"al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1: Structured meaning representations for noun ball and verb catch : lexical information plus expectations that in"
D08-1094,J93-1005,0,0.117728,"ticiples (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pad´o et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1:"
D08-1094,P93-1016,0,0.0216957,", (Lund and Burgess, 1996)). For each pair of a target word and context word, the B OW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions. We also consider a “dependency-based” vector space (S YN, (Pad´o and Lapata, 2007)). In this space, target and context words have to be linked by a “valid” dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&L. Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors ~va : We compute the selectional preference vector for word b and relation r as the weighted 2 More specifically, we used the minimal context specification and plain we"
D08-1094,P98-2127,0,0.736346,"ces of a noun and (left) its lexical vector and (right) inverse object preferences vector (cosine similarity in S YN space) ρ = 0.2, significantly outperforming both baselines. It is interesting, though, that the subj −1 preference itself (“Selpref only”) is already highly significantly correlated with the human judgments. A comparison of the upper half (B OW) with the lower half (S YN) shows that the dependency-based space generally shows better correlation with human judgements. This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998; Pad´o and Lapata, 2007). All instances of the SELPREF model show highly significant correlations. SELPREF and SELPREF - CUT show very similar performance. They do better than both baselines in the B OW space; however, in the cleaner S YN space, their performance is numerically lower than using selectional preferences only (ρ = 0.13 vs. 0.16). SELPREF - POW is always significantly better than SELPREF and SELPREF - CUT, and shows the best result of all tested models (ρ = 0.27, B OW space). The performance is somewhat lower in the S YN space (ρ = 0.22). However, this difference, and the differe"
D08-1094,J03-4004,0,0.230695,"e to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the m"
D08-1094,S07-1009,0,0.569077,"nal vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) tha"
D08-1094,P04-1003,0,0.150376,"problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is unde"
D08-1094,P08-1028,0,0.893726,"7; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical object"
D08-1094,P08-2029,0,0.0123258,"t paraphrase appropriateness based on the similarity between vectors. This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation. Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, notably Information Retrieval and Textual Entailment. Nevertheless, they place their emphasis on different 899 types of information. Current kernels are mostly tree kernels that compare syntactic structure, and use semantic information mostly for smoothing syntactic similarity (Moschitti and Quarteroni, 2008). In contrast, vector-space models focus on the interaction between the lexical meaning of words in composition. 3 A structured vector space model for word meaning in context In this section, we define the structured vector space (SVS) model of word meaning. The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events. For example, in (1a), we assume that upon hearing the phrase “catch a ball”, the hearer will interpret the meaning of “catch” to match typical actions that can be performed with a ball. Similarly, the inter"
D08-1094,J07-2002,1,0.643973,"Missing"
D08-1094,D07-1042,1,0.81262,"Missing"
D08-1094,J98-1004,0,0.949911,"Missing"
D08-1094,P08-1078,0,0.0397749,"Missing"
D08-1094,C00-2137,0,0.0526464,"he selectional preferences of b model the expectations for a, we use b’s selectional preference vector for the given relation as a second baseline, “selpref only”. Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization. 902 landmark slouch decline slouch decline sim high low low high judgment 7 2 3 7 Figure 3: Experiment 1: Human similarity judgements for subject-verb pair with high- and low-similarity landmarks Differences between the performance of models were tested for significance using a stratified shuffling-based randomization test (Yeh, 2000).4 . 5 Exp. 1: Predicting similarity ratings In our first experiment, we attempt to predict human similarity judgments. This experiment is a replication of the evaluation of M&L on their dataset5 . Dataset. The M&L dataset comprises a total of 3,600 human similarity judgements for 120 experimental items. Each item, as shown in Figure 3, consists of an intransitive verb and a subject noun that are combined with a “landmark”, a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject. The dataset was constructed by extracting pairs of"
D08-1094,W07-1401,0,\N,Missing
D08-1094,C98-2122,0,\N,Missing
D09-1046,D07-1007,0,0.0440408,"epts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between usages and because it is publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or"
D09-1046,P07-1005,0,0.0355219,"rned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between usages and because it is publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli,"
D09-1046,P09-1002,1,0.823755,"or thesaurus. However, clear cut sense boundaries are sometimes hard to define, and the meaning of words depends strongly on the context in which they are used (Cruse, 2000; Hanks, 2000). Some researchers in lexical semantics have suggested that word meanings lie on a continuum between i) clear cut cases of ambiguity and ii) vagueness where clear cut boundaries do not hold (Tuggy, 1993). Certainly, it seems that a more complex representation of word sense is needed with a softer, graded representation of meaning rather than a fixed listing of senses (Cruse, 2000). A recent annotation study ((Erk et al., 2009), hereafter GWS) marked a target word in context with graded ratings (on a scale of 1-5) on senses from WordNet (Fellbaum, 1998). Table 1 shows an example of a sentence with the target word in bold, and with the annotator judgments given 2 Related Work WSD has to date been a task where word senses are viewed as having clear cut boundaries. However, there are indications that word meanings do not behave in this way (Kilgarriff, 2006). Researchers in the field of WSD have acknowledged these problems but have used existing lexical resources in the hope that useful applications can be built with t"
D09-1046,N06-2015,0,0.0699944,"publicly available inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subt"
D09-1046,J04-1003,0,0.00597778,"2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object Answering task. However, it does not test its prediction against human annotator data. We concentrate on supervised models in this paper since they generally perform better than their unsupervised or knowledge-based counterparts (Navigli, 2009). We compare them against a baseline model which simply uses the training data to obtain a probability distribution over senses regardless of context, since marginal distributions are highly skewed making a prior distribution very informative (Chan and Ng, 2005; Lapata and Brew, 2004). Along with standard WSD models, we evaluate vector space models that use the training data to locate a word sense in semantic space. Word sense and vector space models have been related in two ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than c"
D09-1046,P93-1016,0,0.0234806,"or training. As it is to be expected that the vectors in this space will be very sparse, we also test a variant of the Prototype model with Sch¨utze-style secondorder vectors (Sch¨utze, 1998), called Prototype/2. Given a (first-order) feature vector, we compute a second-order vector as the centroid of vectors for all lemma features (omitting stopwords) in the first-order vector. For the feature vector in Table 3, ~ ~ this is the centroid of vectors sweet-sour, sauce, ~ ~ . . . , boil. We compute the vectors sweet-sour etc. as dependency vectors (Pad´o and Lapata, 2007) 4 over a Minipar parse (Lin, 1993) of the BNC. and recall, which can be seen as follows. Graded sense assignment is represented by assigning each sense a score between 0.0 and 1.0. The categorial case can be represented in the same way, the difference being that one single sense will receive a score of 1.0 while all other senses get a score of 0.0. With this representation for categorial sense assignment, consider a fixed token t of lemma `. P i∈S` min(assigned`,i,t , gold`,i,t ) will be 1 if the assigned sense is the gold sense, and 0 otherwise. 5 Models for Graded Word Sense Assignment In this section we discuss the computat"
D09-1046,S07-1009,1,0.861164,"As Spearman’s ρ compares the rankings of two sets of judgments, it abstracts from the absolute values of the judgments. It is useful to have a measure that abstracts from absolute values of judgments and magnitude of difference because the GWS dataset contains annotator judgments on a fixed scale, and it is quite possible that human judges will differ in how they use such a scale. Each judgment in the gold-standard can be represented as a 4-tuple hlemma, sense no, sentence no, gold judgmenti. For example, hadd.v, 1 2 The GWS data also contains data from the English Lexical Substitution Task (McCarthy and Navigli, 2007) but we do not use that portion of the data for these experiments. Mitchell and Lapata (2008) note that Spearman’s ρ tends to yield smaller coefficients than its parametric counterparts such as Pearson’s coefficient. 442 1, 1, 0.8i is the first sentence for target add.v, first WordNet sense, with a (normalized) judgment of 0.8. Likewise, each prediction by the model can be represented as a 4-tuple hlemma, sense no, sentence no, predicted judgmenti. We write G for the set of gold tuples, A for the set of assigned tuples, L for the set of lemmas, S` for the set of sense numbers that exist for le"
D09-1046,W03-1810,1,0.385229,"y be transformed to categorial judgments by making more coarse-grained senses. If human word sense judgments are best viewed as graded, it makes sense to explore models of word sense that can predict graded sense assignments. In this paper we look at the issue of graded applicability of word sense from the point of view of automatic graded word sense assignment, using the GWS graded word sense dataset. We make three primary contributions. Firstly, we propose evaluation metrics that can be used on graded word sense judgments. Some of these metrics, like Spearman’s ρ, have been used previously (McCarthy et al., 2003; Mitchell and Lapata, 2008), but we also introduce new metrics based on the traditional precision and recall. Secondly, we investigate how two classes of models perform on the task of graded word sense assignment: on the one hand classical WSD models, on the other hand prototype-based vector space models that can be viewed as simple one-class classifiers. We study supervised models, training on traditional WSD data and evaluating against a graded scale. Thirdly, the evaluation metrics we use also provides a novel analysis of annotator performance on the GWS dataset. Word sense disambiguation"
D09-1046,J98-1004,0,0.561455,"Missing"
D09-1046,H05-1051,0,0.0174344,"rmance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try to predict graded judgments of word sense applicability, using a recent dataset with graded annotation (Erk et al., 2009). Our hope is that models which can mimic graded human judgments on the same task should better reflect the underlying phenomena of word meaning compared to a system that focuses on ma"
D09-1046,W06-2503,1,0.779405,"arpuat and Wu, 2007; Chan et al., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try t"
D09-1046,W04-0807,0,0.0214689,"As the GWS dataset is too small to accommodate both training and testing of a supervised model, we use all the data from GWS for testing our models, and train our models on traditional word sense annotation data. We use as training data all sentences from SemCor and the training portion of SE-3 that are not included in GWS. The quantity of training data available is shown in the last two columns of table 2. # training SemCor SE-3 171 238 14 195 386 236 106 73 125 11 111 160 46 207 88 53 1047 1173 Table 2: Lemmas used in this study with various sense-tagged datasets (e.g. (Miller et al., 1993; Mihalcea et al., 2004)) for comparison. 3 Data In this paper, we use a subset of the GWS dataset (Erk et al., 2009) where three annotators supplied ordinal judgments of the applicability of WordNet (v3.0) senses on a 5 point scale: 1 – completely different, 2 – mostly different, 3 – similar, 4 – very similar and 5 – identical. Table 1 shows a sample annotation. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al., 2004) (hereafter SE-3) and SemCor (Miller et al., 1993). 1 For 8 lemmas, 25 sentences were randomly sampled from SemCor"
D09-1046,H93-1061,0,0.387192,"mapped accordingly. As the GWS dataset is too small to accommodate both training and testing of a supervised model, we use all the data from GWS for testing our models, and train our models on traditional word sense annotation data. We use as training data all sentences from SemCor and the training portion of SE-3 that are not included in GWS. The quantity of training data available is shown in the last two columns of table 2. # training SemCor SE-3 171 238 14 195 386 236 106 73 125 11 111 160 46 207 88 53 1047 1173 Table 2: Lemmas used in this study with various sense-tagged datasets (e.g. (Miller et al., 1993; Mihalcea et al., 2004)) for comparison. 3 Data In this paper, we use a subset of the GWS dataset (Erk et al., 2009) where three annotators supplied ordinal judgments of the applicability of WordNet (v3.0) senses on a 5 point scale: 1 – completely different, 2 – mostly different, 3 – similar, 4 – very similar and 5 – identical. Table 1 shows a sample annotation. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al., 2004) (hereafter SE-3) and SemCor (Miller et al., 1993). 1 For 8 lemmas, 25 sentences were rand"
D09-1046,P08-1028,0,0.373728,"egorial judgments by making more coarse-grained senses. If human word sense judgments are best viewed as graded, it makes sense to explore models of word sense that can predict graded sense assignments. In this paper we look at the issue of graded applicability of word sense from the point of view of automatic graded word sense assignment, using the GWS graded word sense dataset. We make three primary contributions. Firstly, we propose evaluation metrics that can be used on graded word sense judgments. Some of these metrics, like Spearman’s ρ, have been used previously (McCarthy et al., 2003; Mitchell and Lapata, 2008), but we also introduce new metrics based on the traditional precision and recall. Secondly, we investigate how two classes of models perform on the task of graded word sense assignment: on the one hand classical WSD models, on the other hand prototype-based vector space models that can be viewed as simple one-class classifiers. We study supervised models, training on traditional WSD data and evaluating against a graded scale. Thirdly, the evaluation metrics we use also provides a novel analysis of annotator performance on the GWS dataset. Word sense disambiguation is typically phrased as the"
D09-1046,S07-1006,0,0.0658321,"Missing"
D09-1046,P06-1014,0,0.0164749,"., 2007). For monolingual applications however it is less clear whether current state-of-the-art WSD systems for tagging text with dictionary senses are able to have an impact on applications. One way of addressing the problem of low interannotator agreement and system performance is to create an inventory that is coarse-grained enough for humans and computers to do the job reliably (Ide and Wilks, 2006; Hovy et al., 2006; Palmer et al., 2007). Such coarse-grained inventories can be produced manually from scratch (Hovy et al., 2006) or by automatically relating (McCarthy, 2006) or clustering (Navigli, 2006; Navigli et al., 2007) existing word senses. While the reduction in polysemy makes the task easier, we do not know which are the right distinctions to retain. In fact, fine-grained distinctions may be more useful than coarse-grained ones for some applications (Stokoe, 2005). Furthermore, Hanks (2000) goes further and argues that while the ability to distinguish coarse-grained senses is indeed desirable, subtler and more complex representations of word meaning are necessary for text understanding. In this paper, instead of focusing on issues of granularity we try to predict graded judgments of"
D09-1046,J07-2002,0,0.0472049,"Missing"
D09-1046,P05-1016,0,0.0420628,"elated in two ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than clustering occurrences into word senses. The second way in which word sense and vector space models have been related is to assign disambiguated feature vectors to WordNet concepts (Pantel, 2005; Patwardhan and Pedersen, 2006). However those works do not use sense-tagged data and are not aimed at WSD, rather the applications are to insert new concepts into an ontology and to measure the relatedness of concepts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination o"
D09-1046,W06-2501,0,0.00374444,"ways. On the one hand, vector space models have been used for inducing word senses (Sch¨utze, 1998; Pantel and Lin, 2002). The different meanings of a word are obtained by clustering vectors. The clusters must then be mapped to an inventory if a standard WSD dataset is used for evaluation. In contrast, we use sense tagged training data with the aim of building models of given word senses, rather than clustering occurrences into word senses. The second way in which word sense and vector space models have been related is to assign disambiguated feature vectors to WordNet concepts (Pantel, 2005; Patwardhan and Pedersen, 2006). However those works do not use sense-tagged data and are not aimed at WSD, rather the applications are to insert new concepts into an ontology and to measure the relatedness of concepts. We are not concerned in this paper with arguing for or against any particular sense inventory. WordNet has been criticized for being overly finegrained (Navigli et al., 2007; Ide and Wilks, 2006), we are using it here because it is the sense inventory used by Erk et al. (2009). That annotation study used it because it is sufficiently fine-grained to allow for the examination of subtle distinctions between us"
D09-1070,W02-0606,0,0.552097,"stering around a common stem, and generation of new word forms with productive affixes. Intuitively, there are straightforward, but non-trivial, challenges that arise when evaluating a model. One large challenge is distinguishing derivational from inflectional morphology. Most approaches deal with tokens without considering context. Since inflectional morphology is virtually always driven by syntax and word context, such approaches are unable to learn only inflectional morphology or only derivational morphology. Even approaches which take context into consideration (Schone and Jurafsky, 2000; Baroni et al., 2002; Freitag, 2005) cannot learn specifically for one or the other. In addition, the evaluation of both segmentation and clustering involves arbitrary judgment calls. Concerning segmentation, should altimeter and altitude be one morpheme or two? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unprod"
D09-1070,W06-3210,0,0.0285117,"Missing"
D09-1070,W06-3209,0,0.0941825,"many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the"
D09-1070,W02-0603,0,0.0781327,"Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the"
D09-1070,P04-2012,0,0.421757,"t whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et al. (2002) takes a similar approach but uses edit distance to cluster words that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. Th"
D09-1070,W09-1905,1,0.882512,"other approaches that typically require a number of arbitrary thresholds and parameters yet provide little intuitive justification for them. (We give examples of these in §3.) We evaluate our approach on two languages, English and Uspanteko, and compare its performance to two benchmark systems, Morfessor (Creutz and Lagus, 2007) and Linguistica (Goldsmith, 2001). English is commonly used in other studies and permits the use of CELEX as a gold standard for evaluation. Uspanteko is an endangered Mayan language for which we have a set of interlinearized glossed texts (IGT) (Pixabaj et al., 2007; Palmer et al., 2009). IGT provides wordby-word morpheme segmenation, which we use to create a synthetic gold standard. In addition to evaluation against this standard, Telma Kaan Pixabaj—a Mayan linguist who helped create the annotated corpus—reviewed by hand 100 word clusters produced by our system, Morfessor and Linguistica. Note that because English is suffixal and Uspanteko is both prefixal and suffixal, we use a slightly modified model for Uspanteko. The approach introduced in this paper compares favorably to Linguistica and Morfessor, two models that employ much more complex strategies and rely on experimen"
D09-1070,W04-0106,0,0.160943,"ls require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the tendency of raw MDL to"
D09-1070,W00-0712,0,0.405885,"graphy, then the two words are likely to be related morphologically. We measure how integrating these assumptions into our model at different stages affects performance. We define a simple pipeline model. After generating candidate stems and affixes (possibly constrained by document boundaries), a χ2 test based on global corpus counts filters out unlikely affixes. Mutually consistent affix pairs are then clustered to form affix groups. These in turn are used to build morphologically related word clusters, possibly constrained by evidence from co-occurence of word forms in documents. Following Schone and Jurafsky (2000), clusters are evaluated for Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and affixes. This typically involves heuristic search procedures and calibrating multiple arbitrary thresholds. We present a simple approach that uses no thresholds other than those involved in standard application of χ2 significance testing. A key part of our approach is using document boundaries to constrain generation of candidate stems and affixes and clustering morphological variants of a given word stem. We"
D09-1070,N01-1024,0,0.304218,"wo? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unproductive one? Yet, many studies have evaluated their segmentations and clusters by going over their results word by word, cluster by cluster and judging by sight whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et"
D09-1070,W02-0602,0,0.174996,"egant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the tendency of raw MDL to generate descriptions that do not fit linguistic notions of morphology. Despite the sophistication of the models in this group, there are many parameters that need to be set, and heuristic search procedures are crucial for their success (Goldwater, 2007). Snover et al. (2002) present a Bayesian model that uses a prior distribution to refine disjoint clusters of morphologically related words. It disposes with parameter setting by selecting the highest ranking hypothesis. 4 Model2 Our goal is to generate conflation sets: sets of word types that are related through either inflectional or derivational morphology (Schone and Jurafsky, 2000). Solving this task requires learning how individual types are segmented (though the segmentation itself is not evaluated). For present purposes, we assume that the affixal pattern of the language is known: whether it is prefixal, su"
D09-1070,N07-1020,0,0.0468931,"s that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. This group is the only one to evaluate against CELEX (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001; Freitag, 2005). Keshava and Pitler, 2005; Hammarstr¨om, 2006; Dasgupta and Ng, 2007; Demberg, 2007) use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries. LSV has several issues that require fine parameter tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have simi"
D09-1070,P07-1116,0,0.0688802,"do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. This group is the only one to evaluate against CELEX (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001; Freitag, 2005). Keshava and Pitler, 2005; Hammarstr¨om, 2006; Dasgupta and Ng, 2007; Demberg, 2007) use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries. LSV has several issues that require fine parameter tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specifi"
D09-1070,P08-1084,0,0.0723333,"count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that"
D09-1070,P00-1027,0,0.099404,"tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corp"
D09-1070,W05-0617,0,0.711796,"on stem, and generation of new word forms with productive affixes. Intuitively, there are straightforward, but non-trivial, challenges that arise when evaluating a model. One large challenge is distinguishing derivational from inflectional morphology. Most approaches deal with tokens without considering context. Since inflectional morphology is virtually always driven by syntax and word context, such approaches are unable to learn only inflectional morphology or only derivational morphology. Even approaches which take context into consideration (Schone and Jurafsky, 2000; Baroni et al., 2002; Freitag, 2005) cannot learn specifically for one or the other. In addition, the evaluation of both segmentation and clustering involves arbitrary judgment calls. Concerning segmentation, should altimeter and altitude be one morpheme or two? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unproductive one? Yet,"
D09-1070,W99-0904,0,0.174257,"y word, cluster by cluster and judging by sight whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et al. (2002) takes a similar approach but uses edit distance to cluster words that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and re"
D09-1070,J01-2001,0,0.821667,"old and a lower bound on observed counts. These are the only manually set parameters we require—and we in fact use the widely accepted standard values for these thresholds without varying them in our experiments. This is a significant improvement over other approaches that typically require a number of arbitrary thresholds and parameters yet provide little intuitive justification for them. (We give examples of these in §3.) We evaluate our approach on two languages, English and Uspanteko, and compare its performance to two benchmark systems, Morfessor (Creutz and Lagus, 2007) and Linguistica (Goldsmith, 2001). English is commonly used in other studies and permits the use of CELEX as a gold standard for evaluation. Uspanteko is an endangered Mayan language for which we have a set of interlinearized glossed texts (IGT) (Pixabaj et al., 2007; Palmer et al., 2009). IGT provides wordby-word morpheme segmenation, which we use to create a synthetic gold standard. In addition to evaluation against this standard, Telma Kaan Pixabaj—a Mayan linguist who helped create the annotated corpus—reviewed by hand 100 word clusters produced by our system, Morfessor and Linguistica. Note that because English is suffix"
D09-1070,H01-1035,0,0.518764,"hmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values. 1 Introduction Unsupervised morphology acquisition attempts to learn from raw corpora one or more of the following about the written morphology of a language: (1) the segmentation of the set of word types in a corpus (Creutz and Lagus, 2007), (2) the clustering of word types in a corpus based on some notion of morphological relatedness (Schone and Jurafsky, 2000), (3) the generation of out-of-vocabulary items which are morphologically related to other word types in the corpus (Yarowsky et al., 2001). We take a novel approach to segmenting words and clustering morphologically related words. The approach uses no parameters that need to be tuned on data. The two main ideas of the approach are (a) the filtering of affixes by significant co-occurrence, and (b) the integration of knowledge of document boundaries when gener668 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668–677, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP like Uspanteko. whether they capture inflectional paradigms using CELEX (Baayen et al., 1993). We are unaware of other w"
D09-1070,P95-1026,0,0.0409762,"ary linguists to use our model as a preprocessing step for their manual analysis of stems and affixes. To require a documentary linguist–who is likely to have little to no knowledge of NLP methods–to tune parameters is unfeasible. Additionally, data-driven exploration of parameter settings is unlikely to be reliable in language documentation since datasets typically are quite small. To be relevant in this context, a model needs to produce useful results out of the box. Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation (Yarowsky, 1995). Many applications in information retrieval are built on the statistical correlation between documents and terms. However, we are unaware of cases where knowledge of document boundaries has been used for unsupervised learning for morphology. The intuition behind our approach is very simple: if two words in a single document are very similar in terms of orthography, then the two words are likely to be related morphologically. We measure how integrating these assumptions into our model at different stages affects performance. We define a simple pipeline model. After generating candidate stems a"
D09-1070,D08-1109,0,\N,Missing
D10-1020,P10-1132,0,0.126136,"as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterio"
D10-1020,afonso-etal-2002-floresta,0,0.0117489,"54 797328 447079 197422 70125 docs 1801 343 1090 1956 29 avg. 541 2325 410 101 2418 tags 43 80 58 19 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tag"
D10-1020,A00-1031,0,0.0288783,"tributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not w"
D10-1020,E03-1009,0,0.02832,"riments in this table use state sizes that correspond more closely to the size of the tag sets in the respective corpora. (Floresta), and 0.11 (Uspanteko). Clearly, all of the models easily outperform this baseline. F−SCORE VI 203 4 3 2 vi 0.3 f−score 1 0.2 0.1 0 0.0 Number of states. Figure 3 shows the change in accuracy for the different models for different corpora when the overall number of states is varied between 20 and 50. The figure shows results for M-to-1. All models with the exception of HMM + show improvements as the number of states is increased. This brings up the valid concern (Clark, 2003; Johnson, 2007) that a model could posit a very large number of states and obtain high M-to1 scores. However, it is neither the case here nor in any of the studies we cite. Furthermore, as is strongly suggested with HMM +, it does not seem as if all models will benefit from assuming a large number of states. Looking at the results by number of states on VI and f -score for CDHMM(Figure 5), it is clear that Floresta displays the reverse pattern of all other data sets where performance monotonically deteriorates as state sizes are increased. Though the exact reason is unknown, we believe it is"
D10-1020,P07-1035,0,0.0562127,", 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic model given data that had not been filtered of noise in the form of funct"
D10-1020,D08-1036,0,0.0601454,"of experts assumption allows us to capture high variance for certain states. To summarize, the CDHMM is a composite model where both the observed token and the hidden state variable are composite distributions. For the hidden state, this means that there is a “topical” element with high variance across contexts that is embedded in the state sequence for a subset of events. We embed this element through a PoE assumption where transitions into content states are modeled as a product of the transition probability and the local probability of the content state. Inference. We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. In this inference regime, two distributions are of particular interest. One is the posterior density and the other is the conditional distribution, neither of which can be learned in closed form. Letting Λ = (θ, δ, φ, ψ) and h = (α, β, γ, ξ), the posterior density is given as p(Λ|w, t; h) ∝ p(w, t|Λ)p(Λ; h) 199 Note that p(w, t|Λ) is equal to Nd D Y Y d φwi |ti θti |d δti |ti−1 i ψwi |ti δti |ti−1 I[ti ∈C] I[ti ∈F ] (1) where I[·] is the indicator function, D is the number of documents in the corpus and Nd is the numb"
D10-1020,P08-1085,0,0.0673554,"we do in that a parse tree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4 We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. 205 that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Mark"
D10-1020,P07-1094,0,0.533761,"and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being f"
D10-1020,N06-1041,0,0.0141746,"agging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS ta"
D10-1020,C08-1042,0,0.0313772,"Missing"
D10-1020,D07-1031,0,0.106208,"tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008)."
D10-1020,N09-1069,0,0.0203163,"be that Uspanteko has a relatively large number of tags in a very small corpus. 204 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment t"
D10-1020,W09-1905,1,0.791859,"sets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tagsets. • Accuracy: We use a greedy search algorithm to map each unsupervised tag to a gold label such that accuracy is maximized. We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to"
D10-1020,P09-1057,0,0.243953,"ly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained"
D10-1020,W10-2911,0,0.0118274,"al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeat"
D10-1020,W10-2909,0,0.402076,"al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeat"
D10-1020,P93-1034,0,0.0540538,"Missing"
D10-1020,P05-1044,0,0.471829,"ive data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative gen"
D10-1020,D08-1109,0,0.0162426,". 204 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, h"
D10-1020,N03-1033,0,0.0330607,"o show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative generative processes. This also applies to"
D10-1020,D09-1071,0,0.0335811,"Missing"
D10-1020,D09-1072,0,0.242287,"learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4 We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. 205 that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov model that allocates a subset of the states to have distributions conditioned on localized domains can significantly improve performance in unsupervised partof-speech tagging. We have also demonstrated that significant performance gains are possible simply by setting a different emission hyperparameter for a subgroup of the states. It is encouraging that these results hold for both models not just on the WSJ but across a diverse set of languages and measures. We believe our"
D10-1020,J93-2004,0,\N,Missing
D16-1234,W11-2501,0,0.811098,"d, more ambitious semantic tasks are starting to be addressed, such as Question Answering (QA) and Recognizing Textual Entailment (RTE). These systems often depend on the use of lexical resources like WordNet in order to infer entailments for individual words, but these resources are expensive to develop, and always have limited coverage. To address these issues, many works have considered on how lexical entailments can be derived automatically using distributional semantics. Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about wh"
D16-1234,E12-1004,0,0.858176,"be derived automatically using distributional semantics. Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015). Indeed, Levy et al. (2015) showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually entaile"
D16-1234,D14-1082,0,0.0104658,"e two entailment data sets also contain important differences, especially in contrast to the hypernymy data sets. Neither contains any random negative pairs, meaning general semantic similarity measures should be less useful; And both exhibit a variety of non-hypernymy relations, which are less strictly defined and more difficult to model. 3.1 Distributional Vectors In all experiments, we use a standard, count-based, syntactic distributional vector space. We use a corpus composed of the concatenation of Gigaword, Wikipedia, BNC and ukWaC. We preprocess the corpus using Stanford CoreNLP 3.5.2 (Chen and Manning, 2014) for tokenization, lemmatization, POS-tagging and universal dependency parses. We compute a syntactic distributional space for the 250k most frequent lemmas by counting their dependency neighbors across the corpus. We use only the top 1M most frequent dependency attachments as contexts. We use CoreNLP’s “collapsed dependencies”, in which prepositional dependencies are collapsed e.g. “go to the store” emits the tuples (go, prep:to+store) and (store, prep:to−1 +go). After collecting counts, vectors are transformed using PPMI, SVD reduced to 300 dimensions, and normalized to unit length. The use"
D16-1234,P14-1113,0,0.732793,"e detectors. Our model also integrates overall word similarity and Distributional Inclusion, bringing together strengths of several models in the literature. Our model matches or outperforms prior work on multiple data sets. The code, data sets, and model predictions are made available for future research.1 2 Background Research on lexical entailment using distributional semantics has now spanned more than a decade, and has been approached using both unsupervised (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus, 2013) and supervised techniques (Baroni et al., 2012; Fu et al., 2014; Roller et al., 2014; Weeds et al., 2014; Kruszewski et al., 2015; Levy et al., 2015; Turney and Mohammad, 2015; Santus et al., 2016). Most of the work in unsupervised methods is based on the Distributional Inclusion Hypothesis (Weeds et al., 2004; Zhitomirsky-Geffet and Dagan, 2005), which states that the contexts in which a hypernym appear should be a superset over its hyponyms’ contexts. This work focuses primarily on the supervised works in the literature. Formally, we consider methods which treat lexical entailment as a supervised classification problem, which take as input the distribut"
D16-1234,C92-2082,0,0.806734,"efore will also predict that sofa entails animal. Yet it remains unclear why such models make for such strong baselines (Weeds et al., 2014; Kruszewski et al., 2015; Levy et al., 2015). We present a novel qualitative analysis of one prototypicality classifier, giving new insight into why prototypicality classifiers perform strongly in the literature. We find the model overwhelmingly learns to identify hypernyms using Hearst patterns available in the distributional space, like “animals such as cats” and “animals including cats.” These patterns have long been used to identify lexical relations (Hearst, 1992; Snow et al., 2004). We propose a novel model which exploits this behavior as a method of feature extraction, which we call H-feature detectors. Using an iterative procedure similar to Principal Component Analysis, our 2163 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2163–2172, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics model is able to extract and learn using multiple Hfeature detectors. Our model also integrates overall word similarity and Distributional Inclusion, bringing together strengths of sever"
D16-1234,Q15-1027,0,0.0718014,"use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015). Indeed, Levy et al. (2015) showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually entailed, and therefore will also predict that sofa entails animal. Yet it remains unclear wh"
D16-1234,S12-1012,0,0.424537,"for Computational Linguistics model is able to extract and learn using multiple Hfeature detectors. Our model also integrates overall word similarity and Distributional Inclusion, bringing together strengths of several models in the literature. Our model matches or outperforms prior work on multiple data sets. The code, data sets, and model predictions are made available for future research.1 2 Background Research on lexical entailment using distributional semantics has now spanned more than a decade, and has been approached using both unsupervised (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus, 2013) and supervised techniques (Baroni et al., 2012; Fu et al., 2014; Roller et al., 2014; Weeds et al., 2014; Kruszewski et al., 2015; Levy et al., 2015; Turney and Mohammad, 2015; Santus et al., 2016). Most of the work in unsupervised methods is based on the Distributional Inclusion Hypothesis (Weeds et al., 2004; Zhitomirsky-Geffet and Dagan, 2005), which states that the contexts in which a hypernym appear should be a superset over its hyponyms’ contexts. This work focuses primarily on the supervised works in the literature. Formally, we consider methods which treat lexical entai"
D16-1234,W14-1610,0,0.0568231,", we treat hypernymy as positive, and other relations as negative. These two data sets form our hypernymy data sets, but we cannot overstate their important differences: LEDS is balanced, while BLESS contains mostly negative examples; negatives in BLESS include both random pairs and pairs exhibiting other strong semantic relations, while LEDS only contains random pairs. Furthermore, all of the negative examples in LEDS are the same lexical items as the positive items, which has strong implications on the prototypicality argument of Levy et al. (2015). The next data set we consider is Medical (Levy et al., 2014). This data set contains high quality annotations of subject-verb-object entailments extracted from medical texts, and transformed into noun-noun entailments by argument alignments. The data contains 12,600 annotations, but only 945 positive examples encompassing various relations like hypernymy, meronomy, synonymy and contextonymy.3 This makes it one of the most difficult data sets: it is both domain specific and highly unbalanced. The final data set we consider is TM14, a variation on the SemEval 2012 Shared Task of identifying the degree to which word pairs exhibit various relations. These"
D16-1234,N15-1098,0,0.0950786,"focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015). Indeed, Levy et al. (2015) showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually entailed, and therefore will also predict that sofa entails animal."
D16-1234,W15-1501,0,0.0204002,"work. Instead, we propose an alternative interpretation of the hyperplane: that of a feature detector for hypernyms, or an H-feature detector. 4.1 H-Feature Detectors Recall that distributional vectors are derived from a matrix M containing counts of how often words co-occur with the different syntactic contexts. This co-occurrence matrix is factorized using Singular Value Decomposition, producing both W , the ubiquitous word-embedding matrix, and C, the contextembedding matrix (Levy and Goldberg, 2014): M ≈ W C> Since the word and context embeddings implicitly live in the same vector space (Melamud et al., 2015), we can also compare Concat’s hyperplane with the context matrix C. Under this interpretation, the Concat model does not learn what words are hypernyms, but rather what contexts or features are indicative of hypernymy. Table 2 shows the syntactic ˆ contexts with the highest cosine similarity to the H prototype for each of the different data sets. This view of Concat as an H-feature detector produces a radically different perspective on the classifier’s hyperplane. Nearly all of the features learned take the form of Hearst patterns (Hearst, 1992; Snow et al., 2004). The most recognizable and c"
D16-1234,C14-1097,1,0.818376,"Missing"
D16-1234,L16-1722,0,0.356206,"al models in the literature. Our model matches or outperforms prior work on multiple data sets. The code, data sets, and model predictions are made available for future research.1 2 Background Research on lexical entailment using distributional semantics has now spanned more than a decade, and has been approached using both unsupervised (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus, 2013) and supervised techniques (Baroni et al., 2012; Fu et al., 2014; Roller et al., 2014; Weeds et al., 2014; Kruszewski et al., 2015; Levy et al., 2015; Turney and Mohammad, 2015; Santus et al., 2016). Most of the work in unsupervised methods is based on the Distributional Inclusion Hypothesis (Weeds et al., 2004; Zhitomirsky-Geffet and Dagan, 2005), which states that the contexts in which a hypernym appear should be a superset over its hyponyms’ contexts. This work focuses primarily on the supervised works in the literature. Formally, we consider methods which treat lexical entailment as a supervised classification problem, which take as input the distributional vectors for a pair of words, (H, w), and predict on whether the antecedent w entails the consequent H.2 One of the earliest supe"
D16-1234,P16-1158,0,0.382343,"nput. Weeds et al. (2014) found that Concat moderately outperformed Diff, while Roller et al. (2014) found that Asym outperformed Concat. Both Diff and Asym can also be seen as a form of supervised Distributional Inclusion Hypothesis, with the vector difference being analogous to the set-inclusion measures of some unsupervised techniques (Roller et al., 2014). All of these works focused exclusively on hypernymy detection, rather than the more general task of lexical entailment. Recently, other works have begun to analyze Concat and Diff for their ability to go beyond just hypernymy detection. Vylomova et al. (2016) take an extensive look at Diff’s ability to model a wide variety of lexical relations and conclude it is generally robust, and Kruszewski et al. (2015) have success with a neural network model based on the Distributional Inclusion Hypothesis. On the other hand, Levy et al. (2015) analyze both Concat and Diff in their ability to detect general lexical entailment on five data sets: two consisting of only hypernymy, and three covering a wide variety of other entailing word relations. They find that both Concat and Diff fail, and analytically show that they are learning to predict the prototypica"
D16-1234,C04-1146,0,0.671482,"Missing"
D16-1234,C14-1212,0,0.730716,"nal semantics. Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015). Indeed, Levy et al. (2015) showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually entailed, and therefore will also predict that s"
D16-1234,P05-1014,0,0.682369,"e expensive to develop, and always have limited coverage. To address these issues, many works have considered on how lexical entailments can be derived automatically using distributional semantics. Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations (Baroni and Lenci, 2011). Many are based on the Distributional Inclusion Hypothesis, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu which states that the contexts in which a hypernym appears are a superset of its hyponyms’ contexts (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, a great deal of work has pushed toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Levy et al., 2015; Kruszewski et al., 2015), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015). Indeed, Levy et al. (2015) showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models"
D16-1234,N09-1003,0,\N,Missing
D18-1175,J08-1001,0,0.459268,"mble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can flexibly adjust the difficulty of the task. In particular, as we show below, we c"
D18-1175,D08-1005,0,0.61264,"h in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth reasoning, as we show below. 3 Data Natural story salads arise when multiple messy narratives exist to describe the same event or outcome. Often this is because each contribution to the explanation only addresses a small aspect of the larger picture. We can directly simulate the confusion this kind of discourse creates by taking multiple narratives, cutting them into small pieces, and mixing them together. 4 The story salad task is more similar to multichoice narrative"
D18-1175,D15-1075,0,0.0434658,"ument mixture from which they are drawn. Second, we want to capture more in-depth interactions between sentences: our sentence embedding scheme for a sentence s1 should exploit its point 9 Experiments with convolutional encoders here yielded somewhat worse results. 10 Two stories may be on the same topic and still form clearly different narratives. For example, both narratives in Figure 1 are regarding military conflict. of comparison s2 and encode s1 with a view of similarities to and differences with s2 . This type of technique has been useful in tasks like natural language inference (NLI) (Bowman et al., 2015; Peters et al., 2018). To improve contextualization, we add a CNNbased context encoder to the BiLSTM classifier: the reader embeds the whole document salad at hand into a vector. Formally, we compute c = CNN(d), where in this case CNN denotes a single convolution layer with max pooling in the style of Kim (2014) and d is the concatenation of all sentences in the mixture. This component is shown in blue in Figure 3. The context vector c is then appended to z and fed into the bilinear layer. To capture the interaction between two sentences in a pair, we employ a mutual attention mechanism, whic"
D18-1175,P08-1090,0,0.390098,"le, messy narratives, a story salad. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar"
D18-1175,P09-1068,0,0.179449,"h), confirming that the task requires more than just general topical similarity. But there is much room for improvement, in particular on salads generated to be more difficult, where performance is around 15 points lower than on arbitrary mixtures. 2 Related Work Building on early work in script learning (Schank and Abelson, 1977), Chambers and Jurafsky (2008) introduce narrative schema and propose the “narrative cloze” task where the modeling objective is to predict the event happening next. The topic has since seen many extensions and variants coupled with increasingly sophisticated models (Chambers and Jurafsky, 2009) including neural networks (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). This line of work is related to story salads in that our aim of separating entangled narratives in a document mixture also leverages withinnarrative coherence. Our work, however, is very different from narrative cloze: (i) we group sentences/events rather than predicting what happens next; (ii) crucially, the narrative coherence in story salads is in context, in that a narrative clustering is only meaningful with respect to a particular document mixture (see Section 5, 6), while"
D18-1175,N18-1076,1,0.839294,"rd and *- HARD mixtures are challenging for current models. Furthermore, our WIKI - HARD dataset contains salads featuring conflicting information and is an attractive setting for building models with deeper reasoning capabilities. 4 Models We treat the story salad task as a narrative clustering task where, in our dataset, each salad is comprised of two clusters. Accordingly, the first baselines we consider are standard clustering approaches. Baselines. Our first baseline is a simple uniform baseline (hereafter UNIF), where we assign 6 Event tuples are extracted via the extractor presented in Cheng and Erk (2018), and copular verbs are not treated as events, meaning that some sentences translate to null events. all sentences in a document mixture to a single cluster. Under UNIF the clustering accuracy is the percentage of the majority-cluster sentences, e.g. if a mixture has 7 sentences from one narrative and 3 from the other, then the accuracy is 0.7. Additionally, we explore a family of baselines that consist of clustering off-the-shelf sentence embeddings. We choose k-medoids7 (hereafter KM) as our clustering algorithm. For sentence embeddings, we experimented with (i) averaged 300D GloVe embedding"
D18-1175,N18-2031,0,0.0168924,"Under UNIF the clustering accuracy is the percentage of the majority-cluster sentences, e.g. if a mixture has 7 sentences from one narrative and 3 from the other, then the accuracy is 0.7. Additionally, we explore a family of baselines that consist of clustering off-the-shelf sentence embeddings. We choose k-medoids7 (hereafter KM) as our clustering algorithm. For sentence embeddings, we experimented with (i) averaged 300D GloVe embeddings (Pennington et al., 2014), which have been shown to produce surprisingly strong performance in a variety of text classification tasks (Iyyer et al., 2015; Coates and Bollegala, 2018); (ii) skip-thought embeddings (Kiros et al., 2015); and (iii) SCDV (Mekala et al., 2017), a multisense-aware sentence embedding algorithm which builds upon pretrained GloVe embeddings using a Gaussian mixture model. Averaged GloVe embeddings gave the best performance in our experiments; to avoid clutter, we only report those results henceforth. Neural supervised clustering. Our baselines work directly on sentence embeddings and therefore ignore the task-specific supervision available in our labeled training data. Inspired by the work in Bilenko et al. (2004) and Finley and Joachim (2005) on s"
D18-1175,P18-1198,0,0.025666,"ed model (i.e. BILSTM - MT- CTX ) than the models with a single component (i.e. BILSTM - MT and BILSTM - CTX). Overall, the large margin of KM and classifieraided models above the UNIF baseline indicates that separating story salads is a valid task where generalizable patterns can be exploited by machine learning techniques. Why would the mutual attention mechanism help? Plotting the attention weights of randomly selected samples, we see distributionally similar words being attended to in Figure 4a. Intuitively, a BiLSTM compresses a sentence into a single vector, leading to information loss (Conneau et al., 2018). Mutual attention enriches this representation by allowing us access to detailed information in sentences at word-level resolution by capturing lexical similarity. Even more interestingly, we observe a synergistic effect between mutual attention and contextualization: with the context reader added, we see high attention weights on words/phrases which bear little distributional similarity but are important for connecting/contrasting different narratives. For example, in Figure 4b, sega group and family station wagon are selected by the attention, despite not having similar words in the other s"
D18-1175,E17-1104,0,0.0158672,"LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of divid"
D18-1175,P08-1095,0,0.39154,"cular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportuniti"
D18-1175,P15-1162,0,0.0536316,"Missing"
D18-1175,N18-1164,0,0.214775,"ai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth reasoning, as we show"
D18-1175,D14-1181,0,0.0607888,"vailable as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a c"
D18-1175,D17-1069,0,0.158148,"l with respect to a particular document mixture (see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature"
D18-1175,W17-0906,0,0.152888,"to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can flexibly adjust the difficu"
D18-1175,N18-1202,0,0.0218138,"ich they are drawn. Second, we want to capture more in-depth interactions between sentences: our sentence embedding scheme for a sentence s1 should exploit its point 9 Experiments with convolutional encoders here yielded somewhat worse results. 10 Two stories may be on the same topic and still form clearly different narratives. For example, both narratives in Figure 1 are regarding military conflict. of comparison s2 and encode s1 with a view of similarities to and differences with s2 . This type of technique has been useful in tasks like natural language inference (NLI) (Bowman et al., 2015; Peters et al., 2018). To improve contextualization, we add a CNNbased context encoder to the BiLSTM classifier: the reader embeds the whole document salad at hand into a vector. Formally, we compute c = CNN(d), where in this case CNN denotes a single convolution layer with max pooling in the style of Kim (2014) and d is the concatenation of all sentences in the mixture. This component is shown in blue in Figure 3. The context vector c is then appended to z and fed into the bilinear layer. To capture the interaction between two sentences in a pair, we employ a mutual attention mechanism, which is similar to the at"
D18-1175,P16-1027,0,0.393947,". We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. This task is also the subject of an upcoming task at the Text Analysis Conference.1 Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher1 https://tac.nist.gov/2018/SM-KBP/ index.html holgate@utexas.edu katrin.erk@mail.utexas.edu ence judgments could play a large role, the simplest being topical coherence, but also narrative coherence (Chambers and Jurafsky, 2008, 2009; Pichotta and Mooney, 2016; Mostafazadeh et al., 2017), overall textual coherence (Barzilay and Lapata, 2008; Logeswaran et al., 2018), and coherence in the description of entities. This makes it an attractive task for neural models. To make the task accessible to neural models, we propose a simple method for creating simulated story salad data at scale: we mix together sentences from different documents. Figure 1 shows an example mixture of two articles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can"
D18-1175,D16-1264,0,0.033043,"scuss energy policy in a speech on Friday. (B) On Friday, Bush call Gore a “flip-flopper”, say UNK proposal to tap into the reserve be a political ploy. Figure 6: Part of a story salad that is impossible for a human to pick apart (source: NYT- HARD). “UNK” represents out-of-vocabulary tokens, and all the words are lemmatized. Both narratives, i.e. (A) and (B) involve the characters Al Gore and George Bush, and both are on the topic of energy, with strongly overlapping vocabulary. human performance sets the ceiling for the best achievable results (e.g. span-prediction based question answering (Rajpurkar et al., 2016), where all the information needed for the correct answer is available in the input), successfully picking apart narratives in a story salad may require consulting an external knowledge base, which affords machine learning models a clear advantage over humans. For example, recognizing that Commander Kamal is likely to be Afghani based on his name, which is not knowledge every reader possesses, would allow us to successfully cluster the sentence with the U.S.-Afghanistan narrative rather than the Russian-Chechnya narrative. 6 Conclusion We have presented a technique to generate Story Salads, mi"
D18-1175,N09-1023,0,0.0138351,"Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for NLP research in general. The task of picking apart story salads is related to the task of conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009; Jiang et al., 2018), which is a clustering task of dividing a transcript into a set of distinct conversations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse information, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic documents (Bekkerman and Crammer, 2008), which is defined in terms of topical relatedness. In comparison, the story salad task offers opportunities for more in-depth"
D18-1175,N18-2049,1,0.810213,"humans, looking at 20 mixtures each. Here, very interestingly, we find more mixtures that are impossible for humans in NYT- HARD (10 cases, example in Figure 6) than WIKI - HARD (3 cases). This presents a clear discrepancy between difficulty for humans and difficulty for models: the models do better on NYT- HARD which is harder for us. While we would not want to draw strong conclusions from a small sample, this hints at possibilities of future work where world knowledge, which is likely to be orthogonal to the information picked up by the models, can be introduced to improve performance (e.g. Wang et al. (2018)). Note that unlike many other NLP tasks where 1462 (A) The most basic question face the country on energy be how to keep supply and demand in line. The Democrats would say : “what can UNK do to make good use of what UNK have get?” (B) Oil price dominate the 31-minute news conference, hold here near pittsburgh. (B) Vice President Al Gore hold UNK first news conference in 67 day on Friday, defend UNK call for the release of oil from the government’s stockpile and and vow that UNK would “confront friend and foe alike” over the marketing of violent entertainment to child, despite the million in d"
D18-1175,P17-1190,0,0.0179378,"articular document mixture (see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challe"
D18-1175,D17-1026,0,0.0267835,"(see Section 5, 6), while in narrative cloze the next event is predicted on a “global” salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05). level.4 Working with labeled story salad examples, we draw inspiration from previous work on supervised clustering (Bilenko et al., 2004; Finley and Joachim, 2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014; Kiros et al., 2015; Mekala et al., 2017; Wieting and Gimpel, 2017; Wieting et al., 2017) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al., 2004), in particular LSTMs (Hochreiter and Schmidhuber, 1997; Dai and Le, 2015), CNNs (Kim, 2014; Conneau et al., 2017), and attention mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Lin et al., 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) understand the nature of the task and the challenges it sets forth for"
D19-1273,J08-1001,0,0.022779,"program,1 and in a recent Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of comp"
D19-1273,D08-1005,0,0.237795,"ual scenarios is also being considered in the Active Interpretation of Disparate Alternatives (AIDA) program,1 and in a recent Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017"
D19-1273,P08-1090,0,0.679188,"Text Analysis Conference (TAC).2 We frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-worl"
D19-1273,P09-1068,0,0.0527105,"datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. U"
D19-1273,D13-1203,1,0.74865,"Missing"
D19-1273,P08-1095,0,0.0228647,"ooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. Unlike Wang et al. (2018), we do not attempt to identify all the scenarios in the source, but are guided by one particular user’s information need (e.g. the scenario about Khashoggi’s murder, as opposed to all the theories regarding his disappearance, like in Wang et al. (2018)). Further, we do not assume the number of scenarios is known a priori (as Wang et al. (2018) do). We phras"
D19-1273,P11-1118,0,0.0689189,"Missing"
D19-1273,N18-1164,0,0.0126357,"tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and Charniak, 2008, 2011; Jiang et al., 2018; Kummerfeld et al., 2019), and more closely to narrative clustering (Wang et al., 2018), i.e. identifying all the scenarios in an information source by grouping relevant sentences/events. Unlike Wang et al. (2018), we do not attempt to identify all the scenarios in the source, but are guided by one particular user’s information need (e.g. the scenario about Khashoggi’s murder, as opposed to all the theories regarding his disappearance, like in Wang et al. (2018)). Further, we do not assume the number of scenarios is known a priori (as Wang et al. (2018) do). We phrase query-based scenario con"
D19-1273,1985.tmi-1.17,0,0.0830579,"d: (1) A querybased scenario construction task, for which we introduce a model to iteratively build a scenario with compatible events, exploiting ordering. (2) A human-curated evaluation set consisting of multiple accounts of real-world new events, along with a collection of scalably-built synthetic simulation datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation dis"
D19-1273,W17-0906,0,0.301187,"frame the task as query-based scenario discovery: given a topic (e.g., the disappearance of Jamal Khashoggi) and a query (e.g. Jamal Khashoggi was murdered), we want to retrieve 1 https://www.darpa.mil/program/activeinterpretation-of-disparate-alternatives 2 https://tac.nist.gov/2018/SM-KBP/ index.html a scenario, a set of compatible events, from the given reports. We formulate query-based scenario discovery as one-class clustering (Bekkerman and Crammer, 2008). We specifically focus on discovering a scenario of compatible events (Barzilay and Lapata, 2008; Chambers and Jurafsky, 2008, 2009; Mostafazadeh et al., 2017) in a collection of related and unrelated event-denoting sentences, which may contain conflicting and irrelevant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-world news topics. As collecting such"
D19-1273,H05-1115,0,0.13816,"Missing"
D19-1273,N18-1202,0,0.0196191,"Missing"
D19-1273,P16-1027,0,0.0179623,"ratively build a scenario with compatible events, exploiting ordering. (2) A human-curated evaluation set consisting of multiple accounts of real-world new events, along with a collection of scalably-built synthetic simulation datasets, which we show serve as an effective source of training data. (3) Comprehensive experiments and analysis that cast light on the properties of the task and data, as well as on the challenges. 2 Background Our work traces its roots to research in script (Schank and Abelson, 1977; Mooney and DeJong, 1985) and narrative schema learning (Chambers and Jurafsky, 2008; Pichotta and Mooney, 2016). Early work explored tasks such as script modeling (Mooney and DeJong, 1985). Recent work built on the idea that compatibility of events can be learned from corpus data, evaluated on narrative cloze (Chambers and Jurafsky, 2009) and predicting-next-events (Pichotta and Mooney, 2016; Mostafazadeh et al., 2017). We introduce a task with a more practical objective in mind: given a query or an information cue, extract the rest of the pieces to build a compatible scenario. The task is related to conversation disentanglement of multiple entangled conversations in a dialogue transcript (Elsner and C"
D19-1273,D18-1175,1,0.889076,"ant information. We start with a query (see Figure 1) and then iteratively insert sentences into the “scenario-in-construction”. Sentences are chosen based on overall compatibility as well as the ease with which scenario sentences can be arranged into an order. We additionally use an adapted relation network (Santoro et al., 2017) to assess connections between words. For our evaluation, we collect a human-curated set of competing scenarios for real-world news topics. As collecting such data is costly, we follow past work in training our model on synthetic data consisting of document mixtures (Wang et al., 2018) and compare our models directly to theirs. We show that training on such synthetic data yields a model that can substantially outper2712 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2712–2722, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 2: Phase 2 (cf. Table 1) of generating human evaluation data Human100: given the topic Why did MH-17 crash? and the scenario MH-17 had a bomb on board. The annotator searches the web and fi"
E03-1027,P83-1020,0,0.267089,"ested parallelism constraints and so solve this problem for the first time. We argue that well-nested parallelism constraints are powerful enough to model ellipsis, in particular VP-ellipsis. We present a solver for well-nested parallelism constraints which decides satisfiability in nondeterministic polynomial time, and hence proves the NP-completeness of this problem, as dominance constraints are subsumed. 2 CLLS We represent the meaning of sentences by lambda terms, which are seen as trees and then described 115 by formulas of CLLS. The most basic formulas of CLLS are dominance constraints (Marcus et al., 1983). They model scope ambiguity in an underspecified way such that the solved forms of a constraint correspond precisely to the readings of a scopally ambiguous sentence. Next we look at a simple example to see how ellipsis is modeled in this setting. needed: parallelism segments with more than one hole, and jigsaw parallelism, (Erk and Koller, 2001), which is used for cases where the excluded semantic contributions are not subtrees, as in &quot;John went to the station, and every student did too, on a bike.&quot; The approach we describe in this paper extends canonically to segments with more than one hol"
E03-1027,C96-2116,0,\N,Missing
E14-1057,de-marneffe-etal-2006-generating,0,0.0133487,"Missing"
E14-1057,D10-1113,0,0.0382555,"control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a)"
E14-1057,N12-1076,1,0.755835,"any contextspecific substitutes outside the common core. 5 Ranking Paraphrases While there are several studies on modelling lexical substitutes, almost all reported results use McCarthy and Navigli’s S EM E VAL 2007 dataset. We now compare the results of three recent computational models on C O I N C O (our work) and on the S EM E VAL 2007 dataset to highlight similarities and differences between the two datasets. Models. We consider the paraphrase ranking models of Erk and Padó (2008, E P 08), Thater et al. (2010, T FP 10) and Thater et al. (2011, T FP 11). These models have been analysed by Dinu et al. (2012) as instances of the same general framework and have been shown to deliver state-of-the-art performance on the S EM E VAL 2007 dataset, with best results for Thater et al. (2011). The three models share the idea to represent the meaning of a target word in a specific context by 545 corpus syntactically structured syntactically filtered bag of words T FP 11 T FP 10 E P 08 T FP 11/E P 08 T FP 10 T FP 11/E P 08 T FP 10 random COINCO context baseline 47.8 46.2 46.0 44.6 47.4 46.2 47.4 45.8 41.9 38.8 46.2 44.7 40.8 37.5 33.0 S EM E VAL 2007 context baseline 52.5 43.7 48.6 42.7 49.4 43.7 50.1 44.4 4"
E14-1057,1993.eamt-1.1,0,0.520679,"Missing"
E14-1057,D09-1046,1,0.592235,"r substitutes? (b) Do parasets resemble word senses? (c) How similar are the parasets that correspond to the same word sense of a target? These questions have not been addressed before, and we would argue that they could not be addressed before, because previous corpora were either too small or were sampled in a way that was not conducive to this analysis. We use WordNet (Fellbaum, 1998), release 3.1, as a source for both lexical relations and word senses. WordNet is the de facto standard in N LP and is used for both W SD and broader investigations of word meaning (Navigli and Ponzetto, 2012; Erk and McCarthy, 2009). Multi-word substitutes are excluded from all analyses.4 4.1 Relating Targets and Substitutes We first look at the most canonical lexical relations between a target and its substitutes. Table 2 lists the percentage of substitutes that are synonyms (syn), direct/transitive (direct-/trans-) hypernyms (hyper) 3 Please see McCarthy and Navigli (2009) for a possible explanation of the generally low I AA numbers in this field. 543 4 All automatic lexical substitution approaches, including Section 5, omit multi-word expressions. Also, they can be expected to have WordNet coverage and normalisation i"
E14-1057,D08-1094,1,0.881083,".org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution candidates and choose contextual substitutions in each case. Erk and Padó (2008) treated the gold substitution candidates as given and focused on the context-specific ranking of those candidates. In this form, the task has been addressed through three types of (mostly unsupervised) approaches. The first group computes a single type representation and modifies it according C O I N C O – The M ASC All-Words Lexical Substitution Corpus1 Compared to, e. g., W SD, there still is little goldannotated data for lexical substitution. With the exception of the dataset created by Biemann (2013), all existing lexical substitution datasets are fairly small, covering at most several th"
E14-1057,P10-2017,1,0.861873,"study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can pro"
E14-1057,P13-2130,0,0.018836,"in two crucial respects: we annotate all instances of each target, and include all targets regardless of frequency or level of lexical ambiguity. We believe that our corpus is considerably more representative of running text. 541 1 Available as X ML-formatted corpus “Concepts in Context” (C O I N C O) from http://goo.gl/5C0jBH. Also scheduled for release as part of M ASC. 3.2 Crowdsourcing We used the Amazon Mechanical Turk (A MT) platform to obtain substitutes by crowdsourcing. Interannotator variability and quality issues due to nonexpert annotators are well-known difficulties (see, e. g., Fossati et al. (2013)). Our design choices were shaped by “best practices in A MT”, including Mason and Suri (2012) and Biemann (2013). Defining H ITs. An A MT task consists of Human Intelligence Tasks (H ITs), each of which is supposed to represent a minimal, self-contained task. In our case, potential H ITs were annotations of (all target words in) one sentence, or just one target word. The two main advantages of annotating a complete sentence at a time are (a) less overhead, because the sentence has only to be read once; (b) higher reliability, since all words within a sentence will be annotated by the same per"
E14-1057,ide-etal-2008-masc,0,0.033068,"Missing"
E14-1057,P10-2013,0,0.0463841,"d in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution can"
E14-1057,D11-1097,0,0.0405471,"Missing"
E14-1057,S01-1005,0,0.0600907,"(Mitchell and Lapata, 2010). There are, however, important shortcomings of the work in the Lexical Substitution paradigm. All existing datasets (McCarthy and Navigli, 2009; Sinha and Mihalcea, 2014; Biemann, 2013; McCarthy et al., 2013) are either comparatively small, are “lexical sample” datasets, or both. “Lexical sample” datasets consist of sample sentences for each target word drawn from large corpora, with just one target word substituted in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-w"
E14-1057,N10-1013,0,0.0305819,"ask bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose t"
E14-1057,N13-1133,0,0.373781,"es. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can profit from the preexisting annotations and (b) we can release our annotations as part of M ASC. Since we could not annotate the complete M ASC, we selected (complete) text docume"
E14-1057,P10-1097,1,0.962657,"to substitute all content words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotat"
E14-1057,I11-1127,1,0.859251,"tent words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several laye"
E14-1057,D11-1094,0,0.0496376,"Missing"
E14-1057,D13-1198,0,\N,Missing
erk-pado-2004-powerful,P03-1068,1,\N,Missing
erk-pado-2006-shalmaneser,burchardt-etal-2006-salto,1,\N,Missing
erk-pado-2006-shalmaneser,C04-1100,0,\N,Missing
erk-pado-2006-shalmaneser,P97-1003,0,\N,Missing
erk-pado-2006-shalmaneser,W02-0811,0,\N,Missing
erk-pado-2006-shalmaneser,W04-3212,0,\N,Missing
erk-pado-2006-shalmaneser,A00-1031,0,\N,Missing
erk-pado-2006-shalmaneser,P05-1039,0,\N,Missing
erk-pado-2006-shalmaneser,J02-3001,0,\N,Missing
erk-pado-2006-shalmaneser,J05-1004,0,\N,Missing
erk-pado-2006-shalmaneser,P03-1068,1,\N,Missing
erk-pado-2006-shalmaneser,erk-pado-2004-powerful,1,\N,Missing
erk-pado-2006-shalmaneser,N06-1017,1,\N,Missing
erk-pado-2006-shalmaneser,W02-2018,0,\N,Missing
erk-pado-2006-shalmaneser,P93-1016,0,\N,Missing
H05-1084,N03-2008,0,0.0295845,"Missing"
H05-1084,W04-0803,0,0.0724993,"onstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing system performance."
H05-1084,W02-2018,0,0.118704,"Missing"
H05-1084,W04-0817,1,0.784531,"with system performance for models based solely on syntactic features. We conclude that syntactic features approximate a description of grammatical functions, but that semantic features model a different aspect of the world. Much of current research in semantic role assignment is centered on the refinement of syntactic features. Our study suggests that it may be worthwhile to explore the refinement of semantic features as well. The most obvious choice is to investigate features related to selectional preferences. Possible features include goodness of fit relative to pre-computed preferences (Baldewein et al., 2004), named entities (Pradhan et al., 2004), or broad ontological classes like “animate” or “artifact”. Following up on this idea, a natural continuation of the present study would be to create a meta-model that subsumes semantic features. Such a model could use optimal selectional restrictions as a predictor. The next step would then be to construct a combined meta-model that describes the behavior of systems with both syntactic and semantic features. Another interesting research direction that our study suggests is the combination of syntactic and semantic models in co-training. Co-training can"
H05-1084,boas-2002-bilingual,0,0.0605161,"his indicates that syntactic features approximate a description of grammatical functions, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a nu"
H05-1084,W04-2412,0,0.128319,"Missing"
H05-1084,W05-0620,0,0.244124,"umber of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing system performance. Instead, we proceed by error"
H05-1084,P96-1025,0,0.0160785,"Missing"
H05-1084,C04-1100,0,0.0916002,"ons, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework,"
H05-1084,W04-3214,1,0.888328,"Missing"
H05-1084,N04-1030,0,0.323664,"s has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing sy"
H05-1084,P03-1002,0,0.0726952,"approximate a description of grammatical functions, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend them"
H05-1084,W04-3212,0,0.216476,"omatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feat"
H05-1084,P04-1043,0,\N,Missing
H05-1084,J02-3001,0,\N,Missing
heid-etal-2004-querying,mengel-lezius-2000-xml,0,\N,Missing
heid-etal-2004-querying,bird-etal-2000-towards,0,\N,Missing
heid-etal-2004-querying,P03-1068,1,\N,Missing
heid-etal-2004-querying,milde-gut-2002-tasx,1,\N,Missing
I17-1021,D10-1113,0,0.175087,"same shape. For example, most alligators are dangerous. The quantifiers are given probabilistic interpretations, so if most alligators are dangerous, the probability for a random alligator to be dangerous would be 0.95. This makes this dataset a good fit for our probabilistic distributional model. We discuss QMR and the Animal data further in Section 4. Bayesian models in lexical semantics. We use Bayesian models for the sake of interpretability and because the existing definitional property datasets are small. The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al., 2003) to represent co-occurrences of textual context and defini´ S´eaghdha (2010) and Ritter et al. tional features. O (2010) use Bayesian approaches to model selectional preferences. the meaning of role fillers. Also, it has been shown that selectional constraints can be learned distri´ S´eaghdha and Kobutionally (Erk et al., 2010; O rhonen, 2014; Rit"
I17-1021,J10-4007,1,0.884012,"Missing"
I17-1021,W15-0107,0,0.0249255,"Missing"
I17-1021,P14-1132,0,0.141202,"stributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative. Our experiments show that our model can learn properties from a single exposure when given an informative utterance. 1 Introduction When humans encounter an unknown word in text, even with a single instance, they can often infer approximately what it means, as in this example from Lazaridou et al. (2014): We found a cute, hairy wampimuk sleeping behind the tree. People who hear this sentence typically guess that a wampimuk is an animal, or even that it is a mammal. Distributional models, which describe the meaning of a word in terms of its observed contexts (Turney and Pantel, 2010), have been suggested as a model for how humans learn word meanings (Landauer and Dumais, 1997). However, distributional models typically need hundreds of instances of a word to derive a highquality representation for it, while humans can often infer a passable meaning approximation from one sentence only (as in th"
I17-1021,P10-1045,0,0.506856,"Missing"
I17-1021,J14-3005,0,0.0282244,"Missing"
I17-1021,P10-1044,0,0.0242707,"10), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al., 2003) to represent co-occurrences of textual context and defini´ S´eaghdha (2010) and Ritter et al. tional features. O (2010) use Bayesian approaches to model selectional preferences. the meaning of role fillers. Also, it has been shown that selectional constraints can be learned distri´ S´eaghdha and Kobutionally (Erk et al., 2010; O rhonen, 2014; Ritter et al., 2010). However, our point will not be that syntax is needed for fast word learning, but that it helps to observe overarching structure, with syntactic context providing a clear test bed. We test two types of overarching structure for their usefulness in fast mapping. First, we hypothesize that it is helpful to learn about commonalities among context items, which enables mapping from contexts to properties. For example the syntactic contexts eat-dobj and cook-dobj should prefer similar targets: things that are cooked are also things that are eaten (Hypothesis H1). The second hypothesis is that it wi"
I17-1021,D13-1115,1,0.898,"Missing"
I17-1021,P15-2119,0,0.0236462,"pts. 204 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 204–213, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP edge enable fast mapping. Definitional properties. Feature norms are definitional properties collected from human participants. Feature norm datasets are available from McRae et al. (2005) and Vigliocco et al. (2004). In this paper we use feature norms as our target representations of word meaning. There are several recent approaches that learn to map distributional representations to feature norms (Johns and Jones, 2012; Rubinstein et al., 2015; F˘ag˘ar˘as¸an et al., 2015; Herbelot and Vecchi, 2015a). We also map distributional information to feature norms, but we do it based on a single textual instance (one-shot learning). In the current paper we use the Quantified McRae (QMR) dataset (Herbelot and Vecchi, 2015b), which extends the McRae et al. (2005) feature norms by ratings on the proportion of category members that have a property, and the Animal dataset (Herbelot, 2013), which is smaller but has the same shape. For example, most alligators are dangerous. The quantifiers are given probabilistic interpretations, so if most allig"
J10-4007,D08-1007,0,0.396106,"Missing"
J10-4007,E03-1034,0,0.27612,"lity ratings (Experiment 2). Finally, we investigate inverse selectional preferences—preferences of 6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option. 734 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences nouns for the predicates that they co-occur with—again using pseudo-disambiguation (Experiment 3). We compare the EPP model to models from the three model categories presented in Section 2: RESNIK as a hierarchical model; ROOTH ET AL . as a distributional model; and PADO ET AL . as a semantic role–based model. As both Brockmann and Lapata (2003) and Padó (2007) have argued, no WordNet-based model systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios. Among the distributional models, we choose ROOTH ET AL . as a model that performs soft clustering and thus shows a marked difference to the EPP model. To our knowledge, this is the ﬁrst comparison of all three generalization paradigms: semantic hierarchy–based, distributional, and semantic role–based.7 As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human"
J10-4007,C00-1028,0,0.0265236,"a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, ﬁrst introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selection"
J10-4007,N01-1013,0,0.277821,"ds like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, ﬁrst introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our mo"
J10-4007,P97-1003,0,0.0854983,"mary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists ﬁne-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exempliﬁed with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131,582 annotated sentences (roughly three million words). To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. As generalization corpus, we use the Minipar-parsed BNC in both settings. The experimentation with two different primary corpora allows us to directly study the inﬂuence of the disambiguation of predicates and the semantic characterization of argument positions on the performance of selectional preference models. Note, however, that the comparison is complicated by differences between the two corpora: The primary corpus for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the determination of predicates, grammatical functions, and headwords. The primary co"
J10-4007,P08-2008,0,0.0283777,"a pseudodisambiguation task for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processi"
J10-4007,J93-1003,0,0.0402254,"ity measures from Table 2 are applicable to semantic spaces with arbitrary basis elements, with the exception of the Lin measure, whose deﬁnition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation–word pairs (r, v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipﬁan distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that"
J10-4007,P07-1028,1,0.930211,"cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was ﬁrst introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Padó, Padó, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introduce a new similarity measure, nGCM, which achieves excellent performance in many settings. 2. Computational Models of Selectional Preferences In this section, we provide an overview of corpus-based models of selectional preferences. See Table 1 for a summary of the notation that we use. 2 As descriptions"
J10-4007,D08-1094,1,0.351849,"sk for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-ﬁnal wor"
J10-4007,J02-3001,0,0.169582,"ics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sente"
J10-4007,C92-2099,0,0.138643,"ffer from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not signiﬁcantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy speciﬁcally developed on the same corpus. 2.3 Distributional Models Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occurrence—typically obtained from the same corpus as the observed headwords—for generalization. This independence from manually constructed resources gives distributional models a good cost–beneﬁt ratio and makes them especially attractive for domain-speciﬁc applications. These models, like the 727 Computational Linguistics Volume"
J10-4007,P90-1034,0,0.202988,"es. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks. Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was ﬁrst introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Padó, Padó, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse prefere"
J10-4007,J93-1005,0,0.0401984,"ency remains a major inﬂuence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items. 1. Introduction Selectional preferences or selectional constraints describe knowledge about possible and plausible ﬁllers for a predicate’s argument positions. They model the fact that there is often a semantically coherent set of concepts that can ﬁll a given argument position. Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD, ∗ Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguis"
J10-4007,P99-1004,0,0.232072,"ntation of selectional preferences in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use a subject– object context speciﬁcation that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popular in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefﬁcient in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice emerged in ﬁrst place. Padó and Lapata (2007) found good results with Lin’s measure for predominant word sense identiﬁcation. Because it is unclear whether the ﬁndings about best similarity measures generalize to new tasks, we will investigate a range of similarity measures shown in Table 2: Cosine, the Dice and Jaccard coefﬁcients, Hindle’s (1990) and Lin’s (1998) mutual information-based metrics, and an adaptation of Nosofsky’s (1986) Generalized Context Model (GC"
J10-4007,P93-1016,0,0.0302579,"(subj). In both settings, the two potential headwords (here called headword and confounder, to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas. The verb–dependency–headword tuples of the SYN PRIMARY setting yield much more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation. We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences. For the SEM PRIMARY setting, we require a primary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists ﬁne-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exempliﬁed with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131"
J10-4007,P98-2127,0,0.162994,"Missing"
J10-4007,A00-2034,0,0.0392864,"2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions. Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automat"
J10-4007,J03-4004,0,0.20841,"word sense disambiguation (WSD, ∗ Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argume"
J10-4007,P04-1036,0,0.0354815,"which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reﬂects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identiﬁcation (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that proﬁt from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the inﬂuence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of S"
J10-4007,D07-1039,0,0.0435715,"Missing"
J10-4007,P04-1003,0,0.0416762,"models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identiﬁcation (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that proﬁt from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the inﬂuence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences Figure 1 An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter). ﬁllers of the agent and patient position of shoot, respectively. In order to judge whether a hunter is a plausible agent of shoot, the vector space representation of hunter is compared t"
J10-4007,I08-2105,0,0.0114188,"ctional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-ﬁnal word order (as in"
J10-4007,J07-2002,1,0.965308,"rds spaces tend to group words by topics. They ignore the syntactic relation between context items and the target, which is a problem for selectional preference modeling. The top table in Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though they show complementary plausibility ratings. The reason is that deer and hunter often co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities). The bottom table in Figure 1(b) indicates a way out of this problem, namely the use of word-relation pairs as basis elements (Grefenstette 1994; Padó and Lapata 2007). This space splits the co-occurrences with context words such as shoot based on the grammatical relation between target and context word, and this split looks different for different words: whereas deer occurs exclusively as the object of shoot, hunter predominantly occurs as the subject. We ﬁnd the reverse pattern for escape. In consequence, 731 Computational Linguistics Volume 36, Number 4 Table 2 Similarity measures explored in this article. Notation: We assume Basis = {b1 , . . . , bn }. We write I for mutual information, and BE(a) for the set of basis elements that co-occur at least once"
J10-4007,D07-1042,1,0.769375,"Missing"
J10-4007,N07-1071,0,0.0329787,"ke.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences"
J10-4007,P93-1024,0,0.212817,"Missing"
J10-4007,P99-1014,0,0.0557299,"eference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identiﬁed with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen 1 Some approaches also ﬁx a role and headword list and generalize from seen predicates to other, similar predicates. 724 Erk, Padó, and Padó A Flexib"
J10-4007,schulte-im-walde-2010-comparing,0,0.0178483,"Missing"
J10-4007,P08-1057,0,0.0712579,"eighted by sim(w1 , w). The similarity sim(w1 , w) is computed on vector space representations. Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative approach to the prediction of selectional preferences. The features they use are mainly cooccurrence statistics, enriched with morphological context features to alleviate sparse data problems for low-frequency argument heads. They train one SVM per verb– argument position pair, using unobserved verb–argument combinations as negative examples, which makes their approach independent of manually annotated training data. Schulte im Walde et al. (2008) present a model that combines features of the semantic hierarchy–based and the distributional approaches by integrating WordNet into an EM-based clustering model; Schulte im Walde (2010) shows that integrating noun–modiﬁer relations improves the prediction of human plausibility judgments. 2.4 Semantic Role–Based Models The third class of models takes advantage of semantic resources beyond simple semantic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the prediction of selectional preferences for semantic roles rather than grammatical functions. From a lingui"
J10-4007,H93-1052,0,0.0511239,"model to primary corpora with rich information that are too small for efﬁcient generalization, such as domain-speciﬁc corpora or corpora with deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization corpus is available. We empirically demonstrate the beneﬁt of this distinction. We use FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as generalization corpus, modeling selectional preferences for semantic roles with nearperfect coverage and low error rate.2 We evaluate our model on two tasks. The ﬁrst task is pseudo-disambiguation (Yarowsky 1993), where the model decides which of two randomly chosen words is a better ﬁller for the given argument position. This task tests model properties that are needed for concrete semantic analysis tasks, most notably word sense disambiguation, but also for semantic role labeling. The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional"
J10-4007,P06-1107,0,0.0171864,"Missing"
J10-4007,P09-2019,0,0.0790797,"Missing"
J10-4007,J13-3006,0,\N,Missing
J10-4007,E09-1094,0,\N,Missing
J10-4007,E06-1044,1,\N,Missing
J10-4007,C98-2122,0,\N,Missing
J13-3003,D10-1115,0,0.0611901,"Missing"
J13-3003,P08-2063,0,0.182386,"Missing"
J13-3003,burchardt-etal-2006-salsa,1,0.867888,"Missing"
J13-3003,2007.tmi-papers.6,0,0.0326144,"Missing"
J13-3003,D07-1007,0,0.0507655,"rin.erk@mail.utexas.edu, nlgaylord@utexas.edu. ∗∗ Visiting Scholar, Department of Theoretical and Applied Linguistics, University of Cambridge, Sidgwick Avenue, Cambridge, CB3 9DA, UK. E-mail: diana@dianamccarthy.co.uk. Submission received: 3 November 2011; revised version received: 30 April 2012; accepted for publication: 25 June 2012. doi:10.1162/COLI a 000142 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 3 1. Introduction Word sense disambiguation (WSD) is a task that has attracted much work in computational linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview), including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, M`arquez, and Wicentowski 2007; Erk and Strapparava 2010), which were originally organized expressly as a forum for shared tasks in WSD. In WSD, polysemy is typically modeled through a dictionary, where the senses of a word are understood to be mutually disjoint. The meaning of an occurrence of a word is then characterized through the best-ﬁtting among its dictionary senses. The assumption of senses that are mutua"
J13-3003,D09-1003,0,0.0173312,"Missing"
J13-3003,D10-1113,0,0.101169,"Missing"
J13-3003,D09-1046,1,0.928864,"Missing"
J13-3003,P09-1002,1,0.734848,"Missing"
J13-3003,D08-1094,1,0.895479,"Mervis 1975; Hampton 2007). This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies? Recently, there have been several proposals for modeling word meaning in context that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word. The SemEval Lexical Substitution task (McCarthy and Navigli 2009) represents each occurrence through multiple weighted paraphrases. Other approaches represent meaning in context through a vector ¨ space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Furstenau, and Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again, this raises the question of annotation: Can human annotators give ﬁne-grained judgments about degrees of similarity between word occurrences, like these computational models predict? The question that we explore in this paper is: Can word meaning be described through annotation in the form of graded judgments? We want to know whether annotators can provide graded meaning annotation in a consistent fashion. Also, we want to know whether annotators will use"
J13-3003,P10-2017,1,0.917282,"Missing"
J13-3003,D11-1129,0,0.022525,"Missing"
J13-3003,N06-2015,0,0.0241474,"Missing"
J13-3003,W09-2413,0,0.0273464,"Missing"
J13-3003,W02-0816,1,0.776181,"Missing"
J13-3003,S07-1009,1,0.89977,"Missing"
J13-3003,W03-2408,0,0.0867752,"Missing"
J13-3003,H93-1061,0,0.77528,"Missing"
J13-3003,P08-1028,0,0.0717742,"on 2007). This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies? Recently, there have been several proposals for modeling word meaning in context that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word. The SemEval Lexical Substitution task (McCarthy and Navigli 2009) represents each occurrence through multiple weighted paraphrases. Other approaches represent meaning in context through a vector ¨ space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Furstenau, and Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again, this raises the question of annotation: Can human annotators give ﬁne-grained judgments about degrees of similarity between word occurrences, like these computational models predict? The question that we explore in this paper is: Can word meaning be described through annotation in the form of graded judgments? We want to know whether annotators can provide graded meaning annotation in a consistent fashion. Also, we want to know whether annotators will use the whole graded scale, or"
J13-3003,S07-1006,0,0.0333095,"Missing"
J13-3003,passonneau-etal-2010-word,0,0.0761967,"Missing"
J13-3003,S07-1016,0,0.0862518,"Missing"
J13-3003,J91-4003,0,0.514636,"Missing"
J13-3003,I11-1079,1,0.911679,"Missing"
J13-3003,N10-1013,0,0.059551,"Missing"
J13-3003,J98-1004,0,0.229525,"Missing"
J13-3003,W04-0811,0,0.0600943,"Missing"
J13-3003,H05-1051,0,0.071159,"Missing"
J13-3003,P10-1097,0,0.105721,"Missing"
J13-3003,D11-1094,0,0.0385112,"Missing"
J13-3003,W10-2807,0,0.0433242,"Missing"
J13-3003,P10-4014,0,0.016529,"Missing"
J13-3003,D08-1105,0,0.0469699,"Missing"
J13-3003,W04-0807,0,\N,Missing
J13-3003,W09-2412,1,\N,Missing
J13-3003,S10-1002,1,\N,Missing
J16-2003,apidianaki-2008-translation,1,0.788727,"different, but somewhat related. We want to know to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built f"
J16-2003,E09-1010,1,0.794351,"d for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will move to automatic"
J16-2003,apidianaki-etal-2014-semantic,1,0.867919,"Missing"
J16-2003,D09-1056,0,0.0820066,"Missing"
J16-2003,P05-1074,0,0.0289879,"ses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will"
J16-2003,N12-1095,0,0.0584039,"Missing"
J16-2003,D07-1007,0,0.030657,"Missing"
J16-2003,J13-3008,0,0.0609858,"Missing"
J16-2003,eom-etal-2012-using,0,0.0612982,"Missing"
J16-2003,P09-1002,1,0.923919,"Missing"
J16-2003,J13-3003,1,0.892716,"Missing"
J16-2003,N06-2015,0,0.0425995,"Missing"
J16-2003,W02-0808,0,0.182124,"Missing"
J16-2003,S13-2049,0,0.0416348,"Missing"
J16-2003,W09-2413,0,0.0378648,"Missing"
J16-2003,S10-1011,0,0.0219161,"pted for publication: 25 January 2016. doi:10.1162/COLI a 00247 © 2016 Association for Computational Linguistics Computational Linguistics Volume 42, Number 2 1. Introduction In computational linguistics, the field of word sense disambiguation (WSD)—where a computer selects the appropriate sense from an inventory for a word in a given context—has received considerable attention.1 Initially, most work focused on manually constructed inventories such as WordNet (Fellbaum 1998) but there has subsequently been a great deal of work on the related field of word sense induction (WSI) (Pedersen 2006; Manandhar et al. 2010; Jurgens and Klapaftis 2013) prior to disambiguation. This article concerns the phenomenon of word meaning and current practice in the fields of WSD and WSI . Computational approaches to determining word meaning in context have traditionally relied on a fixed sense inventory produced by humans or by a WSI system that groups token instances into hard clusters. Either sense inventory can then be applied to tag sentences on the premise that there will be one best-fitting sense for each token instance. However, word meanings do not always take the form of discrete senses but vary on a continuum b"
J16-2003,S07-1009,1,0.800354,"Missing"
J16-2003,S10-1002,1,0.886946,"Missing"
J16-2003,P08-1028,0,0.0875116,"Missing"
J16-2003,palmer-etal-2000-semantic,0,0.22024,"Missing"
J16-2003,passonneau-etal-2010-word,0,0.168918,"s (Tuggy 1993). For example, the noun crane is a clear-cut case of ambiguity between lifting device and bird, whereas the exact meaning of the noun thing can only be retrieved via the context of use rather than via a representation in the mental lexicon of speakers. Cases of polysemy such as the verb paint, which can mean painting a picture, decorating a room, or painting a mural on a house, lie somewhere between these two poles. Tuggy highlights the fact that boundaries between these different categories are blurred. Although specific context clearly plays a role (Copestake and Briscoe 1995; Passonneau et al. 2010) some lemmas are inherently much harder to partition than others (Kilgarriff 1998; Cruse 2000). There are recent attempts to address some of these issues by using alternative characterizations of word meaning that do not involve creating a partition of usages into senses (McCarthy and Navigli 2009; Erk, McCarthy, and Gaylord 2013), and by asking WSI systems to produce soft or graded clusterings (Jurgens and Klapaftis 2013) where tokens can belong to a mixture of the clusters. However, these approaches do not overtly consider the location of a lemma on the continuum, but doing so should help in"
J16-2003,W97-0213,0,0.220678,"ssible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefeve"
J16-2003,D07-1043,0,0.159065,"Missing"
J16-2003,J06-2001,0,0.0881139,"Missing"
J16-2003,D09-1067,0,0.028907,"Missing"
J16-2003,S07-1044,0,0.0306505,"now to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent w"
J16-2003,W09-2419,0,\N,Missing
J16-2003,passonneau-etal-2012-masc,0,\N,Missing
J16-4007,E12-1004,0,0.0613411,"vely, to identify the individual and holistic value of each feature set and systematic patterns. However, this evaluation may also be used as a framework by future lexical semantics research to see its value in end-to-end textual entailment systems. For example, we could have also included features corresponding to the many measures of distributional inclusion that were developed to predict hypernymy (Weeds, Weir, and McCarthy 2004; 795 Computational Linguistics Volume 42, Number 4 Kotlerman et al. 2010; Lenci and Benotto 2012; Santus 2013), or other supervised lexical entailment classifiers (Baroni et al. 2012; Fu et al. 2014; Weeds et al. 2014; Levy et al. 2015; Kruszewski, Paperno, and Baroni 2015). Evaluation is broken into four parts: first, we overview performance of the entire entailment rule classifier on all rules, both lexical and phrasal. We then break down these results into performance on only lexical rules and only phrasal rules. Finally, we look at only the asymmetric features to address concerns raised by Levy et al. (2015). In all sections, we evaluate the lexical rule classifier on its ability to generalize to new word pairs, as well as the full system’s performance when the entail"
J16-4007,2014.lilt-9.5,0,0.0774062,"Missing"
J16-4007,J10-4006,0,0.0369767,"e task. We use the large window size to ensure the BoW vectors captured more topical similarity, rather than syntactic similarity, which is modeled by the dependency vectors. 787 Computational Linguistics Volume 42, Number 4 Figure 4 Distribution of entailment relations on lexical items by cosine. Highly similar pairs (0.90–0.99) are less likely entailing than moderately similar pairs (0.70–0.89). Dependency Vectors. We extract (lemma/POS, relation, context/POS) tuples from each of the Stanford Collapsed CC Dependency graphs. We filter tuples with lemmas not in our 51k chosen types. Following Baroni and Lenci (2010), we model inverse relations and mark them separately. For example, “red/JJ car/NN” will generate tuples for both (car/NN, amod, red/JJ) and (red/JJ, amod−1 , car/NN). After extracting tuples, we discard all but the top 100k (relation, context/POS) pairs and build a vector space using lemma/POS as rows, and (relation, context/POS) as columns. The matrix is transformed with Positive Pointwise Mutual Information, and reduced to 300 dimensions using Singular Value Decomposition (SVD). We do not vary these parameters, but chose them as they performed best in prior work (Roller, Erk, and Boleda 201"
J16-4007,W11-2501,0,0.0761049,"Missing"
J16-4007,D10-1115,0,0.105774,"Missing"
J16-4007,S13-1002,1,0.923963,"et al. Meaning Using Logical and Distributional Models For logic-based semantics, one of the challenges is to adapt the representation to the assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk [2015]) and semantic textual similarity (Beltagy, Erk, and Mooney 2014), and we are investigating applications to question answering. We have demonstrated the modularity of the system by testing both MLNs (Richardson and Domingos 2006) and Probabilistic Soft Logic (Broecheler, Mihalkova, and Getoor 2010) as probabilistic inference engines (Beltagy et al. 2013; Beltagy, Erk, and Mooney 2014). The primary aim of th"
J16-4007,W15-0119,1,0.951889,"tion for a more graded representation of words and short phrases, providing information on near-synonymy and lexical entailment. Uncertainty and gradedness at the lexical and phrasal level should inform inference at all levels, so we rely on probabilistic inference to integrate logical and distributional semantics. Thus, our system has three main components, all of which present interesting challenges. 764 Beltagy et al. Meaning Using Logical and Distributional Models For logic-based semantics, one of the challenges is to adapt the representation to the assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk"
J16-4007,P14-1114,1,0.717939,"Missing"
J16-4007,D13-1160,0,0.145898,"in SemEval 2014, the best result was achieved by systems that did not compute a sentence representation in a compositional manner. We present a model that performs deep compositional semantic analysis and achieves state-of-the-art performance (Section 7.2). 2. Background Logical Semantics. Logical representations of meaning have a long tradition in linguistic semantics (Montague 1970; Dowty, Wall, and Peters 1981; Alshawi 1992; Kamp and Reyle 1993) and computational semantics (Blackburn and Bos 2005; van Eijck and Unger 2010), and are commonly used in semantic parsing (Zelle and Mooney 1996; Berant et al. 2013; Kwiatkowski et al. 2013). They handle many complex semantic phenomena, such as negation and quantifiers, and they identify discourse referents along with the predicates that apply to them and the relations that hold between them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view"
J16-4007,S14-2114,0,0.220937,"Missing"
J16-4007,W08-2222,0,0.398307,"tween them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P12-1015,0,0.0273672,"related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To combine logical and probabilistic information, we utiliz"
J16-4007,P04-1014,0,0.0800852,"Missing"
J16-4007,2015.lilt-10.4,0,0.509016,"than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To combine logical and probabilistic information, we utilize MLNs (Richardson and Domingos 2006). MLNs are well suited for our approach because they provide an elegant fra"
J16-4007,copestake-flickinger-2000-open,0,0.116329,"Missing"
J16-4007,N12-1076,0,0.280892,"Missing"
J16-4007,D08-1094,1,0.83338,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P14-1113,0,0.0214839,"Missing"
J16-4007,N13-1092,0,0.0993755,"Missing"
J16-4007,W11-0112,1,0.818128,"Missing"
J16-4007,P05-1014,0,0.0345764,"t to the sources of lexical and phrasal knowledge it uses, and in this article we utilize PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013) and WordNet, along with distributional models. But we are specifically interested in distributional models, in particular, in how well they can predict lexical and phrasal entailment. Our system provides a unique framework for evaluating distributional models on recognizing textual entailment (RTE) because the overall sentence representation is handled by the logic, so we can zoom in on the performance of distributional models at predicting lexical (Geffet and Dagan 2005) and phrasal entailment. The evaluation of distributional models on RTE is the third aim of our article. We build a lexical entailment classifier that exploits both task-specific features as well as distributional information, and present an in-depth evaluation of the distributional components. We now provide a brief sketch of our framework (Garrette, Erk, and Mooney 2011; Beltagy et al. 2013). Our framework is three components. The first is the logical form, which is the primary meaning representation for a sentence. The second is the distributional information, which is encoded in the form o"
J16-4007,S13-1001,0,0.567318,"c similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010). They are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Lund and Burgess 1996; Landauer and Dumais 1997). Therefore, distributional models are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the graded nature of linguistic meaning, but they do not adequately capture logical structure (Grefenstette 2013). Distributional models have also been extended to compute vector representations for larger phrases, for example, by adding the vectors for the individual words (Landauer and Dumais 1997) or by a component-wise product of word vectors (Mitchell and Lapata 2008, 2010), or through more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli 2010; Grefenstette and Sadrzadeh 2011). Integrating Logic-Based and Distributional Semantics. It does not seem particularly useful at this point to speculate about phenomena that either a distributional approach or a"
J16-4007,D11-1129,0,0.043449,"chutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pinkal 2010). But at this point, fully representing structure and logical form using distributional models of phrases and sentences is still an open problem. Also, current distributional representations do not support logical inference that captures the semantics of negation, logical connectives, and quantifiers. Therefore, distributional models and logical representations of natural language meaning are complementary in their strengths, as has frequently been remarked (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Grefenstette and Sadrzadeh 2011; Baroni, Bernardi, and Zamparelli 2014). Our aim has been to construct a general-purpose natural language understanding system that provides in-depth representations of sentence meaning amenable to automated inference, but that also allows for flexible and graded inferences involving word meaning. Therefore, our approach combines logical and distributional methods. Specifically, we use first-order logic as a basic representation, providing a sentence representation that can be easily interpreted and manipulated. However, we also use distributional information for a more graded representation"
J16-4007,D15-1003,0,0.0518462,"and world knowledge should be front and center in all inferential processes. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang, Jordan, and Klein 2011). They construct phrasal entailment rules based on a logic-based alignment, and use distributional similarity of aligned words to filter rules that do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels fe"
J16-4007,P88-1012,0,0.122535,"∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen, Boxer uses a neo-Davidsonian framework (Parsons 1990): y is an event variable, and the semantic roles agent and patient are turned into predicates linking y to the agent x and patient z. As we discuss later, we combine Boxer’s logical form with weighted rules and perform probabilistic inference. Lewis and Steedman (2013) also integrate logical and distributional approaches, but use distributional information to create predicates for a standard binary logic and do not use probabilistic inference. Much earlier, Hobbs et al. (1988) combined logical form with weights in an abductive framework. There, the aim was to model the interpretation of a passage as its best possible explanation. Distributional Semantics. Distributional models (Turney and Pantel 2010) use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010). They are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur"
J16-4007,Q15-1027,0,0.0421207,"Missing"
J16-4007,D13-1161,0,0.364509,"and Reyle 1993), like first-order logic, represent many linguistic phenomena like negation, quantifiers, or discourse entities. Some of these phenomena (especially negation scope and discourse entities over paragraphs) cannot be easily represented in syntax-based representations like Natural Logic (MacCartney and Manning 2009). In addition, firstorder logic has standardized inference mechanisms. Consequently, logical approaches have been widely used in semantic parsing where it supports answering complex natural language queries requiring reasoning and data aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic and probabilities). Distributional models (Turney and Pantel 2010) use contextual similarity to predict the graded semantic similarity of words and phrases (Landauer and Dumais ¨ 1997; Mitchell and Lapata 2010), and to model polysemy (Schutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pi"
J16-4007,S14-2055,0,0.0675792,"the SemEval 2012 STS MSR-Video Description data.4 Randomly selected sentences from these two sources were first simplified to remove some linguistic phenomena that the data set was not aiming to cover. Then, additional sentences were created as variations over these sentences, by paraphrasing, negation, and reordering. RTE pairs were then created that consisted of a simplified original sentence paired with one of the transformed sentences (generated from either the same or a different original sentence). We would like to mention two particular systems that were evaluated on SICK. The first is Lai and Hockenmaier (2014), which was the top-performing system at the original shared task. It uses a linear classifier with many hand-crafted features, including alignments, word forms, POS tags, distributional similarity, WordNet, and a unique feature called Denotational Similarity. Many of these hand-crafted features are later incorporated in our lexical entailment classifier, described in Section 5.2. The Denotational Similarity uses a large database of human- and machine-generated image captions to cleverly capture some world knowledge of entailments. The second system is Bjerva et al. (2014), which also particip"
J16-4007,S12-1012,0,0.0410122,"Missing"
J16-4007,N15-1098,0,0.458345,"provide a data set of all lexical and phrasal rules needed for the SICK data set (10,211 rules). This is a valuable resource for testing lexical entailment systems on entailment relations that are actually useful in an end-to-end RTE system (Section 5.1). We evaluate a state-of-the-art compositional distributional approach (Paperno, Pham, and Baroni 2014) on the task of phrasal entailment (Section 5.2.5). We propose a simple weight learning approach to map rule weights to MLN weights (Section 6.3). The question “Do supervised distributional methods really learn lexical inference relations?” (Levy et al. 2015) has been studied before on a variety of lexical entailment data sets. For the first time, we study it on data from an actual RTE data set and show that distributional information is useful for lexical entailment (Section 7.1). Marelli et al. (2014a) report that for the SICK data set used in SemEval 2014, the best result was achieved by systems that did not compute a sentence representation in a compositional manner. We present a model that performs deep compositional semantic analysis and achieves state-of-the-art performance (Section 7.2). 2. Background Logical Semantics. Logical representat"
J16-4007,Q13-1015,0,0.110586,"he assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk [2015]) and semantic textual similarity (Beltagy, Erk, and Mooney 2014), and we are investigating applications to question answering. We have demonstrated the modularity of the system by testing both MLNs (Richardson and Domingos 2006) and Probabilistic Soft Logic (Broecheler, Mihalkova, and Getoor 2010) as probabilistic inference engines (Beltagy et al. 2013; Beltagy, Erk, and Mooney 2014). The primary aim of the current article is to describe our complete system in detail— all the nuts and bolts necessary to bring together the three"
J16-4007,D14-1107,0,0.0324867,"lead to erroneous entailments. If we can obtain multiple parses for a text T and query H, and hence multiple logical forms, this should increase our chances of getting a good estimate of the probability of H given T. The default CCG parser that Boxer uses is C&C (Clark and Curran 2004). This parser can be configured to produce multiple ranked parses (Ng and Curran 2012); however, we found that the top parses we get from C&C are usually not diverse enough and map to the same logical form. Therefore, in addition to the top C&C parse, we use the top parse from another recent CCG parser, EasyCCG (Lewis and Steedman 2014). Therefore, for a natural language text NT and query NH , we obtain two parses each, say ST1 and ST2 for T and SH1 and SH2 for H, which are transformed to logical forms T1 , T2 , H1 , H2 . We now compute probabilities for all possible combinations of representations of NT and NH : the probability of H1 given T1 , the probability of H1 given T2 , and conversely also the probabilities of H2 given either T1 or T2 . If the task is textual entailment with the three categories: entailment, neutral, and contradiction, then, as described in Section 4.1, we also compute the probability of ¬H1 given ei"
J16-4007,W09-3714,0,0.193266,"automatically from corpus data. There is no single representation for natural language meaning at this time that fulfills all of these requirements, but there are representations that fulfill some of them. Logic-based representations (Montague 1970; Dowty, Wall, and Peters 1981; Kamp and Reyle 1993), like first-order logic, represent many linguistic phenomena like negation, quantifiers, or discourse entities. Some of these phenomena (especially negation scope and discourse entities over paragraphs) cannot be easily represented in syntax-based representations like Natural Logic (MacCartney and Manning 2009). In addition, firstorder logic has standardized inference mechanisms. Consequently, logical approaches have been widely used in semantic parsing where it supports answering complex natural language queries requiring reasoning and data aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic a"
J16-4007,S14-2001,0,0.0594867,"build on in this article. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical 768 Beltagy et al. Meaning Using Logical and Distributional Models forms. The main difference between the two approaches lies in the role of gradience. Lewis and Steedman view weights and probabilities as a problem to be avoided. We believe that the uncertainty inherent in both language processing and world knowledge should be front and center in all inferential processes. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang, Jordan, and Klein 2011). They construct phrasal entailment rules based on a logic-based alignment, and use distributional similarity of aligned words to filter rules that do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextu"
J16-4007,marelli-etal-2014-sick,0,0.218007,"Missing"
J16-4007,N13-1090,0,0.0100112,"BNC, ukWaC, and a 2014-01-07 copy of Wikipedia. All corpora are preprocessed using the Stanford CoreNLP parser. We collapse particle verbs into a single token, and all tokens are annotated with a (short) POS tag so that the same lemma with a different POS is modeled separately. We keep only content words (NN, VB, RB, JJ) appearing at least 1,000 times in the corpus. The final corpus contains 50,984 types and roughly 1.5B tokens. Bag-of-Words Vectors. We filter all but the 51k chosen lemmas from the corpus, and create one sentence per line. We use Skip-Gram Negative Sampling to create vectors (Mikolov et al. 2013). We use 300 latent dimensions, a window size of 20, and 15 negative samples. These parameters were not tuned, but chosen as reasonable defaults for the task. We use the large window size to ensure the BoW vectors captured more topical similarity, rather than syntactic similarity, which is modeled by the dependency vectors. 787 Computational Linguistics Volume 42, Number 4 Figure 4 Distribution of entailment relations on lexical items by cosine. Highly similar pairs (0.90–0.99) are less likely entailing than moderately similar pairs (0.70–0.89). Dependency Vectors. We extract (lemma/POS, relat"
J16-4007,P08-1028,0,0.0399511,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P12-1052,0,0.060547,"Missing"
J16-4007,P14-1009,0,0.0181022,"Missing"
J16-4007,P15-1142,0,0.0233947,"Missing"
J16-4007,D08-1068,0,0.0135133,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,W09-1406,0,0.00953484,"t do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To com"
J16-4007,W08-2125,0,0.0353324,"for some ground clauses. For example, ogre(A) means that Anna is an ogre. Marginal inference for MLNs calculates the probability P(Q|E, R) for a query formula Q. Alchemy (Kok et al. 2005) is the most widely used MLN implementation. It is a software package that contains implementations of a variety of MLN inference and learning algorithms. However, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem. MLNs have been used for various NLP applications, including unsupervised coreference resolution (Poon and Domingos 2008), semantic role labeling (Riedel and Meza-Ruiz 2008), and event extraction (Riedel et al. 2009). Recognizing Textual Entailment. The task that we focus on in this article is RTE (Dagan et al. 2013), the task of determining whether one natural language text, the Text T, entails, contradicts, or is not related (neutral) to another, the Hypothesis H. “Entailment” here does not mean logical entailment: The Hypothesis is entailed if a human annotator 770 Beltagy et al. Meaning Using Logical and Distributional Models judges that it plausibly follows from the Text. When using naturally occurring sentences, this is a very challenging task that should b"
J16-4007,C14-1097,1,0.8051,"Missing"
J16-4007,J98-1004,0,0.0450268,"a aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic and probabilities). Distributional models (Turney and Pantel 2010) use contextual similarity to predict the graded semantic similarity of words and phrases (Landauer and Dumais ¨ 1997; Mitchell and Lapata 2010), and to model polysemy (Schutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pinkal 2010). But at this point, fully representing structure and logical form using distributional models of phrases and sentences is still an open problem. Also, current distributional representations do not support logical inference that captures the semantics of negation, logical connectives, and quantifiers. Therefore, distributional models and logical representations of natural language meaning are complementary in their strengths, as has frequently been remarked (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Grefenstett"
J16-4007,D12-1130,0,0.00980463,"Missing"
J16-4007,P06-1101,0,0.0276372,"Missing"
J16-4007,P10-1097,0,0.0569234,"Missing"
J16-4007,P14-1008,0,0.0490958,"Missing"
J16-4007,C14-1212,0,0.0187584,"Missing"
J16-4007,C04-1146,0,0.247925,"Missing"
J16-4007,I11-1038,0,0.0728394,"Missing"
J16-4007,P08-2063,0,\N,Missing
N06-1017,W02-0811,0,0.0728103,"sis of all three experiments in this paper: • a bag-of-words context, with a window size of one sentence; • bi- and trigrams centered on the target word; • grammatical function information: for each dependent of the target, (1) its function label, (2) its headword, and (3) a combination of both are used as features. (4) The concatenation of all function labels constitutes another feature. For PPs, function labels are extended by the preposition. As an example, Figure 2 shows a BNC sentence and its grammatical function features. • for verb targets, the target voice. The feature set is based on Florian et al. (2002) but contains additional syntax-related features. Each word-related feature is represented as four features for word, lemma, part of speech, and named entity. S HALMANESER trains one Naive Bayes classifier per lemma to be disambiguated. For this experiment, θ 0.5 0.75 0.9 0.98 Precision 0.6524 (σ 0.115) 0.7855 (σ 0.0086) 0.7855 (σ 0.0093) 0.7847 (σ 0.0073) 0.0011 0.0527 0.1006 0.1744 Recall (σ 0.0004) (σ 0.0013) (σ 0.0021) (σ 0.0025) x Table 1: Experiment 1: Results for label unknown sense, WSD confidence level approach. θ: confidence threshold. σ: std. dev. all system parameters were set to t"
N06-1017,P93-1016,0,0.0313618,"l setup, the unknown sense occurrences of each lemma all belong to the same sense. However, this does not bias the experiment since none of the models we study take advantage of the shape of the test set in any way. Rather, each test item is classified individually, without recourse to the other test items. Data. All experiments in this paper were performed on the FrameNet 1.2 annotated data pertaining to ambiguous lemmas. After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas. They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004). 4 Experiment 1: WSD confidence scores for unknown sense detection In this section we test a very simple model of unknown sense detection: Classifiers often return a confidence score along with the assigned label. We will try to detect unknown senses by a threshold on confidence scores, declaring anything below the threshold as unknown. Note that this method can only be applied to lemmas that have more than one sense, since for single-sense lemmas the system will always return the maximum confidence score. Data. While"
N06-1017,J98-1004,0,0.306569,"Missing"
N06-1017,P98-1013,0,0.187981,"allow semantic parsing, by which we mean a combination of WSD and the automatic assignment of 128 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128–135, c New York, June 2006. 2006 Association for Computational Linguistics semantic roles to free text. In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses. Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the S HALMANESER (Erk and Pado, 2006) shallow semantic parser. The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions. FrameNet is lacking a sense of “expectation” or “being mentally prepared” for the verb prepare, so prepared has been assigned the sense C OOKING CREATION, a possible but improbable analysis2 . Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing, e.g. for drawing inferences. Unknown sense detection can prevent such mistakes. All sense inventories face the problem of missing senses, either because of their small overall size (as is the case for some non-English Word"
N06-1017,callmeier-etal-2004-deepthought,0,0.0230044,"Missing"
N06-1017,P05-1004,0,0.0218781,"method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the S HALMANESER WSD 2 Unfortunately, the semantic roles have been mis-assigned by the system. The word I should fill the F OOD role, while for a hound could be assigned the optional R ECEIVER role. 129 system. The result is th"
N06-1017,erk-pado-2006-shalmaneser,1,0.809753,"respects the given word senses. The main motivation for this study comes from shallow semantic parsing, by which we mean a combination of WSD and the automatic assignment of 128 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128–135, c New York, June 2006. 2006 Association for Computational Linguistics semantic roles to free text. In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses. Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the S HALMANESER (Erk and Pado, 2006) shallow semantic parser. The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions. FrameNet is lacking a sense of “expectation” or “being mentally prepared” for the verb prepare, so prepared has been assigned the sense C OOKING CREATION, a possible but improbable analysis2 . Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing, e.g. for drawing inferences. Unknown sense detection can prevent such mistakes. All sense inventories face the problem of missing senses, eith"
N06-1017,H05-1047,0,0.0155165,"expressions that can evoke them. These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions. Frames are linked by IS-A and other relations. Currently, FrameNet contains 609 frames with 8,755 lemmaframe pairs, of which 5,308 are exemplified in annotated sentences from the British National Corpus. The annotation comprises 133,846 sentences. As FrameNet is a growing resource, many lemmas are still lacking senses, and many senses are still lacking annotation. This is problematic for the use of FrameNet analyses as a basis for inferences over text, as e.g. in Tatu and Moldovan (2005). For example, the verb prepare from Figure 1 is associated with the frames C OOKING CREATION: prepare food ACTIVITY PREPARE: get ready for an activity ACTIVITY READY STATE: be ready for an activity W ILLINGNESS: be willing of which only the C OOKING CREATION sense has been annotated. The sense in Figure 1 is not covered yet: ACTIVITY READY STATE would be more appropriate than C OOKING CREATION, but still not optimal, since the sentence refers to a mental state rather than the preparation of an activity. 3 wave Experimental setup and data s Experimental setup. To evaluate an unknown sense dete"
N06-1017,N03-1036,0,0.0271675,"rsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the S HALMANESER WSD 2 Unfortunately, the semantic roles have been mis-assigned by the system. The word I should fill the F OOD role, while for a hound could be assigned the optional R ECEIVER role. 129 system. Th"
N06-1017,C98-1013,0,\N,Missing
N16-1131,D14-1082,0,0.0256291,"Missing"
N16-1131,D10-1113,0,0.233328,"y and Navigli, 2009) is a task in which word meaning in context is described not through dictionary senses but through substitutes (paraphrases) chosen by annotators. For example, consider the following usage of the adjective bright: “The bright girl was reading a book.” Valid lexical substitutions for bright include adjectives like smart and intelligent, but not words like luminous or colorful. Originally introduced as a SemEval task in 2007, lexical substitution has often been used to evaluate the ability of distributional models to handle polysemy (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010; Van de Cruys et al., 2011; Melamud et al., 2015b; Melamud et al., 2015a; Kawakami and Dyer, 2015). Recent models include a simple but high-performing method by Melamud et al. (2015b), which uses the Skip-gram model of Mikolov et al. (Mikolov et al., 2013) to compute the probability of a substitute given a sentence context, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu and integrates it with the probability of the substitute given the target. The current state of the art is held by another model of Melamud (Melamud et al., 2015a), which uses"
N16-1131,D08-1094,1,0.888315,"Missing"
N16-1131,P15-1001,0,0.101478,"Missing"
N16-1131,E14-1057,1,0.889686,"Missing"
N16-1131,P14-2050,0,0.238316,"tics vised (Szarvas et al., 2013) and unsupervised (Melamud et al., 2015a) settings. The latter is the current state-of-art system, and is based around generating and pruning second-order word representations using language models. In this work, we limit our comparisons to the model of Melamud et al. (2015b), a method which performs nearly state-of-art, is extremely easy to implement, and is a good testbed for focused hypotheses. They propose a novel measure which uses dependency-based word and context embeddings derived from Skip-gram Negative Sampling algorithm (SGNS) (Mikolov et al., 2013; Levy and Goldberg, 2014a). Their measure addCos for estimating the appropriateness of a substitute s as a substitute for t in the context C = {c1 , c2 , . . .} is defined as follows:2 X addCos(s|t, C) = cos(s, t) + cos(s, c). The values Zt and ZC are normalizing constants to make sure each distribution sums to one. This measure has two free parameters, W and b, which act as a linear transformation over the context vectors. These parameters are estimated from the original corpus, and are trained to maximize the prediction of a target from only its syntactic contexts (c.f. Section 4.4). Given this formulation, a natur"
N16-1131,S07-1009,0,0.622929,"rly on the all-words ranking task. Interestingly, analysis of PIC shows it improves over baselines by incorporating frequency biases into predictions. 2 Prior Work In the lexical substitution task, an annotator is given a target word in context and generates one or more substitutes. As multiple annotators label a target, the result is a weighted list of substitutes, where weights indicate how many annotators chose a particular substitute (McCarthy and Navigli, 2009). There have been numerous approaches on the lexical substitution task of varying complexity and using various lexical resources (McCarthy and Navigli, 2007). Some approaches focus on explicitly modeling an in-context vector (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Van de Cruys et al., 2011; Kremer et al., 2014; Kawakami and Dyer, 2015), while others approach it using more sophisticated pipelines, in both super1 Code and models available at https://github.com/ stephenroller/naacl2016. 1121 Proceedings of NAACL-HLT 2016, pages 1121–1126, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics vised (Szarvas et al., 2013) and unsupervised (Melamud et al., 2015a) settings. The latter is the c"
N16-1131,N15-1050,0,0.410935,"ng in context is described not through dictionary senses but through substitutes (paraphrases) chosen by annotators. For example, consider the following usage of the adjective bright: “The bright girl was reading a book.” Valid lexical substitutions for bright include adjectives like smart and intelligent, but not words like luminous or colorful. Originally introduced as a SemEval task in 2007, lexical substitution has often been used to evaluate the ability of distributional models to handle polysemy (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010; Van de Cruys et al., 2011; Melamud et al., 2015b; Melamud et al., 2015a; Kawakami and Dyer, 2015). Recent models include a simple but high-performing method by Melamud et al. (2015b), which uses the Skip-gram model of Mikolov et al. (Mikolov et al., 2013) to compute the probability of a substitute given a sentence context, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu and integrates it with the probability of the substitute given the target. The current state of the art is held by another model of Melamud (Melamud et al., 2015a), which uses a more complex architecture. In this paper we bu"
N16-1131,W15-1501,0,0.616355,"ng in context is described not through dictionary senses but through substitutes (paraphrases) chosen by annotators. For example, consider the following usage of the adjective bright: “The bright girl was reading a book.” Valid lexical substitutions for bright include adjectives like smart and intelligent, but not words like luminous or colorful. Originally introduced as a SemEval task in 2007, lexical substitution has often been used to evaluate the ability of distributional models to handle polysemy (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010; Van de Cruys et al., 2011; Melamud et al., 2015b; Melamud et al., 2015a; Kawakami and Dyer, 2015). Recent models include a simple but high-performing method by Melamud et al. (2015b), which uses the Skip-gram model of Mikolov et al. (Mikolov et al., 2013) to compute the probability of a substitute given a sentence context, Katrin Erk Department of Linguistics The University of Texas at Austin katrin.erk@mail.utexas.edu and integrates it with the probability of the substitute given the target. The current state of the art is held by another model of Melamud (Melamud et al., 2015a), which uses a more complex architecture. In this paper we bu"
N16-1131,N13-1133,0,0.222038,"task of varying complexity and using various lexical resources (McCarthy and Navigli, 2007). Some approaches focus on explicitly modeling an in-context vector (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Van de Cruys et al., 2011; Kremer et al., 2014; Kawakami and Dyer, 2015), while others approach it using more sophisticated pipelines, in both super1 Code and models available at https://github.com/ stephenroller/naacl2016. 1121 Proceedings of NAACL-HLT 2016, pages 1121–1126, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics vised (Szarvas et al., 2013) and unsupervised (Melamud et al., 2015a) settings. The latter is the current state-of-art system, and is based around generating and pruning second-order word representations using language models. In this work, we limit our comparisons to the model of Melamud et al. (2015b), a method which performs nearly state-of-art, is extremely easy to implement, and is a good testbed for focused hypotheses. They propose a novel measure which uses dependency-based word and context embeddings derived from Skip-gram Negative Sampling algorithm (SGNS) (Mikolov et al., 2013; Levy and Goldberg, 2014a). Their"
N16-1131,P10-1097,0,0.379496,"Missing"
N16-1131,D11-1094,0,0.044786,"Missing"
N16-1131,biemann-2012-turk,0,\N,Missing
N18-1076,P98-1013,0,0.511831,"Missing"
N18-1076,P08-1090,0,0.15487,"s. Event knowledge is key to determining implicit arguments. In our example, diseases are maybe the single most typical things to break out, and diseases also typically kill people. The task of identifying implicit arguments was first addressed by Gerber and Chai (2010) and Ruppenhofer et al. (2010). However, the datasets for the task were very small, and to our knowledge there has been very little further development on the task since then. In this paper, we address the data issue by training models for implicit argument prediction on a simple cloze task, similar to the narrative cloze task (Chambers and Jurafsky, 2008), for which data can be generated automatically at scale. This allows us to train a neural network to perform the task, building on two insights. First, event knowledge is crucial for implicit argument detection. Therefore we build on models for narrative event prediction (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016a), using them to judge how coherent the narrative would be when we fill in a particular entity as the missing (implicit) argument. Second, the omitted arguments tend to be salient, as Ebola is in the text from which the above example is taken. So in addition to narr"
N18-1076,P09-1068,0,0.299788,"Missing"
N18-1076,I17-1010,0,0.100217,"Missing"
N18-1076,E14-4040,0,0.27951,"allows us to train a neural network to perform the task, building on two insights. First, event knowledge is crucial for implicit argument detection. Therefore we build on models for narrative event prediction (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016a), using them to judge how coherent the narrative would be when we fill in a particular entity as the missing (implicit) argument. Second, the omitted arguments tend to be salient, as Ebola is in the text from which the above example is taken. So in addition to narrative coherence, our model takes into account entity salience (Dunietz and Gillick, 2014). In an evaluation on a large automatically generated dataset, our model clearly outperforms even strong baselines, and we find salience features to be important to the success of the model. We also evaluate against a variant of the Gerber and Chai (2012) model that does not rely on gold features, finding that our simple neural model outperforms their much more complex model. Our paper thus makes two major contributions. 1) We propose an argument cloze task to generate synthetic training data at scale for implicit argument prediction. 2) We show that neural event Implicit arguments are not syn"
N18-1076,P10-1160,0,0.0919144,"most powerful indicator for entity salience among all features. We expect similar results in our experiments, however we include all three features in our event composition model for now, and conduct an ablation test afterwards. Feature 1st loc head count mentions # doc # test cases Avg # entities Description Index of the sentence where the first mention of the entity appears Number of times the head word of the entity appears A vector containing the numbers of named, nominal, pronominal, and total mentions of the entity 5.2 The Gerber and Chai (G&C) Dataset The implicit argument dataset from Gerber and Chai (2010) (referred as G&C henceforth) consists of 966 human-annotated implicit argument instances on 10 nominal predicates. To evaluate our model on G&C, we convert the annotations to the input format of our model as follows: We map nominal predicates to their verbal form, and semantic role labels to syntactic argument types based on the NomBank frame definitions. One of the examples (after mapping semantic role labels) is as follows: Gillick (2014). The entity salience features are directly passed into the pair composition network as additional input. We also add an extra feature for argument positio"
N18-1076,J12-4003,0,0.471978,"ooney, 2016a), using them to judge how coherent the narrative would be when we fill in a particular entity as the missing (implicit) argument. Second, the omitted arguments tend to be salient, as Ebola is in the text from which the above example is taken. So in addition to narrative coherence, our model takes into account entity salience (Dunietz and Gillick, 2014). In an evaluation on a large automatically generated dataset, our model clearly outperforms even strong baselines, and we find salience features to be important to the success of the model. We also evaluate against a variant of the Gerber and Chai (2012) model that does not rely on gold features, finding that our simple neural model outperforms their much more complex model. Our paper thus makes two major contributions. 1) We propose an argument cloze task to generate synthetic training data at scale for implicit argument prediction. 2) We show that neural event Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task"
N18-1076,N06-2015,0,0.0227722,"Therefore, we train the model to minimize cross-entropy as follows: Pair Composition Network The pair composition network (light blue area in Figure 2) computes a coherence score coh between 0 and 1, given the vector representations of a context event and a target event. The coherence score should be high when the target event contains the correct argument, and low otherwise. So we construct the m 1 X − log(coh(eci , epi ))−log(1−coh(eci , eni )) m i=1 (3) 834 where eci , epi , and eni are the context, positive, and negative events of the ith training sample respectively. 4.3 from OntoNotes (Hovy et al., 2006), which contains human-labeled dependency and coreference annotation for a large corpus. So the extracted events and entities in the evaluation set are gold. Note that this is only for evaluation; in training we do not rely on any gold annotations (Section 6.1). There are four English sub-corpora in OntoNotes Release 5.04 that are annotated with dependency labels and coreference chains. Three of them, which are mainly from broadcast news, share similar statistics in document length, so we combine them into a single dataset and name it ON-S HORT as it consists mostly of short documents. The fou"
N18-1076,D15-1195,0,0.131256,"Missing"
N18-1076,P13-1116,0,0.0366036,"Missing"
N18-1076,W09-2417,0,0.145964,"Missing"
N18-1076,P14-5010,0,0.00375488,"g samples. We refer to the resulting models as E VENT C OMP -8M and E VENT C OMP -40M. Experiments 6.1 Implementation Details We train our neural model using synthetic data as described in Section 3. For creating the training data, we do not use gold parses or gold coreference chains. We use the 20160901 dump of English Wikipedia5 , with 5,228,621 documents in total. For each document, we extract plain text and break it into paragraphs, while discarding all structured data like lists and tables6 . We construct a sequence of events and entities from each paragraph, by running Stanford CoreNLP (Manning et al., 2014) to obtain dependency parses and coreference chains. We lemmatize all verbs and arguments. We incorporate negation and particles in verbs, and normalize passive constructions. We represent each argument by the corresponding entity’s representative mention if it is linked to an entity, otherwise by its head lemma. We keep verbs and arguments with counts over 500, together with the 50 most frequent prepositions, leading to a vocabulary of 53,345 tokens; all other words are replaced with an out-of-vocabulary token. The most frequent verbs (with counts over 100,000) are down-sampled. For training"
N18-1076,W04-2705,0,0.150613,"Missing"
N18-1076,N16-1173,0,0.386583,"Missing"
N18-1076,Q17-1003,0,0.0492424,"Missing"
N18-1076,J05-1004,0,0.0440165,"Missing"
N18-1076,P16-1027,0,0.446165,"were very small, and to our knowledge there has been very little further development on the task since then. In this paper, we address the data issue by training models for implicit argument prediction on a simple cloze task, similar to the narrative cloze task (Chambers and Jurafsky, 2008), for which data can be generated automatically at scale. This allows us to train a neural network to perform the task, building on two insights. First, event knowledge is crucial for implicit argument detection. Therefore we build on models for narrative event prediction (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016a), using them to judge how coherent the narrative would be when we fill in a particular entity as the missing (implicit) argument. Second, the omitted arguments tend to be salient, as Ebola is in the text from which the above example is taken. So in addition to narrative coherence, our model takes into account entity salience (Dunietz and Gillick, 2014). In an evaluation on a large automatically generated dataset, our model clearly outperforms even strong baselines, and we find salience features to be important to the success of the model. We also evaluate against a variant of the Gerber and"
N18-1076,S10-1008,0,\N,Missing
N18-2049,K16-1002,0,0.0253448,"ectional preference. candy is selectionally preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and t"
N18-2049,D13-1185,0,0.0206428,"tinguishing semantic plausibility from selectional preference. candy is selectionally preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Eq"
N18-2049,N01-1013,0,0.274003,"Missing"
N18-2049,P10-2017,1,0.860967,"Missing"
N18-2049,P17-1025,0,0.408245,"y, semantic plausibility is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016). Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals. In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3). Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation. Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility. 303 Proceedings of NAACL-HLT 2018, pages 303–308 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work (a) Have Turkers write down plausible or implausible S - V and V- O selections; (b) Randomly generate S - V- O triples from collected S - V and V- O pairs; (c) Send resulting S - V- O triples to Turkers to filter for ones with high agreement (by ma"
N18-2049,D17-1019,0,0.0336672,"n the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and then hold it. What kinds of semantic knowledge are necessary for distinguishing a physically plausible e"
N18-2049,N16-1098,0,0.0736901,"preferred because it is distributionally common patient in the event man-swallow-*, as opposed to the bizarre and rarely seen (if at all) patient paintball. However both are semantically plausible according to our world knowledge: they are small-sized objects that are swallowable by a man. desk is both distributionally unlikely and implausible (i.e. oversized for swallowing). Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance. Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem. Introduction Intuitively, a man can swallow a candy or paintball but not a desk. Equally so, one cannot plausibly eat a cake and then hold it. What kinds of semantic knowledge are"
N18-2049,P10-1045,0,0.22193,"Missing"
N18-2049,D16-1017,0,0.106709,"Missing"
N18-2049,D14-1004,0,0.273445,"Missing"
N18-2049,I17-1021,1,0.800029,"y is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016). Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals. In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3). Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation. Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility. 303 Proceedings of NAACL-HLT 2018, pages 303–308 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work (a) Have Turkers write down plausible or implausible S - V and V- O selections; (b) Randomly generate S - V- O triples from collected S - V and V- O pairs; (c) Send resulting S - V- O triples to Turkers to filter for ones with high agreement (by majority vote). (a) en"
N18-2049,Q17-1027,0,0.0794419,"Missing"
P01-1011,C00-1067,1,0.783806,"ant. Existing underspecification approaches (Reyle, 1993; van Deemter and Peters, 1996; Pinkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach"
P01-1011,P83-1020,0,0.196211,"r and Peters, 1996; Pinkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach extends the work presented in (Bodirsky et al., 2001), which"
P01-1011,P95-1021,0,0.0247362,"inkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach extends the work presented in (Bodirsky et al., 2001), which defines -reduction con"
P03-1068,P98-1013,0,0.0248009,"e most serious bottlenecks for language technology. To train tools for the acquisition of semantic information for such lexica, large, extensively annotated resources are necessary. In this paper, we present current work of the SALSA (SAarbr¨ucken Lexical Semantics Annotation and analysis) project, whose aim is to provide such a resource and to investigate efficient methods for its utilisation. In the current project phase, the focus of our research and the backbone of the annotation are semantic role relations. More specifically, our role annotation is based on the Berkeley FrameNet project (Baker et al., 1998; Johnson et al., 2002). In addition, we selectively annotate word senses and anaphoric links. The TIGER corpus (Brants et al., 2002), a 1.5M word German newspaper corpus, serves as sound syntactic basis. Besides the sparse data problem, the most serious problem for corpus-based lexical semantics is the lack of specificity of the data: Word meaning is notoriously ambiguous, vague, and subject to contextual variance. The problem has been recognised and discussed in connection with the SENSEVAL task (Kilgarriff and Rosenzweig, 2000). Annotation of frame semantic roles compounds the problem as it"
P03-1068,H94-1020,0,0.0480689,"quisition of word-semantic information, e.g. the construction of domainindependent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation. 1 Introduction Corpus-based methods for syntactic learning and processing are well-established in computational linguistics. There are comprehensive and carefully worked-out corpus resources available for a number of languages, e.g. the Penn Treebank (Marcus et al., 1994) for English or the NEGRA corpus (Skut et al., 1998) for German. In semantics, the situation is different: Semantic corpus annotation is only in its initial stages, and currently only a few, mostly small, corpora are available. Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative (Kilgarriff, 2001), a notable exception being the Prague Treebank (Hajiˇcov´a, 1998) . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often u"
P03-1068,C98-1013,0,\N,Missing
P07-1028,E03-1034,0,0.57964,"Missing"
P07-1028,W05-0620,0,0.0490374,"ne application of selectional preferences: semantic role labeling. The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word). In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005). In the CoNLL-05 shared task, participating systems showed about 10 points F-score difference between in-domain and out-of-domain test data. Concerning (1), we focus on selectional preferences as the strongest candidate for informative semantic features. Concerning (2), the corpusbased similarity metrics that we use for selectional preference induction open up interesting possibilities of mixing domains. We evaluate the similarity-based model against Resnik’s WordNet-based model as well as the EM-based clustering approach. In the evaluation, the similarity-model shows lower error rates than b"
P07-1028,N01-1013,0,0.0689186,"of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin’s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created 216 lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: sem"
P07-1028,P97-1003,0,0.0272365,"exicon for English that groups words in semantic classes called frames and lists semantic roles for each frame. The FrameNet 1.3 annotated data comprises 139,439 sentences from the British National Corpus (BNC). For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 5001000, and more than 1000 occurrences. The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus. To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. Our generalization corpus is the BNC. It was parsed using Minipar (Lin, 1993), which is considerably faster than the Collins parser but failed to parse about a third of all sentences. Instantiation used in this paper. Our aim is to compute selectional preferences for semantic roles. So we choose a particular instantiation of the similarity-based model that makes use of the fact that the two-corpora approach allows us to use different notions of “predicate” and “argument” in the primary and generalization corpus. Our primary corpus will consist of manually semantically annotated data,"
P07-1028,J02-3001,0,0.475293,"rics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik’s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin’s (1998) mutual information"
P07-1028,J93-1005,0,0.1281,"e a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik’s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional"
P07-1028,P90-1034,0,0.541944,"Missing"
P07-1028,P93-1016,0,0.0335028,"roles for each frame. The FrameNet 1.3 annotated data comprises 139,439 sentences from the British National Corpus (BNC). For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 5001000, and more than 1000 occurrences. The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus. To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. Our generalization corpus is the BNC. It was parsed using Minipar (Lin, 1993), which is considerably faster than the Collins parser but failed to parse about a third of all sentences. Instantiation used in this paper. Our aim is to compute selectional preferences for semantic roles. So we choose a particular instantiation of the similarity-based model that makes use of the fact that the two-corpora approach allows us to use different notions of “predicate” and “argument” in the primary and generalization corpus. Our primary corpus will consist of manually semantically annotated data, and we will use semantic verb classes as pred2 icates and semantic roles as arguments."
P07-1028,P98-2127,0,0.400216,"Missing"
P07-1028,J03-4004,0,0.389032,"tional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik’s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similari"
P07-1028,P99-1014,0,0.926592,"have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin’s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created 216 lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: semantic role labeling. The argument positions for which we compute sel"
P07-1028,P98-1013,0,\N,Missing
P07-1028,C98-1013,0,\N,Missing
P07-1028,J06-1003,0,\N,Missing
P07-1028,C98-2122,0,\N,Missing
P09-1002,S07-1002,0,0.0639388,"dictionary senses or similarity between uses. In that case, it would be useful to have more extensive datasets with graded annotation, even though this annotation paradigm is more time consuming and thus more expensive than traditional word sense annotation. As a next step, we will automatically cluster the judgments we obtained in the WSsim and Usim experiments to further explore the degree to which the annotation gives rise to sense grouping. We will also use the ratings in both experiments to evaluate automatically induced models of word meaning. The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from OntoNotes. We hope that the Usim dataset will be particularly useful for evaluating methods which relate usages without necessarily producing hard clusters. Also, we will extend the current dataset using more annotators and exploring additional lexicon resources. Acknowledgments. We acknowledge support from the UK Royal Society for a Dorothy Hodkin Fellowship to the second author. We thank Sebastian Pado for many helpful discussions, and Andrew Young for help with the interface"
P09-1002,S07-1009,1,0.878001,"data taken from SemCor and SE-3 so that we can compare the annotations. Although we use WordNet for the annotation, our study is not a study of WordNet per se. We choose WordNet because it is sufficiently fine-grained to examine subtle differences in usage, and because traditionally annotated datasets exist to which we can compare our results. Predefined dictionaries and lexical resources are not the only possibilities for annotating lexical items with meaning. In cross-lingual settings, the actual translations of a word can be taken as the sense labels (Resnik and Yarowsky, 2000). Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. It uses paraphrases for words in context as a way of annotating meaning. The task was proposed following a background of discussions in the WSD community as to the adequacy of predefined word senses. The LEXSUB dataset comprises open class words (nouns, verbs, adjectives and adverbs) with token instances of each word appearing in the context of one sentence taken from the English Internet Corpus (Sharoff, 2006). The methodology can only work where there are paraphrases, so the"
P09-1002,W04-0807,0,0.0587557,"Missing"
P09-1002,D08-1094,1,0.107902,"the full spectrum of ratings, as shown in Figures 1 and 4. This may be because of a graded perception of the similarity of uses as well as senses, or because some uses and senses are very similar. Table 4 shows that for a large number of WSsim items, multiple senses that were not significantly positively correlated got high ratings. This seems to indicate that the ratings we obtained cannot simply be explained by more coarse-grained senses. It may hence be reasonable to pursue computational models of word meaning that are graded, maybe even models that do not rely on dictionary senses at all (Erk and Pado, 2008). Comparison to previous word sense annotation. Our graded WSsim annotations do correlate with traditional “best fitting sense” annotations from SemCor and SE-3; however, if annotators perceive similarity between uses and senses as graded, traditional word sense annotation runs the risk of introducing bias into the annotation. Comparison to lexical substitutions. There is a strong correlation between both Usim and WSsim and the overlap in paraphrases that annotators generated for LEXSUB. This is very encouraging, and especially interesting because LEXSUB annotators freely generated paraphrases"
P09-1002,S07-1006,0,0.142338,"Missing"
P09-1002,N06-2015,0,0.147668,"Missing"
P09-1002,H05-1051,0,0.0559669,"Missing"
P09-1002,J06-4002,0,\N,Missing
P10-2017,W09-0201,0,0.0109048,"one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textu"
P10-2017,P08-1078,0,0.101516,"ll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the 2 Related Work Among distributional models of word, there are some approaches that address polysemy, either by inducing a fix"
P10-2017,W09-0208,1,0.834592,"Missing"
P10-2017,W09-2506,0,0.183666,"mong distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of contexts into senses (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentential context After a fire extinguisher is used, it must always be returned for recharging and its use recorded. fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theor"
P10-2017,W02-0811,0,0.0271141,"settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-words contexts provide complementary information (Florian et al., 2002; Szpektor et al., 2008), and we hope that they can be integrated in a more sophisticated exemplar model. Finally, we will to explore task-based evaluations. Relation extraction and textual entailment in particular are tasks where similar models have been used before (Szpektor et al., 2008). Acknowledgements. This work was supported in part by National Science Foundation grant IIS0845925, and by a Morris Memorial Grant from the New York Community Trust. TDP09 NA 36.5 actTP 39.9 39.6 Table 4: Comparison to other models on two subsets of LexSub (GAP evaluation) datapoints) and Erk and Pad´o (200"
P10-2017,P06-1046,0,0.025763,"ved over using s itself only for verbs (Tab. 3). This suggests the possibility of considering T ’s activated paraphrase candidates as the representation of T in the context s, rather than some vector of T itself, in the spirit of Kintsch (2001). While it is encouraging that the best parameter settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-words contexts provide complementary information (Florian et al., 2002; Szpektor et al., 2008), and we hope that they can be integrated in a more sophisticated exemplar model. Finally, we will to explore task-based evaluations. Relation extraction and textual entailment in particular are tasks where similar models have been used before (Szpektor et al., 2008). Acknowledgements."
P10-2017,J03-4004,0,0.0184317,"oing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor"
P10-2017,P08-1028,0,0.545312,"target. For example, among the 2 Related Work Among distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of contexts into senses (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentential context After a fire extinguisher is used, it must always be returned for recharging and its use recorded. fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity"
P10-2017,J98-1004,0,0.837204,"Missing"
P10-2017,D08-1094,1,\N,Missing
P11-1108,N10-1083,0,0.0790379,"Missing"
P11-1108,W06-2912,0,0.411632,"tion has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s resu"
P11-1108,E99-1016,0,0.279855,"level predictions. This result suggests that improvements to low-level constituent prediction will ultimately lead to further gains in overall constituent parsing. Here, we present such an improvement by using probabilistic finite-state models for phrasal segmentation from raw text. The task for these models is chunking, so we evaluate performance on identification of multiword chunks of all constituent types as well as only noun phrases. Our unsupervised chunkers extend straightforwardly to a cascade that predicts higher levels of constituent structure, similar to the supervised approach of Brants (1999). This forms an overall unsupervised parsing system that outperforms CCL by a wide margin. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics WSJ relieved                was        one for      Ward       Mrs. (a) Chunks: (Mrs. Ward), (for one), and (was relieved) Negra 1 All CTB came from              Research               Cray Chunks NPs Chnk ∩ NPs Chunks NPs Chnk ∩ NPs Chunks NPs Chnk ∩ NPs Ta"
P11-1108,N09-1009,0,0.0417624,"2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency"
P11-1108,P07-3008,0,0.148321,"ation and comparison.1 More importantly, until recently it was the only unsupervised raw text constituent parser to produce results competitive with systems which use gold POS tags (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006) – and the recent improved raw-text parsing results of Reichart and Rappoport (2010) make direct use of CCL without modification. There are other raw-text parsing systems of note, EMILE (Adriaans et al., 2000), ABL (van Zaanen, 2000) and ADIOS (Solan et al., 2005); however, there is little consistent treebank-based evaluation of these models. One study by Cramer (2007) found that none of the three performs particularly well under treebank evaluation. Finally, CCL outperforms most published POS-based models when those models are trained on unsupervised word classes rather than gold POS tags. The only exception we are aware of is H¨anig’s (2010) unsuParse+, which outperforms CCL on Negra, though this is shown only for sentences with ten or fewer words. Phrasal punctuation. Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation – punctuation symbols tha"
P11-1108,P03-1066,0,0.0301646,"course of EM. These correctly predicted constituents are not counted as such in the constituent chunking or base NP evaluations, but they factor directly into improved accuracy when this model is part of a cascade. 7 Related work Our task is the unsupervised analogue of chunking (Abney, 1991), popularized by the 1999 and 2000 Conference on Natural Language Learning shared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compare context-free parsers with finite-state partial parsing methods. They find that full parsing maintains a number of benefits, in spite of the greater training time required: they can train on less data more effectively"
P11-1108,W10-2901,0,0.116113,"Missing"
P11-1108,N09-1012,0,0.153363,"Missing"
P11-1108,H05-1099,0,0.0207212,"hared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compare context-free parsers with finite-state partial parsing methods. They find that full parsing maintains a number of benefits, in spite of the greater training time required: they can train on less data more effectively than chunkers, and are more robust to shifts in textual domain. Brants (1999) reports a supervised cascaded chunking strategy for parsing which is strikingly similar to the methods proposed here. In both, Markov models are used in a cascade to predict hierarchical constituent structure; and in both, the parameters for the model at each level are estimated independently. T"
P11-1108,P02-1017,0,0.849996,"ing, or simply unsupervised chunking, we mean the segmentation of raw text into (non-overlapping) multiword constituents. The models are intended to cap- CCL benchmark. We use Seginer’s CCL as a ture local constituent structure – the lower branches benchmark for several reasons. First, there is a of a constituent tree. For this reason we evaluate free/open-source implementation facilitating exper1078 imental replication and comparison.1 More importantly, until recently it was the only unsupervised raw text constituent parser to produce results competitive with systems which use gold POS tags (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006) – and the recent improved raw-text parsing results of Reichart and Rappoport (2010) make direct use of CCL without modification. There are other raw-text parsing systems of note, EMILE (Adriaans et al., 2000), ABL (van Zaanen, 2000) and ADIOS (Solan et al., 2005); however, there is little consistent treebank-based evaluation of these models. One study by Cramer (2007) found that none of the three performs particularly well under treebank evaluation. Finally, CCL outperforms most published POS-based models when those models are trained on unsupervised word"
P11-1108,P04-1061,0,0.873424,"tures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL"
P11-1108,J93-2004,0,0.0461076,"lass of predictions for both models. Two of the top classes of errors, MD VB and TO VB, represent verb phrase constituents, which are often predicted by the HMM chunker, but not by the PRLG. The class represented by NNP NNP corresponds with the tendency of the HMM chunker to split long proper names: for example, it systematically splits new york stock exchange into two chunks, (new york) (stock exchange), whereas the PRLG chunker predicts a single four-word chunk. The most interesting class is DT JJ, which represents the difficulty the HMM chunker has at dis3 For the Penn Treebank tagset, see Marcus et al. (1993). 1 Start with raw text: there is no asbestos in our products now 2 Apply chunking model: there (is no asbestos) in (our products) now Text : Mr. Vinken is chairman of Elsevier N.V. Level 1 : Level 2 : 3 Create pseudowords: there is in our now 4 Apply chunking model (and repeat 1–4 etc.): (there is ) (in our ) now 5 Unwind and create a tree: Level 3 : Mr. Vinken Mr. Vinken is chairman of of 1 is chairman 1 Elsevier N.V. in is no asbestos our tinguishing determiner-adjective from determinernoun pairs. The PRLG chunker systematically gets DT JJ NN trigrams as chunks. The greater context provided"
P11-1108,D10-1020,1,0.83575,"ish, German, and Chinese. Like CCL, our models operate from raw (albeit segmented) text, and like it our models decode very quickly; however, unlike CCL, our models are based on standard and well-understood computational linguistics technologies (hidden Markov models and related formalisms), and may benefit from new research into these core technologies. For instance, our models may be improved by the application of (unsupervised) discriminative learning techniques with features (Berg-Kirkpatrick et al., 2010); or by incorporating topic models and document information (Griffiths et al., 2005; Moon et al., 2010). UPPARSE, the software used for the experiments in this paper, is available under an open-source license to facilitate replication and extensions.6 Acknowledgments. This material is based upon work supported in part by the U. S. Army Research Laboratory and the U. S. Army Research Office under grant number W911NF-10-1-0533. Support for the first author was also provided by Mike Hogg Endowment Fellowship, the Office of Graduate Studies at The University of Texas at Austin. This paper benefited from discussion in the Natural Language Learning reading group at UT Austin, especially from Collin B"
P11-1108,P92-1017,0,0.745108,"relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Segine"
P11-1108,W95-0107,0,0.0654013,"often mark phrasal boundaries within a sentence. These are used in two ways: i) they impose a hard constraint on constituent spans, in that no constituent (other than sentence root) may extend over a punctuation symbol, and ii) they contribute to the model, specifically in terms of the statistics of words seen adjacent to a phrasal boundary. We follow this convention and use the following set: . ? ! ; , -- ◦ � The last two are ideographic full-stop and comma.2 4 Unsupervised partial parsing We learn partial parsers as constrained sequence models over tags encoding local constituent structure (Ramshaw and Marcus, 1995). A simple tagset is unlabeled BIO, which is familiar from supervised chunking and named-entity recognition: the tag B 1 http://www.seggu.net/ccl This set is essentially that of Seginer (2007). While it is clear from our analysis of CCL that it does make use of phrasal punctuation in Chinese, we are not certain whether ideographic comma is included. 2 1079 denotes the beginning of a chunk, I denotes membership in a chunk and O denotes exclusion from any chunk. In addition we use the tag STOP for sentence boundaries and phrasal punctuation. HMMs and PRLGs. The models we use for unsupervised par"
P11-1108,D10-1067,0,0.153666,"hich are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s results, they do so by selecting training sets to best match the particular test sentences—CCL itself is used without modification. Ponvert et al. (2010) explore an alternative strategy of unsupervised partial parsing: directly predicting low-level constituents based solely on word co-occurrence frequencies. Essentially, this means segmenting raw text into multiword constituents. In that paper, we show—somewhat surprisingly—that CCL’s performance is mostly dependent on its effectiveness at identifying low-level constituents. In fact, simply extracting non-hierarchical mul"
P11-1108,J10-1001,0,0.0484557,"onstituent structure; and in both, the parameters for the model at each level are estimated independently. There are major differences, though: the models here are learned from raw text without tree annotations, using EM to train parameters; Brants’ cascaded Markov models use supervised maximum likelihood estimation. Secondly, between the separate levels of the cascade, we collapse constituents into symbols which are treated as tokens in subsequent chunking levels; the Markov models in the higher cascade levels in Brants’ work actually emit constituent structure. A related approach is that of Schuler et al. (2010), who report a supervised hierarchical hidden Markov model which uses a right-corner transform. This allows the model to predict more complicated trees with fewer levels than in Brants’ work or this paper. 1085 8 Conclusion In this paper we have introduced a new subproblem of unsupervised parsing: unsupervised partial parsing, or unsupervised chunking. We have proposed a model for unsupervised chunking from raw text that is based on standard probabilistic finitestate methods. This model produces better local constituent predictions than the current best unsupervised parser, CCL, across dataset"
P11-1108,P07-1049,0,0.581647,", 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s results, they do so by selecting training sets to best match the particular test sentences—CCL itself is used without modification. Ponvert et"
P11-1108,P04-1062,0,0.0804928,"c Rec F 53.6 50.0 51.7 48.2 43.6 45.8 60.0 49.4 54.2 German / Negra Prec Rec F 33.4 32.6 33.0 30.8 50.3 38.2 38.8 47.4 42.7 Chinese / CTB Prec Rec F 37.0 21.6 27.3 43.0 29.8 35.2 50.4 32.8 39.8 Table 6: Unlabeled PARSEVAL scores for cascaded models. source implementation by F. Luque,4 we compare on WSJ and Negra to the constituent context model (CCM) of Klein and Manning (2002). CCM learns to predict a set of brackets over a string (in practice, a string of POS tags) by jointly estimating constituent and distituent strings and contexts using an iterative EM-like procedure (though, as noted by Smith and Eisner (2004), CCM is deficient as a generative model). Note that this comparison is methodologically problematic in two respects. On the one hand, CCM is evaluated using gold standard POS sequences as input, so it receives a major source of supervision not available to the other models. On the other hand, the other models use punctuation as an indicator of constituent boundaries, but all punctuation is dropped from the input to CCM. Also, note that CCM performs better when trained on short sentences, so here CCM is trained only on the 10-wordor-less subsets of the training datasets.5 The results from the"
P11-1108,N10-1116,0,0.0312993,"are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is al"
P11-1108,W00-0726,0,0.173471,"whether the longer chunks predicted correspond to actual constituents or not. Fig. 6c shows that the PRLG, when constrained by phrasal punctuation, does continue to improve its constituent prediction accuracy over the course of EM. These correctly predicted constituents are not counted as such in the constituent chunking or base NP evaluations, but they factor directly into improved accuracy when this model is part of a cascade. 7 Related work Our task is the unsupervised analogue of chunking (Abney, 1991), popularized by the 1999 and 2000 Conference on Natural Language Learning shared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compa"
P11-1108,C00-2139,0,0.0602638,"Missing"
P14-1114,S12-1051,0,0.348977,"ine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach. 1 Introduction When will people say that two sentences are similar? This question is at the heart of the Semantic Textual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approac"
P14-1114,S12-1059,0,0.0117816,"Missing"
P14-1114,D10-1115,0,0.0390138,"ces into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical represe"
P14-1114,S13-1002,1,0.0702222,"tual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1 Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similari"
P14-1114,P11-1062,0,0.0223414,"could learn that the type of an object determined by a noun should be weighted more than a property specified by an adjective. As a result, “black dog” would be appropriately judged more similar to “white dog” than to “black cat.” One of the advantages of using a probabilistic logic is that additional sources of knowledge can easily be incorporated by adding additional soft inference rules. To complement the soft inference rules capturing distributional lexical and phrasal similarities, PSL rules could be added that encode explicit paraphrase rules, such as those mined from monolingual text (Berant et al., 2011) or multi-lingual parallel text (Ganitkevitch et al., 2013). This paper has focused on STS; however, as shown by Beltagy et al. (2013), probabilistic logic is also an effective approach to recognizing textual entailment (RTE). By using the appropriate functions to combine truth values for various logical connectives, PSL could also be adapted for RTE. Although we have shown that PSL outperforms MLNs on STS, we hypothesize that MLNs may still be a better approach for RTE. However, it would be good to experimentally confirm this intuition. In any case, the high computational complexity of MLN in"
P14-1114,D13-1160,0,0.0265957,"p and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best p"
P14-1114,W08-2222,0,0.0929732,"n entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similarity score for all pairs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, wher"
P14-1114,P11-1020,0,0.0187176,"rkov logic instead of PSL for probabilistic inference. MLN inference is very slow in some cases, so we use a 10 minute timeout. When MLN times out, it backs off to a simpler sentence representation as explained in section 2.6. Evaluation This section evaluates the performance of PSL on the STS task. 4.1 Datasets We evaluate our system on three STS datasets. • PSL: Our proposed PSL system for combining logical and distributional information. • msr-vid: Microsoft Video Paraphrase Corpus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logical forms but does not use distributional information on lexical or phrasal similarity. It tests the impact of the probabilistic logic only • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pai"
P14-1114,P04-1014,0,0.0854417,"rs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, where w = → − − f (sim(→ a , b )). Both a and b can be words or phrases. Phrases are defined in terms of Boxer’s output. A phrase is more than one unary atom sharing the same variable like “a litt"
P14-1114,C04-1051,0,0.0120882,"llected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logical forms but does not use distributional information on lexical or phrasal similarity. It tests the impact of the probabilistic logic only • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors empl"
P14-1114,N13-1092,0,0.0538761,"Missing"
P14-1114,W11-0112,1,0.676139,"sets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extended this framework by generating distributional inference rules from phrase similarity and tailoring the system to the STS task. STS is treated as computing the probability of two textual entailments T |= H and H |= T , where T and H are the two sentences who"
P14-1114,D11-1129,0,0.0746619,"utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire kno"
P14-1114,D13-1161,0,0.0148238,"ition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously"
P14-1114,Q13-1015,0,0.051073,"system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Two of the datasets we use for evaluation are from the 2012 competition. We did not utilize the new datasets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extende"
P14-1114,P08-1028,0,0.735442,"f the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1 Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarity ratings at the word and phrase level. Sentence similarity is then modelled as mutual entailme"
P14-1114,S13-1001,0,0.374618,"S. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (LanMarkov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic"
P19-1062,D14-1162,0,0.0846982,"dj ) (1) Models We present two models: one for text classification, and one for sentence ordering. Both are based on the L&L model, with a design change to cause stronger percolation of information up the tree (we also experiment without this change). ci = Text classification The left-hand side of Figure 1 presents an overview of the model: the model operates first at the sentence-level to create sentence representations, and then at the document-level to create a document representation from the previously created sentence representations. In more detail, the model composes GloVe embeddings (Pennington et al., 2014) into a sentence representation using structured attention (from which a tree can be derived), then sentence representations into a single document representation for class predicn X aik ek (3) k=1 where aik is the probability that k is the child of i, and ek is the semantic vector of the child. The children vectors are then passed through a non-linear function, resulting in the updated semantic vector e0i for parent node i. e0i = tanh(Wr [ei , ci ]) 3 (4) https://github.com/nlpyang/structured We found similar results for using both parents and children as well as using parents only. 4 647 Yel"
P19-1062,D13-1158,0,0.0651747,"Missing"
P19-1062,prasad-etal-2008-penn,0,0.0301692,"ically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available at https://github.com/ elisaF/structured 646 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 646–653 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics document class document-level max-pooling structured+compose attention sentences sentence structured+compose attention word"
P19-1062,P17-1092,0,0.673652,"ations, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.1 1 Introduction Discourse describes how a document is organized, and how discourse units are rhetorically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available"
P19-1062,D18-1548,0,0.0479993,"Missing"
P19-1062,P15-1098,0,0.0174817,"e, and the model from the timestep with highest development performance is chosen. We report accuracies on the test set, and tree analyses on the development set. Our implementation is built on the L&L released implementation, with changes as noted in Section 3. Preprocessing and training details are in Appendix A. We evaluate the model on four text classification tasks and one sentence order discrimination task. 4.1 Settings Datasets Details and statistics are included in Appendix A.5 Yelp (in L&L, 5-way classification) comprises customer reviews from the Yelp Dataset Challenge (collected by Tang et al. (2015)). Each review is labeled with a 1 to 5 rating (least to most positive). Debates (in L&L, binary classification) are transcribed debates on Congressional bills from the U.S. House of Representatives (compiled by Thomas et al. (2006), preprocessed by Yogatama 4.3 Results We report accuracy (as in prior work) in Table 1, and perform two ablations: removing the structured attention at the document level, and removing it at both document and sentence levels. Additionally, we run experiments on the original code 5 Of the document-level datasets used in L&L (SNLI was sentence-level), we omit IMDB an"
P19-1062,J93-2004,0,\N,Missing
P19-1062,W06-1639,0,\N,Missing
P19-1062,W01-1605,0,\N,Missing
P19-1062,P14-1074,0,\N,Missing
P19-1062,J08-1001,0,\N,Missing
P19-1062,Q13-1028,0,\N,Missing
P19-1062,J17-4001,0,\N,Missing
P19-1062,N19-1173,0,\N,Missing
P19-1062,Q18-1005,0,\N,Missing
P19-1062,D07-1015,0,\N,Missing
S07-1018,fillmore-etal-2004-framenet,1,0.922936,"0,000 word senses (lexical units). It also contains roughly 150,000 annotation sets, of which 139,000 are lexicographic examples, with each sentence annotated for a single predicator. The remainder are from full-text annotation in which each sentence is annotated for all predicators; 1,700 sentences are annotated in the full-text portion of the database, accounting for roughly 11,700 annotation sets, or 6.8 predicators (=annotation sets) per sentence. Nearly all of the frames are connected into a single graph by frame-to-frame relations, almost all of which have associated FE-to-FE relations (Fillmore et al., 2004a) 2.1 Frame Semantics of texts The ultimate goal is to represent the lexical semantics of all the sentences in a text, based on the relations between predicators and their dependents, including both phrases and clauses, which may, in turn, include other predicators; although this has been a long-standing goal of FN (Fillmore and Baker, 2001), automatic means of doing this are only now becoming available. 100 Consider a sentence from one of the testing texts: (1) This geography is important in understanding Dublin. In the frame semantic analysis of this sentence, there are two predicators whic"
S07-1018,W04-1906,0,0.0558664,"Missing"
S07-1018,W04-0803,0,0.128768,"on The testing data for this task turned out to be especially challenging with regard to new frames, since, in an effort to annotate especially thoroughly, almost 40 new frames were created in the process of annotating these three specific passages. One result of this was that the test passages had more unseen frames than a random unseen passage, which probably lowered the recall on frames. It appears that this was not entirely compensated by giving partial credit for related frames. This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). Unlike that task, the testing data was previously unseen, participants had to determine the correct frames as a first step, and participants also had to determine FE boundaries, which were given in the Senseval-3. A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al., 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. Thus, the FN SRL results are translatable fa"
S07-1018,N04-1030,0,0.0394658,"frames than a random unseen passage, which probably lowered the recall on frames. It appears that this was not entirely compensated by giving partial credit for related frames. This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). Unlike that task, the testing data was previously unseen, participants had to determine the correct frames as a first step, and participants also had to determine FE boundaries, which were given in the Senseval-3. A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al., 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. Thus, the FN SRL results are translatable fairly directly into formal representations which can be used for reasoning, question answering, etc. (Scheffczyk et al., 2006; Frank and Semecky, 2004; Sinha and Narayanan, 2005). Despite the problems with recall, the participants have expressed a determination to work to improve these results, and the FN staff are"
S07-1018,E03-1037,0,0.0534733,"Missing"
S07-1018,W03-0410,0,0.0374211,"Missing"
S13-1002,S12-1051,0,0.0454478,"fidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic amb"
S13-1002,S12-1059,0,0.0221299,"Missing"
S13-1002,D10-1115,0,0.448308,"the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly"
S13-1002,P11-1062,0,0.0850071,"e make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1 , p2 ), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules “on the fly”, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1 , c2 ) where c1 ∈ S1 and c2 ∈ S2 , a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-variable predicate"
S13-1002,H05-1079,0,0.851584,"omputing the probability of a query literal given a set of weighted clauses as background knowledge and evidence. Tasks: RTE and STS Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, implies another, the hypothesis. Consider (1) below. (1) p: Oracle had fought to keep the forms from being released h: Oracle released a confidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity,"
S13-1002,W08-2222,0,0.249599,"nd Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our app"
S13-1002,W11-2504,0,0.0394132,"Missing"
S13-1002,P11-1020,0,0.0148009,"demonstrates the advantage of using a model that operationalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 17 5 5.1 Task 2: Semantic Textual Similarity Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video. The gold standard score is the average of the Turkers’ annotations. In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions. We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well. In addition, the STS 2012 data included sentences from the MSR Paraphra"
S13-1002,P04-1014,0,0.100456,"(Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference. An MLN consists of a s"
S13-1002,J12-1002,0,0.00930197,"s very sensitive to parsing errors, and the C&C parser, on which Boxer is based, produces many errors on this dataset, even for simple sentences. When the C&C CCG parse is wrong, the resulting logical form is wrong, and the resulting similarity score is greatly affected. Dropping variable binding makes the systems more robust to parsing errors. Second, in contrast to RTE, the STS dataset does not really test the important role of syntax and logical form in deter18 Future Work rectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore “coarser” logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sente"
S13-1002,D08-1094,1,0.834937,"Missing"
S13-1002,W11-0112,1,0.929827,"970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional similarity rules, and only ev"
S13-1002,D11-1129,0,0.45908,"based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional simil"
S13-1002,C08-1043,0,0.0498596,"Missing"
S13-1002,P08-1028,0,0.223342,"to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we d"
S13-1002,P12-1052,0,0.0289542,"vely combining logical representations with distributional information automatically acquired from text. In this section, we discuss some of limitations of the current work and directions for future research. As noted before, parse errors are currently a significant problem. We use Boxer to obtain a logical representation for a sentence, which in turn relies on the C&C parser. Unfortunately, C&C misparses many sentences, which leads to inaccurate logical forms. To reduce the impact of misparsing, we plan to use a version of C&C that can produce the top-n parses together with parse re-ranking (Ng and Curran, 2012). As an alternative to re-ranking, one could obtain logical forms for each of the topn parses, and create an MLN that integrates all of them (together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributi"
S13-1002,J98-1004,0,0.284282,"Missing"
S13-1002,C08-1107,0,0.309922,"y describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reasonable results on both tasks"
S13-1002,P10-1097,0,0.040878,"Missing"
S14-2141,S12-1051,0,0.0737721,"Explanation) finds the overall interpretation with the maximum probability given a set of evidence. This optimization problem is a second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or is not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 3.1 Distributional Representation Approach Logical Representation The first component in the system is Boxer (Bos, 2008), which maps the input sentences into logical 797 phrases using vector addition across the component predicates. We also tried computing phrase vectors using component-wise vector multiplication (Mitchell and Lapata, 2010), but found it performed marginally worse than addition. 3.3 A general problem w"
S14-2141,W11-2501,0,0.029055,"n inference rule: a → b |w, where the rule weight w is → − − a function of sim(→ a , b ), and sim is a similarity → − − measure of the distributional vectors → a , b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference → − → − ( a − b ) as features in a logistic regression classifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relation. The model is trained using the noun-noun subset of the BLESS data set (Baroni and Lenci, 2011). The final similarity weight is given by the model’s estimated probability that the word relationship is either hypernymy → − − or meronomy: asym(→ a , b ) = P (hyper(a, b))+ P (mero(a, b)). Distributional representations for words are derived by counting co-occurrences in the ukWaC, WaCkypedia, BNC and Gigaword corpora. We use the 2000 most frequent content words as basis dimensions, and count co-occurrences within a two word context window. The vector space is weighted using Positive Pointwise Mutual Information. Phrases are defined in terms of Boxer’s output to be more than one unary atom"
S14-2141,D10-1115,0,0.0521769,"d” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on dist"
S14-2141,S13-1002,1,0.910574,"Missing"
S14-2141,P14-1114,1,0.903726,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W14-2402,1,0.661264,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W08-2222,0,0.399373,"troduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactl"
S14-2141,N13-1092,0,0.182293,"Missing"
S14-2141,S13-1001,0,0.0323607,"rated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Box"
S14-2141,C14-1097,1,0.904999,"rvation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system b"
S14-2141,S12-1012,0,0.024692,"2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical i"
S14-2141,S14-2001,0,0.0930159,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,marelli-etal-2014-sick,0,0.0679359,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,P08-1028,0,0.0432548,", and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability"
W04-0817,W04-2413,1,0.86329,"Missing"
W04-0817,W98-1505,0,0.0255768,"med by off-the-shelf statistical tools for data processing and modelling. After listing our data preparation steps (Sec. 2) and features (Sec. 3), we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results. 2 Detlef Prescher University of Amsterdam Amsterdam, The Netherlands prescher@science.uva.nl Data and Instances Parsing. To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic contextfree parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998). We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes. The resulting nodes form the instances of our classification. We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads. NP (C OGNIZER) VP (NONE) Peter V (NONE) does VP (NONE) not know NP (C ONTENT) the answer Figure 1: Example parse tree with role labels Semantic clustering. We used clustering to generalise over possible fillers of roles. In a first model, we derived a probability distribution  for pairs     , where  is a target:role co"
W04-0817,W03-1007,0,0.12709,"Missing"
W04-0817,J02-3001,0,0.465326,"Missing"
W04-0817,W02-2018,0,0.0410263,"l, where the probability of a class  given an feature vector ) ( is defined as   & )( $+, * -.!/1032 0 43576 !8 , - where is a normalisation constant, - 9  )   the ) value of feature for class  , and : the weight assigned to 9 - . The model is trained by optimising the weights : subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. Maximum Entropy (Maxent) models have been successfully applied to semantic role labelling (Fleischman et al., 2003). We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. Memory-based Learning. Our second learner implements an instance of a memory-based learning (MBL) algorithm, namely the ; -nearest neighbour algorithm. This algorithm classifies test instances by assigning them the label of the most similar examples from the training set. Its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme. We used the implementation provided by TiMBL (Daelemans et al., 2003) with the default parameters, i.e. ; =1 and the weighted overlap similarity metric with gain"
W04-0817,C00-2094,1,0.853748,"t:role pair =@> for a cluster  as follows: <  =@>1$ A BDCFEHG A 2I4 6 JLK 8 MIN 9 OP   & O where <   =?>1 is the total frequency of all head lemmas O that have been seen with =@> , weighted by the class-membership probability of O in  . This appropriateness measure <  =@>1 is built on top of the class-based frequencies 9  O   & O rather than on the frequencies 9  O or the class-membership probabilities   & OP in isolation: For some tasks the combination of lexical and semantic information has been shown to outperform each of the single information sources (Prescher et al., 2000). Our similarity notion is now formalised as follows: With a threshold Q as a parameter, two frame elements =@> , =@>R count as similar if for some class  , <   =@>S $T Q and <   =@>  $T Q . In the syntactic clustering model, a role filler was described as a combination of the path from instance to target, the instance’s preposition, and the target voice. The appropriateness of a target:role pair is defined as for the above model. For time reasons, only verbal targets were considered. Figure 2 shows excerpts of two “syntactic” clusters in the form of target:frame.role members. Group 6"
W04-2413,W04-2412,0,0.0410263,"that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set. 1 Introduction This paper describes a statistical approach to semantic role labelling addressing the CoNLL shared task 2004, which is based on the the current release of the English PropBank data (Kingsbury et al., 2002). For further details of the task, see (Carreras and Màrquez, 2004). We address the main challenge of the task, the absence of deep syntactic information, with three main ideas: Proper constituents being unavailable, we use chunk sequences as instances for classification. The classification is performed by a maximum entropy model, which can integrate features from heterogeneous data sources. We model the fit between verb and argument candidate by clusters induced with EM on the training data, which we use as features during classification. Sections 2 through 4 describe the systems’ architecture. First, we compute chunk sequences for all sentences (Sec. 2). Th"
W04-2413,W03-1007,0,0.0725491,"NNP POS VBG NN VBZ VBG PRP TO NN NNS [NP ] [NP ] [VP ] [NP] [VP ] [NP ] [S ] Figure 1: Part of a sentence with part of speech, chunk and clause information Frequency in training data 25000 LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). Sequence frequencies Divider frequencies 20000 15000 10000 5000 0 3.2 Figure 2: Frequency distribution for the 20 most frequent sequences and dividers in the training data separated from it by e.g. a typical A1 sequence. Generalised divider chunk sequences separating actual arguments and targets in the training set show a Zipfian distribution similar to the chunk sequences (see Fig. 2). As instances to be classified, we consider all sequences whose generalised sequence and divider each appear at least 10 times for an argument in the training corpus, and whose generalised sequence and divider"
W04-2413,W02-2018,0,0.0352341,"VP_PP_NP_NP (in words: a bonus in the form of charitable donations made from an employer ’s treasury). The chunk sequence approach also allows us to consider the divider chunk sequences that separate arguments and targets. For example, A0s are usually divided from the target by the empty divider, while A2 arguments are Britain ’s manufacturing industry is transforming itself to boost exports NNP POS VBG NN VBZ VBG PRP TO NN NNS [NP ] [NP ] [VP ] [NP] [VP ] [NP ] [S ] Figure 1: Part of a sentence with part of speech, chunk and clause information Frequency in training data 25000 LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). Sequence frequencies Divider frequencies 20000 15000 10000 5000 0 3.2 Figure 2: Frequency distribution for the 20 most frequent sequences and dividers in the training data separated from it by"
W04-2413,P99-1014,1,0.757592,"get lemma is passive. Divider Features. These are shallow and higher-level features related to the divider sequences: the divider itself, its superchunk, and we state whether, judging by the divider, the sequence is an argument. A similar feature judges this by the combination of divider and sequence. Features based on EM-Based Clustering. We use EM-based clustering to measure the fit between a target verb, an argument position of the verb, and the head lemma (or head named entity) of a sequence. EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al., 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. In our application, we aim at deriving a probability distribution +, on verb-argument pairs + from the training data. Using the key idea that + is conditioned on an unobserved class .-0/ , we define the probability of a pair +1 2'+435(+67 -98 3;: 8 6 as: <'+ = >   (+ 2> <  ''+   #@?A @# ?A    B+43   ' '+6   >  #@?A The last line is warranted by the assumption that +C3 and +6 are independent and are only"
W07-1528,U03-1008,0,0.0231727,"and (4) a free translation of each sentence. Another characteristic of language documentation projects is the tentative nature of many analyses, given that linguistic analysis is often occurring in tandem with the annotation process, sometimes for the first time in the recorded history of the language. Furthermore, language documentation projects require long-term accessibility of the collected language data as well as easy accessibility to community members as well as to linguists. In this paper we propose a new XML format for representing IGT, which we call IGT-XML. We build on the model of Hughes et al (2003) (the BHB model from now on), who first proposed using the IGT structure directly as a basis for an XML format. While their format shows closely integrated annotation layers using XML embedding, our model has a more loosely coupled and flexible representation of different annotation layers, to accommodate (a) selective manual reannotation of individual layers, and (b) the (semi-)automatic extension of annotation, without the format posing an a priori restriction on the annotation levels that can be added. The IGTXML representation is thus a first step toward partial automation of the productio"
W07-1528,kuhn-mateo-toledo-2004-applying,0,0.0211633,"3, c Prague, June 2007. 2007 Association for Computational Linguistics with language data. Plan of the paper. After discussing interlinearized glosses in Section 2, we show the BHB model and corresponding XML format in Section 3. Section 4 presents the IGT-XML format that we propose. Section 5 demonstrates the applicability of IGT-XML to data from different languages and different documentation projects, and Section 6 concludes. 2 Interlinearized glossed text IGT is a way of encoding linguistic data commonly used to present linguistic examples. The example below is a segment of IGT taken from Kuhn and Mateo-Toledo (2004). The language is Q’anjob’al, a Mayan language of Guatemala. (1) Maxab’ ek’elteq ix unin yet sq’inib’alil tu. (2) max-ab’ ek’-el-teq ix unin y-et COM-EV pass-DIR-DIR CL child E3S-when s-q’inib’-al-il tu E3S-early-ABS-ABS DEM ’The child came out early that morning (they say)’ 2 The format of the IGT in this example is typical of the presentation of individual examples in the linguistics literature. The raw, unannotated text (1) is associated with three layers of annotation, shown in (2). The first annotation layer shows the same text with each word segmented into its constituent morphemes. The"
W07-1528,mengel-lezius-2000-xml,0,0.0305113,"Missing"
W07-1528,W01-1506,0,\N,Missing
W08-0201,W08-0208,1,0.843127,"a range of external tools and have adapted tools from our own research for various aspects of our courses. In this section, we describe our experience using these as part of our courses. We have used Python as the common language in our courses. We are pleased with it: it is straightforward for beginning programmers to learn, its interactive prompt facilitates in-class instruction, it is text-processing friendly, and it is useful for gluing together other (e.g., Java and C++) applications. 3.1 External tools and resources NLTK. We use the Natural Language Toolkit (NLTK) (Loper and Bird, 2002; Bird et al., 2008) in both undergraduate and graduate courses for in-class demos, tutorials, and homework assignments. We use the toolkit and tutorials for several course components, including introductory Python programming, text processing, rule-based part-of-speech tagging and chunking, and grammars and parsing. NLTK is ideal for both novice and advanced programmers. The tutorials and extensive documentation provide novices with plenty of support outside of the classroom, and the toolkit is powerful enough to give plenty of room for advanced students to play. The demos are also very useful in classes and ser"
W08-0201,W05-0103,0,0.199936,"was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs and experiment with their behavior so easily. Language and Computers. We had fortunately already planned the first replacement course: Language and Computers, based on the course designed at the Department of Linguistics at the Ohio State University (Brew et al., 2005). This course introduces computational linguistics to a general audience and is ideal for students who want exposure to computational methods without having to learn to program. We designed and taught it jointly, and added several new aspects to the course. Whereas OSU’s course fulfills a Mathematical and Logical Analysis requirement, our course fulfills a Science requirement for liberal arts majors. These requirements were met by course content that requires understanding and thinking about formal methods. The topics we added to our course were question answering, cryptography,2 and world kno"
W08-0201,W02-0102,0,0.0225202,"cs students who may have to deal with formalism wars in their own linguistic subfield. Also, XFST allows one to use lenient composition to encode Optimality Theory constraints and in so doing show interesting and direct contrasts and comparisons between paper-and-pencil linguistics and rigorous computational implementations. As with other implementation-oriented activities in our classes, we created a wiki page for XFST tutorials.3 These were adapted and expanded from Xerox PARC materials and Mark Gawron’s examples. Eisner’s HMM Materials. Simply put: the spreadsheet designed by Jason Eisner (Eisner, 2002) for teaching hidden Markov models is fantastic. We used that plus Eisner’s HMM homework assignment for Computational Linguistics II in Fall 2007. The spreadsheet is great for interactive classroom exploration of HMMs—students were very engaged. The homework allows students to implement an HMM from scratch, giving enough detail to alleviate much of the needless frustration that could occur with this task while ensuring that students need to put in significant effort and understand the concepts in order to make it work. It also helps that the new edition of Jurafsky and Martin’s textbook discus"
W08-0201,J00-4006,0,0.0049271,"ational linguistics, but that they would not actually have to do computational linguistics. Many stayed with it, but there were still others who could have gone much further if it had not been necessary to slow down to cover basic material like for loops. Note that several linguistics majors were among the compationally savvy students. In fairness to the students who struggled, it was certainly ill-advised to ask students with no previous background to learn Python and XFST in a single semester. One of the key points of confusion was regular expression syntax. The syntax used in the textbook (Jurafsky and Martin, 2000) transfers easily to regular expressions in Python, but is radically different from that of XFST. For students who had never coded anything in their life, this proved extremely frustrating. On the other hand, for computationally savvy students, XFST was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs an"
W08-0201,W02-0109,0,0.440661,"rse with the R statistics package. It was used for statistical analyses: students loved it because they could produce meaningful results immediately and visualize them. The course includes only a very short two-session introduction to working with R. We were worried that this would overtax students because R is its own programming language. But interestingly they had no problems with learning this second programming language (after Python). This is particularly striking as most of the students had no programming experience prior to the class. We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. But as it, too, offers visualization and rapid access to meaningful results, we intend to use it in the future. In particular, the NLTK allows very easy access to Toolbox data (Robinson et al., 2007), which we feel will greatly improve the utility and appeal of the course for the significant number of documentary linguistics students in the department. Seminars. We also offer several seminars in our areas of interest. These include Categorial Grammar, Computational Syntax, and Lexical Acquisition. These courses have attracted “noncomputational” linguistics st"
W08-0201,D07-1041,1,0.830627,"e. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these skills and needs is the core of this course, which covers corpus formats (XML,"
W08-0201,P07-3002,0,0.0159617,"on for their project, rather than rushing everything in last minute. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these sk"
W08-0201,P06-4018,0,\N,Missing
W09-0208,D08-1007,0,0.0048007,"get + object selpref target + subject selpref 0.005 0.022 0.024 mean sim(val)−sim(inval) 0.026 0.028 target + object selpref target + subject selpref 0.020 mean sim(val)−sim(inval) 0.025 Figure 4: Scatterplot of &quot;out of ten&quot; accuracy against model discriminativity between valid and invalid paraphrases. Left: L EX S UB -PARA, right: S EM C OR -PARA. 0 5 10 15 20 25 30 0 5 exponent 10 15 20 25 30 exponent Figure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for different reweighting values. Left: L EX S UB -PARA, right: S EM C OR -PARA. mulations (Bergsma et al., 2008), or by exemplar-based models that are able to deal better with the ambiguity present in the preferences of very general words. Another important topic for further research is the computation of token vectors that incorporate more than one context word. The current results we obtain for “both ” are promising but limited; it appears that the successful integration of multiple context words requires strategies that go beyond simplistic addition or intersection of observed contexts. in the datasets. We describe an auxiliary quantity, discriminativity, that measures the ability of the model’s pred"
W09-0208,D08-1021,0,0.0131733,"Missing"
W09-0208,P07-1028,1,0.0724908,"well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive. Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects. This"
W09-0208,J02-3001,0,0.00279255,"ivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a"
W09-0208,D07-1042,1,0.418701,"lished (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive. Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects. This appears to contradic"
W09-0208,J93-1005,0,0.176233,"l of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by"
W09-0208,J98-1004,0,0.362466,"Missing"
W09-0208,P93-1016,0,0.0784823,"an a joint syntactic context preference function because (a) this separation models the conceptual difference between predicates and arguments, and (b) it allows for a simpler, more elegant formulation of the computation of meaning in context in Eq. 3. Vector space. We use a dependency-based vector space that counts a target word and a context word 59 as co-occurring in a sentence if they are connected by an “informative” path in the dependency graph for the sentence.2 We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). It uses raw co-occurrence counts and 2000 dimensions. Next, we test component-wise multiplication (mult). This operation is more difficult to interpret in terms of vector space, since it does not correspond to the standard inner or outer vector products. The most straightforward interpretation is to reinterpret the second vector as a diagonal matrix, i.e., as a linear transformation of the first vector. Large entries in the second vector increase the weight of the corresponding contexts; small entries decrease it. Mitchell and Lapata (2008) found this method to yield the best results. The th"
W09-0208,S07-1009,0,0.237709,"nce not through a sense label, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Structured Vector Space (SVS) (Erk and Padó, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-pract"
W09-0208,W06-2503,0,0.0145521,"w occurrences. One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most succe"
W09-0208,P04-1003,0,0.0246093,"token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measu"
W09-0208,W03-2408,0,0.0281447,"bj subj both 0.5 21.7 20.6 21.1 22.6 21.1 24.5 20.9 20.1 20.1 1 20.7 20.1 20.3 24.8 23.9 24.5 19.5 19.6 19.8 2 23.2 22.9 23.2 25.0 24.4 25.6 23.6 22.5 25.2 5 24.3 24.4 24.4 24.4 24.4 24.3 24.4 24.2 24.5 10 24.2 23.3 23.3 24.2 23.5 20.0 24.3 23.9 24.3 20 21.8 19.7 18.9 21.4 19.8 17.4 21.9 19.6 19.0 Table 2: OOT accuracy on the S EM C OR -PARA dataset across models and reweighting values (best results for each line boldfaced). Random baseline: 19.6. Target type vector baseline: 20.8 need to add direct hypernyms. Direct hypernyms have been used in annotation tasks to characterize WordNet senses (Mihalcea and Chklovski, 2003), an indicator that they are usually close enough in meaning to function as pseudo-paraphrases. Again, we parsed the corpus with Minipar and identified all sense-tagged instances of the verbs from L EX S UB -PARA, to keep the two corpora as comparable as possible. For each instance wi of word w, we collected all synonyms and direct hypernyms of the synset as the set of appropriate paraphrases. The list of synonyms and direct hypernyms of all other senses of w, whether they occur in SemCor or not, were considered inappropriate paraphrases for the instance wi . This method does not provide us wi"
W09-0208,P08-1028,0,0.182264,"l, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Structured Vector Space (SVS) (Erk and Padó, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-practice” parameter settings for"
W09-0208,D08-1094,1,\N,Missing
W09-1109,W09-3711,1,0.64946,"ces of feature values in occurrences of the target. To encode these two types of information, we study richer models of word meaning in vector space beyond single point representations. Many models of categorization in psychology represent a concept as a region, characterized by feature vectors with dimension weights (Smith et al., 1988; Hampton, 1991; Nosofsky, 1986). Taking our cue from these approaches, we study two models that represent a word as a region in vector space rather than a point. The first model is one that we have recently introduced for representing hyponymy in vector space (Erk, 2009). We now test its suitability as a general region model for word meaning. This model can be viewed as a prototypestyle model that induces a region surrounding a central vector. As it does not record co-occurrences of feature values, we contrast it with a second model, an exemplar-style model using a k-nearest neighbor analysis, which can represent both degree of variance in each dimension and value co-occurrences. Both models induce regions representations without labeled data. The idea on which both models are based is to use word token vectors to estimate a Proceedings of the Thirteenth Conf"
W09-1109,P05-1014,0,0.0265115,"ector space models have featured most prominently in information retrieval (Manning et al., 2008), but have also been used for ontology 58 learning (Lin, 1998; Snow et al., 2006; Gorman and Curran, 2006) and word sense-related tasks (McCarthy et al., 2004; Sch¨utze, 1998). In psychology, vector space models have been used to model synonymy (Landauer and Dumais, 1997; Pad´o and Lapata, 2007), lexical priming phenomena (Lowe and McDonald, 2000), and similarity judgments (McDonald and Ramscar, 2001). There have also been studies on inducing hyponymy information from vector space representations. Geffet and Dagan (2005) use a dimension re-weighting scheme, then predict entailment when the most highly weighted dimensions of two verbs stand in a subset relation. However, they find that while recall of this method is good (whenever some senses of two words stand in an entailment relation, topweighted dimensions of their vectors stand in a subset relation), precision is problematic. Weeds, Weir and McCarthy (2004) introduce the notion of distributional generality (x is more distributionally general than y if x occurs in more contexts than y) and find that for hyponym-hypernym pairs from WordNet, hyponyms are typ"
W09-1109,P06-1046,0,0.0295789,"urgess, 1996; Landauer and Dumais, 1997; Lowe, 2001; Jones and Mewhort, 2007; Sahlgren and Karlgren, 2005) represent words as points in a highdimensional semantic space. The dimensions of the space represent the contexts in which each target word has been observed. Distance between vectors in semantic space predicts the degree of semantic similarity between the corresponding words, as words with similar meaning tend to occur in similar contexts. Because of this property, vector space models have been used successfully both in computational linguistics (Manning et al., 2008; Snow et al., 2006; Gorman and Curran, 2006; Sch¨utze, 1998) and in cognitive science (Landauer and Dumais, 1997; Lowe and McDonald, 2000; McDonald and Ramscar, 2001). Given the known problems with defining globally appropriate senses (Kilgarriff, 1997; Hanks, 2000), vector space models are espe57 cially interesting for their ability to represent word meaning without relying on dictionary senses. Vector space models typically compute one vector per target word (what we will call word type vectors), summing co-occurrence counts over all corpus tokens of the target. If the target word is polysemous, the representation will constitute a u"
W09-1109,P93-1016,0,0.0204345,"National Corpus (BNC) to compute the vector space and as our source of target word tokens. We need token vectors for training the two region models, and we need separate, previously unseen token vectors as test data. So we split the written portion of the BNC in half at random, leaving files intact. This yielded a training and a test set. We computed word type vectors from the training half of the BNC, using a syntax-based vector space (Pad´o and Lapata, 2007) of 500 dimensions, with raw co-occurrence counts as dimension values. We used the dv package1 to compute type vectors from a Minipar (Lin, 1993) parse of the BNC. We computed token vectors by combining the target verb’s type vector with the type vector of the word occurring as the target’s direct object. We test three methods for combining type vectors: First, component-wise multiplication (below called mult), which showed best results in Mitchell and Lapata’s (2008) analysis. Second, component-wise averaging (below called avg), a variant of type vector addition, a method often used for computing token vectors. Third, we consider component-wise minimum (min), which can be viewed as a kind of intersection of the contexts with which the"
W09-1109,P98-2127,0,0.0737947,"words (Lund and Burgess, 1996) or syntactic paths (Pad´o and Lapata, 2007). In the simplest case, the value on a dimension is the raw co-occurrence count between the target word and the context item for which the dimension stands. Raw counts are often transformed, for example using a log-likelihood transformation (Lowe, 2001). Sometimes the vector space as a whole is transformed using dimensionality reduction (Landauer and Dumais, 1997). In NLP, vector space models have featured most prominently in information retrieval (Manning et al., 2008), but have also been used for ontology 58 learning (Lin, 1998; Snow et al., 2006; Gorman and Curran, 2006) and word sense-related tasks (McCarthy et al., 2004; Sch¨utze, 1998). In psychology, vector space models have been used to model synonymy (Landauer and Dumais, 1997; Pad´o and Lapata, 2007), lexical priming phenomena (Lowe and McDonald, 2000), and similarity judgments (McDonald and Ramscar, 2001). There have also been studies on inducing hyponymy information from vector space representations. Geffet and Dagan (2005) use a dimension re-weighting scheme, then predict entailment when the most highly weighted dimensions of two verbs stand in a subset r"
W09-1109,P04-1036,0,0.0230118,"mplest case, the value on a dimension is the raw co-occurrence count between the target word and the context item for which the dimension stands. Raw counts are often transformed, for example using a log-likelihood transformation (Lowe, 2001). Sometimes the vector space as a whole is transformed using dimensionality reduction (Landauer and Dumais, 1997). In NLP, vector space models have featured most prominently in information retrieval (Manning et al., 2008), but have also been used for ontology 58 learning (Lin, 1998; Snow et al., 2006; Gorman and Curran, 2006) and word sense-related tasks (McCarthy et al., 2004; Sch¨utze, 1998). In psychology, vector space models have been used to model synonymy (Landauer and Dumais, 1997; Pad´o and Lapata, 2007), lexical priming phenomena (Lowe and McDonald, 2000), and similarity judgments (McDonald and Ramscar, 2001). There have also been studies on inducing hyponymy information from vector space representations. Geffet and Dagan (2005) use a dimension re-weighting scheme, then predict entailment when the most highly weighted dimensions of two verbs stand in a subset relation. However, they find that while recall of this method is good (whenever some senses of two"
W09-1109,P08-1028,0,0.477693,"scribe areas in which points encode similar meanings. This description is flexible, depending on the target word in question, rather than uniform for all words through a fixed distance threshold from the target’s type vector. One possible application of region models of word meaning is in the task of determining the appropriateness of a paraphrase in a given context (Connor and Roth, 2007). This task is highly relevant for textual entailment (Szpektor et al., 2008). Current vector space approaches typically compare the target word’s token vector to the type vector of the potential paraphrase (Mitchell and Lapata, 2008; Erk and Pado, 2008). A region model could instead test the target’s token vector for inclusion in the potential paraphrase’s region. 2 Related work This section discusses existing vector space models and compares vector space models in computational linguistics to feature-based models of human concept representation in psychology. Vector space models. Vector space models represent the meaning of a target word as a vector in a high-dimensional space (Lund and Burgess, 1996; Landauer and Dumais, 1997; Sahlgren and Karlgren, 2005; Pad´o and Lapata, 2007; Jones and Mewhort, 2007). Dimensions sta"
W09-1109,J07-2002,0,0.0519389,"Missing"
W09-1109,J98-1004,0,0.347845,"Missing"
W09-1109,P06-1101,0,0.0814223,"meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997; Lowe, 2001; Jones and Mewhort, 2007; Sahlgren and Karlgren, 2005) represent words as points in a highdimensional semantic space. The dimensions of the space represent the contexts in which each target word has been observed. Distance between vectors in semantic space predicts the degree of semantic similarity between the corresponding words, as words with similar meaning tend to occur in similar contexts. Because of this property, vector space models have been used successfully both in computational linguistics (Manning et al., 2008; Snow et al., 2006; Gorman and Curran, 2006; Sch¨utze, 1998) and in cognitive science (Landauer and Dumais, 1997; Lowe and McDonald, 2000; McDonald and Ramscar, 2001). Given the known problems with defining globally appropriate senses (Kilgarriff, 1997; Hanks, 2000), vector space models are espe57 cially interesting for their ability to represent word meaning without relying on dictionary senses. Vector space models typically compute one vector per target word (what we will call word type vectors), summing co-occurrence counts over all corpus tokens of the target. If the target word is polysemous, the represent"
W09-1109,P08-1078,0,0.0340809,"t in vector space, the task is predict the word of which it is a token vector. By representing the meaning of words as regions in vector space, we can describe areas in which points encode similar meanings. This description is flexible, depending on the target word in question, rather than uniform for all words through a fixed distance threshold from the target’s type vector. One possible application of region models of word meaning is in the task of determining the appropriateness of a paraphrase in a given context (Connor and Roth, 2007). This task is highly relevant for textual entailment (Szpektor et al., 2008). Current vector space approaches typically compare the target word’s token vector to the type vector of the potential paraphrase (Mitchell and Lapata, 2008; Erk and Pado, 2008). A region model could instead test the target’s token vector for inclusion in the potential paraphrase’s region. 2 Related work This section discusses existing vector space models and compares vector space models in computational linguistics to feature-based models of human concept representation in psychology. Vector space models. Vector space models represent the meaning of a target word as a vector in a high-dimensi"
W09-1109,C04-1146,0,0.0933316,"Missing"
W09-1109,D08-1094,1,\N,Missing
W09-1109,C98-2122,0,\N,Missing
W09-3207,W09-0201,1,0.819944,"paired t-tests for the relevant degrees of freedom. Even though the SVD-based and pure-vector models are among the top achievers in general, we see that in different tasks different random walk models achieve comparable or even better performances. In particular, for phrasal associates and conceptual associates, the best results are obtained by random walks based on direct measures. the full graph are in Table 1, line 1. The SVD model clearly outperforms the pure-vector based approach and the graph-based approaches. Its performance is above that of previous models trained on the same corpus (Baroni and Lenci, 2009). The best model that we report is based on web search engine results (Chen et al., 2006). Among the graph-based random walk models, flexible walk with parameter 0.5 and fixed 1-step walk with indirect relatedness measures using dot product similarity achieve the highest performance. Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit. . . ) and abstract (feelings, times. . . ) concepts. Our results on this clustering task are given in Table 1 (line 2). The difference between SVD and pure-vector models is negli"
W09-3207,P06-1127,0,0.0895379,"models are among the top achievers in general, we see that in different tasks different random walk models achieve comparable or even better performances. In particular, for phrasal associates and conceptual associates, the best results are obtained by random walks based on direct measures. the full graph are in Table 1, line 1. The SVD model clearly outperforms the pure-vector based approach and the graph-based approaches. Its performance is above that of previous models trained on the same corpus (Baroni and Lenci, 2009). The best model that we report is based on web search engine results (Chen et al., 2006). Among the graph-based random walk models, flexible walk with parameter 0.5 and fixed 1-step walk with indirect relatedness measures using dot product similarity achieve the highest performance. Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit. . . ) and abstract (feelings, times. . . ) concepts. Our results on this clustering task are given in Table 1 (line 2). The difference between SVD and pure-vector models is negligible and they both obtain the best performance in terms of both cluster entropy (not sho"
W09-3207,D08-1095,0,0.080606,"e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can simulate the smoothing effects obtained by methods"
W09-3207,J07-2002,0,0.438925,"Missing"
W09-3207,N04-3012,0,0.0255047,"aph, it corresponds to two nodes labeled e1 and e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can s"
W09-3207,W09-0203,0,0.109769,"Missing"
W09-3207,C02-1114,0,0.0821388,"two nodes labeled e1 and e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can simulate the smoothing eff"
W09-3711,D08-1094,1,0.921054,"ule is applicable. Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences. 1 Introduction Semantic space models represent the meaning of a word as a vector in a highdimensional space, where the dimensions stand for contexts in which the word occurs [14, 10, 21, 20]. They have been used successfully in NLP [15], as well as in psychology [10, 13, 16]. Semantic space models, which are induced automatically from corpus data, can be used to characterize the meaning of an occurrence of a word in a specific sentence [17, 3] without recourse to dictionary senses. This is interesting especially in the light of the recent debate about the problems of dictionary senses [9, 7]. However, 104 Proceedings of the 8th International Conference on Computational Semantics, pages 104–115, c Tilburg, January 2009. 2009 International Conference on Computational Semantics   animal horse  dim1 29   dim1 1003     dim2 0  v  dim2 5 ... ...      animal horse Figure 1: Modeling hyponymy in semantic space: as subsumption between feature structures (left) or as subregion inclusion (right) it makes sense to characterize"
W09-3711,C92-2082,0,0.0677089,"el for extending the representation of word meaning 105 in co-occurrence space from a point to a region. In doing so, it makes use of the property that points in co-occurrence space that are close together represent similar meanings. We do not assume that the subregion relation will hold between induced hyponym and hypernym representations, no more than that the subsumption relation would hold between them. Instead, we will argue that the region representations make it possible to encode hyponymy information collected from another source, for example WordNet [4] or a hyponymy induction scheme [8, 25]. Plan of the paper. Sec. 2 gives a short overview of existing geometric models of meaning. In Sec. 3 we discuss the significance of a point in conceptual space and in co-occurrence space, finding that the two frameworks differ fundamentally in this respect, but that we can still represent word meanings as regions in co-occurrence space. Building on this, Sec. 4 introduces a region model of word meaning in co-occurrence space that can be learned automatically from corpus data. Sec. 5 reports on experiments testing the model on the task of predicting hyponymy relations between occurrences of wo"
W09-3711,P93-1016,0,0.175541,"ries by using P (in|~x) without a threshold. It may thus be able to model borderline uses of a word, and unclear boundaries between senses [2]. 5 Experiments on hyponymy In this section, we report on experiments on hyponymy in co-occurrence space. We test whether different co-occurrence space models can predict, given meaning representations (summation vectors, occurrence vectors, or regions) of two words, whether one of the two words is a hypernym of the other. In all tests, the models do not see the words, just the co-occurrence space representations. Experimental setting. We used a Minipar [11] dependency parse of the British National Corpus (BNC) as the source of data for all experiments below. The written portion of the BNC was split at random into two halves: a training half and a test half. We used WordNet 3.0 as the “ground truth” against which to evaluate models. We work with two main sets of lemmas: first, the set of monosemous verbs according to WordNet (we refer to this set as Mon), and second, the set of hypernyms of the verbs in Mon (we call this set Hyp). We concentrate on monosemous words in the current 110 paper since they will allow us to evaluate property (P3) most d"
W09-3711,P98-2127,0,0.203538,"representation: The region model identifies hyponym occurrences with very high precision. If anything, the region is too narrow, classifying many actual hyponyms as negatives. 6 Inference in co-occurrence space In this section we take a step back to ask what it means for co-occurrence space to support inferences, taking the inferences in Ex. (1) and (2) as an example. The inference in Ex. (1), which involves a paraphrase, is supported in two ways: (I1) Paraphrase candidates – words that may be substituted for acquire in some contexts – can be read off a co-occurrence space represen112 tation [12]. They are the words whose summation vectors are closest to the summation vector of acquire in space. In this way, co-occurrence space can be used for the construction of context-dependent paraphrase rules. (I2) Given an occurrence ~o of acquire, the appropriateness of applying the paraphrase rule substituting buy for acquire is estimated based on the distance ~ of buy [17, 3]. This can be used between ~o and the summation vector buy to select the single best paraphrase candidate for the given occurrence of acquire, or to produce a ranking of all paraphrase candidates [3]. Concerning the hypon"
W09-3711,P08-1028,0,0.635746,"ule is applicable. Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences. 1 Introduction Semantic space models represent the meaning of a word as a vector in a highdimensional space, where the dimensions stand for contexts in which the word occurs [14, 10, 21, 20]. They have been used successfully in NLP [15], as well as in psychology [10, 13, 16]. Semantic space models, which are induced automatically from corpus data, can be used to characterize the meaning of an occurrence of a word in a specific sentence [17, 3] without recourse to dictionary senses. This is interesting especially in the light of the recent debate about the problems of dictionary senses [9, 7]. However, 104 Proceedings of the 8th International Conference on Computational Semantics, pages 104–115, c Tilburg, January 2009. 2009 International Conference on Computational Semantics   animal horse  dim1 29   dim1 1003     dim2 0  v  dim2 5 ... ...      animal horse Figure 1: Modeling hyponymy in semantic space: as subsumption between feature structures (left) or as subregion inclusion (right) it makes sense to characterize"
W09-3711,J07-2002,0,0.240188,"a model for learning a region representation for word meaning in semantic space, based on the fact that points at close distance tend to represent similar meanings. We show that this model can be used to predict, with high precision, when a hyponymybased inference rule is applicable. Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences. 1 Introduction Semantic space models represent the meaning of a word as a vector in a highdimensional space, where the dimensions stand for contexts in which the word occurs [14, 10, 21, 20]. They have been used successfully in NLP [15], as well as in psychology [10, 13, 16]. Semantic space models, which are induced automatically from corpus data, can be used to characterize the meaning of an occurrence of a word in a specific sentence [17, 3] without recourse to dictionary senses. This is interesting especially in the light of the recent debate about the problems of dictionary senses [9, 7]. However, 104 Proceedings of the 8th International Conference on Computational Semantics, pages 104–115, c Tilburg, January 2009. 2009 International Conference on Computational Semantics"
W09-3711,J98-1004,0,0.421259,"de and Prejudice. There are many variations on co-occurrence space representations, for example using syntactic context rather than word co-occurrence [20]. The most important property of co-occurrence space models is that similarity between target words can be estimated as distance in space, using measures such as 106 letter surprise admirer 1 0 all 8 7 allow 1 0 almost 2 0 am 2 4 and 56 22 angry 1 0 ... ... ... Table 1: Some co-occurrence counts for letter, surprise in Austen’s Pride and Prejudice Euclidean distance or cosine similarity. Co-occurrence space models have been used both in NLP [15, 25, 22] and in psychology [10, 13, 16]. They have mostly been used to represent the meaning of a word by summing over all its occurrences. We will call these vectors summation vectors. A few studies have developed models that represent the meaning of an occurrence of a word in a specific sentence [10, 22, 17, 3]. The occurrence vector for a word in a specific sentence is typically computed by combining its summation vector with that of a single context word in the same sentence, for example the direct object of a target verb. For Ex. (1) this would mean computing the meaning representation of this oc"
W09-3711,P06-1101,0,0.533682,"el for extending the representation of word meaning 105 in co-occurrence space from a point to a region. In doing so, it makes use of the property that points in co-occurrence space that are close together represent similar meanings. We do not assume that the subregion relation will hold between induced hyponym and hypernym representations, no more than that the subsumption relation would hold between them. Instead, we will argue that the region representations make it possible to encode hyponymy information collected from another source, for example WordNet [4] or a hyponymy induction scheme [8, 25]. Plan of the paper. Sec. 2 gives a short overview of existing geometric models of meaning. In Sec. 3 we discuss the significance of a point in conceptual space and in co-occurrence space, finding that the two frameworks differ fundamentally in this respect, but that we can still represent word meanings as regions in co-occurrence space. Building on this, Sec. 4 introduces a region model of word meaning in co-occurrence space that can be learned automatically from corpus data. Sec. 5 reports on experiments testing the model on the task of predicting hyponymy relations between occurrences of wo"
W09-3711,C98-2122,0,\N,Missing
W10-2803,W09-1109,1,0.701337,"catch if it is close enough to the attachment site. If we know the WordNet sense of contract for which rule (2) holds – it happens to be sense 4 –, we can attach the rule to a vector for sense 4 of contract, rather than a vector computed from all occurrences of the lemma. Note that when we use dictionaries as a source for inference rules, for example by creating an inference rule like (2) for each two words that share a synset and for each direct hyponym/hypernym pair, we do know the WordNet sense to which each inference rule attaches. Mapping dictionary senses to regions in vector space. In Erk (2009) we expand on the idea of tying inference rules to attachment sites by representing a word sense not as a point but as a region in vector space. The extent of the regions is estimated through the use of both positive exemplars (occurrences of the word sense in question), and negative exemplars (occurrences of other words). The computational models we use are inspired by cognitive models of concept representation that represent concepts as regions (Smith et al., 1988; Hampton, 1991), in particular adopting Shepard’s law (Shepard, 1987), which states that perceived similarity to an exemplar decr"
W10-2803,P08-2063,0,0.018083,"t correlation (p  0.001) between the judgments of each pair of annotators. Furthermore, there was a strong correlation on judgments given with and without the use of dictionary senses (USim versus WSsim) for the same data. 2) Eleven CIRA members have been convicted of criminal charges and others are awaiting trial. Figure 1: From (Erk et al., 2009): A sense pair from the USim dataset, for the target charge.n. Annotator judgments: 2,3,4 and varies strongly with the experimental setting. Some studies found evidence for a separate representation (Klein and Murphy, 2001; Pylkkanen et al., 2006). Brown (2008) finds a linear change in semantic similarity effects with sense distance, which could possibly point to a continuous representation of word meaning without clear sense boundaries. But while there is no definitive answer yet on the question of the mental representation of polysemy, a computational model that does not rely on distinct senses has the advantage of making fewer assumptions. It also avoids the tough lexicographic problem mentioned above, of deciding on a best set of senses for a given domain. 19 3 Vector space models of word meaning in isolation (C)), distributional models are an o"
W10-2803,N06-2015,0,0.0451412,"Missing"
W10-2803,D09-1046,1,0.850911,"he number of times each word occurs in a given sentence. The activated exemplars are then simply the ones whose vectors are most similar to the vector of s. The results that we achieved with the exemplar-based model on the Lexical Substitution dataset were considerably better than Mapping dictionary senses to points in vector space. Dictionary senses can be mapped to points in vector space very straightforwardly if we have sense-annotated corpus data. In that case, we can compute a (prototype) vector for a sense from all corpus occurrences annotated with that sense. We used this simple model (Erk and McCarthy, 2009) to predict the graded sense applicability judgments from the WSsim dataset. (See Section 2 for more information on this dataset.) The predictions of the vector space model significantly correlate with annotator judgments. In comparison with an approach that uses the confidence levels of a standard WSD model as predictions, the vector space model shows higher recall but lower precision – for definitions of precision and recall that are adapted to the graded case. 1 These two examples are due to Ray Mooney. Instead of the binary selection of each exemplar that this model uses, it would also be"
W10-2803,D08-1094,1,0.928527,"Missing"
W10-2803,W09-0208,1,0.720545,"Missing"
W10-2803,P10-2017,1,0.883175,"Missing"
W10-2803,J07-2002,0,0.196388,"Missing"
W10-2803,passonneau-etal-2010-word,0,0.0766209,"Missing"
W10-2803,P04-1036,0,0.0905891,"Missing"
W10-2803,S07-1016,0,0.0211362,"Missing"
W10-2803,N10-1013,0,0.0177984,"atch -1 subj ! throw catch organise obj-1 obj ... subj-1 ball obj cold baseball drift mod ... red golf elegant cold baseball drift ! ball mod ... Figure 2: From (Erk and Pad´o, 2008): Left: Vector representations for verb catch and noun ball. Lexical information plus selectional preferences. Right: Computing context-specific meaning by combining predicate and argument via selectional preference vectors ata (2008)), and component-wise minimum. Then there are multiple prototype approaches that statically cluster synonyms or occurrences to induce word senses(Sch¨utze, 1998; Pantel and Lin, 2002; Reisinger and Mooney, 2010). Exemplar-based approaches represent a word in isolation as a collection of its occurrences or paraphrases, then select only the contextually appropriate exemplars for a given occurrence context (Kintsch, 2001; Erk and Pad´o, 2010). In this paper we focus on the first and third group of approaches, as they do not rely on knowledge of how many word senses (clusters) there should be. preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj −1 ), and one for the verbs of which is an object (obj −1 ). The vector for catch in a given context, say in the context"
W10-2803,W04-0807,0,0.0335262,"Missing"
W10-2803,P08-1028,0,0.560606,"re are a few vector space models of meaning in context, though they differ in what it is that they model. One group of models computes a single vector for a whole sentence, encoding both the words and the syntactic structure (Smolensky, 1990; B. Coecke and Clark, 2010). In this case, the dimensionality of the vectors varies with the syntactic complexity of the sentence in question. A second group also computes a single vector for a whole expression, but the vector for a larger expression is a combination of the word vectors for the words occurring in the expression (Landauer and Dumais, 1997; Mitchell and Lapata, 2008). Syntactic structure is not encoded. The resulting vector, of the same dimensionality as the word vectors, is then a combination of the contexts in which the words of the sentence occur. A third group of approaches derives a separate vector for each word in a given sentence (Erk and Pad´o, 2008; Thater et al., 2009; Erk and Pad´o, 2010). While an approach of the second type would derive a single, joint vector for, say, the expression catch a ball, an approach from the third group would derive two vectors, one for the word catch in the context of ball, and one for the word ball in the context"
W10-2803,J98-1004,0,0.685586,"Missing"
W10-2803,W04-0811,0,0.0380176,"Missing"
W10-2803,W09-2506,0,0.124669,"s with the syntactic complexity of the sentence in question. A second group also computes a single vector for a whole expression, but the vector for a larger expression is a combination of the word vectors for the words occurring in the expression (Landauer and Dumais, 1997; Mitchell and Lapata, 2008). Syntactic structure is not encoded. The resulting vector, of the same dimensionality as the word vectors, is then a combination of the contexts in which the words of the sentence occur. A third group of approaches derives a separate vector for each word in a given sentence (Erk and Pad´o, 2008; Thater et al., 2009; Erk and Pad´o, 2010). While an approach of the second type would derive a single, joint vector for, say, the expression catch a ball, an approach from the third group would derive two vectors, one for the word catch in the context of ball, and one for the word ball in the context of catch. In this third group, the dimensionality of a vector for a word in context is the same as for a word in isolation. This section gives a brief overview of the use of vector spaces to model concepts and word meaning in cognition and computational linguistics. In two of the current main theories of concept rep"
W10-2803,P09-1002,1,\N,Missing
W11-0112,C04-1180,0,0.0229644,"g weights to first-order logical rules, combining a diverse set of inference rules, and performing inference in a probabilistic way. While this is a large and complex task, this paper proposes a series of first steps toward our goal. In this paper, we focus on three natural language phenomena and their interaction: implicativity and factivity, word meaning, and coreference. Our framework parses natural language into a logical form, adds rule weights computed by external NLP modules, and performs inferences using an MLN. Our end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al., 2004) to parse natural 105 language into a logical form. We use Alchemy (Kok et al., 2005) for MLN inference. Finally, we use the exemplar-based distributional model of Erk and Pad´o (2010) to produce rule weights. 2 Background Logic-based semantics. Boxer (Bos et al., 2004) is a software package for wide-coverage semantic analysis that provides semantic representations in the form of Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005) describe a system for Recognizing Textual Entailment (RTE) that uses Boxer t"
W11-0112,H05-1079,0,0.872595,"nces that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context. 1 Introduction Logic-based representations of natural language meaning have a long history. Representing the meaning of language in a first-order logical form is appealing because it provides a powerful and flexible way to express even complex propositions. However, systems built solely using first-order logical forms tend to be very brittle as they have no way of integrating uncertain knowledge. They, therefore, tend to have high precision at the cost of low recall (Bos and Markert, 2005). Recent advances in computational linguistics have yielded robust methods that use weighted or probabilistic models. For example, distributional models of word meaning have been used successfully to judge paraphrase appropriateness. This has been done by representing the word meaning in context as a point in a high-dimensional semantics space (Erk and Pad´o, 2008; Thater et al., 2010; Erk and Pad´o, 2010). However, these models typically handle only individual phenomena instead of providing a meaning representation for complete sentences. It is a long-standing open question how best to integr"
W11-0112,P04-1014,0,0.0375653,"erforms inferences using an MLN. Our end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al., 2004) to parse natural 105 language into a logical form. We use Alchemy (Kok et al., 2005) for MLN inference. Finally, we use the exemplar-based distributional model of Erk and Pad´o (2010) to produce rule weights. 2 Background Logic-based semantics. Boxer (Bos et al., 2004) is a software package for wide-coverage semantic analysis that provides semantic representations in the form of Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005) describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem prover to check for logical entailment. Distributional models for lexical meaning. Distributional models describe the meaning of a word through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able to predict semantic simila"
W11-0112,D08-1094,1,0.167087,"Missing"
W11-0112,P10-2017,1,0.26339,"Missing"
W11-0112,N06-2015,0,0.00739758,"tion of hypernymy and polarity are given in (12). The rule in (12a) states that in a positive environment, the hyponym entails the hypernym while the rule in (12b) states that in a negative environment, the opposite is true: the hypernym entails the hyponym. (12) (a) ∀ l p1 p2 x.[(hypernym(p1 , p2 ) ∧ true(l) ∧ pred(l, p1 , x)) → pred(l, p2 , x)]] (b) ∀ l p1 p2 x.[(hypernym(p1 , p2 ) ∧ f alse(l) ∧ pred(l, p2 , x)) → pred(l, p1 , x)]] Making use of coreference information As a test case for incorporating additional resources into Boxer’s logical form, we used the coreference data in OntoNotes (Hovy et al., 2006). However, the same mechanism would allow us to transfer information into Boxer output from arbitrary additional NLP tools such as automatic coreference analysis tools or semantic role labelers. Our system uses coreference information into two distinct ways. The first way we make use of coreference data is to copy atoms describing a particular variable to those variables that corefer. Consider again example (4) which has a two-sentence premise. This inference requires recognizing that the “he” in the second sentence of the premise refers to “George Christopher” from the first sentence. Boxer a"
W11-0112,W09-3714,0,0.130866,"(the “arranging that”) is the theme of “forget to” and DRS x5 (the “failing”) is the theme of “arrange that”. In order to write logical rules about the truth conditions of nested propositions, the structure has to be flattened. However, it is clearly not sufficient to just conjoin all propositions at the top level. Such an approach, applied to example (2), would yield (hope(x1 ) ∧ theme(x1 , x2 ) ∧ build(x2 ) ∧ . . .), leading to the wrong inference that the stadium was built. Instead, we add a new argument to each predicate that 1 2 Examples (1) and (16) and Figure 2 are based on examples by MacCartney and Manning (2009) Examples (2), (3), (4), and (18) are modified versions of sentences from document wsj 0126 from the Penn Treebank 107 x0 x1 named(x0,ed,per) named(x1,dave,per) x2 x3 ¬ forget(x2) event(x2) agent(x2,x0) theme(x2,x3) x4 x5 x3: arrange(x4) event(x4) agent(x4,x0) theme(x4,x5) x6 x5: fail(x6) event(x6) agent(x6,x1) transforms to −−−−−−−→ named(l0, ne per ed d s0 w0, z0) named(l0, ne per dave d s0 w7, z1) not(l0, l1) pred(l1, v forget d s0 w3, e2) event(l1, e2) rel(l1, agent, e2, z0) rel(l1, theme, e2, l2) prop(l1, l2) pred(l2, v arrange d s0 w5, e4) event(l2, e4) rel(l2, agent, e4, z0) rel(l2, the"
W11-0112,P08-1028,0,0.0243781,"and then uses a theorem prover to check for logical entailment. Distributional models for lexical meaning. Distributional models describe the meaning of a word through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able to predict semantic similarity between words based on distributional similarity and they can be learned in an unsupervised fashion. Recently distributional models have been used to predict the applicability of paraphrases in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Erk and Pad´o, 2010). For example, in “The wine left a stain”, “result in” is a better paraphrase for “leave” than is “entrust”, while the opposite is true in “He left the children with the nurse”. Usually, the distributional representation for a word mixes all its usages (senses). For the paraphrase appropriateness task, these representations are then reweighted, extended, or filtered to focus on contextually appropriate usages. Markov Logic. An MLN consists of a set of weighted first-order clauses. It provides a way of softening first-order logic b"
W11-0112,W06-3907,0,0.107176,"p, x)]] (6) We use three different predicate symbols to distinguish three types of atomic concepts: predicates, named entities, and relations. Predicates and named entities represent words that appear in the text. For example, named(l0, ne per ed d s0 w0, z0) indicates that variable z0 is a person named “Ed” while pred(l1, v forget d s0 w3, e2) says that e2 is a “forgetting to” event. Relations capture the relationships between words. For example, rel(l1, agent, e2, z0) indicates that z0, “Ed”, is the “agent” of the “forgetting to” event e2. 5 Handling the phenomena Implicatives and factives Nairn et al. (2006) presented an approach to the treatment of inferences involving implicatives and factives. Their approach identifies an “implication signature” for every implicative or factive verb that determines the truth conditions for the verb’s nested proposition, whether in a positive or negative environment. Implication signatures take the form “x/y” where x represents the implicativity in the the positive environment and y represents the implicativity in the negative environment. Both x and y have three possible values: “+” for positive entailment, meaning the nested proposition is entailed, “-” for n"
W11-0112,D09-1001,0,0.0278519,"d Z be the normalization constant. Then the probability of a particular truth assignment x to the variables in X is defined as:     X X X 1 1 P (X = x) = exp  g(x) = exp  wi wi ni (x) (1) Z Z fi ∈F g∈Gfi fi ∈F P where g(x) is 1 if g is satisfied and 0 otherwise, and ni (x) = g∈Gf g(x) is the number of groundings i of fi that are satisfied given the current truth assignment to the variables in X. This means that the probability of a truth assignment rises exponentially with the number of groundings that are satisfied. Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)). However, this paper marks the first attempt at representing deep logical semantics in an MLN. While it is possible learn rule weights in an MLN directly from training data, our approach at this time focuses on incorporating weights computed by external knowledge sources. Weights for word meaning rules are computed from the distributional model of lexical meaning and then injected into the MLN. Rules governing implicativity and coreference are given infinite weight (hard constraints). 3 Evaluation and phenomena Textual entailment offers a good framework for testing whether a system performs"
W11-0112,P10-1097,0,0.0451223,"Missing"
W13-0109,D10-1115,0,0.661417,"ints in vector space to mental concepts. We extend this framework to a joint semantics of logic and distributions by linking intensions of logical expressions to mental concepts. 1 Introduction Distributional similarity can model a surprising range of phenomena (e.g., Lund et al. (1995); Landauer and Dumais (1997)) and is useful in many NLP tasks (Turney and Pantel, 2010). Recently, it has been suggested that a general-purpose framework for representing natural language semantics should be distributional, such that it could represent word similarity and phrase similarity (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012). Another suggestion has been to combine distributional representations and logical form, with the argument that the strengths of the two frameworks are in complementary areas (Garrette et al., 2011). One important difference between logic and distributional representations is that logics have a semantics. For example, a model1 in model-theoretic semantics provides a truth assignment to each sentence of a logical language. More generally, it associates expressions of a logic with set-theoretic structures, for example the constant cat0 could be i"
W13-0109,P11-1062,0,0.0122363,"nyms with respect to a usage u = hs, fix, δ, ωi if Iu (u) = Iu (hs[correct/fix], correct, δ, ωi). The main challenge for incorporating polysemy is to have intensions change based on the context of use. (2) Distributional similarity of larger phrases. There is considerable work both on the distributional similarity of phrases and sentences (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) and on the distributional similarity of phrases with open argument slots, such as “X solves Y” and “X finds a solution to Y” (Lin and Pantel, 2001; Szpektor and Dagan, 2008; Berant et al., 2011). We would like to use these results to do distributionally driven replacement of multi-word paraphrases in a joint distributional and logical framework. But this requires a semantics for distributional representations of larger phrases. If we assume some sort of conceptual structures as semantics, the next 9 question is whether all logical expressions should be associated with conceptual structures: Should the intension of a variable be something conceptual? (3) Gradience. In this paper we have assumed that the link from usage to concept is binary – either present or not –, and also that ther"
W13-0109,P12-1015,0,0.0429593,"such as paraphrasing beyond the word level, or graded concept membership. 3.1 Distributional representations Typically, the distributional representation for a target word t is computed from the occurrences, or usages, of t in a given corpus. Minimally, a usage is a sequence of words in which the target appears at least once. We will allow for two additional pieces of information in a usage, namely larger discourse context, and non-linguistic context. (Recently, there have been distributional approaches that make use of non-linguistic context, in particular image data (Feng and Lapata, 2010; Bruni et al., 2012).) Let W be a set of words (the lexicon), and let Seq(W ) be the set of finite sequences over W . Then a usage over W is a tuple hs, t, δ, ωi, where s ∈ Seq(W ) is a sequence of words such that a word form of t ∈ W occurs in s at least once, δ ∈ ∆ ∪ {N A} is a (possibly empty) discourse context, and 4 We write αA to indicate that expression α is of type A. Fox and Lappin mention that one could add the constraint that if α, α0 differ only in the names of bound variables, then I(α) = I(α0 ). We do not do that here, since we are only concerned with replacing non-logical constants in the current p"
W13-0109,J12-1002,0,0.191004,"rk to a joint semantics of logic and distributions by linking intensions of logical expressions to mental concepts. 1 Introduction Distributional similarity can model a surprising range of phenomena (e.g., Lund et al. (1995); Landauer and Dumais (1997)) and is useful in many NLP tasks (Turney and Pantel, 2010). Recently, it has been suggested that a general-purpose framework for representing natural language semantics should be distributional, such that it could represent word similarity and phrase similarity (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012). Another suggestion has been to combine distributional representations and logical form, with the argument that the strengths of the two frameworks are in complementary areas (Garrette et al., 2011). One important difference between logic and distributional representations is that logics have a semantics. For example, a model1 in model-theoretic semantics provides a truth assignment to each sentence of a logical language. More generally, it associates expressions of a logic with set-theoretic structures, for example the constant cat0 could be interpreted as the set of all cats in a given worl"
W13-0109,N10-1011,0,0.113271,"portant questions open, such as paraphrasing beyond the word level, or graded concept membership. 3.1 Distributional representations Typically, the distributional representation for a target word t is computed from the occurrences, or usages, of t in a given corpus. Minimally, a usage is a sequence of words in which the target appears at least once. We will allow for two additional pieces of information in a usage, namely larger discourse context, and non-linguistic context. (Recently, there have been distributional approaches that make use of non-linguistic context, in particular image data (Feng and Lapata, 2010; Bruni et al., 2012).) Let W be a set of words (the lexicon), and let Seq(W ) be the set of finite sequences over W . Then a usage over W is a tuple hs, t, δ, ωi, where s ∈ Seq(W ) is a sequence of words such that a word form of t ∈ W occurs in s at least once, δ ∈ ∆ ∪ {N A} is a (possibly empty) discourse context, and 4 We write αA to indicate that expression α is of type A. Fox and Lappin mention that one could add the constraint that if α, α0 differ only in the names of bound variables, then I(α) = I(α0 ). We do not do that here, since we are only concerned with replacing non-logical const"
W13-0109,W11-0112,1,0.938199,"henomena (e.g., Lund et al. (1995); Landauer and Dumais (1997)) and is useful in many NLP tasks (Turney and Pantel, 2010). Recently, it has been suggested that a general-purpose framework for representing natural language semantics should be distributional, such that it could represent word similarity and phrase similarity (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012). Another suggestion has been to combine distributional representations and logical form, with the argument that the strengths of the two frameworks are in complementary areas (Garrette et al., 2011). One important difference between logic and distributional representations is that logics have a semantics. For example, a model1 in model-theoretic semantics provides a truth assignment to each sentence of a logical language. More generally, it associates expressions of a logic with set-theoretic structures, for example the constant cat0 could be interpreted as the set of all cats in a given world. But what is the interpretation of a distributional representation? What does a point in vector space, where the dimensions are typically uninterpretable symbols, stand for? 2 In this paper, we pro"
W13-0109,D11-1129,0,0.163213,"l concepts. We extend this framework to a joint semantics of logic and distributions by linking intensions of logical expressions to mental concepts. 1 Introduction Distributional similarity can model a surprising range of phenomena (e.g., Lund et al. (1995); Landauer and Dumais (1997)) and is useful in many NLP tasks (Turney and Pantel, 2010). Recently, it has been suggested that a general-purpose framework for representing natural language semantics should be distributional, such that it could represent word similarity and phrase similarity (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012). Another suggestion has been to combine distributional representations and logical form, with the argument that the strengths of the two frameworks are in complementary areas (Garrette et al., 2011). One important difference between logic and distributional representations is that logics have a semantics. For example, a model1 in model-theoretic semantics provides a truth assignment to each sentence of a logical language. More generally, it associates expressions of a logic with set-theoretic structures, for example the constant cat0 could be interpreted as the set of all cats"
W13-0109,P98-2127,0,0.130915,"dress problems in the granularity of intensions. Crucially, some hyper-intensional approaches have intensions that are abstract objects, with minimal requirements on the nature of these objects. So we can build on them to link some intensions to conceptual structure. Why design a semantics for distributional representations? Our aim is not to explicitly construct conceptual models; that would be at least as hard as constructing an ontology. Rather, our aim is to support inferences. Distributional representations induce synonyms and paraphrases automatically based on distributional similarity (Lin, 1998; Lin and Pantel, 2001). As Garrette et al. (2011) point out, and as illustrated in Figure 1, these can be used as inference rules within logical form. But when is such inference projection valid? Our main aim for constructing a joint semantics is to provide a principled basis for answering this question. In the current paper, we construct a first semantics along the lines sketched above. In order to be able to take this first step, we simplify distributional predictions greatly by discretizing them. We want to stress, however, that this is a temporary restriction; our eventual aim is to make"
W13-0109,N10-1013,0,0.0243334,"stic context. We write U(W, ∆, Ω) for the set of all usages over W (and ∆ and Ω). For any usage u = hs, t, δ, ωi, we write target(u) = t. Given a set U ⊆ U(W, ∆, Ω) of usages, we write Ut = {u ∈ U |target(u) = t} for the usages of a target word t. Furthermore, we write WU = {t ∈ W |Ut 6= ∅} for the set of words that have usages in U . In distributional approaches, the vector space representation for a target word t is computed from such a set U of usages, typically by mapping U to a single point in vector space (Lund et al., 1995; Landauer and Dumais, 1997) or a set of points (Sch¨utze, 1998; Reisinger and Mooney, 2010). This makes it possible to use linear algebra in modeling semantics. However, for our current purposes, we do not need to specify any particular mapping to a vector space, and can simply work with the underlying set U of usages: A finite set U of usages over W constitutes a distributional representation for WU . The distributional representation for a word t ∈ W is Ut . 3.2 A semantics for distributional representations We want to interpret distributional representations over conceptual structure. But what is conceptual structure? We know that concepts are linked by different semantic relatio"
W13-0109,J98-1004,0,0.0974705,"Missing"
W13-0109,C08-1107,0,0.0121793,"ix” and “correct” are synonyms with respect to a usage u = hs, fix, δ, ωi if Iu (u) = Iu (hs[correct/fix], correct, δ, ωi). The main challenge for incorporating polysemy is to have intensions change based on the context of use. (2) Distributional similarity of larger phrases. There is considerable work both on the distributional similarity of phrases and sentences (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) and on the distributional similarity of phrases with open argument slots, such as “X solves Y” and “X finds a solution to Y” (Lin and Pantel, 2001; Szpektor and Dagan, 2008; Berant et al., 2011). We would like to use these results to do distributionally driven replacement of multi-word paraphrases in a joint distributional and logical framework. But this requires a semantics for distributional representations of larger phrases. If we assume some sort of conceptual structures as semantics, the next 9 question is whether all logical expressions should be associated with conceptual structures: Should the intension of a variable be something conceptual? (3) Gradience. In this paper we have assumed that the link from usage to concept is binary – either present or not"
W13-0109,C98-2122,0,\N,Missing
W14-2402,S13-1001,0,0.0770814,"s a complex set of relationships between them 7 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7–11, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponym"
W14-2402,S12-1051,0,0.032773,"erall interpretation with the maximum probability given a set of evidence. It turns out that this optimization problem is second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved efficiently in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach A semantic parser is three components, a formal language, an ontology, and an inference mechanism. This section explains the details of these components in our semantic parser. It also points out the future work related to each part of the system. 3.1 Formal Language: first-order logic Natural sentences are mapped to logical form using Boxer (Bos, 2008), which maps the input sentences into a lexically-based lo"
W14-2402,D10-1115,0,0.28562,"s 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks"
W14-2402,D13-1161,0,0.0210497,"he ontology. Inference is what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicte"
W14-2402,S13-1002,1,0.924408,"e hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 2.1 2.3 Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder"
W14-2402,P14-1114,1,0.442945,"tween “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 2.1 2.3 Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly enc"
W14-2402,S12-1012,0,0.248494,"ited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in nature, so the on-the-fly ontology and its predicted semantic relations are also graded. This means, that standard first-order logic is insufficient because it is binary by nature. Probabi"
W14-2402,D13-1160,0,0.0431878,"what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations i"
W14-2402,P08-1028,0,0.0743507,"26 2014. 2014 Association for Computational Linguistics 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantic"
W14-2402,W08-2222,0,0.180759,"ntal issue. Background Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). 2.2 Markov Logic Network 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing"
W14-2402,P04-1014,0,0.0414115,"lational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). 2.2 Markov Logic Network 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted infer"
W14-2402,N13-1092,0,0.106997,"Missing"
W14-2402,D11-1129,0,\N,Missing
W14-3009,ide-etal-2008-masc,0,0.132205,"and event types Katrin Erk Department of Linguistics The University of Texas at Austin Austin, Texas 78712 katrin.erk@mail.utexas.edu In memoriam Charles Fillmore, 1929-2014 Abstract et al., 2004), Italian (Tonelli and Pianta, 2008; Lenci et al., 2010), as well as frame-semantic annotation for German (Erk et al., 2003). The definition of frames proceeds in a corpusbased fashion, driven by the data (Ellsworth et al., 2004). We stand in this tradition by reporting on a recent annotation effort (Kremer et al., 2014) that collected lexical substitutes for content words in part of the MASC corpus (Ide et al., 2008). If we view substitute sets as indications of the relevant frame, then this data can give us interesting indicators on perceived frames in a naturally occurring text. Lexical substitution is an annotation task in which annotators provide one-word paraphrases (lexical substitutes) for individual target words in a sentence context. Lexical substitution yields a fine-grained characterization of word meaning that can be done by non-expert annotators. We discuss results of a recent lexical substitution annotation effort, where we found strong contextual modulation effects: Many substitutes were no"
W14-3009,E14-1057,1,0.883913,"Missing"
W14-3009,lenci-etal-2010-building,0,0.0732091,"Missing"
W14-3009,tonelli-pianta-2008-frame,0,0.0381215,"Missing"
W14-3009,P03-1068,1,\N,Missing
W15-0119,S12-1051,0,0.0630535,"Missing"
W15-0119,S13-1002,1,0.863354,"ki et al., 2013; Berant et al., 2013), or fine-grained opinion analysis. With the more complex tasks, there has been a renewed interest in phenomena like negation (Choi and Cardie, 2008) or the factivity of embedded clauses (MacCartney and Manning, 2009; Lotan et al., 2013) – phenomena that used to be standardly handled by logic-based semantics. Bos (2013) identifies the use of broad-coverage lexical resources as one aspect that is crucial to the success of logic-based approaches. Another crucial aspect is the ability to reason with uncertain, probabilistic information (Garrette et al., 2011; Beltagy et al., 2013). Lexical information typically comes with weights, be it weights of paraphrase rules (Lin and Pantel, 2001; Ganitkevitch et al., 2013), confidence ratings of word sense disambiguation systems, or distributional similarity values, and reasoning with such information and finding the overall best interpretation requires the ability to handle weights. This is possible in the framework of probabilistic logic (Nilsson, 1986). In this paper we do not talk about why one should use probabilistic logic for natural language semantics (we argue for the need for probabilistic logic in previous work (Belta"
W15-0119,D13-1160,0,0.162088,"Missing"
W15-0119,W08-2222,0,0.668981,"Missing"
W15-0119,D08-1083,0,0.0429909,"Missing"
W15-0119,copestake-flickinger-2000-open,0,0.0329452,"Missing"
W15-0119,P79-1022,0,0.39377,"Missing"
W15-0119,N13-1092,0,0.188194,"Missing"
W15-0119,W11-0112,1,0.878315,"ntic Parsing (Kwiatkowski et al., 2013; Berant et al., 2013), or fine-grained opinion analysis. With the more complex tasks, there has been a renewed interest in phenomena like negation (Choi and Cardie, 2008) or the factivity of embedded clauses (MacCartney and Manning, 2009; Lotan et al., 2013) – phenomena that used to be standardly handled by logic-based semantics. Bos (2013) identifies the use of broad-coverage lexical resources as one aspect that is crucial to the success of logic-based approaches. Another crucial aspect is the ability to reason with uncertain, probabilistic information (Garrette et al., 2011; Beltagy et al., 2013). Lexical information typically comes with weights, be it weights of paraphrase rules (Lin and Pantel, 2001; Ganitkevitch et al., 2013), confidence ratings of word sense disambiguation systems, or distributional similarity values, and reasoning with such information and finding the overall best interpretation requires the ability to handle weights. This is possible in the framework of probabilistic logic (Nilsson, 1986). In this paper we do not talk about why one should use probabilistic logic for natural language semantics (we argue for the need for probabilistic logic"
W15-0119,D13-1161,0,0.0886623,"Missing"
W15-0119,Q13-1015,0,0.0797928,"Missing"
W15-0119,D14-1107,0,0.250501,"Missing"
W15-0119,N13-1091,0,0.0267024,"Missing"
W15-0119,W07-1431,0,0.0948201,"Missing"
W15-0119,C08-1066,0,0.205809,"Missing"
W15-0119,W09-3714,0,0.154516,"Missing"
W15-0119,marelli-etal-2014-sick,0,0.158238,"Missing"
W15-0119,P07-2009,0,\N,Missing
W16-6112,P15-1136,0,0.0126208,"ing, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used sem"
W16-6112,J07-1005,0,0.0665381,"These observations motivate the coreference features presented in section 4.3. In Table 2, standard care is not a member of any chains. More generally, we can expect salience to help more with intervention arms than control.3 3 Related work 3.1 Automated Identification of Arms Previous work has identified PICO elements either at the word or sentence level. Most research has extracted information from medical abstracts, although some studies have used the full text of the articles (De Bruijn et al., 2008; Zhao et al., 2012; Wallace et al., 2016). One of the seminal studies in PICO extraction (Demner-Fushman and Lin, 2007) collapsed intervention and comparator, where interventions were short noun phrases based largely on recognition of semantic types (mapped to UMLS concepts) and a few manually constructed rules. The intervention/comparator extractor returned a list of all the interventions under study, and the extractor was evaluated at the sentence level. However, it is important to distinguish between experimental and control treatments as the bias for the experimental 3 Cases of joint coreference such as all participants referring to both arms in the example abstract are not addressed in this paper, but pos"
W16-6112,D08-1069,0,0.0648262,"Missing"
W16-6112,N07-1030,0,0.0810657,"Missing"
W16-6112,Q14-1037,0,0.0233569,"hen modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitive closure to guarantee you end up with legal assignments (Finkel and Manning, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic"
W16-6112,S15-2051,0,0.0395324,"ly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Records (EHRs), and biomolecular studies. In fact, there have been corpora (i2b2/VA Corpus(Uzuner et al., 2012), GENIA Event Corpus(Kim et al., 2008)) and shared tasks (SemEval-2015 shared task on Analysis of Clinical Text (Task 14)(Elhadad et al., 2015), BioNLP09 shared task(Kim et al., 2009), ShARe/CLEF eHealth 2013 Evaluation Lab Task 1(Pradhan et al., 2013)) created specifically to advance this area. Given that resources such as FrameNet and WordNet are based mostly on news (e.g. British National Corpus, U.S. newswire), a large number of resources have been created to aid in natural language processing of medical texts. By far the largest and most complex is the Unified Medical Language System (UMLS)7 , consisting of three main components: Metathesaurus with terms and codes from many vocabularies (including CPT, ICD-10-CM, MeSH, RxNorm, a"
W16-6112,P08-2012,0,0.0234167,"ng information from partially-completed coreference chains to guide later decisions. Features include whether a mention head word matches any of the head words in the antecedent cluster. • The antecedent tree model (Yu and Joachims, 2009) builds a graph from a document, where the nodes are the mentions and arcs are the links between mention pairs that are coreferent candidates. The coreference chains are then modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitive closure to guarantee you end up with legal assignments (Finkel and Manning, 2008). For example, if John Smith is coreferent with Smith, and Smith with Jane Smith, then it should not follow that John Smith and Jane Smith are coreferent. Other work has shown that joint models improve performance. Denis et al. (2007) recognized that anaphoricity (whether an entity is the first mention) and coreference should be treated as a joint task since one informs the other. Durrett and Klein (2014) models coreference together with named entity recognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and"
W16-6112,C08-1033,0,0.0248128,"for extracting eventargument relations from biomedical texts in the Genia Event Corpus. Jindal and Roth (2013) used very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific discourse model (i.e. a single patient, several doctors and a few family members) to resolve entities of type ”person”. Despite the active interest in coreference resolution, there has been much less research investigating its application to clinical trial texts. Most of the literature that does exist is applied to the bio-medical field, focusing more on full-text articles (Gasperin and Briscoe, 2008; Huang et al., 2010; Kilicoglu et al., 2016) than on abstracts (Castano et al., 2002; Yang et al., 2004). To the best of the authors’ knowledge, there have been no papers using coreference features to identify arms in clinical trial abstracts. 8 https://metamap.nlm.nih.gov http://www.drugbank.ca 10 http://banner.sourceforge.net 11 http://biotext.berkeley.edu 9 4 Experiment The goal of this experiment is to explore empirically whether incorporating coreference features improves the performance of a classifier for arm identification, as compared to a baseline model without coref features (note"
W16-6112,W09-1401,0,0.0405063,"anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Records (EHRs), and biomolecular studies. In fact, there have been corpora (i2b2/VA Corpus(Uzuner et al., 2012), GENIA Event Corpus(Kim et al., 2008)) and shared tasks (SemEval-2015 shared task on Analysis of Clinical Text (Task 14)(Elhadad et al., 2015), BioNLP09 shared task(Kim et al., 2009), ShARe/CLEF eHealth 2013 Evaluation Lab Task 1(Pradhan et al., 2013)) created specifically to advance this area. Given that resources such as FrameNet and WordNet are based mostly on news (e.g. British National Corpus, U.S. newswire), a large number of resources have been created to aid in natural language processing of medical texts. By far the largest and most complex is the Unified Medical Language System (UMLS)7 , consisting of three main components: Metathesaurus with terms and codes from many vocabularies (including CPT, ICD-10-CM, MeSH, RxNorm, and SNOMED CT), Semantic Network with sem"
W16-6112,C02-1139,0,0.0628798,"ach token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head wo"
W16-6112,P02-1014,0,0.172486,"ach token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head wo"
W16-6112,N15-1082,0,0.0173032,"ognition and linking named entities to Wikipedia entities. Combinations of these models have also yielded improved results, such as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used semantic role labeling to link the time and locations for event mentions, and for verbal mentions they linked their participants. More recently, Kilicoglu et al. (2016) focused on sortal anaphoras which they found to commonly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, corefe"
W16-6112,N12-1091,0,0.017602,"gBank9 , a database of drug names, BANNER10 , a named entity recognizer for biomedical texts, BioText for identifying entities and relations in bioscience texts, and BioFrameNet11 , an extension of FrameNet for molecular biology (and BioWordNet(Poprat et al., 2008) was a failed attempt at extending WordNet also to the biomolecular field). However, when applied to clinical trial texts, these tools prove useful mainly for identifying only medical terms and drug names, and thus more linguistically-motivated resources are still lacking for clinical trial texts. In the area of clinical narratives, Raghavan et al. (2012) took advantage of the temporal features present in these texts to help determine whether two medical concepts corefer with each other. Their 2014 paper (Raghavan et al., 2014) expanded on this idea to identify medical events spanning across narratives, such as admission notes, medical reports, and discharge notes. Yoshikawa et al. (2011) exploited coreference information for extracting eventargument relations from biomedical texts in the Genia Event Corpus. Jindal and Roth (2013) used very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific dis"
W16-6112,D10-1048,0,0.0195134,"n features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical features (e.g., head word of the mention), and string matching. • The mention-ranking model (Denis and Baldridge, 2008), reframes the task as a ranking function rather than a classification function, ranking all the candidate antecedents of a mention to determine which candidate antecedent is the most probable. • The entity-centric model makes use of entitylevel information, focusing on features of mention clusters, and not just pairs (Raghunathan et al., 2010). The coreference clusters are built up incrementally, using information from partially-completed coreference chains to guide later decisions. Features include whether a mention head word matches any of the head words in the antecedent cluster. • The antecedent tree model (Yu and Joachims, 2009) builds a graph from a document, where the nodes are the mentions and arcs are the links between mention pairs that are coreferent candidates. The coreference chains are then modeled as latent trees in the graph. Constraints are imposed on these models for improved results, such as enforcing a transitiv"
W16-6112,J01-4004,0,0.373443,"nstead classifies each token directly, followed by an inference process to constrain the labels to more accurate results. As with previous studies, outcome results were the hardest because they are more variable. A significant limitation of this study is that the abstracts were limited to two-arm trials, and in a specific domain. 3.2 Automated Coreference Resolution Coreference resolution is a long-studied task that remains a challenging problem. Most recent work on coreference resolution builds mainly on one of four models. • The first and most widely-used approach is the mention-pair model (Soon et al., 2001; Ng and Cardie, 2002b). A classifier first identifies all the pairs of mentions which are coreferent. These pairs are then grouped into coreferent chains by clustering techniques such as closest-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002b; Ng and Cardie, 2002a). 4 http://rctbank.ucsf.edu/home/cplus In closest-first, you link to the closest preceding mention, whereas in best-first, you choose the likeliest one. Common features in these models include distance between the two mentions, syntactic features (e.g., POS tags), semantic features (e.g., named entity type), lexical fe"
W16-6112,C04-1033,0,0.0499361,"sed very specific domain knowledge to resolve coreference in clinical narratives, such as creating a specific discourse model (i.e. a single patient, several doctors and a few family members) to resolve entities of type ”person”. Despite the active interest in coreference resolution, there has been much less research investigating its application to clinical trial texts. Most of the literature that does exist is applied to the bio-medical field, focusing more on full-text articles (Gasperin and Briscoe, 2008; Huang et al., 2010; Kilicoglu et al., 2016) than on abstracts (Castano et al., 2002; Yang et al., 2004). To the best of the authors’ knowledge, there have been no papers using coreference features to identify arms in clinical trial abstracts. 8 https://metamap.nlm.nih.gov http://www.drugbank.ca 10 http://banner.sourceforge.net 11 http://biotext.berkeley.edu 9 4 Experiment The goal of this experiment is to explore empirically whether incorporating coreference features improves the performance of a classifier for arm identification, as compared to a baseline model without coref features (note that we do not aim to necessarily achieve state-of-the-art results on this task). The task of the classif"
W16-6112,Q15-1037,0,0.116138,"as Clark and Manning (2015) stacking 89 mention-pair and entity-centric systems (which the current paper uses as its off-the-shelf coreference resolver). Many coreference resolvers exploit deeper linguistic knowledge, beyond the features mentioned above. Chowdhury and Zweigenbaum (2013) eliminated less-informative training instances prior to model training by creating a list of criteria based on semantic and syntactic intuitions such as a mismatch in semantic types. Peng et al. (2015) created predicate schemas to constrain inference, such as two predicates with a semantically shared argument. Yang et al. (2015) used semantic role labeling to link the time and locations for event mentions, and for verbal mentions they linked their participants. More recently, Kilicoglu et al. (2016) focused on sortal anaphoras which they found to commonly occur in biomedical literature, resolving anaphors that carry a specific semantic type, or sort, such as these drugs. Many of these studies take advantage of linguistic resources such as WordNet5 and FrameNet6 . In the medical area, coreference resolution has been most closely studied for analyzing clinical narrative text such as that found in Electronic Health Reco"
W19-2704,D15-1263,0,0.0238212,"menters: (1) DPLP3 uses features from syntactic and dependency parses for a linear support vector classifier; (2)T WO - PASS (Feng and Hirst, 2014) is a CRF segmenter that derives features from syntax parses but also uses global features to perform a second pass of segmentation; (3) N EURAL (Wang et al., 2018) is a neural BiLSTM-CRF model that uses ELMo embeddings (Peters et al., 2018). We choose these segmenters because they are widelyused and publicly available (most RST parsers do not include a segmenter). DPLP has been cited in several works showing discourse helps on different NLP tasks (Bhatia et al., 2015). T WO - PASS, until recently, achieved SOTA on discourse segmentation when using parsed (not gold) syntax 3 D OMAIN Table 3: F1, precision (P) and recall (R) of RST discourse segmenters on two domains (best numbers for News are underlined, for Medical are bolded). Agreement. Annotators achieved on average a high level of agreement for identifying EDU boundaries with kappa=0.90 (averaged over 11 texts). However, we note that document length and complexity of the discourse influence this number. On a document of 35 tokens, the annotators exhibited perfect agreement. For the Discussion sections"
W19-2704,P17-1092,0,0.0264977,"the more challenging Discussion section. During training, we recommend using medical-specific word embeddings and preprocessing pipeline.6 Addressing even one of these issues may yield a multiplied effect on segmentation improvements as the Medical domain is by nature highly repetitive and formulaic. However, a future avenue of research would be to first understand what impact these segmentation errors have on downstream tasks. For example, using RST trees generated by the lowest-performing DPLP parser nevertheless provides small gains to text categorization tasks such as sentiment analysis (Ji and Smith, 2017). On the other hand, understanding the verb form, which proved to be difficult in the Medical domain, has been shown to be useful in distinguishing text on experimental results from text describing more abstract concepts (such as background and introductory information), which may be a more relevant task than sentiment analysis (de Waard and Maat, 2012). Table 5: Average inter-annotator agreement per section, ordered from highest to lowest, the corresponding average F1 of the NEURAL segmenter, and number of tokens (there are 2 documents per section, except 1 for Summary). fit. This finding is"
W19-2704,P17-2037,0,0.0685629,"for understanding both the strengths and limits of existing RST segmenters, and the next concrete steps towards a better segmenter for the medical domain. 2 Corpus #docs #tokens #sents #EDUs RST-DT SMALL M EDICAL 11 11 4009 3356 159 169 403 399 Table 2: Corpus statistics. pus of RST-segmented medical articles in English. Unlike several other works, we include all parts of the article, and not just the abstract. Segmenters in non-news domains. While corpora have expanded to other domains, most automated discourse segmenters remain focused (and trained) on news. An exception is the segmenter in Braud et al. (2017a) which was trained on different domains for the purpose of developing a segmenter for under-resourced languages. However, they make the simplifying assumption that a single corpus represents a single (and distinct) domain, and do not include the medical domain. In this work, we study the viability of using news-trained segmenters on the medical domain. 3 Corpus Creation Medical Corpus. The M EDICAL corpus consists of 2 clinical trial reports from PubMed Central, randomly selected for their shorter lengths for ease of annotation. We expect the language and discourse to be representative of th"
W19-2704,J15-3002,0,0.246117,"Missing"
W19-2704,D17-1258,0,0.0777561,"for understanding both the strengths and limits of existing RST segmenters, and the next concrete steps towards a better segmenter for the medical domain. 2 Corpus #docs #tokens #sents #EDUs RST-DT SMALL M EDICAL 11 11 4009 3356 159 169 403 399 Table 2: Corpus statistics. pus of RST-segmented medical articles in English. Unlike several other works, we include all parts of the article, and not just the abstract. Segmenters in non-news domains. While corpora have expanded to other domains, most automated discourse segmenters remain focused (and trained) on news. An exception is the segmenter in Braud et al. (2017a) which was trained on different domains for the purpose of developing a segmenter for under-resourced languages. However, they make the simplifying assumption that a single corpus represents a single (and distinct) domain, and do not include the medical domain. In this work, we study the viability of using news-trained segmenters on the medical domain. 3 Corpus Creation Medical Corpus. The M EDICAL corpus consists of 2 clinical trial reports from PubMed Central, randomly selected for their shorter lengths for ease of annotation. We expect the language and discourse to be representative of th"
W19-2704,P14-5010,0,0.00410734,"all documents were re-segmented by both annotators, and disagreements were resolved by discussion. F1 P R DPLP News Medical 82.56 75.29 81.75 78.69 83.37 72.18 TWO - PASS News Medical 95.72 84.69 97.19 86.23 94.29 83.21 N EURAL News Medical 97.32 91.68 95.68 94.86 99.01 88.70 trees. N EURAL now holds SOTA in RST discourse segmentation. We evaluate the segmenter’s ability to detect all EDU boundaries present in the gold data (not just intra-sentential) using the metrics of precision (P), recall (R) and F1. The DPLP and TWO - PASS segmenters, both of which employ the Stanford Core NLP pipeline (Manning et al., 2014), were updated to use the same version of this software (2018-10-05). 5 Results Table 3 lists our results on News and Medical for correctly identifying EDU boundaries using the three discourse segmenters. As expected, the News domain outperforms the Medical domain, regardless of which segmenter is used. In the case of the DPLP segmenter, the gap between the two domains is about 7.4 F1 points. Note that the performance of DPLP on News lags considerably behind the state of the art (-14.76 F1 points). When switching to the TWO - PASS segmenter, the performance on News increases dramatically (+13"
W19-2704,W17-3610,0,0.0210038,"e medical documents. The corpus statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are ade"
W19-2704,N18-1202,0,0.00903885,"News and is also able to more successfully close the gap on Medical, with only a 5.64 F1 difference, largely attributable to lower recall. Experiment We automatically segment the documents in RSTDT SMALL and MEDICAL using three segmenters: (1) DPLP3 uses features from syntactic and dependency parses for a linear support vector classifier; (2)T WO - PASS (Feng and Hirst, 2014) is a CRF segmenter that derives features from syntax parses but also uses global features to perform a second pass of segmentation; (3) N EURAL (Wang et al., 2018) is a neural BiLSTM-CRF model that uses ELMo embeddings (Peters et al., 2018). We choose these segmenters because they are widelyused and publicly available (most RST parsers do not include a segmenter). DPLP has been cited in several works showing discourse helps on different NLP tasks (Bhatia et al., 2015). T WO - PASS, until recently, achieved SOTA on discourse segmentation when using parsed (not gold) syntax 3 D OMAIN Table 3: F1, precision (P) and recall (R) of RST discourse segmenters on two domains (best numbers for News are underlined, for Medical are bolded). Agreement. Annotators achieved on average a high level of agreement for identifying EDU boundaries wit"
W19-2704,W01-1605,0,0.234423,"ds arbitrary and uninformative analyses (Taboada and Mann, 2006). XML formatting was stripped, and figures and tables were removed. The sections for Acknowledgements, Competing Interests, and Prepublication History were not included. For comparison with the News domain, we created RST-DT-SMALL by sampling an equal number of Wall Street Journal articles from the “Test” portion of the RST-DT that were similar in length to the medical documents. The corpus statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the med"
W19-2704,P09-2004,0,0.0394657,"nals the start of an EDU (e.g., in the RST discourse relations of temporal and circumstance), but is not a boundary in this case because there is no verbal element. Other problematic words include “that”, signalling relative clauses (often, but not always treated as embedded EDUs), and “and” which may indicate a coordinated sentence or clause (treated as a separate EDU) but also a coordinated verb phrase (not a separate EDU). Note this phenomenon is different from distinguishing between discourse vs. nondiscourse usage of a word, or sense disambiguation of a discourse connective as studied in Pitler and Nenkova (2009). infinitival “to” The syntactic construction of to+verb can act either as a verbal complement (treated as the same EDU) or a clausal complement (separate EDU). In the Table 4 example, the infinitival “to buy” is a complement of the verb “move” and should remain in the same EDU, but the segmenter incorrectly segmented it. tokenization This error type covers cases where the tokenizer fails to detect token boundaries, specifically punctuation. These tokenization errors lead to downstream segmentation errors since punctuation marks, often markers of EDU boundaries, are entirely missed when mangle"
W19-2704,redeker-etal-2012-multi,0,0.0794475,"Missing"
W19-2704,N03-1030,0,0.248968,"Missing"
W19-2704,W17-3604,0,0.0219226,"arized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are adequately resolved in the forthcoming shared task of the"
W19-2704,D18-1116,0,0.0238744,"Missing"
W19-2704,P18-2071,0,0.0249392,"statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are adequately resolved in the forthcomi"
W19-2704,P07-1062,0,\N,Missing
