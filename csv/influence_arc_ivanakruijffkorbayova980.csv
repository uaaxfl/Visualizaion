2005.sigdial-1.11,P05-1030,1,0.734055,"Missing"
2005.sigdial-1.11,C00-1073,0,0.0435766,"Missing"
2005.sigdial-1.11,P01-1066,0,\N,Missing
2005.sigdial-1.11,2005.sigdial-1.6,0,\N,Missing
2020.crac-1.13,W18-0701,0,0.0526567,"Missing"
2020.crac-1.13,W19-5946,1,0.870933,"Missing"
2020.crac-1.13,W99-0201,0,0.404849,"got a push, when the 6th and 7th Message Understanding Conferences (MUC 6, MUC -7) took place. MUC-7 Coreference Task Definitions (Hirschman and Chinchor, 1998) defined co-reference as a symmetric identity relation between two noun phrases (NPs) if both of them refer to the same entity. Among the researchers who worked on co-reference annotation schemes around this time are McEnery et al. (1997), Ge (1998), Rocha (1999). There also appeared some works investigating not only relations between entities introduced by NPs, but also event co-reference and temporal relations between events, e.g. by Bagga and Baldwin (1999), or Setzer and Gaizauskas (2000). At the same time, interest in the annotation of a wider range of semantic relations emerged. Among these relations is anaphoric reference. While co-reference is an equivalence relation, anaphoric reference is not - the interpretation of an anaphoric expression always depends on its antecedent. Deemter and Kibble (2000) discussed the differences between anaphora and co-reference in detail. They stressed that they can coincide, but are not interchangeable, and pointed out that co-reference is not to be mixed with bound anaphora, where an anaphor relates to a ge"
2020.crac-1.13,J96-2004,0,0.721905,"he annotation, under the guidance of the second author. All spurious cases were discussed by both authors and the annotation was updated based on the decision. We adjusted and extended our annotation scheme in the process. We did not involve multiple independent annotators, because our aim was mainly to get an overview of the reference resolution issues. To test the reliability of the resulting annotation scheme, another person annotated a small subset of the corpus, consisting of one dialogue, which contained 57 utterances. We measured inter-annotator agreement using Cohen’s kappa following (Carletta, 1996). We obtained a kappa score of 0.704 for the Entities layer, 1.0 for Comments, 0.895 for Roles, 0.573 for Reference Units and 0.845 for Reference Links. This shows good agreement, except for reference units. 4.1 Annotation Scheme Our annotation scheme has four separate layers: Entities, Roles, Reference units and Comments. We use separate layers, so that each layer can have its own set of markable expressions and a separate corresponding tag set. We keep the tag sets flat for practical reasons. At the Entities layer we annotate mission-relevant objects (POIs), locations, mission participants ("
2020.crac-1.13,caselli-prodanof-2010-annotating,0,0.0330355,"tentially be linked to something that was introduced earlier (cf. also the concept of cohesion (Halliday and Hasan, 1976)). Speaking of anaphora, sometimes researches try to concentrate only on the identity relation between entities, e.g. (Poesio, 2004) or Aktas¸ et al. (2018). But often anaphora is understood in a broader sense. Nissim et al. (2004) and Elango (2005) discussed in various types of anaphors and antecedents. Zinsmeister and Dipper (2010) researched the annotation of abstract (discourse-deictic) anaphora. Anaphoric relations between different types of events were also studied by Caselli and Prodanof (2010). Poesio et al. (2008), annotating the ARRAU corpus, introduced an anaphoric relation between single anaphoric expression and plural antecedents, as well as references to events, actions and plans. Kruijff-Korbayov´a and Kruijff (2004) developed a discourse-level annotation scheme that covered a 123 broad range of discourse reference properties, e.g. semantic sort, delimitation, quantification, familiarity status, and anaphoric links (co-reference and bridging). There also emerged some classifications of anaphora types and other relations. So, Tetreault et al. (2004) annotated the Monroe corpu"
2020.crac-1.13,2020.lrec-1.8,0,0.032869,". One of the more fine-grained approaches to co-reference annotation was presented by Hasler et al. (2006). Aiming at creating corpora for event processing, they investigated NP and event co-reference and created a co-reference annotation scheme. They introduced relations between NPs (identity, synonymy, generalisation, specialisation and other) and co-reference types (NP, copula, apposition, bracketed text, speech pronoun and other). Among more recent papers on co-reference annotation are the works by Cohen et al. (2017) about identity and appositive relations in biomedical journal articles, Dakle et al. (2020) on co-reference in emails, Wright-Bettner et al. (2019) on cross-document co-reference. While all the above mentioned works mostly deal with text data, Poesio et al. (1999) developed the MATE ‘meta-scheme’ for anaphora annotation in dialogue. This generic scheme consists of a core scheme for annotating identity relations (co-reference) between the entities introduced by NPs, and three extensions for annotating references to the visual situation, bridging and anaphoric relations involving an extended range of anaphoric expressions and antecedents. The bridging extension was later realized in t"
2020.crac-1.13,W16-4011,0,0.0686382,"Missing"
2020.crac-1.13,M98-1029,0,0.187891,"ystematise, due to proliferating terminologies and combinations of heterogeneous phenomena, such as different types of references, referents and relations. Since a proper discussion of the similarities and differences would exceed the space we have available here, we present the relevant previous work in a close to chronological order. The first annotation scheme for anaphoric relations appeared in 1992 (Fligelstone, 1992). In the late 1990s this field of research got a push, when the 6th and 7th Message Understanding Conferences (MUC 6, MUC -7) took place. MUC-7 Coreference Task Definitions (Hirschman and Chinchor, 1998) defined co-reference as a symmetric identity relation between two noun phrases (NPs) if both of them refer to the same entity. Among the researchers who worked on co-reference annotation schemes around this time are McEnery et al. (1997), Ge (1998), Rocha (1999). There also appeared some works investigating not only relations between entities introduced by NPs, but also event co-reference and temporal relations between events, e.g. by Bagga and Baldwin (1999), or Setzer and Gaizauskas (2000). At the same time, interest in the annotation of a wider range of semantic relations emerged. Among th"
2020.crac-1.13,W04-0206,1,0.706593,"Missing"
2020.crac-1.13,D17-1018,0,0.0245025,"ne in dialogue systems, clearly does not suffice for this domain, a proper discourse representation is required. As for referential links, basic anaphora together with identity relationships dominate. Bridging is next, and then there are various different cases which are not that frequent but quite tricky for reference resolution. Analyzing different kinds of bridging in more detail and properly describing the other kinds of links remains a topic for our future work. The annotated corpus is currently being used for testing existing co-reference resolution models, including the AllenNLP model (Lee et al., 2017), NeuralCoref by HuggingFace (Wolf et al., 2020) and the CoreNLP framework (Manning et al., 2014). The results of these experiments will help determine our future steps for reference resolution. 130 Acknowledgements This work was done as part of the project “A-DRZ: Setting up the German Rescue Robotics Center”, funded by the German Ministry of Education and Research (BMBF), grant No. I3N14856.2 We would like to thank our colleagues from the A-DRZ project for discussions, Tatiana Anakina for additional reference annotation and the reviewers of the CRAC 2020 Workshop on Computational Models of R"
2020.crac-1.13,P14-5010,0,0.00429413,"Missing"
2020.crac-1.13,W97-1300,0,0.443177,"ve available here, we present the relevant previous work in a close to chronological order. The first annotation scheme for anaphoric relations appeared in 1992 (Fligelstone, 1992). In the late 1990s this field of research got a push, when the 6th and 7th Message Understanding Conferences (MUC 6, MUC -7) took place. MUC-7 Coreference Task Definitions (Hirschman and Chinchor, 1998) defined co-reference as a symmetric identity relation between two noun phrases (NPs) if both of them refer to the same entity. Among the researchers who worked on co-reference annotation schemes around this time are McEnery et al. (1997), Ge (1998), Rocha (1999). There also appeared some works investigating not only relations between entities introduced by NPs, but also event co-reference and temporal relations between events, e.g. by Bagga and Baldwin (1999), or Setzer and Gaizauskas (2000). At the same time, interest in the annotation of a wider range of semantic relations emerged. Among these relations is anaphoric reference. While co-reference is an equivalence relation, anaphoric reference is not - the interpretation of an anaphoric expression always depends on its antecedent. Deemter and Kibble (2000) discussed the diff"
2020.crac-1.13,nissim-etal-2004-annotation,0,0.117135,"bridging extension was later realized in the GNOME annotation project (Poesio, 2004). Poesio et al. (1999) also formulated the difficulties that any designer of an annotation scheme for anaphora faces, namely, that almost every word or phrase in a coherent text can potentially be linked to something that was introduced earlier (cf. also the concept of cohesion (Halliday and Hasan, 1976)). Speaking of anaphora, sometimes researches try to concentrate only on the identity relation between entities, e.g. (Poesio, 2004) or Aktas¸ et al. (2018). But often anaphora is understood in a broader sense. Nissim et al. (2004) and Elango (2005) discussed in various types of anaphors and antecedents. Zinsmeister and Dipper (2010) researched the annotation of abstract (discourse-deictic) anaphora. Anaphoric relations between different types of events were also studied by Caselli and Prodanof (2010). Poesio et al. (2008), annotating the ARRAU corpus, introduced an anaphoric relation between single anaphoric expression and plural antecedents, as well as references to events, actions and plans. Kruijff-Korbayov´a and Kruijff (2004) developed a discourse-level annotation scheme that covered a 123 broad range of discourse"
2020.crac-1.13,W99-0309,0,0.36633,"ed NP and event co-reference and created a co-reference annotation scheme. They introduced relations between NPs (identity, synonymy, generalisation, specialisation and other) and co-reference types (NP, copula, apposition, bracketed text, speech pronoun and other). Among more recent papers on co-reference annotation are the works by Cohen et al. (2017) about identity and appositive relations in biomedical journal articles, Dakle et al. (2020) on co-reference in emails, Wright-Bettner et al. (2019) on cross-document co-reference. While all the above mentioned works mostly deal with text data, Poesio et al. (1999) developed the MATE ‘meta-scheme’ for anaphora annotation in dialogue. This generic scheme consists of a core scheme for annotating identity relations (co-reference) between the entities introduced by NPs, and three extensions for annotating references to the visual situation, bridging and anaphoric relations involving an extended range of anaphoric expressions and antecedents. The bridging extension was later realized in the GNOME annotation project (Poesio, 2004). Poesio et al. (1999) also formulated the difficulties that any designer of an annotation scheme for anaphora faces, namely, that"
2020.crac-1.13,poesio-artstein-2008-anaphoric,0,0.10353,"Missing"
2020.crac-1.13,W04-2327,0,0.385034,"right-Bettner et al. (2019) on cross-document co-reference. While all the above mentioned works mostly deal with text data, Poesio et al. (1999) developed the MATE ‘meta-scheme’ for anaphora annotation in dialogue. This generic scheme consists of a core scheme for annotating identity relations (co-reference) between the entities introduced by NPs, and three extensions for annotating references to the visual situation, bridging and anaphoric relations involving an extended range of anaphoric expressions and antecedents. The bridging extension was later realized in the GNOME annotation project (Poesio, 2004). Poesio et al. (1999) also formulated the difficulties that any designer of an annotation scheme for anaphora faces, namely, that almost every word or phrase in a coherent text can potentially be linked to something that was introduced earlier (cf. also the concept of cohesion (Halliday and Hasan, 1976)). Speaking of anaphora, sometimes researches try to concentrate only on the identity relation between entities, e.g. (Poesio, 2004) or Aktas¸ et al. (2018). But often anaphora is understood in a broader sense. Nissim et al. (2004) and Elango (2005) discussed in various types of anaphors and an"
2020.crac-1.13,W99-0208,0,0.280371,"relevant previous work in a close to chronological order. The first annotation scheme for anaphoric relations appeared in 1992 (Fligelstone, 1992). In the late 1990s this field of research got a push, when the 6th and 7th Message Understanding Conferences (MUC 6, MUC -7) took place. MUC-7 Coreference Task Definitions (Hirschman and Chinchor, 1998) defined co-reference as a symmetric identity relation between two noun phrases (NPs) if both of them refer to the same entity. Among the researchers who worked on co-reference annotation schemes around this time are McEnery et al. (1997), Ge (1998), Rocha (1999). There also appeared some works investigating not only relations between entities introduced by NPs, but also event co-reference and temporal relations between events, e.g. by Bagga and Baldwin (1999), or Setzer and Gaizauskas (2000). At the same time, interest in the annotation of a wider range of semantic relations emerged. Among these relations is anaphoric reference. While co-reference is an equivalence relation, anaphoric reference is not - the interpretation of an anaphoric expression always depends on its antecedent. Deemter and Kibble (2000) discussed the differences between anaphora"
2020.crac-1.13,setzer-gaizauskas-2000-annotating,0,0.356066,"7th Message Understanding Conferences (MUC 6, MUC -7) took place. MUC-7 Coreference Task Definitions (Hirschman and Chinchor, 1998) defined co-reference as a symmetric identity relation between two noun phrases (NPs) if both of them refer to the same entity. Among the researchers who worked on co-reference annotation schemes around this time are McEnery et al. (1997), Ge (1998), Rocha (1999). There also appeared some works investigating not only relations between entities introduced by NPs, but also event co-reference and temporal relations between events, e.g. by Bagga and Baldwin (1999), or Setzer and Gaizauskas (2000). At the same time, interest in the annotation of a wider range of semantic relations emerged. Among these relations is anaphoric reference. While co-reference is an equivalence relation, anaphoric reference is not - the interpretation of an anaphoric expression always depends on its antecedent. Deemter and Kibble (2000) discussed the differences between anaphora and co-reference in detail. They stressed that they can coincide, but are not interchangeable, and pointed out that co-reference is not to be mixed with bound anaphora, where an anaphor relates to a generic antecedent, which does not"
2020.crac-1.13,W04-0214,0,0.126313,"s were also studied by Caselli and Prodanof (2010). Poesio et al. (2008), annotating the ARRAU corpus, introduced an anaphoric relation between single anaphoric expression and plural antecedents, as well as references to events, actions and plans. Kruijff-Korbayov´a and Kruijff (2004) developed a discourse-level annotation scheme that covered a 123 broad range of discourse reference properties, e.g. semantic sort, delimitation, quantification, familiarity status, and anaphoric links (co-reference and bridging). There also emerged some classifications of anaphora types and other relations. So, Tetreault et al. (2004) annotated the Monroe corpus, consisting of task-oriented dialogues from an emergency rescue domain. They focused on co-referential pronouns and NPs (identity relation), but also presented a classification of relations for non-co-referential pronouns with the following relation types: indexicals, action, demonstrative, functional, set, hard and dummy. Botley (2006) distinguished three types of abstract anaphora: label anaphora (encapsulates stretches of text), which has several sub-types, situation anaphora (for linking events, processes, states, facts, propositions) and text deixis. Another c"
2020.crac-1.13,D19-6201,0,0.0614675,"Missing"
2021.codi-sharedtask.3,N18-2108,0,0.0516159,"that the M2M model aims to overcome. The first is pairing the same mentions of different entities because of the same context. The second is the problem with the different contexts of the same entities when they are referred to by the same expressions. The third one is the different mentions of the same entities in different contexts. Most coreference resolution models have been adapted to rank the antecedents based on average embeddings (Lee et al., 2017, 2018) and contextualized embeddings (Joshi et al., 2019, 2020). Furthermore, Xu and Choi (2020) has adapted the model implementation from (Lee et al., 2018) and used the contextualized span embeddings (Joshi et al., 2020) to obtain the mention candidates through span enumeration and aggressive pruning. The M2M system combines the average and contextualized embeddings instead of using them alone, to avoid context variation of the same mentions and capture lexical similarities of different mentions. Therefore, the M2M system is designed with the aim of suppressing the context variations of different or the same entities. This simply means that the different mentions might be paired because their contexts are similar, and on the contrary, the same e"
2021.codi-sharedtask.3,P02-1011,0,0.190483,"uistic text, e.g., a sequence of clauses or utterances. The exact boundaries of discourse segments are often hard to define even for humans (Webber, 1991; Artstein and Poesio, 2006). In addition, the resolution of discourse deixis mentions requires cross-sentence information about rhetorical relations and focus/attention which is often not annotated. All this makes discourse deixis resolution a very challenging task. There exist only a few frameworks focusing on the resolution of discourse deixis mentions. Most early works that we know of (e.g., Eckert and Strube (2000), Navarretta (2000), or Byron (2002)) are rule-based. They use hand-crafted rules to pick out anaphors, which are usually demonstrative pronouns. The right antecedent for each anaphor is found relying on candidates’ dialogue act types. Early machine learning approaches are presented, e.g., by Strube and Müller (2003) and 2.4.1 Data The scorer is trained on all the CCST train and development data. Before that the light_dev file was used to work on the heuristic rules. We built our dataset as follows. For each utterance containing a discourse deixis anaphor (anchor utterance) we create a pair of examples (see Fig. 4) by replacing"
2021.codi-sharedtask.3,D17-1021,0,0.0438052,"Missing"
2021.codi-sharedtask.3,W00-1007,0,0.277502,"i.e. a chunk of a linguistic text, e.g., a sequence of clauses or utterances. The exact boundaries of discourse segments are often hard to define even for humans (Webber, 1991; Artstein and Poesio, 2006). In addition, the resolution of discourse deixis mentions requires cross-sentence information about rhetorical relations and focus/attention which is often not annotated. All this makes discourse deixis resolution a very challenging task. There exist only a few frameworks focusing on the resolution of discourse deixis mentions. Most early works that we know of (e.g., Eckert and Strube (2000), Navarretta (2000), or Byron (2002)) are rule-based. They use hand-crafted rules to pick out anaphors, which are usually demonstrative pronouns. The right antecedent for each anaphor is found relying on candidates’ dialogue act types. Early machine learning approaches are presented, e.g., by Strube and Müller (2003) and 2.4.1 Data The scorer is trained on all the CCST train and development data. Before that the light_dev file was used to work on the heuristic rules. We built our dataset as follows. For each utterance containing a discourse deixis anaphor (anchor utterance) we create a pair of examples (see Fig."
2021.codi-sharedtask.3,D14-1162,0,0.0862527,"member 2 ""book"" ""it"" cluster 2 probability: 0.05 Figure 2: Workspace Clustering Example WCS Input and Architecture After extracting mentions we pre-process all sentences in the documents using BERT (pre-trained cased version) and extract various features for each mention, including the mention position within the document, the head lemma, head number,6 animacy classification,7 speaker embedding, BERT embedding (Devlin et al., 2018) of the mention’s head, averaged BERT embedding for the span of the mention, Numberbatch concept embedding (Speer et al., 2017) for the head lemma, GloVe embedding (Pennington et al., 2014) for the head lemma, averaged GloVe span embedding and the masked language model (MLM) embedding for the mention. For the MLM embedding we use BERT (pre-trained cased base model) and take the current and the previous dialogue turns, mask the mention’s head in the turn and ask the model to predict the top candidate for the masked token. We take the GloVe embedding of this candidate as the MLM embedding. When processing a new mention we compare its representation to each of the workspace clusters. Note that the very first position in the workspace is always reserved for the singleton case. All e"
2021.codi-sharedtask.3,2020.tacl-1.5,0,0.0132083,"ame mentions of different entities because of the same context. The second is the problem with the different contexts of the same entities when they are referred to by the same expressions. The third one is the different mentions of the same entities in different contexts. Most coreference resolution models have been adapted to rank the antecedents based on average embeddings (Lee et al., 2017, 2018) and contextualized embeddings (Joshi et al., 2019, 2020). Furthermore, Xu and Choi (2020) has adapted the model implementation from (Lee et al., 2018) and used the contextualized span embeddings (Joshi et al., 2020) to obtain the mention candidates through span enumeration and aggressive pruning. The M2M system combines the average and contextualized embeddings instead of using them alone, to avoid context variation of the same mentions and capture lexical similarities of different mentions. Therefore, the M2M system is designed with the aim of suppressing the context variations of different or the same entities. This simply means that the different mentions might be paired because their contexts are similar, and on the contrary, the same expressions might not be paired together because their context or"
2021.codi-sharedtask.3,N18-1202,0,0.0135657,"beginning of each chunk tagged B, any subsequent tokens in the chunk tagged I, and tokens not in any chunk tagged O. We implemented a Bidirectional LSTM - Conditional Random Field (BiLSTM-CRF) Model, shown to be effective in sequence tagging (Huang et al., 2015). As the standard IOB2 format only annotates flat chunks, our model only predicts the markables with the widest scope in cases of nesting. We flattened the training data accordingly when converting it to the IOB2 format. The architecture of the model is as follows: the input data (in IOB2 format) is first passed through an ELMo layer (Peters et al., 2018), where contextualized word representations (embeddings) were extracted for each word. The ELMo embeddings are then passed to the BiLSTM-CRF model. Finally, the CRF model predicts the tags for the input tokens. In a post-processing step, we used SpaCy (en_core_web_trf ) to extract any nested noun chunks from the markables predicted by the model. Maximum sentence length was set to 50, and we used the Adam optimizer with a learning rate of 0.0005 and trained the model for 4 epochs. The training and development sets are created as an 8:2 split of the documents in each CCST dataset. This was done"
2021.codi-sharedtask.3,D19-1588,0,0.0130186,"so refer to different entities in the real world. These noted points cause three different challenges that the M2M model aims to overcome. The first is pairing the same mentions of different entities because of the same context. The second is the problem with the different contexts of the same entities when they are referred to by the same expressions. The third one is the different mentions of the same entities in different contexts. Most coreference resolution models have been adapted to rank the antecedents based on average embeddings (Lee et al., 2017, 2018) and contextualized embeddings (Joshi et al., 2019, 2020). Furthermore, Xu and Choi (2020) has adapted the model implementation from (Lee et al., 2018) and used the contextualized span embeddings (Joshi et al., 2020) to obtain the mention candidates through span enumeration and aggressive pruning. The M2M system combines the average and contextualized embeddings instead of using them alone, to avoid context variation of the same mentions and capture lexical similarities of different mentions. Therefore, the M2M system is designed with the aim of suppressing the context variations of different or the same entities. This simply means that the d"
2021.codi-sharedtask.3,2020.crac-1.13,1,0.811972,"Missing"
2021.codi-sharedtask.3,2021.codi-sharedtask.1,0,0.352167,"Missing"
2021.codi-sharedtask.3,J18-3007,0,0.012943,"anaphor pair is valid or not. Marasovi´c et al. (2017) introduce a model that relies on a neural network (a Siamese Net) to rank potential antecedent candidates given an anaphor. The input for the network, i.e. sentences containing anaphors and antecedent candidates, is represented as feature vectors (embeddings) using bi-LSTM. Works focusing on automatic identification of discourse deixis anaphors are also not many. An algorithm based on hand-crafted features to recognize discourse deictic it, this and that is suggested, e.g., by Müller (2008). Other works, e.g., by Eckert and Strube (2000), Kolhatkar et al. (2018), or Roussel et al. (2018) present analyses of anaphors with non-nominal antecedents and their properties, but no actual implementations. Our approach combines machine learning and hand-crafted rules. The machine learning part is inspired by the mention-ranking model for abstract anaphora resolution by Marasovi´c et al. (2017). Given a discourse deixis mention that needs to be resolved, the task is to choose the most suitable antecedent among several candidates. To do so, we train a scorer implemented as a Siamese Net with the goal to assign a higher score to true anaphorantecedent pairs and l"
2021.codi-sharedtask.3,P03-1022,0,0.235599,"ation about rhetorical relations and focus/attention which is often not annotated. All this makes discourse deixis resolution a very challenging task. There exist only a few frameworks focusing on the resolution of discourse deixis mentions. Most early works that we know of (e.g., Eckert and Strube (2000), Navarretta (2000), or Byron (2002)) are rule-based. They use hand-crafted rules to pick out anaphors, which are usually demonstrative pronouns. The right antecedent for each anaphor is found relying on candidates’ dialogue act types. Early machine learning approaches are presented, e.g., by Strube and Müller (2003) and 2.4.1 Data The scorer is trained on all the CCST train and development data. Before that the light_dev file was used to work on the heuristic rules. We built our dataset as follows. For each utterance containing a discourse deixis anaphor (anchor utterance) we create a pair of examples (see Fig. 4) by replacing the anaphor with its true antecedent (a positive example) and with a false candidate (a negative example), which is usually the utterance before the true antecedent. The idea is to make the Siamese Net learn that the true antecedent fits semantically into the utterance containing t"
2021.codi-sharedtask.3,D17-1018,0,0.0545241,"on systems. Additionally, the same expressions might also refer to different entities in the real world. These noted points cause three different challenges that the M2M model aims to overcome. The first is pairing the same mentions of different entities because of the same context. The second is the problem with the different contexts of the same entities when they are referred to by the same expressions. The third one is the different mentions of the same entities in different contexts. Most coreference resolution models have been adapted to rank the antecedents based on average embeddings (Lee et al., 2017, 2018) and contextualized embeddings (Joshi et al., 2019, 2020). Furthermore, Xu and Choi (2020) has adapted the model implementation from (Lee et al., 2018) and used the contextualized span embeddings (Joshi et al., 2020) to obtain the mention candidates through span enumeration and aggressive pruning. The M2M system combines the average and contextualized embeddings instead of using them alone, to avoid context variation of the same mentions and capture lexical similarities of different mentions. Therefore, the M2M system is designed with the aim of suppressing the context variations of dif"
2021.codi-sharedtask.3,2020.emnlp-main.695,0,0.0288649,"Missing"
2021.codi-sharedtask.3,2020.emnlp-main.686,0,0.773652,"g set with 550 documents and a development set with 61 documents. We did not use any additional data for training/development. The system was tested on the CCST test set in the Eval-AR track. cluster coherence loss mention workspace clusters referring/non-referring classiﬁcation referring loss cluster assignment workspace clusters update history clusters Figure 1: Workspace Coreference System Overview Our implementation differs from other submissions to the Shared Task. Both winning models (lxucs and UTD_NLP) are based on the pairwise scoring of mentions and extend the coreference model from (Xu and Choi, 2020). The model submitted by KU_NLP is based on pointer networks whereas our system is more similar in spirit to the incremental approach introduced in (Xia et al., 2020). Similarly to their work we are building clusters of mentions incrementally and remove old mentions from the workspace if the corresponding clusters have not been updated for a long time.3 However, there are some important differences between our model and the work by Xia et al. (2020). For example, we do not use separate representations for clusters, but rather take the latest 5 mentions from each cluster and compute an average"
bateman-etal-2000-resources,P96-1026,0,\N,Missing
bateman-etal-2000-resources,J94-4004,0,\N,Missing
baumann-etal-2004-muli,J98-2001,0,\N,Missing
baumann-etal-2004-muli,W01-1612,0,\N,Missing
baumann-etal-2004-muli,H94-1020,0,\N,Missing
C00-1069,A97-1017,0,0.0313464,"Missing"
C12-3012,N12-3005,1,0.353842,"Missing"
C12-3012,W11-2814,1,0.836376,"Missing"
C12-3012,P11-2115,1,0.885607,"Missing"
C12-3012,W11-2011,1,0.831464,"pped to actions by maximizing a long-term reward signal (Sutton and Barto, 1998). While RL-based dialogue systems are promising, they still need to overcome several limitations to reach practical and wide-spread application. One of these limitations is the curse of dimensionality, the problem that the state space grows exponentially according to the variables taken into account. Another limitation is that attempts to address the first problem often involve rule-based reductions of the state space (Litman et al., 2000; Singh et al., 2002; Heeman, 2007; Williams, 2008; Cuayáhuitl et al., 2010b; Dethlefs et al., 2011) which can lead to reduced flexibility of system behaviour in terms of letting the user say and/or do anything at any time during the dialogue. Finally, even when function approximation techniques have been used to scale up in small-scale and singletask systems (Henderson et al., 2008; Li et al., 2009; Pietquin, 2011; Jurcícek et al., 2011), their application to more complex dialogue contexts has yet to be demonstrated. Our motivation for increased dialogue flexibility in this paper is the assumption that users at times deviate from the system’s expected user behaviour. In reduced state spaces"
C12-3012,D12-1008,1,0.888014,"Missing"
C12-3012,N07-1034,0,0.027347,"interaction with an environment, where situations are mapped to actions by maximizing a long-term reward signal (Sutton and Barto, 1998). While RL-based dialogue systems are promising, they still need to overcome several limitations to reach practical and wide-spread application. One of these limitations is the curse of dimensionality, the problem that the state space grows exponentially according to the variables taken into account. Another limitation is that attempts to address the first problem often involve rule-based reductions of the state space (Litman et al., 2000; Singh et al., 2002; Heeman, 2007; Williams, 2008; Cuayáhuitl et al., 2010b; Dethlefs et al., 2011) which can lead to reduced flexibility of system behaviour in terms of letting the user say and/or do anything at any time during the dialogue. Finally, even when function approximation techniques have been used to scale up in small-scale and singletask systems (Henderson et al., 2008; Li et al., 2009; Pietquin, 2011; Jurcícek et al., 2011), their application to more complex dialogue contexts has yet to be demonstrated. Our motivation for increased dialogue flexibility in this paper is the assumption that users at times deviate"
C12-3012,J08-4002,0,0.0210826,"problem that the state space grows exponentially according to the variables taken into account. Another limitation is that attempts to address the first problem often involve rule-based reductions of the state space (Litman et al., 2000; Singh et al., 2002; Heeman, 2007; Williams, 2008; Cuayáhuitl et al., 2010b; Dethlefs et al., 2011) which can lead to reduced flexibility of system behaviour in terms of letting the user say and/or do anything at any time during the dialogue. Finally, even when function approximation techniques have been used to scale up in small-scale and singletask systems (Henderson et al., 2008; Li et al., 2009; Pietquin, 2011; Jurcícek et al., 2011), their application to more complex dialogue contexts has yet to be demonstrated. Our motivation for increased dialogue flexibility in this paper is the assumption that users at times deviate from the system’s expected user behaviour. In reduced state spaces this may lead to unseen dialogue states in which the system cannot react properly to the new situation, as is exemplified Proceedings of COLING 2012: Demonstration Papers, pages 95–102, COLING 2012, Mumbai, December 2012. 95 by dialogue 3S in Figure 1. So whilst a full state space re"
C12-3012,W12-1619,0,0.026688,"Missing"
C12-3012,C00-1073,0,0.0523784,"s. An RL agent learns its behaviour from interaction with an environment, where situations are mapped to actions by maximizing a long-term reward signal (Sutton and Barto, 1998). While RL-based dialogue systems are promising, they still need to overcome several limitations to reach practical and wide-spread application. One of these limitations is the curse of dimensionality, the problem that the state space grows exponentially according to the variables taken into account. Another limitation is that attempts to address the first problem often involve rule-based reductions of the state space (Litman et al., 2000; Singh et al., 2002; Heeman, 2007; Williams, 2008; Cuayáhuitl et al., 2010b; Dethlefs et al., 2011) which can lead to reduced flexibility of system behaviour in terms of letting the user say and/or do anything at any time during the dialogue. Finally, even when function approximation techniques have been used to scale up in small-scale and singletask systems (Henderson et al., 2008; Li et al., 2009; Pietquin, 2011; Jurcícek et al., 2011), their application to more complex dialogue contexts has yet to be demonstrated. Our motivation for increased dialogue flexibility in this paper is the assum"
C12-3012,E09-1081,0,0.074338,"Missing"
E03-1057,E03-2004,1,0.447311,"Missing"
E03-2004,E03-1057,1,0.448883,"Missing"
H05-1002,C00-1021,0,0.0724901,"Missing"
H05-1002,P01-1036,0,0.0664719,"Missing"
H05-1002,E03-1057,1,0.891237,"Missing"
H05-1002,H94-1035,0,0.0425674,"rts that link the sentence content  We use the Praguian terminology for this distinction. to the discourse context, and other parts that advance the discourse, i.e., add or modify information; and (ii) a background/kontrast2 distinction between parts of the utterance which contribute to distinguishing its actual content from alternatives the context makes available. Information Structure is an important factor in determining the felicity of a sentence in a given context. Applications in which IS is crucial are textto-speech systems, where IS helps to improve the quality of the speech output (Prevost and Steedman, 1994; Kruijff-Korbayov´a et al., 2003; Moore et al., 2004), and machine translation, where IS improves target word order, especially that of free word order languages (Stys and Zemke, 1995). Existing theories, however, state their principles using carefully selected illustrative examples. Because of this, they fail to adequately explain how different linguistic dimensions cooperate to realize Information Structure. In this paper we describe data-driven, machine learning approaches for automatic identification of Information Structure; we describe what aspects of IS we deal with and report results"
H05-1002,vesela-etal-2004-annotators,0,0.0633445,"Missing"
kruijff-korbayova-etal-2006-annotation,cmejrek-etal-2004-prague,0,\N,Missing
kruijff-korbayova-etal-2006-annotation,H05-1002,1,\N,Missing
kruijff-korbayova-etal-2006-annotation,J96-2004,0,\N,Missing
kruijff-korbayova-etal-2006-annotation,P00-1056,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,2005.sigdial-1.11,1,\N,Missing
kruijff-korbayova-etal-2006-sammie,J97-1002,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,W04-2327,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,W05-1624,1,\N,Missing
kruijff-korbayova-etal-2006-sammie,poesio-2000-annotating,0,\N,Missing
N12-3005,J86-3001,0,0.757967,"t and flexible. These two forms of dialogue control are shown in Figure 1. It can be observed that strict HDC is based on a pure top down execution, and flexible HDC is based on a combined hierarchical and graph-based execution. 1 Introduction Hierarchical Dialogue Control (HDC) consists of behaviours or discourse segments at different levels of granularity executed from higher to lower level. For example, a dialogue agent can invoke a sub-dialogue agent, which can also invoke a subsub-dialogue agent, and so on. Task-oriented dialogues have shown evidence of following hierarchical structures (Grosz and Sidner, 1986; Litman and Allen, 1987; Clark, 1996). Practically speaking, HDC offers the following benefits. First, modularity helps to specify sub-dialogues that may be easier to specify than the entire full dialogues. Second, sub-dialogues may include only relevant dialogue knowledge (e.g. subsets of dialogue acts), thus reducing significantly their com∗ *Funding by the EU-FP7 project ALIZ-E (ICT-248116) is gratefully acknowledged. The main limitation of strict HDC is that human-machine interactions are rigid, i.e. the user cannot change the imposed dialogue structure. A more natural way of interaction"
P04-1004,P02-1041,0,0.0687926,"nd Kruijff-Korbayov´a, 2004a). Implementation The syntactic analysis is performed using openCCG8 , an open source parser for Multi-Modal Combinatory Categorial Grammar (MMCCG). MMCCG is a lexicalist grammar formalism in which application of combinatory rules is controlled though context-sensitive specification of modes on slashes (Baldridge and Kruijff, 2003). The linguistic meaning, built in parallel with the syntax, is represented using Hybrid Logic Dependency Semantics (HLDS), a hybrid logic representation which allows a compositional, unification-based construction of HLDS terms with CCG (Baldridge and Kruijff, 2002). An HLDS term is a relational structure where dependency relations between heads and dependents are encoded as modal relations. The syntactic categories for a lexical entry FORMULA, corresponding to mathematical , and . expressions of type “formula”, are , For example, in one of the readings of “B enthaelt ”, “enthaelt” represents the meaning contain taking dependents in the relations Actor and Patient, shown schematically in Fig. 2.  &quot; enthalten:contain Other commonly found TRs include NormCriterion, e.g.  ) +& [nach deMorgan-Regel-2] NORM ist =...) [according to De Morga"
P04-1004,E03-1036,0,0.104381,"l relation as in complementation, and PROP-FROM is a HasProperty relation of type Direction-From or From-Source. More details on the investigation into tectogrammatical relations that build up linguistic meaning of informal mathematical text can be found in (Wolska and Kruijff-Korbayov´a, 2004a). Implementation The syntactic analysis is performed using openCCG8 , an open source parser for Multi-Modal Combinatory Categorial Grammar (MMCCG). MMCCG is a lexicalist grammar formalism in which application of combinatory rules is controlled though context-sensitive specification of modes on slashes (Baldridge and Kruijff, 2003). The linguistic meaning, built in parallel with the syntax, is represented using Hybrid Logic Dependency Semantics (HLDS), a hybrid logic representation which allows a compositional, unification-based construction of HLDS terms with CCG (Baldridge and Kruijff, 2002). An HLDS term is a relational structure where dependency relations between heads and dependents are encoded as modal relations. The syntactic categories for a lexical entry FORMULA, corresponding to mathematical , and . expressions of type “formula”, are , For example, in one of the readings of “B enthaelt ”, “enthaelt” represents"
P04-1004,W04-0911,1,0.508833,"Missing"
P04-1004,wolska-etal-2004-annotated,1,0.865834,"Missing"
P06-4015,E03-1036,0,0.0783909,"Missing"
P06-4015,2005.sigdial-1.20,1,0.854194,"Missing"
P06-4015,W05-1624,1,0.876704,"Missing"
W01-0810,C00-1069,1,\N,Missing
W01-0810,E95-1034,0,\N,Missing
W01-0810,W94-0314,0,\N,Missing
W01-0810,H89-1022,0,\N,Missing
W01-0810,A97-1039,0,\N,Missing
W04-0206,W04-2707,1,0.677563,"h (app. 7,000 tokens). The MULI corpus has been created by extracting a continuous stretch of 21 relatively short texts from the Tiger treebank, and a set of 10 texts from the Penn Treebank. The selection was made so that the texts would be comparable in genre (ﬁnancial news/announcements). The morphological, part-of-speech and syntactic information encoded in the treebanks can be reused for our purposes. We add annotations of syntactically marked constructions, prosodic features and discourse semantics. Our approach to annotation at the levels of syntax, prosody and discourse is outlined in (Bauman et al., 2004a; Bauman et al., 2004b). In this paper, we provide 1 http://www.coli.uni-sb.de/cl/projects/tiger/ 2 http://www.cis.upenn.edu/~treebank/home.html more details about the discourse-level annotation. In §2 we overview the methodological concerns and desiderata we adhere to in designing our annotation schemes. In §3 we present the discourse-level annotation scheme in detail. In §4 we illustrate the multi-level investigation perspective. §5 we brieﬂy describe the annotation tools we use. In §6 we conclude and sketch future work. 2 Methodology Text samples of varying origin, genre, language and size"
W04-0206,baumann-etal-2004-muli,1,0.893429,"h (app. 7,000 tokens). The MULI corpus has been created by extracting a continuous stretch of 21 relatively short texts from the Tiger treebank, and a set of 10 texts from the Penn Treebank. The selection was made so that the texts would be comparable in genre (ﬁnancial news/announcements). The morphological, part-of-speech and syntactic information encoded in the treebanks can be reused for our purposes. We add annotations of syntactically marked constructions, prosodic features and discourse semantics. Our approach to annotation at the levels of syntax, prosody and discourse is outlined in (Bauman et al., 2004a; Bauman et al., 2004b). In this paper, we provide 1 http://www.coli.uni-sb.de/cl/projects/tiger/ 2 http://www.cis.upenn.edu/~treebank/home.html more details about the discourse-level annotation. In §2 we overview the methodological concerns and desiderata we adhere to in designing our annotation schemes. In §3 we present the discourse-level annotation scheme in detail. In §4 we illustrate the multi-level investigation perspective. §5 we brieﬂy describe the annotation tools we use. In §6 we conclude and sketch future work. 2 Methodology Text samples of varying origin, genre, language and size"
W04-0206,C00-1021,0,0.0272466,"sketch future work. 2 Methodology Text samples of varying origin, genre, language and size have been previously annotated with theoryspeciﬁc notions of IS by various authors. Such data are typically not publicly available, and even if they can be obtained, it is very hard if not impossible to compare and reuse diﬀerent annotations. More promising in this respect are annotations that include or add some aspect(s) of IS to an existing corpus or treebank. The most systematic eﬀort of this kind that we are familiar with is the TopicFocus annotation in the Prague Dependency Treebank (Bur´ an ˇ ov´ a et al., 2000). In contrast to other projects in which IS is annotated and investigated, we do not annotate theory-biased abstract categories like Topic-Focus or Theme-Rheme. Since we are particularly interested in the correlations and co-occurrences of features on diﬀerent linguistic levels that can be interpreted as indicators of the abstract IS categories, we needed an annotation scheme to be as theoryneutral as possible: It should allow for a description of the phenomena, from which ’any’ theoryspeciﬁc explanatory mechanisms can subsequently be derived (Skut et al., 1997). We therefore concentrate inste"
W04-0206,P03-1068,0,0.0198246,"stigations can help to extend existing accounts of information structure, and can also be used to verify (or falsify) predictions made by such accounts. The corpus also makes it possible to construct computational models from the corpus data. Theory-neutrality enhances reusability of linguistic resources, because it facilitates the integration with other, theory-neutral resources. To some extent we have already explored this in MULI, combining e.g. Tiger annotation with discourse-level annotation. Another possibility to explore is the to integrate MULI annotation with, e.g., the SALSA corpus (Erk et al., 2003), which provides more detailed semantico-pragmatic information in the style of FrameNet. Our initial investigation also reveals where additional annotation would be needed. For instance, the text example discussed above constitutes a concession scheme, which we cannot identify without annotating discourse/rhetorical relations. This in turn requires extending the annotation scheme to non-nominal markables. Acknowledgements We would like to thank Saarland University for funding the MULI pilot project. Thanks also to Stella Neumann, Erich Steiner, Elke Teich, Stefan Baumann, Caren Brinckmann, Sil"
W04-0206,ide-etal-2000-xces,0,0.0815649,"Missing"
W04-0206,H94-1020,0,0.165868,"Missing"
W04-0206,W01-1612,0,0.077702,"antic attributes include, among others, animacy, ontological status, countability, quantiﬁcation and generic vs. speciﬁc reference, which reﬂect similar distinctions as we make in our annotation scheme. Besides the semantic properties that characterize discourse entities individually, our annotation scheme of course also covers referential relations between discourse entities, including both identity and bridging. We build on and extend the MUC-7 coreference speciﬁcation and the coreference/bridging classiﬁcations described in (Passoneau, 1996), (Carletta et al., 1997), (Poesio, 2000) and (M¨ uller and Strube, 2001). We represent anaphoric relations between linguistic expressions through links between the corresponding markables. The type of relation is annotated as an attribute of the markable corresponding to the anaphor. 3 Discourse-Level Annotation Information structure theories describe the phenomena at hand at a surface level, at a semantic level, or at both levels simultaneously, i.e., an expression belongs to some IS partition, in virtue of some information-status of the corresponding discourse entity. For the investigation of IS at the (discourse) semantic level, we thus need more information ab"
W04-0206,W03-2117,0,0.0295951,"was annotated with the EMU Speech Database System10 (Cassidy and Harrington, 2001a), which produces 10 http://emu.sourceforge.net/ Figure 2: Prosodic annotation of example sentence (2) in EMU several ﬁles in which time stamps are associated with the respective annotated labels. Syntactic Level For the syntactic annotation, we used the XML editor XML-Spy11 . The annotation scheme is deﬁned in a DTD, which is used to check the well-formedness and the validity of the annotation. Discourse Level The discourse-level annotation is done with the MMAX annotation tool developed at EML, Heidelberg (M¨ uller and Strube, 2003). MMAX is a light-weight tool written in Java that runs under both Windows and Unix/Linux. It supports multilevel annotation of XML-encoded data using annotation schemes deﬁned as DTDs. MMAX implements the above-mentioned general concepts of markables with attributes and standing in link relations to one another. To exploit and reuse annotated data in the MMAX format, there is the MMAX XML Discourse API. Integration The tools inevitably employ diﬀerent data formats: on the prosodic level data is stored in the EMU data format, on the syntactic level in Tiger XML and on the discourse level in MM"
W04-0206,A97-1014,0,0.235391,"Dependency Treebank (Bur´ an ˇ ov´ a et al., 2000). In contrast to other projects in which IS is annotated and investigated, we do not annotate theory-biased abstract categories like Topic-Focus or Theme-Rheme. Since we are particularly interested in the correlations and co-occurrences of features on diﬀerent linguistic levels that can be interpreted as indicators of the abstract IS categories, we needed an annotation scheme to be as theoryneutral as possible: It should allow for a description of the phenomena, from which ’any’ theoryspeciﬁc explanatory mechanisms can subsequently be derived (Skut et al., 1997). We therefore concentrate instead on features pertaining, on the one hand, to the surface realization of linguistic expressions (the levels of syntax and prosody), and, on the other hand, to the semantic character of the discourse referents (the discourse level). In designing our annotation schemes, we followed the guidelines of the Text Encoding Initiative3 and the Discourse Resource Initiative (Carletta et al., 1997). In line with these standards, we deﬁne for each annotation level (i) the markable expressions, (ii) the attributes of markables, and (iii) the links between markables (if any)"
W04-0911,P02-1041,0,0.0630751,"K((A ∪ B) ∩ (C ∪ D)), the list of bracketed sub-expressions includes: A∪B, C∪D, (A ∪ B) ∩ (C ∪ D), etc. Next, the pre-processed input is parsed with a lexically-based syntactic/semantic parser built on Multi-Modal Combinatory Categorial Grammar (Baldridge, 2002; Baldridge and Kruijff, 2003). The task of the deep parser is to produce an FGDbased linguistic meaning representation of syntactically well-formed sentences and fragments. The linguistic meaning is represented in the formalism of Hybrid Logic Dependency Semantics. Details on the semantic construction in this formalism can be found in (Baldridge and Kruijff, 2002). To derive our set of TRs we generalize and simplify the collection of Praguian tectogrammatical relations from (Hajiˇcov´a et al., 2000). One reason for simplification is to distinguish which relations are to be understood metaphorically given the domain-specific sub-language. The most commonly occurring relations in our context (aside from the roles of Actor and Patient) are Cause, Condition, and Result-Conclusion (which coincide with the rhetorical relations in the argumentative structure of the proof): Da [A ⊆ K(B) gilt]<CAUSE>, alle x, die in A sind sind nicht in B [As A⊆K(B) applies, al"
W04-0911,E03-1036,0,0.0982954,"re identified, verified as to syntactic validity, categorized, and substituted with default lexical entries encoded in the parser grammar for mathematical expression categories. For example, the expression K((A ∪ B) ∩ (C ∪ D)) = (K(A ∪ B) ∩ K(C ∪ D)) given its top node operator, =, is of type formula, its “left side” is the expression K((A ∪ B) ∩ (C ∪ D)), the list of bracketed sub-expressions includes: A∪B, C∪D, (A ∪ B) ∩ (C ∪ D), etc. Next, the pre-processed input is parsed with a lexically-based syntactic/semantic parser built on Multi-Modal Combinatory Categorial Grammar (Baldridge, 2002; Baldridge and Kruijff, 2003). The task of the deep parser is to produce an FGDbased linguistic meaning representation of syntactically well-formed sentences and fragments. The linguistic meaning is represented in the formalism of Hybrid Logic Dependency Semantics. Details on the semantic construction in this formalism can be found in (Baldridge and Kruijff, 2002). To derive our set of TRs we generalize and simplify the collection of Praguian tectogrammatical relations from (Hajiˇcov´a et al., 2000). One reason for simplification is to distinguish which relations are to be understood metaphorically given the domain-specif"
W04-0911,P04-1004,1,0.5032,"Missing"
W04-0911,wolska-etal-2004-annotated,1,0.842778,"Missing"
W04-0911,P03-1068,0,\N,Missing
W05-1624,W01-1607,0,0.049452,"Missing"
W06-2711,W05-1624,1,0.889632,"Missing"
W06-2711,kruijff-korbayova-etal-2006-sammie,1,0.841033,"Missing"
W06-2711,2005.sigdial-1.11,1,0.874647,"Missing"
W08-0129,P06-4015,1,0.896052,"Missing"
W08-1117,P06-4015,1,0.782701,"Missing"
W08-1117,2005.sigdial-1.20,0,0.378516,"1: SAMMIE system architecture. U: Play the third one. S: [song “From Me To You” plays] The system puts the user in control of the interaction. Input can be given through any modality and is not restricted to answers to system queries. On the contrary, the user can provide new tasks as well as any information relevant to the current task at any time. This is achieved through modeling the interaction as a collaborative problem solving (CPS) process, modeling the tasks and their progression as recipes and a multimodal interpretation that fits any user input into the context of the current task (Blaylock and Allen, 2005). To support dialogue flexibility, we model discourse context, the CPS state and the driver’s attention state by an enriched information state (Kruijff-Korbayov´a et al., 2006a). by selecting the content to present, distributing it appropriately over the available output modalities and finally co-ordinating and synchronizing the output. Modality-specific output modules generate spoken output and an update of the graphical display. All modules interact with the extended information state in which all context information is stored. Many tasks in the SAMMIE system are modeled by a rule-based appr"
W08-1117,W06-1405,0,0.129601,"ction 6 we provide a discussion and conclusions. 2 Previous Work Although recently developed dialogue systems adapt their output to the users in various ways, this 129 usually concerns content selection rather than surface realization. There is to our knowledge no system that varies the style of its output in the interpersonal dimension as we have done in SAM MIE . Work on animated conversational agents has addressed various issues concerning agents displaying their personality, but this usually concerns emotional states and personality traits, rather than the personal/impersonal alteration. (Isard et al., 2006) model personality and alignment in generated dialogues between pairs of agents using OpenCCG and an over-generation and ranking approach, guided by a set of language models. Their approach probably could produce the personal/impersonal style variation as an effect of personality or a side-effect of syntactic alignment. The question whether a system should generate output in personal or impersonal style has been addressed by (Nass and Brave, 2005): They observe that agents that use “I” are generally perceived more like a person than those that do not. However, systems tend to be more positivel"
W08-1117,W08-1117,1,0.0512899,"Missing"
W08-1117,kruijff-korbayova-etal-2006-sammie,1,0.256782,"Missing"
W09-0622,E91-1028,0,0.179782,"ext set. Else TAA1 moves up one level of abstraction and adds the set of all child nodes to the context set. This loop continues until r is in the context set. At that point TAA1 stops and returns the constructed context set (cf. Algorithm 1). TAA1 is formulated to be neutral to the kind of GRE algorithm that it is used for. It can be used with the original Incremental Algorithm (Dale and Reiter, 1995), augmented by a recursive call if a relation to another entity is selected as a discriminatory feature. It could in principle also be used with the standard approach to GRE involving relations (Dale and Haddock, 1991), but we agree with Paraboni et al. (2007) that the mutually qualified references that it can produce2 are not easily resolvable if they pertain to circumstances where a confirmatory search is costly (such as in largescale space). More recent approaches to avoiding infinite loops when using relations in GRE make use of a graph-based knowledge representation (Krahmer et al., 2003; Croitoru and van Deemter, 2007). TAA1 is compatible with these approaches, as well as with the salience based approach of (Krahmer and Theune, 2002). 2 An example for such a phenomenon is the expression “the ball on t"
W09-0622,P97-1027,0,0.710096,"reader, they function as attention-directing elements that are introduced to build up common ground by incrementally extending the hearer’s focus of attention. Moreover, representing some spatial relations as two-place predicates between two entities and some as oneplace predicates is an arbitrary decision. Most GRE approaches are applied to very limited, visual scenes – so-called small-scale space. The domain of such systems is usually a small visual scene, e.g. a number of objects, such as cups and tables, located in the same room), or other closed-context scenarios (Dale and Reiter, 1995; Horacek, 1997; Krahmer and Theune, 2002). Recently, Kelleher and Kruijff (2006) have presented an incremental GRE algorithm for situated dialogue with a robot about a table-top setting, i.e. also about small-scale space. In all these cases, the context set is assumed to be identical to the visual scene that is shared between the interlocutors. The intended referent is thus already in the hearer’s focus of attention. We present an approach for context determination (or extension), that imposes less restrictions on its knowledge base, and which can be used as a sub-routine in existing GRE algorithms. 127 3 4"
W09-0622,P06-1131,1,0.797324,"ts that are introduced to build up common ground by incrementally extending the hearer’s focus of attention. Moreover, representing some spatial relations as two-place predicates between two entities and some as oneplace predicates is an arbitrary decision. Most GRE approaches are applied to very limited, visual scenes – so-called small-scale space. The domain of such systems is usually a small visual scene, e.g. a number of objects, such as cups and tables, located in the same room), or other closed-context scenarios (Dale and Reiter, 1995; Horacek, 1997; Krahmer and Theune, 2002). Recently, Kelleher and Kruijff (2006) have presented an incremental GRE algorithm for situated dialogue with a robot about a table-top setting, i.e. also about small-scale space. In all these cases, the context set is assumed to be identical to the visual scene that is shared between the interlocutors. The intended referent is thus already in the hearer’s focus of attention. We present an approach for context determination (or extension), that imposes less restrictions on its knowledge base, and which can be used as a sub-routine in existing GRE algorithms. 127 3 4 Situated Dialogue in Large-Scale Space building1 Imagine the situ"
W09-0622,J03-1003,0,0.325722,"Missing"
W09-0622,J07-2004,0,0.201449,"Missing"
W19-5946,J97-1002,0,0.661135,"Missing"
W19-5946,H91-1064,0,0.631452,"Missing"
W19-5946,D18-1547,0,0.0481311,"Missing"
W19-5946,bunt-etal-2012-iso,0,0.297724,"Missing"
W19-5946,P99-1030,0,0.442336,"Missing"
W19-5946,W17-7404,0,0.275469,"Missing"
W19-5946,L16-1020,0,0.0467538,"Missing"
W19-5946,L16-1503,0,0.0737316,"Missing"
W19-5946,W17-2626,0,0.048891,"Missing"
W19-5946,W04-2319,0,0.613597,"Missing"
W19-5946,J00-3003,0,0.574311,"Missing"
W19-5946,D17-1231,0,0.0299495,"Missing"
W19-5946,N04-4025,0,0.166135,"Missing"
W19-5947,I17-1074,0,0.370746,"in the conversation. One way to model conversation is as a partially observable Markov decision process (Young et al., 2013). In this framework system response generation is modeled as a stochastic policy, and research into statistically optimizing dialogue policies with Reinforcement Learning (RL) is an active area of research (Gasic and Young, 2014; Lemon and Pietquin, 2007). However, learning optimal dialogue policies with RL can be challenging since large state and action spaces require large amounts of training data to densely sample the space (Lemon and Pietquin, 2007; Wen et al., 2016; Li et al., 2017). Additionally, networks trained with RL learn in a trial-anderror process, guided by a potentially delayed reward function. This exploration process can lead to poor performance in the early training stages, which in turn can lead to a negative user experience (Su et al., 2016). To address these issues supervised learning has been used for pre-training of dialogue policies (Su 411 Proceedings of the SIGDial 2019 Conference, pages 411–417 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Corpus ATIS Frames COMM ATT COMM BBN COMM CMU COMM SRI # Slots 79 2"
W19-5947,P15-1166,0,0.0872636,"Missing"
W19-5947,E17-1052,0,0.0314196,"en et al. (2016) treat dialogue as a sequence to sequence mapping problem and design a dialogue manager where each component is modularly connected and trainable from data. Previous work also learns state-tracking and other NLU tasks simultaneously. Hakkani-Tur et al. (2016) use a bi-directional LSTM to jointly model slot filling, intent determination, and domain classification for different domains. Chen et al. (2016) use a knowledge-guided structural attention network (K-SAN) to model intent prediction and slot filling simultaneously. Both published results on the ATIS corpus (Price, 1990). Padmakumar et al. (2017) train a semantic parser and policy network in batches, giving the policy network access to the updated semantic parser after every batch. Zhao and Eskenazi (2016) jointly learn policies for state tracking and dialogue strategies using Deep Recurrent Q-Network (DRQN). Li et al. (2017) use a single RNN with LSTM to jointly learn user intent as well as slot filling. Their dialogue manager is initialized by supervised learning of labels generated by a rule system, then end-to-end train3 Data We evaluated our models on three corpora: the Maluuba Frames (El Asri et al., 2017), DARPA COMMUNICATOR (G"
W19-5947,W17-5526,0,0.0446285,"Missing"
W19-5947,D14-1162,0,0.0864704,"rk of three convolutional layers with different filter widths each followed by max pooling. This is then connected to the BLSTM1 architecture. The goal was to explore the possibility of extracting features with a CNN layer that could then be used by the BLSTM1 network. 4.2 Training All network development and training was done in Keras (Chollet et al., 2015) and the code will be released with the final version of this paper. We experimented with batch sizes of 15, 25, 50 and 100, hidden layers of 25, 50 and 100 units, and drop-out ratios of 0,0.25, and 0.5 on the fullyconnected layers. GloVe (Pennington et al., 2014) word embeddings were used as pre-trained word embeddings. The Adam optimizer was used with a learning rate of 0.001. All weights were initialized with glorot uniform. The BLSTM layers used tanh as the activation function. During training the validation loss was monitored and early stopping was used to prevent over-fitting. The CNN1 network design was inspired by Yoon (2014) and is illustrated in Figure 1b. This network uses 4 filters of different widths each followed by max pooling over time. Filter widths, the number of feature maps, and the number of nodes in the fully connected layer were"
W19-5947,H90-1020,0,0.265276,"gue systems: Wen et al. (2016) treat dialogue as a sequence to sequence mapping problem and design a dialogue manager where each component is modularly connected and trainable from data. Previous work also learns state-tracking and other NLU tasks simultaneously. Hakkani-Tur et al. (2016) use a bi-directional LSTM to jointly model slot filling, intent determination, and domain classification for different domains. Chen et al. (2016) use a knowledge-guided structural attention network (K-SAN) to model intent prediction and slot filling simultaneously. Both published results on the ATIS corpus (Price, 1990). Padmakumar et al. (2017) train a semantic parser and policy network in batches, giving the policy network access to the updated semantic parser after every batch. Zhao and Eskenazi (2016) jointly learn policies for state tracking and dialogue strategies using Deep Recurrent Q-Network (DRQN). Li et al. (2017) use a single RNN with LSTM to jointly learn user intent as well as slot filling. Their dialogue manager is initialized by supervised learning of labels generated by a rule system, then end-to-end train3 Data We evaluated our models on three corpora: the Maluuba Frames (El Asri et al., 20"
W19-5947,J86-3001,0,0.675872,"Missing"
W19-5947,C00-2137,0,0.134322,"with multiple LSTM and BLSTM models, but noted that comparable results were achieved on each and therefore only report results on the BLSTM models. We do the same and only report on experiments with their BLSTM architecture. The results on the ATIS corpus are the metrics reported by the authors (and confirmed by us). For each corpus many of the multi-task models achieved a higher metric score than the Baseline B models on the test data, however significance testing showed not all of these improvements were statistically significant. Significance testing was done with randomized approximation (Yeh, 2000). Table 3 lists the best F-measure values for each model for the primary task of system action selection. The majority of the multi-task models, as well as the Baseline B models on the Frames, BBN, and SRI corpora, achieved a higher F-measure than the Baseline A models. (We did not test for statistical significance between the MTL models and the Baseline A). The multi-task CNN models showed statistically significant improvement on three data sets and were faster to train than the BLSTM models, even when larger. Half of the BLSTM models achieved significant improvement on the Frames corpus, but"
W19-5947,D14-1181,0,0.00948762,"with the final version of this paper. We experimented with batch sizes of 15, 25, 50 and 100, hidden layers of 25, 50 and 100 units, and drop-out ratios of 0,0.25, and 0.5 on the fullyconnected layers. GloVe (Pennington et al., 2014) word embeddings were used as pre-trained word embeddings. The Adam optimizer was used with a learning rate of 0.001. All weights were initialized with glorot uniform. The BLSTM layers used tanh as the activation function. During training the validation loss was monitored and early stopping was used to prevent over-fitting. The CNN1 network design was inspired by Yoon (2014) and is illustrated in Figure 1b. This network uses 4 filters of different widths each followed by max pooling over time. Filter widths, the number of feature maps, and the number of nodes in the fully connected layer were chosen based on the suggestions of Zhang and Wallace (2015). Early experiments on the BLSTM networks showed a potential benefit to using userintent classification alone as an auxiliary task, therefore these experiments used only user-intent classification as the auxiliary task. We also conducted experiments with networks inspired by Google’s Inception architecture (Szegedy e"
W19-5947,W16-3601,0,0.0321676,"rom data. Previous work also learns state-tracking and other NLU tasks simultaneously. Hakkani-Tur et al. (2016) use a bi-directional LSTM to jointly model slot filling, intent determination, and domain classification for different domains. Chen et al. (2016) use a knowledge-guided structural attention network (K-SAN) to model intent prediction and slot filling simultaneously. Both published results on the ATIS corpus (Price, 1990). Padmakumar et al. (2017) train a semantic parser and policy network in batches, giving the policy network access to the updated semantic parser after every batch. Zhao and Eskenazi (2016) jointly learn policies for state tracking and dialogue strategies using Deep Recurrent Q-Network (DRQN). Li et al. (2017) use a single RNN with LSTM to jointly learn user intent as well as slot filling. Their dialogue manager is initialized by supervised learning of labels generated by a rule system, then end-to-end train3 Data We evaluated our models on three corpora: the Maluuba Frames (El Asri et al., 2017), DARPA COMMUNICATOR (Georgila et al., 2009, 2005) and ATIS (Price, 1990) data sets. The Frames corpus is a collection of human-human dialogues that captures realistic behaviors in natur"
W19-5947,N16-1004,0,0.0193365,"e tested, and the results of the evaluation. 2 Related work Multi-Task Learning: In MTL the training signals of related tasks are used to learn features that are relevant to multiple tasks, including a primary task of interest. In learning these shared features the model learns a representation that improves generalization on that primary task. Caruana (1998) and Zhang and Yang (2017) describe a number of tasks where the shared representation learned with MTL improves generalization. MTL has also been shown to improve a number NLP tasks (Toshniwal et al., 2017; Arik et al., 2017; Dong et al.; Zoph and Knight, 2016; Johnson et al., 2016). See Ruder (2017) and Zhang and Yang (2017) for additional examples of MTL for NLP. Goal-Oriented Dialogue systems: Wen et al. (2016) treat dialogue as a sequence to sequence mapping problem and design a dialogue manager where each component is modularly connected and trainable from data. Previous work also learns state-tracking and other NLU tasks simultaneously. Hakkani-Tur et al. (2016) use a bi-directional LSTM to jointly model slot filling, intent determination, and domain classification for different domains. Chen et al. (2016) use a knowledge-guided structural at"
wolska-etal-2004-annotated,W03-2117,0,\N,Missing
wolska-etal-2004-annotated,P02-1041,0,\N,Missing
