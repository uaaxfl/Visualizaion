1992.tmi-1.1,C88-1007,0,0.0236387,"xample, an acceptable translation for the English sentence John swam across the river in Spanish would be Juan cruzó el río nadando where English swim across NP is translationally equivalent to cruzar NP nadando (&apos;cross NP swimming&apos;; Beaven, 1992). Translation mismatches of this complexity present difficulties with respect to both lexical representation and generation. Because the mismatch in this case arises from diverging regimes of lexicalization, 1 it would be desirable to state the equivalence at the lexical level; this can be done using lexical transfer techniques, e.g. bilingual signs (Beaven & Whitelock 1988; Whitelock, 1988; Zajac, 1989; Estival et al., 1990; Tsujii, 1991). At the same time, one side of the equivalence (the Spanish side in our example) involves a phrase with a &apos;gap&apos; inside (i.e. the &apos;goal&apos; argument) which can only be filled after the input source-language string is analyzed. For generation purposes, it would thus be more convenient to establish the translation equivalence through structural correspondences which relate phrasal descriptions in the source and target languages, following the treatment of head switching mismatches proposed by Kaplan et al. (1989). However, the use o"
1992.tmi-1.1,P91-1028,0,0.061436,"Missing"
1992.tmi-1.1,A92-1012,1,0.915133,"specific lexical semantic classes which are unmotivated from the perspective of the monolingual grammars. The goal of this paper is to present a strongly lexicalist approach to translation equivalence where lexical transfer can be made to drive generation without direct reference to phrasal transfer. 2 Background Within the ACQUILEX project, 1 we are testing the feasibility of constructing a multilingual Lexical Knowledge Base (LKB) for a large subset of nouns and verbs using monolingual lexicons semiautomatically derived from English, Spanish, Dutch and Italian machine-readable dictionaries (Copestake, 1992; Sanfilippo & Poznański, 1992; Ageno et al, 1992; Vossen, 1992; Calzolari, 1991). The ACQUILEX LKB provides a lexicon development environment which uses a typed graph-based unification formalism as representation language.2 It allows the user to define an inheritance network of types with associated features, and to create lexicons where such types are semi-automatically assigned to lexical templates which encode word-sense specific information extracted from machine-readable dictionaries. Consider, for example, the LKB entry relative to the first sense of the verb swim in the Longman Diction"
1992.tmi-1.1,P90-1017,0,0.160824,"s of thematic information (Dowty, 1991; Jackendoff, 1990) within a neo-Davidsonian approach to verb semantics (Sanfilippo, 1990, 1992). A box around a type as in the case of np-cat in Figure 1 indicates that the feature structure associated with the type has been shrunk to ease graphical representation. 3 Figure 2: LKB entries for the Italian and Spanish translations of swim In principle, the use of a common type system in the domain of semantic representation could be made to provide the kind of conceptual representation which is used in interlingual approaches to MT (Lytinen & Schank, 1982; Dorr, 1990). This is not the case, however, in the ACQUILEX LKB where semantic decomposition is only partially executed; for example, language-specific predicates (i.e. names of word senses such as swim_l_1_1) are still needed to differentiate word meanings. Consequently, our treatment makes it possible to exploit some of the advantages of an interlingual approach without a specific commitment to expressing all aspects of word meaning in terms of a language-independent semantic representation. While use of an interlingual semantic representation is appealing in that it should be the best solution to the"
1992.tmi-1.1,E89-1037,0,0.0768854,"bilingual signs (Beaven & Whitelock 1988; Whitelock, 1988; Zajac, 1989; Estival et al., 1990; Tsujii, 1991). At the same time, one side of the equivalence (the Spanish side in our example) involves a phrase with a &apos;gap&apos; inside (i.e. the &apos;goal&apos; argument) which can only be filled after the input source-language string is analyzed. For generation purposes, it would thus be more convenient to establish the translation equivalence through structural correspondences which relate phrasal descriptions in the source and target languages, following the treatment of head switching mismatches proposed by Kaplan et al. (1989). However, the use of phrasal transfer to cope with lexically governed mismatches requires the creation of specialised equivalents of phrase structure rules restricted to specific lexical semantic classes which are unmotivated from the perspective of the monolingual grammars. The goal of this paper is to present a strongly lexicalist approach to translation equivalence where lexical transfer can be made to drive generation without direct reference to phrasal transfer. 2 Background Within the ACQUILEX project, 1 we are testing the feasibility of constructing a multilingual Lexical Knowledge Bas"
1992.tmi-1.1,A92-1011,1,0.901248,"semantic classes which are unmotivated from the perspective of the monolingual grammars. The goal of this paper is to present a strongly lexicalist approach to translation equivalence where lexical transfer can be made to drive generation without direct reference to phrasal transfer. 2 Background Within the ACQUILEX project, 1 we are testing the feasibility of constructing a multilingual Lexical Knowledge Base (LKB) for a large subset of nouns and verbs using monolingual lexicons semiautomatically derived from English, Spanish, Dutch and Italian machine-readable dictionaries (Copestake, 1992; Sanfilippo & Poznański, 1992; Ageno et al, 1992; Vossen, 1992; Calzolari, 1991). The ACQUILEX LKB provides a lexicon development environment which uses a typed graph-based unification formalism as representation language.2 It allows the user to define an inheritance network of types with associated features, and to create lexicons where such types are semi-automatically assigned to lexical templates which encode word-sense specific information extracted from machine-readable dictionaries. Consider, for example, the LKB entry relative to the first sense of the verb swim in the Longman Dictionary of Contemporary English (P"
1992.tmi-1.1,E91-1048,0,0.0291036,"Missing"
1992.tmi-1.1,P89-1001,0,0.0186365,"English sentence John swam across the river in Spanish would be Juan cruzó el río nadando where English swim across NP is translationally equivalent to cruzar NP nadando (&apos;cross NP swimming&apos;; Beaven, 1992). Translation mismatches of this complexity present difficulties with respect to both lexical representation and generation. Because the mismatch in this case arises from diverging regimes of lexicalization, 1 it would be desirable to state the equivalence at the lexical level; this can be done using lexical transfer techniques, e.g. bilingual signs (Beaven & Whitelock 1988; Whitelock, 1988; Zajac, 1989; Estival et al., 1990; Tsujii, 1991). At the same time, one side of the equivalence (the Spanish side in our example) involves a phrase with a &apos;gap&apos; inside (i.e. the &apos;goal&apos; argument) which can only be filled after the input source-language string is analyzed. For generation purposes, it would thus be more convenient to establish the translation equivalence through structural correspondences which relate phrasal descriptions in the source and target languages, following the treatment of head switching mismatches proposed by Kaplan et al. (1989). However, the use of phrasal transfer to cope wit"
1995.iwpt-1.8,P89-1018,0,0.0134528,"eebanked corpus of transcribed British radio pro grammes punctuated by the corpus compilers. Both corpora were retagged with determinate punctuation and PoS labelling using _the Acquilex HMM tagger (Elworthy, 1993, 1994) trained on text tagged with a slightly modified version of CLAWS-II labels (Garside et al. , 1987) . 5.1 Coverage and Average Ambiguity To examine the efficiency and coverage of the grammar we applied it to our retagged versions of Susanne and SEC. We used the ANLT chart parser (Carroll, 1 993) , but modified just to count th_e number of possible parses in the parse forests (Billot & Lang, 1989) rather than actually · unpacking them. We also imposed a per-sentence time-out of 30 seconds CPU time, running in Franz· Allegro Common Lisp 4.2 on an HP PA-RISC 715/100 workstation with 96 Mbytes of physical memory. Ve define the &apos;coverage&apos; of the grammar to be the inverse of the proportion of sentences for which no analysis was found-a weak measure since discovery of one or more global analyses does not entail that the correct analysis i� recovered. For both corpora, the majority of sentences analysed successfully received under 100 parses, although there is a long tail in the distribution"
1995.iwpt-1.8,P94-1040,1,0.838779,"that improved coverage of heterogeneous text can be achieved by exploiting textual and grammatical con straints on PoS and punctuation sequences. The experiments show that grammatical coverage can be greatly increased by relaxing subcategorisation constraints, and that text grammatical or punctuation-cued constraints can reduce ambiguity and increase coverage during parsing. To our knowledge these are the first experiments which objectively demonstrate the utility of punctuation for resolving syntactic ambiguity and improving parser coverage. They extend work by Jones ( 1994) and Briscoe and Carroll (1994) by applying a wide-coverage text grammar to substantial quantities of naturally-punctuated text and by quantifying the contribution of punctuation to ambiguity resolution in a well-defined probabilistic parse selection model. Accurate enough parse selection for practical applications will require a more lexic.alised system. Magerman&apos;s ( 1995) parser is an extension of the history-based parsing approach devel oped at IBM (e.g. Black, 1993) in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history. In future work, we intend to explor"
1995.iwpt-1.8,A88-1019,0,0.0430331,"on subcategorisation and uses punctuation to reduce ambiguity. The analyses produced by this parser could be utilised for phrase-finding applications, recovery of subcategorisation frames, and other &apos;intermediate&apos; level parsing problems. 2 Part-of-speech Tag Sequence Grammar Several robust parsing systems exploit the comparative success of part-of-speech (PoS) taggers, such as Fidditch (Hindle, 1989) or MITFP (de · Marcken, 1990) , by reducing the input to a determinate sequence of extended PoS labels of the type which can be practically disambiguated in context using a (H)MM PoS tagger (e.g. Church, 1988) . Such approaches, by definition, cannot exploit subcategorisation, and probably achieve some of their robustness as a result. However, such parsers typically also employ heuristic rules, such as &apos;low&apos; attachment of PPs to produce unique &apos;canonical&apos; analyses. This latter step complicates the recovery of predicate argument structure and does not integrate with a probabilistic approach to parsing. We utilised the ANLT metagrammatical formalism to develop a feature-based, declara tive description of PoS label sequences for English. This grammar compiles into a DCG-like grammar of approximately"
1995.iwpt-1.8,C94-1042,0,0.046863,"Missing"
1995.iwpt-1.8,P90-1031,0,0.0200749,"Missing"
1995.iwpt-1.8,E93-1040,0,0.0546539,"Missing"
1995.iwpt-1.8,A94-1009,0,\N,Missing
1995.iwpt-1.8,P89-1015,0,\N,Missing
1995.iwpt-1.8,J93-1002,1,\N,Missing
1995.iwpt-1.8,P95-1037,0,\N,Missing
2020.codi-1.11,P05-1018,0,0.0977503,"e coherence models. 1 Introduction Coherence refers to the properties of a text that indicate how meaningful (sub-)sentential constituents are connected to convey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textu"
2020.codi-1.11,J08-1001,0,0.166317,"Missing"
2020.codi-1.11,N04-1015,0,0.129221,"s (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed"
2020.codi-1.11,N10-1099,0,0.0647927,"Missing"
2020.codi-1.11,D16-1245,0,0.0682306,"Missing"
2020.codi-1.11,P18-1198,0,0.351309,"51.6 61.6 86.6 83.3 76.6 86.6 50.0 53.3 53.3 60.0 64.2 71.4 66.0 69.1 LCDbert 75.4 96.7 71.0 94.8 86.6 78.3 86.6 93.3 80.0 76.6 82.8 72.2 Egridcnn 84.6 88.1 53.4 68.8 83.3 71.6 76.6 80.0 53.3 56.6 70.4 65.8 Table 5: PRA performance on the CLA (bottom) and CC datasets (top; ‘fine-tuned’ shows results for models tuned on the respective cloze training sets). Task SubjNum ObjNum CoordInv CorruptAgr MTL 64.9 64.5 58.5 53.2 MTLbert 75.4 72.1 63.4 69.7 STL 62.2 61.1 53.0 57.7 STLbert 71.5 70.7 63.7 68.6 LC 52.7 54.5 53.0 52.2 Models LCDrnnlm 71.2 65.0 56.6 64.2 LCDbert 88.0 86.5 78.4 94.3 Best from Conneau et al. (2018) 95.1 (Seq2Tree) 95.1 (Seq2Tree) 76.2 (NMT En-De) - Human 88.0 86.5 85.0 - Table 6: Classification accuracy on probing tasks. ‘Human’ shows the human upper bound on the task. However, the exception is LCDbert (with PRA 80 on lexical perturbations and 76.6 on corrupt pronoun) suggesting a better ability at capturing semantics and resolving references. Across all six CLA datasets (‘All data’; Table 5), we find that, overall, LCDbert is the top performing model (average PRA). The ‘All data’ row reports the result of comparing a coherent example against its incoherent counterparts across the diffe"
2020.codi-1.11,D18-1465,0,0.0147804,"vantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019). However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence. In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence an"
2020.codi-1.11,N19-1423,0,0.0310514,"word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the network as an auxiliary task. MTL with BERT embeddings (MTLbert ): We replicate the previous MTL model but now use BERT embeddings (Devlin et al., 2019) to initialize the input words. Single-task learning (STL, Farag and Yannakoudakis, 2019): This model has the same architecture as MTL but only performs the coherence prediction task, excluding the grammatical role auxiliary objective. STL with BERT (STLbert ): This is the same as STL but uses BERT embeddings. Local Coherence Discriminator with Language modeling (LCDrnnlm , Xu et al., 2019): The model generates sentence representations via an RNN language model, where word embeddings are initialized using GloVe. It then generates a representation for two consecutive sentences via concatenating"
2020.codi-1.11,N07-1055,0,0.054098,"e, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created"
2020.codi-1.11,P11-2022,0,0.0260707,"ve been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether th"
2020.codi-1.11,P19-1060,1,0.790353,"2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li"
2020.codi-1.11,C14-1089,0,0.0172828,"1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis,"
2020.codi-1.11,W07-2321,0,0.0630942,"onvey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Othe"
2020.codi-1.11,J95-2003,0,0.907072,"Missing"
2020.codi-1.11,P13-1010,0,0.0192766,"e the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner e"
2020.codi-1.11,N19-1419,0,0.0209616,"elate to aspects of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.1 2 Neural Coherence Models We experiment with a number of existing and stateof-the-art neural approaches to coherence assessment, that have publicly available implementations, and present details of the models below. Across all the BERT-based models, we use bert-large-uncased and layer 16 following Liu et al. (2019) and Hewitt and Manning (2019). Multi-task learning (MTL, Farag and Yannakoudakis, 2019): The model applies a Bi-LSTM on input GloVe word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the network as an auxiliary task."
2020.codi-1.11,P18-1052,0,0.0238123,"l., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering th"
2020.codi-1.11,W18-5023,0,0.024545,"representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019). However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence. In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence and analyze model ability to capture syntactic and semant"
2020.codi-1.11,D17-1019,0,0.0762258,"tual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 20"
2020.codi-1.11,P11-1100,0,0.0573953,"Missing"
2020.codi-1.11,Q16-1037,0,0.0828633,"Missing"
2020.codi-1.11,D12-1106,0,0.150333,"al models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence."
2020.codi-1.11,D19-1231,0,0.0404688,"fts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019)."
2020.codi-1.11,N16-1098,0,0.0616619,"Missing"
2020.codi-1.11,D14-1162,0,0.0900615,"Missing"
2020.codi-1.11,C14-1090,0,0.0197233,"e assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et a"
2020.codi-1.11,P06-2103,0,0.0821693,"izes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguy"
2020.codi-1.11,N19-1112,0,0.0223964,"ons and how it might relate to aspects of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.1 2 Neural Coherence Models We experiment with a number of existing and stateof-the-art neural approaches to coherence assessment, that have publicly available implementations, and present details of the models below. Across all the BERT-based models, we use bert-large-uncased and layer 16 following Liu et al. (2019) and Hewitt and Manning (2019). Multi-task learning (MTL, Farag and Yannakoudakis, 2019): The model applies a Bi-LSTM on input GloVe word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the"
2020.codi-1.11,P17-1121,0,0.261038,"2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks s"
2020.codi-1.11,P19-1067,0,0.0694207,"pturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 201"
A97-1052,P87-1027,1,0.698428,"ons Programme project LE1-211i 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering', and by SERC/EPSRC Advanced Fellowships to both authors. We would like to thank the COMLEX Syntax development team for allowing us access to pre-release data (for an early experiment), and for useful feedback. 356 Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency"
A97-1052,P91-1027,0,0.37465,"ized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few from sentence subanalyses which begin/end at the boundaries of (specified) predicates. verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et al. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classes--a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which de"
A97-1052,J93-2002,0,0.87711,"parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis"
A97-1052,1995.iwpt-1.8,1,0.645459,"Missing"
A97-1052,P94-1040,1,0.810599,"lace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the G A T E project stemmer (Cunningham et al., 1995). 3. A p r o b a b i l i s t i c L R p a r s e r , trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994, 1995; Carroll & Briscoe, 1996). 4. A p a t t e r n s e t e x t r a c t o r which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, 357 (lb) is parsed successfully by the probabilistic LR parser, and the ranked analyses are returned. Then the patternset extractor locates the subanalyses containing attribute and constructs a patternset. The highest ranked analysis and pattern for this example are shown in Figure 12 . Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument. In the case of P P"
A97-1052,W96-0209,1,0.646554,"ith lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the G A T E project stemmer (Cunningham et al., 1995). 3. A p r o b a b i l i s t i c L R p a r s e r , trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994, 1995; Carroll & Briscoe, 1996). 4. A p a t t e r n s e t e x t r a c t o r which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, 357 (lb) is parsed successfully by the probabilistic LR parser, and the ranked analyses are returned. Then the patternset extractor locates the subanalyses containing attribute and constructs a patternset. The highest ranked analysis and pattern for this example are shown in Figure 12 . Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument. In the case of P P (I)2) arguments, the pattern al"
A97-1052,P90-1031,0,0.0811131,"Missing"
A97-1052,A94-1009,0,0.0263756,"ted his failure, he said, to no< blank> one buying his books. b he_PPHS1 attribute_VVD his_APP$ failure_NN1 ,_, he_PPHS1 say_VVD ,_, to_II no<blank>one_PN buy_ VVG his_APP$ book_NN2 System 2.1 O v e r v i e w The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: 1. A t a g g e r , a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994). 2. A l e m m a t i z e r is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the G A T E project stemmer (Cunningham et al., 1995). 3. A p r o b a b i l i s t i c L R p a r s e r , trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'latt"
A97-1052,C94-1042,0,0.653301,"ng and Knowledge extraction for Language Engineering', and by SERC/EPSRC Advanced Fellowships to both authors. We would like to thank the COMLEX Syntax development team for allowing us access to pre-release data (for an early experiment), and for useful feedback. 356 Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)langu"
A97-1052,A92-1022,0,0.018785,"Missing"
A97-1052,C92-2099,0,0.0314067,"we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences"
A97-1052,P93-1032,0,0.782191,"grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few from sentence subanalyses which begin/end at the boundaries of (specified) predicates. verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et al. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classes--a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which demonstrates that this sy"
A97-1052,W93-0108,0,0.0636711,"filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raisin"
A97-1052,C94-2123,0,0.0295959,"using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1"
A97-1052,C92-2066,0,0.0177604,"and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed 'lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few from sentence subanalyses which begin/end at the boundaries of (specified) predicates. verbal subcategorizat"
A97-1052,W93-0109,0,0.74354,"the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few from sentence subanalyses which begin/end at the boundaries of (specified) predicates. verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et al. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classes--a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which demonstrates that this system is capable of acquirin"
A97-1052,J87-3002,1,\N,Missing
A97-1052,H91-1067,0,\N,Missing
A97-1052,E95-1016,0,\N,Missing
A97-1052,A92-1011,0,\N,Missing
A97-1052,J93-1002,1,\N,Missing
andersen-etal-2008-bnc,W07-1022,0,\N,Missing
andersen-etal-2008-bnc,P06-4020,1,\N,Missing
andersen-etal-2008-bnc,C94-1103,0,\N,Missing
andersen-etal-2008-bnc,P06-2006,1,\N,Missing
briscoe-carroll-2002-robust,1995.iwpt-1.8,1,\N,Missing
briscoe-carroll-2002-robust,A00-2018,0,\N,Missing
briscoe-carroll-2002-robust,A00-2022,1,\N,Missing
briscoe-carroll-2002-robust,J93-2006,0,\N,Missing
briscoe-carroll-2002-robust,W98-1114,1,\N,Missing
briscoe-carroll-2002-robust,A94-1009,0,\N,Missing
briscoe-carroll-2002-robust,J02-2003,0,\N,Missing
briscoe-carroll-2002-robust,P00-1017,0,\N,Missing
briscoe-carroll-2002-robust,J03-4003,0,\N,Missing
briscoe-carroll-2002-robust,P01-1034,0,\N,Missing
briscoe-carroll-2002-robust,P99-1061,1,\N,Missing
briscoe-carroll-2002-robust,P90-1031,0,\N,Missing
briscoe-carroll-2002-robust,P98-2247,0,\N,Missing
briscoe-carroll-2002-robust,C98-2242,0,\N,Missing
briscoe-carroll-2002-robust,grover-etal-2000-lt,0,\N,Missing
briscoe-carroll-2002-robust,J93-1002,1,\N,Missing
briscoe-carroll-2002-robust,P94-1040,1,\N,Missing
briscoe-carroll-2002-robust,W01-1808,1,\N,Missing
briscoe-carroll-2002-robust,P01-1019,0,\N,Missing
C02-1013,W97-0810,0,0.0752176,"Missing"
C02-1013,P98-1010,0,0.0381019,"Missing"
C02-1013,A00-2031,0,0.0712413,"Missing"
C02-1013,W97-0307,0,0.0581427,"Missing"
C02-1013,J93-2002,0,0.0371654,"ss than one, and so will not receive full credit from the weighted precision and recall measures. However, these results only tell part of the story. An application using grammatical relation analyses might be interested only in GRs that the parser is fairly confident of being correct. For instance, in unsupervised acquisition of lexical information (such as subcategorisation frames for verbs) from text, the usual methodology is to (partially) analyse the text, retaining only reliable hypotheses which are then filtered based on the amount of evidence for them over the corpus as a whole. Thus, Brent (1993) only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) so there can be no other attachment possibilities. In recent work on unsupervised learning of prepositional phrase disambiguation, Pantel and Lin (2000) derive training instances only from relevant data appearing in syntactic contexts that are guaranteed to be unambiguous. In our system, the weights on GRs indicate how certain the parser is of the associated relations being correct. We therefore investigated whether more highly weighted GRs are in"
C02-1013,A97-1052,1,0.825922,"ched using lexicalised probabilistic models over headdependent tuples. Bouma, van Noord and Malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection. Lin (1998), Srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against ‘gold standard’ relations, rather than matching (labelled) phrase structure bracketings. Research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (Briscoe and Carroll, 1997; McCarthy, 2000), word classes for disambiguation (Clark and Weir, 2001), and collocations (Lin 1999), has used grammatical relation/head/dependent tuples. Such 1 A previous version of this paper was presented at IWPT’01; this version contains new experiments and results. Ted Briscoe Computer Laboratory University of Cambridge JJ Thomson Avenue Cambridge CB3 0FD, UK tuples also constitute a convenient intermediate representation in applications such as information extraction (Palmer et al., 1993; Yeh, 2000), and document retrieval on the Web (Grefenstette, 1997). A variety of different approa"
C02-1013,W99-0629,0,0.0465788,"Missing"
C02-1013,W98-1114,1,0.91687,"Missing"
C02-1013,N01-1013,0,0.0112118,"van Noord and Malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection. Lin (1998), Srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against ‘gold standard’ relations, rather than matching (labelled) phrase structure bracketings. Research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (Briscoe and Carroll, 1997; McCarthy, 2000), word classes for disambiguation (Clark and Weir, 2001), and collocations (Lin 1999), has used grammatical relation/head/dependent tuples. Such 1 A previous version of this paper was presented at IWPT’01; this version contains new experiments and results. Ted Briscoe Computer Laboratory University of Cambridge JJ Thomson Avenue Cambridge CB3 0FD, UK tuples also constitute a convenient intermediate representation in applications such as information extraction (Palmer et al., 1993; Yeh, 2000), and document retrieval on the Web (Grefenstette, 1997). A variety of different approaches have been taken for robust extraction of relation/head/dependent tup"
C02-1013,P99-1061,1,0.911215,"Missing"
C02-1013,P99-1041,0,0.0121153,"dency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection. Lin (1998), Srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against ‘gold standard’ relations, rather than matching (labelled) phrase structure bracketings. Research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (Briscoe and Carroll, 1997; McCarthy, 2000), word classes for disambiguation (Clark and Weir, 2001), and collocations (Lin 1999), has used grammatical relation/head/dependent tuples. Such 1 A previous version of this paper was presented at IWPT’01; this version contains new experiments and results. Ted Briscoe Computer Laboratory University of Cambridge JJ Thomson Avenue Cambridge CB3 0FD, UK tuples also constitute a convenient intermediate representation in applications such as information extraction (Palmer et al., 1993; Yeh, 2000), and document retrieval on the Web (Grefenstette, 1997). A variety of different approaches have been taken for robust extraction of relation/head/dependent tuples, or grammatical relations"
C02-1013,A00-2034,0,0.0125027,"abilistic models over headdependent tuples. Bouma, van Noord and Malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection. Lin (1998), Srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against ‘gold standard’ relations, rather than matching (labelled) phrase structure bracketings. Research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (Briscoe and Carroll, 1997; McCarthy, 2000), word classes for disambiguation (Clark and Weir, 2001), and collocations (Lin 1999), has used grammatical relation/head/dependent tuples. Such 1 A previous version of this paper was presented at IWPT’01; this version contains new experiments and results. Ted Briscoe Computer Laboratory University of Cambridge JJ Thomson Avenue Cambridge CB3 0FD, UK tuples also constitute a convenient intermediate representation in applications such as information extraction (Palmer et al., 1993; Yeh, 2000), and document retrieval on the Web (Grefenstette, 1997). A variety of different approaches have been ta"
C02-1013,A00-2022,1,0.793332,"Missing"
C02-1013,P00-1014,0,0.016652,"Missing"
C02-1013,P01-1060,0,0.169153,"Missing"
C02-1013,P00-1017,0,0.0358537,"Missing"
C02-1013,J03-4003,0,\N,Missing
C08-1033,W06-3316,1,0.834065,"hich defines the discourse new scores. We reckon that the considerable drop on recall numbers for the associative cases would make the system less viable, while the low precision for discourse new cases shows that many anaphoric cases are left unresolved. We view our strategy, based on the clustering of negative samples and consecutive cluster size reduction, to be more effective at proportionally eliminating negative samples that are less frequent and that are more likely to be noisy. We compare our model to a rule-based baseline system that we have previously developed. The baseline system (Gasperin, 2006) for each anaphoric expression: 1) selects as coreferent antecedent the closest preceding NP that has the same head noun, same biotype and agrees in number with the anaphor, and 2) selects as associative antecedent the closest preceding NP that has the same head noun, same biotype but disagrees in number with the anaphor, or that has the same head noun or a modifier matching the anaphor head (or vice-versa) or matching modifiers, agrees in number but has different biotypes. The baseline system does not distinguish between different types of associative cases, although it aims to cover biotype"
C08-1033,W98-1119,0,0.0516718,"Missing"
C08-1033,P02-1014,0,0.490311,"ass and to NP form are likely to represent noisy samples (similar to positive ones), are eliminated, and bigger clusters are shrunk; however the shape of the distribution of the negative samples is preserved. For example, our biggest cluster (feature values are: fA =‘pn’, fa =‘pn’, hm=‘no’, hmm=‘no‘, mm=‘no’, bm=‘yes’, gp=‘yes’, num=‘yes’, sr=‘none’, d=‘16<’, dm=‘50<’) with 33,998 instances is reduced to 3,399 – still considerably more numerous than any positive sample. Other works have used a different strategy to reduce the imbalance between positive and negative samples (Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002), where only samples composed by a negative antecedent that is closer than the annotated one are considered. We compare the performance of both strategies in Section 5.1 and show that ours is more effective. The higher the number of negative samples, the higher the precision of the resolution, but the lower the recall. 5 Results Given the small size of our corpus, we did not hold out a test set. Instead, we have measured the average performance achieved by the model in a 10fold cross-validation setting, using the whole of the annotated corpus. We consider as antecedent ca"
C08-1033,P06-4020,1,0.849415,"Missing"
C08-1033,J01-4004,0,0.933944,"ing to anaphoric class and to NP form are likely to represent noisy samples (similar to positive ones), are eliminated, and bigger clusters are shrunk; however the shape of the distribution of the negative samples is preserved. For example, our biggest cluster (feature values are: fA =‘pn’, fa =‘pn’, hm=‘no’, hmm=‘no‘, mm=‘no’, bm=‘yes’, gp=‘yes’, num=‘yes’, sr=‘none’, d=‘16<’, dm=‘50<’) with 33,998 instances is reduced to 3,399 – still considerably more numerous than any positive sample. Other works have used a different strategy to reduce the imbalance between positive and negative samples (Soon et al., 2001; Ng and Cardie, 2002; Strube et al., 2002), where only samples composed by a negative antecedent that is closer than the annotated one are considered. We compare the performance of both strategies in Section 5.1 and show that ours is more effective. The higher the number of negative samples, the higher the precision of the resolution, but the lower the recall. 5 Results Given the small size of our corpus, we did not hold out a test set. Instead, we have measured the average performance achieved by the model in a 10fold cross-validation setting, using the whole of the annotated corpus. We cons"
C08-1033,W03-2607,0,0.0243906,"e relations. Our set of features is clearly failing to cover some of these aspects, and a deeper feature study should be the best way to boost the scores. However, despite lower, these performance 261 Class coreferent biotype set-member discourse new P 56.3 28.5 35.4 44.3 Perfect R 54.7 35.0 38.2 53.4 F 55.5 31.4 36.7 48.4 P 69.4 31.2 38.5 44.3 Relaxed R 67.4 37.9 41.5 53.4 F 68.3 34.2 40.0 48.4 Table 3: Performance of the probabilistic model scores are higher that the ones from previous approaches for newspaper texts, which used for instance the WordNet (Poesio et al., 1997) or the Internet (Bunescu, 2003) as source of semantic knowledge. We have analysed our features and observed that the string matching features hm, hmm, and mm, the number agreement feature num, biotype matching bm, and distance in markables dm are the core features and achieve reasonable performance. However, fA and fa play an important role, they increase the precision of coreferent cases and boost considerably the performance of the associative ones. This is due to the different distribution of NP types across the relations as shown is Table 2. The remaining features focused on specific cases: gp improved biotype recall, b"
C08-1033,W02-1040,0,0.0537271,"Missing"
C08-1033,W05-1306,0,0.0214853,"P (fa |C, fA ) P (d, dm|C, fA , fa ) P (sr|C, fA , fa , d, dm) P (bm, gp|C, fA , fa , d, dm, sr) P (num|C, fA , fa ,d, dm, sr, bm, gp) P (hm, hmm, mm|C, fA ,fa , d, dm, sr, bm, gp, num) = P (fA ) P (fa |fA ) P (d, dm|fA , fa ) P (sr|fA , fa , d, dm) P (bm, gp|fA , fa , d, dm, sr) P (num|fA , fa , d, dm, sr, bm, gp) P (hm, hmm, mm|fA , fa , d, dm, sr, bm, gp, num) (2) Following the decomposition, we eliminate some of the dependencies among the features that 4 (3) Training There are very few biomedical corpora annotated with anaphora information, and all of them are built from paper abstracts (Cohen et al., 2005), instead of full papers. As anaphora is a phenomenon that develops through a text, we believe that short abstracts are not the best source to work with and decided to concentrate on full papers. In order to collect the statistics to train our model, we have manually annotated anaphoric relations between biomedical entities in 5 full-text articles (approx. 33,300 words)4 , which are part of the Drosophila molecular biology literature. The corpus and annotation process are described in (Gasperin et al., 2007). To the best of our knowledge, this corpus is the first corpus of biomedical full-text"
C08-1033,W06-3328,1,0.566196,"Missing"
C08-1033,C04-1033,0,0.219843,"rs are being linked to the correct coreference chain, despite not being linked to the closest antecedent. This happens mainly in cases where there is no string matching between the closest antecedent and the anaphor, causing an earlier mention of the same entity with matching head and/or modifiers to get higher probability. We believe we can approximate ‘perfect’ to ‘relaxed’ results if we extend the string matching features to represent the whole coreference chain, that is, consider a positive matching when the anaphor match any of the elements in a chain, similarly to the idea presented in (Yang et al., 2004). We believe that the lower overall performance for associative cases is due to the difficulty of selecting features that capture all aspects involved in associative relations. Our set of features is clearly failing to cover some of these aspects, and a deeper feature study should be the best way to boost the scores. However, despite lower, these performance 261 Class coreferent biotype set-member discourse new P 56.3 28.5 35.4 44.3 Perfect R 54.7 35.0 38.2 53.4 F 55.5 31.4 36.7 48.4 P 69.4 31.2 38.5 44.3 Relaxed R 67.4 37.9 41.5 53.4 F 68.3 34.2 40.0 48.4 Table 3: Performance of the probabili"
C08-1033,poesio-etal-2002-acquiring,0,\N,Missing
C08-1033,M95-1005,0,\N,Missing
C08-1033,W97-1301,0,\N,Missing
C08-1033,C96-1021,0,\N,Missing
C08-1033,W03-0403,0,\N,Missing
C08-1033,J00-4005,0,\N,Missing
C08-1033,A88-1003,0,\N,Missing
C08-1033,C88-1021,0,\N,Missing
C08-1033,W02-1112,0,\N,Missing
C08-1033,W02-1008,0,\N,Missing
C08-1033,C92-2082,0,\N,Missing
C08-1033,W07-1022,0,\N,Missing
C08-1033,C02-1139,0,\N,Missing
C08-1033,W02-0312,0,\N,Missing
C08-1033,W04-3111,0,\N,Missing
C08-1033,briscoe-carroll-2002-robust,1,\N,Missing
C08-1033,J95-2003,0,\N,Missing
C08-1033,J94-4002,0,\N,Missing
C08-1033,P95-1017,0,\N,Missing
C08-1033,T78-1013,0,\N,Missing
C08-1033,J78-3020,0,\N,Missing
C08-1033,J01-4003,0,\N,Missing
C08-1033,P04-1075,0,\N,Missing
C08-1033,J96-2004,0,\N,Missing
C08-1033,J00-4003,0,\N,Missing
C08-1033,P04-1020,0,\N,Missing
C08-1033,P98-1011,0,\N,Missing
C08-1033,C98-1011,0,\N,Missing
C08-1033,P98-2143,0,\N,Missing
C08-1033,C98-2138,0,\N,Missing
C08-1033,W04-2327,0,\N,Missing
C08-1033,W04-0707,0,\N,Missing
C14-1164,andersen-etal-2008-bnc,1,0.764096,"BNC and the AN combinations containing these frequent words. We use about 8K nouns, 4K adjectives and 64K ANs following Kochmar and Briscoe (2013). The semantic space is represented by a matrix encoding word co-occurrences, where the rows represent the 76K elements mentioned above, and the columns represent a selected set of 10K context elements. The 10K context elements include the most frequent nouns, adjectives and verbs from the corpus. The word co-occurrence counts are estimated using the BNC. The corpora have been lemmatized, tagged and parsed with the RASP system (Briscoe et al., 2006; Andersen et al., 2008; Yannakoudakis et al., 2011), and all statistics are extracted at the lemma level. 5 Further details of the annotation experiment are described in the dataset release. 1744 We transform the raw sentence-internal co-occurrence counts into Local Mutual Information scores (Baroni and Zamparelli, 2010; Evert, 2005), and perform dimensionality reduction applying Singular Value Decomposition to the noun and adjective matrix rows, projecting the AN rows onto the same reduced space following Baroni and Zamparelli (2010). The original 76K × 10K matrix is reduced to a 76K × 300 matrix. This allows us t"
C14-1164,D10-1115,0,0.468107,"content word combinations directly since these will be very sparse and will often remain unattested even in an extremely large corpus. A promising solution is provided by compositional distributional semantic models, which combine distributional vectors for the component words using some function over such vectors. Compositional distributional semantic representations have been previously used to detect semantic anomaly in AN combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional semantic representations for the semantically acceptable and anomalous combinations, suggesting that compositional distributional models can be used to detect semantic anomaly without relying directly on corpus statistics. Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish between correct and incorrect ANs extracted from learner texts. Their results support the assumption that there is a distinguishable difference between the composite vector"
C14-1164,P06-4020,1,0.836836,"d adjectives from the BNC and the AN combinations containing these frequent words. We use about 8K nouns, 4K adjectives and 64K ANs following Kochmar and Briscoe (2013). The semantic space is represented by a matrix encoding word co-occurrences, where the rows represent the 76K elements mentioned above, and the columns represent a selected set of 10K context elements. The 10K context elements include the most frequent nouns, adjectives and verbs from the corpus. The word co-occurrence counts are estimated using the BNC. The corpora have been lemmatized, tagged and parsed with the RASP system (Briscoe et al., 2006; Andersen et al., 2008; Yannakoudakis et al., 2011), and all statistics are extracted at the lemma level. 5 Further details of the annotation experiment are described in the dataset release. 1744 We transform the raw sentence-internal co-occurrence counts into Local Mutual Information scores (Baroni and Zamparelli, 2010; Evert, 2005), and perform dimensionality reduction applying Singular Value Decomposition to the noun and adjective matrix rows, projecting the AN rows onto the same reduced space following Baroni and Zamparelli (2010). The original 76K × 10K matrix is reduced to a 76K × 300 m"
C14-1164,W12-2006,0,0.196925,"Missing"
C14-1164,D11-1010,0,0.241696,"Missing"
C14-1164,R13-1047,1,0.498669,"entations have been previously used to detect semantic anomaly in AN combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional semantic representations for the semantically acceptable and anomalous combinations, suggesting that compositional distributional models can be used to detect semantic anomaly without relying directly on corpus statistics. Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish between correct and incorrect ANs extracted from learner texts. Their results support the assumption that there is a distinguishable difference between the composite vectors for the correct and incorrect ANs, but they did not address the question of how to integrate these semantic models into an error detection system. Recent work by Lazaridou et al. (2013) has shown that measures used for quantifying the degree of semantic anomaly in phrases derived from their compositional distributional semantic representations can be used"
C14-1164,D13-1196,0,0.0198677,"le and anomalous combinations, suggesting that compositional distributional models can be used to detect semantic anomaly without relying directly on corpus statistics. Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish between correct and incorrect ANs extracted from learner texts. Their results support the assumption that there is a distinguishable difference between the composite vectors for the correct and incorrect ANs, but they did not address the question of how to integrate these semantic models into an error detection system. Recent work by Lazaridou et al. (2013) has shown that measures used for quantifying the degree of semantic anomaly in phrases derived from their compositional distributional semantic representations can be used as features by a classifier to help resolve syntactic ambiguities. Our goals are to test, using a new and larger AN dataset, whether semantic models can distinguish between correct and incorrect AN combinations, which cannot be dealt with using simpler error detection approaches, and to implement an error detection system using these semantically-based features. 3 Data Annotation We present and release a dataset of AN combi"
C14-1164,W09-2107,0,0.432052,"Missing"
C14-1164,P08-1028,0,0.457085,"ements. Distributional models are less suitable for representing content word combinations directly since these will be very sparse and will often remain unattested even in an extremely large corpus. A promising solution is provided by compositional distributional semantic models, which combine distributional vectors for the component words using some function over such vectors. Compositional distributional semantic representations have been previously used to detect semantic anomaly in AN combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional semantic representations for the semantically acceptable and anomalous combinations, suggesting that compositional distributional models can be used to detect semantic anomaly without relying directly on corpus statistics. Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish between correct and incorrect ANs extracted from learner texts. Their results support the assumption that"
C14-1164,C10-2103,0,0.217629,"tion, we showed that a classifier that uses output of the semantic models as features outperforms the comparison-based baseline system and shows good accuracy. In this section, we analyse the classifier’s performance in more detail. We note that, from an educational point of view, it is important for an EDC system to have high precision. For example, it has been shown that grammatical error detection systems with high precision maximize learning effect, and that systems with high precision but lower recall are more useful in language learning than systems with high recall and lower precision (Nagata and Nakatani, 2010). This suggests that learners might be misled and confused if they are frequently notified by a system that something is an error when it is not. Since precision is measured as the proportion of true positives (TP) to the sum of true positives and false positives (FP): 1748 P = TP TP + FP (4) an EDC system that achieves precision less than 0.5 is, in fact, misleading for language learners: for example, precision of less than 0.5 on the class of errors means that the system misidentifies correct use as an error more frequently than it correctly detects an error. Our classifier achieves good pre"
C14-1164,P11-1093,0,0.0998467,"es on (1) article and (2) preposition errors: (1) I am 0*/a student. (2) Last October, I came in*/to Tokyo. In (1) an EDC system would consider {a, an, the} as possible corrections for the missing article. To correct the preposition in in (2), an EDC system would consider the most frequent prepositions {on, from, for, of, about, to, at, with, by}, among which at or to would have a higher chance to be appropriate corrections as these are most often confused with in. Confusion sets can be learnt from learner texts, and probabilities can be set up according to the distribution of the confusions (Rozovskaya and Roth, 2011). EDC is usually cast as a multi-class classification task, with the number of classes equal to the number of target corrections. Detection and correction can occur simultaneously: an error is detected when an EDC system suggests using a word different from the one originally used by the learner, and the suggested word can be used as a correction. Each occurrence of a function word is represented with a feature vector, where features are derived from the surrounding context. This is usually highly informative for function words: for example, a context of I am and student or a similar noun requ"
C14-1164,W11-1301,0,0.446402,"nal space with each dimension encoding a word’s co-occurrence with one of its contextual elements. Distributional models are less suitable for representing content word combinations directly since these will be very sparse and will often remain unattested even in an extremely large corpus. A promising solution is provided by compositional distributional semantic models, which combine distributional vectors for the component words using some function over such vectors. Compositional distributional semantic representations have been previously used to detect semantic anomaly in AN combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional semantic representations for the semantically acceptable and anomalous combinations, suggesting that compositional distributional models can be used to detect semantic anomaly without relying directly on corpus statistics. Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish between"
C14-1164,P11-1019,1,0.946151,"new and larger AN dataset, whether semantic models can distinguish between correct and incorrect AN combinations, which cannot be dealt with using simpler error detection approaches, and to implement an error detection system using these semantically-based features. 3 Data Annotation We present and release a dataset of AN combinations which, on the one hand, exemplify the typical errors committed by language learners in the choice of content words within such combinations, and, on the other hand, are challenging for an EDC system. For that, we examined the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011), used the error annotation (Nicholls, 2003), and analysed the typical errors in AN combinations committed by language learners. We have compiled a list of 61 adjectives that are most problematic for learners. Most typically, learners confuse semantically related words: for example, they are unable to distinguish between synonyms, near-synonyms or co-hyponyms and choose an appropriate one from the set. Our list of adjectives contains some frequent ones that are confused with each other due to their similarity in meaning. For example, the adjectives within the set {big, large, great} are freque"
C14-1164,I08-2082,0,0.0574263,"how compositional distributional semantic models can be applied to detect semantic anomalies in this dataset; • demonstrate that the output of these models can be used to derive features for error detection in AN combinations. 2 2.1 Previous work Error Detection in Content Words Previous work on EDC for content words has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., ¨ 2008; Yi et al., 2008; Ostling and Knutsson, 2009). In the first case, the task is reduced to the search for the most suitable correction among the alternatives typically composed of synonyms, homophones or L1-related paraphrases (Dahlmeier and Ng, 2011), while the more challenging error detection step is omitted. In the second case, error detection is integrated into suggestion of alternatives and their comparison to the originally used word combination according to some metric of collocational strength. Such approaches aim to improve the fluency of non-native texts by correcting erroneous idioms or collocations,"
C14-1164,W14-1701,1,\N,Missing
C14-1164,W13-3601,0,\N,Missing
C14-1164,W11-2838,0,\N,Missing
C16-1079,P15-1068,1,0.911732,"Missing"
C16-1079,N12-1067,0,0.277258,"ith a target sentence to map errors to corrections (sometimes referred to as ‘correction detection’; see example in Table 1). However, unlike in MT, the source and target sentences in GEC are in the same language and so a majority of tokens match. This means alignment is comparatively more straightforward and so it is more feasible to annotate texts manually, rather than automatically. In fact two of the largest publicly available GEC datasets, the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) and the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier and Ng, 2012), were aligned and annotated manually. We took a guide tour on center city . We took a guided tour of the city center . Table 1: A sample alignment between an original uncorrected sentence and its corrected version. Nevertheless, automatic alignment of GEC data still has several advantages over manual alignment, not least because the latter is slow, laborious work. This is especially important for datasets that do not always contain explicit alignments, such as Lang-8 (Mizumoto et al., 2011), or GEC system output that needs to be aligned to the original uncorrected sentence. Another important"
C16-1079,W13-1703,0,0.12684,"lar etymology, spelling or function will align. This is better than the simple surface matching used by the standard token-level Levenshtein distance and hence, we argue, results in more natural, human-like alignments (see Table 2 (b)). The final alignment is retrieved by collecting the operations that make up the optimal path in the cost matrix. Given that the cost is now dependent upon a variable function, it is often the case that there is just a single optimal alignment. 3.3 Data We evaluated our improved alignment algorithm using the public FCE (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and CoNLL test sets (Ng et al., 2013; Ng et al., 2014). While the CoNLL data is available in a pretokenised format, the FCE data is not, and so to keep things comparable, we only worked with the untokenized CoNLL data. It should be noted that processing each of these datasets in a standard way is not at all straightforward. For example, unlike the CoNLL data, the FCE contains nested edits; e.g. [entery → entry → entrance] indicates a spelling error followed by a replacement noun error. Similarly, the NUCLE corpus can be quite noisy and it is not uncommon for annotators to select entire parag"
C16-1079,I11-1017,0,0.253201,"annakoudakis et al., 2011) and the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier and Ng, 2012), were aligned and annotated manually. We took a guide tour on center city . We took a guided tour of the city center . Table 1: A sample alignment between an original uncorrected sentence and its corrected version. Nevertheless, automatic alignment of GEC data still has several advantages over manual alignment, not least because the latter is slow, laborious work. This is especially important for datasets that do not always contain explicit alignments, such as Lang-8 (Mizumoto et al., 2011), or GEC system output that needs to be aligned to the original uncorrected sentence. Another important benefit of an automatic alignment is that it tends to be more consistent than a human alignment. For example, within both the FCE and NUCLE, strings such as has eating are inconsistently corrected as [has → was] or [has eating → was eating] even though they fundamentally equate to the same thing. In fact, the latter seems less desirable given the token eating does not actually change. A similar case is [has eating → was eaten], which is inconsistently realised either as one edit, as above, T"
C16-1079,W13-3601,0,0.0388181,". This is better than the simple surface matching used by the standard token-level Levenshtein distance and hence, we argue, results in more natural, human-like alignments (see Table 2 (b)). The final alignment is retrieved by collecting the operations that make up the optimal path in the cost matrix. Given that the cost is now dependent upon a variable function, it is often the case that there is just a single optimal alignment. 3.3 Data We evaluated our improved alignment algorithm using the public FCE (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and CoNLL test sets (Ng et al., 2013; Ng et al., 2014). While the CoNLL data is available in a pretokenised format, the FCE data is not, and so to keep things comparable, we only worked with the untokenized CoNLL data. It should be noted that processing each of these datasets in a standard way is not at all straightforward. For example, unlike the CoNLL data, the FCE contains nested edits; e.g. [entery → entry → entrance] indicates a spelling error followed by a replacement noun error. Similarly, the NUCLE corpus can be quite noisy and it is not uncommon for annotators to select entire paragraphs or even essays as edits with com"
C16-1079,W14-1701,1,0.867593,"than the simple surface matching used by the standard token-level Levenshtein distance and hence, we argue, results in more natural, human-like alignments (see Table 2 (b)). The final alignment is retrieved by collecting the operations that make up the optimal path in the cost matrix. Given that the cost is now dependent upon a variable function, it is often the case that there is just a single optimal alignment. 3.3 Data We evaluated our improved alignment algorithm using the public FCE (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and CoNLL test sets (Ng et al., 2013; Ng et al., 2014). While the CoNLL data is available in a pretokenised format, the FCE data is not, and so to keep things comparable, we only worked with the untokenized CoNLL data. It should be noted that processing each of these datasets in a standard way is not at all straightforward. For example, unlike the CoNLL data, the FCE contains nested edits; e.g. [entery → entry → entrance] indicates a spelling error followed by a replacement noun error. Similarly, the NUCLE corpus can be quite noisy and it is not uncommon for annotators to select entire paragraphs or even essays as edits with comments such as “Rew"
C16-1079,J03-1002,0,0.0092523,"-based function to decide which alignments should be merged. Our method beats all previous approaches on the tested datasets, achieving state-of-the-art results for automatic error extraction. 1 Introduction Within the field of Machine Translation (MT), one of the first steps of data processing is to align a source sentence with a target sentence. This is necessary because we want to determine which tokens and phrases in the source language map to which equivalent tokens or phrases in the target language. As this would be extremely time consuming to do manually, several tools, such as GIZA++ (Och and Ney, 2003), have been made available to do this automatically. Within the related field of Grammatical Error Correction (GEC), we similarly want to align a source sentence with a target sentence to map errors to corrections (sometimes referred to as ‘correction detection’; see example in Table 1). However, unlike in MT, the source and target sentences in GEC are in the same language and so a majority of tokens match. This means alignment is comparatively more straightforward and so it is more feasible to annotate texts manually, rather than automatically. In fact two of the largest publicly available GE"
C16-1079,Q16-1013,0,0.149418,"ribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 825 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 825–835, Osaka, Japan, December 11-17 2016. or two edits: [has → was] and [eating → eaten]. Ultimately, it seems desirable to regularise such edits and hence reduce ambiguity in the data. If all datasets are treated in the same way, this would also make them fully compatible with each other. Finally, automatic alignment can also simplify the annotation of new data. For instance, Sakaguchi et al. (2016) recently claimed that forcing annotators to annotate grammatical errors within the confines of an error scheme often led to unnatural sounding sentences and that unconstrained editing correlated more with human judgements. As such, if we no longer ask humans to explicitly mark edit boundaries in new data, we would need to extract this information automatically. This is particularly useful for English as a Second Language (ESL) teaching, where teachers could edit text freely and then let a computer delimit the edits. 2 Background There is very little previous work on automatic alignment of sen"
C16-1079,N12-1037,0,0.122739,"Missing"
C16-1079,P14-2098,0,0.0429921,"is crucial for deriving meaningful edits. Unfortunately, however, the most common method of aligning sentences is to use the Levenshtein distance, which only optimises in terms of insertions, deletions and substitutions. This means that, while optimal in terms of edit operation, the alignments do not take linguistic information into account and are hence not optimal in terms of human intuition (see Table 2 (a)). Human alignments, on the other hand, do make use of linguistic information, so we propose automatic alignments should do the same. 3.1 Damerau-Levenshtein First, however, as noted by Xue and Hwa (2014), another limitation of Levenshtein is that it is unable to handle word order errors. For example, [only can → can only] is realised as [only → Ø], [can → can] and [Ø → only]; in other words, reorderings are treated as deletions followed by insertions of identical tokens. Since we also need to preserve word order errors in the data, we argue that the Damerau-Levenshtein distance is better suited for the task than standard Levenshtein because it allows for token transpositions. 826 function DL_distance_extended(a, b): declare d[0..length(a), 0..length(b)] for i := 0 to length(a) inclusive do d["
C16-1079,P11-1019,1,0.826269,"the related field of Grammatical Error Correction (GEC), we similarly want to align a source sentence with a target sentence to map errors to corrections (sometimes referred to as ‘correction detection’; see example in Table 1). However, unlike in MT, the source and target sentences in GEC are in the same language and so a majority of tokens match. This means alignment is comparatively more straightforward and so it is more feasible to annotate texts manually, rather than automatically. In fact two of the largest publicly available GEC datasets, the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) and the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier and Ng, 2012), were aligned and annotated manually. We took a guide tour on center city . We took a guided tour of the city center . Table 1: A sample alignment between an original uncorrected sentence and its corrected version. Nevertheless, automatic alignment of GEC data still has several advantages over manual alignment, not least because the latter is slow, laborious work. This is especially important for datasets that do not always contain explicit alignments, such as Lang-8 (Mizumoto et al., 2011), or"
C88-1012,C86-1016,0,0.0285964,"ent, and describe the implementation of a system which has supported efficient development of a large computational grammar of English? 1. Tools for Grammar Development A number of researzh projects within the broad area of natural language processing (NLP) and theoretical linguistics make use of special purpose programs, which are beginning to be known under the general term of &quot;gm.nmar development environments&quot; (GDEs). Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al. (1986) and Karttunen (1986). In all instances the software packages cited above fall in the class of computational tools used in theoretical (rather than applied) Projects. Thus Kaplan's Grammar-writer's Workbench is an implementation of a particular linguistic theory (Lexical Functional Grammar;, Kaplan and Bresnan, 1982); similarly, Evans' ProGram incorporated an early version of Generalized Phrase Structure Grammar (GPSG, Gazdar and Pullum, 1982), whilst PATR-II is a &quot;virtual linguistic machine&quot;, developed by Shieber as a tool for experimenting with a variety of syntactic theories. These systems differ in their goals"
C88-1012,J85-2001,0,0.0293572,"or the notation is, it incorporates a handle for explicit intervention into the interpretation of the grammar at hand. Sometimes the nature of the task for which the g~ammar is being developed justifies a form~J notation incolporating 'hooks' for explicit procedures. Thus a number of matchine translation (MT) projects~ especially ones employing a ~ransfer strategy, make use of format systems for grammar specification, which, in addition to mapping surface strings into con~esponding language structures, identify operations to be associated with nodes and / or subtrees (Vauquois & Boitet, 1985; Nagao et al., 1985). In general, the effects of the temptation to allow, for example, the EVALuation of arbitrary LISP expressions on the ares of the ATN or the addition of &quot;procedural programming facilities&quot; to the rule-based skeleton of 1BM's PLNLP have been discussed at length in the recent literature addressing the issues of declarative formalisms from a theoretical perspective (see Shieber, 1986a, and references therein). However, from the point of view of developing a realistic grammar with substantial coverage, the opening of the procedural 'back door', while perhaps useful fo: 'patching' the inadequacies"
C88-1012,P85-1021,0,0.0297239,"l;~)rtance. For i~stance, it would bc inappropriate to adopt a direct imp!ementation of, say GPSG, since tire rate of change of the theory itself is likely to make such an implementation obsolete (or at least incapable of irmorporating subsequent linguistic analyses) quite rapidly - . file bdcf lifcspan of Ewms' ProGram is a case in point. ()nly when theou and grammar are beiug developed in very close collaboration, or even wifltin the same group -- - as in, for example, the })ewlctt.-Packard NLP project, whose cornerstotm is the linguistic framewolk of Head-Driven t'hrase Structm'e Grannnar (Proudian and Pollard, 1985; PollaN aud Sag, 1987) - - could such ~ul approach work. l}owever, itr mJ effint like om&quot;a, it is of critical impmtauce to strike the right balance between i)eit~g failhfu[ to the spirit nf a tbeo~y mid being uncommii:ted with respect to a particular vcrsien of it, as well as remaiuing tlexiNe within tile overall iianlcwoN of 'close' or related theories. Attempts to be too flexible, however, arc iikely to lead tit situations of wqich the PATII..II system is an example: the ability to model a wide t' rage of theoretical devices and mr(lyrical ti'amcworks is penalised by its unsuitability &quot;for"
C88-1012,P87-1034,0,0.0332838,"Missing"
C88-1012,C86-1066,0,0.06067,"Missing"
C88-1012,P84-1075,0,0.0908724,"this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English? 1. Tools for Grammar Development A number of researzh projects within the broad area of natural language processing (NLP) and theoretical linguistics make use of special purpose programs, which are beginning to be known under the general term of &quot;gm.nmar development environments&quot; (GDEs). Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al. (1986) and Karttunen (1986). In all instances the software packages cited above fall in the class of computational tools used in theoretical (rather than applied) Projects. Thus Kaplan's Grammar-writer's Workbench is an implementation of a particular linguistic theory (Lexical Functional Grammar;, Kaplan and Bresnan, 1982); similarly, Evans' ProGram incorporated an early version of Generalized Phrase Structure Grammar (GPSG, Gazdar and Pullum, 1982), whilst PATR-II is a &quot;virtual linguistic machine&quot;, developed by Shieber as a tool for"
C88-1012,J85-1003,0,0.079222,"ics: whatever the basis for the notation is, it incorporates a handle for explicit intervention into the interpretation of the grammar at hand. Sometimes the nature of the task for which the g~ammar is being developed justifies a form~J notation incolporating 'hooks' for explicit procedures. Thus a number of matchine translation (MT) projects~ especially ones employing a ~ransfer strategy, make use of format systems for grammar specification, which, in addition to mapping surface strings into con~esponding language structures, identify operations to be associated with nodes and / or subtrees (Vauquois & Boitet, 1985; Nagao et al., 1985). In general, the effects of the temptation to allow, for example, the EVALuation of arbitrary LISP expressions on the ares of the ATN or the addition of &quot;procedural programming facilities&quot; to the rule-based skeleton of 1BM's PLNLP have been discussed at length in the recent literature addressing the issues of declarative formalisms from a theoretical perspective (see Shieber, 1986a, and references therein). However, from the point of view of developing a realistic grammar with substantial coverage, the opening of the procedural 'back door', while perhaps useful fo: 'patch"
C88-1012,C86-1050,0,\N,Missing
C90-2008,P85-1008,0,0.257452,"sarily a long event which, whilst plausible, is not entailed under this interpretation of long. In order to avoid this effect using unification-based techniques it is necessary to explicitly copy the structure that specifies the telic role. We suggested in section 1 that NPs, such as the fact, can denote propositions 'directly'. Similarly, we think that there is no metonymy involved in examples such as John enjoyed the experience /film-making and so forth. In these cases, we claim that the NPs in question denote events 'directly'. Thus, we are lead to an 'ontologically promiscuous' semantics (Hobbs, 1985). However, recent developments in model-theoretic semantics which treat properties as basic entities (e.g. Chierchia & Turner, 1988) support this position. Indeed the interpretation of eventdenoting NPs in complement position with enjoy strongly suggests that these NPs must be analysed as denoting propositional functions since their 'missing argument' must be associated with the subject of enjoy. For instance, John likes marriage can mean that John likes the institution but John enjoys marriage can only mean that he enjoys being in the state of marriage (to someone). Figure 2b In (la) we show"
C90-2008,J87-3004,0,0.028895,"Missing"
C90-2008,E89-1024,0,0.0425772,"Missing"
C90-2008,P89-1005,0,0.0285092,"position. Indeed the interpretation of eventdenoting NPs in complement position with enjoy strongly suggests that these NPs must be analysed as denoting propositional functions since their 'missing argument' must be associated with the subject of enjoy. For instance, John likes marriage can mean that John likes the institution but John enjoys marriage can only mean that he enjoys being in the state of marriage (to someone). Figure 2b In (la) we show the formula which can be read off the DAG in Figure 2b given straightforward assumptions about the semantic interpretation of the formalism (e.g. Moore, 1989). The lexical entry for enjoy specifies that its complement must denote an event which can be syntactically an NP or progressive VP and that, if the NP is type-shifted, the relic role supplies the understood predicate. The resulting formulae associated with the VP and S are shown in (lb,c). (1) a) ~ x e' ~ y ?read(e' x y) & book(y) b) ~ x 3 e e' y past(e) & enjoy(e x e') & ?read(e' x y) & book(y) c) 3 e e' y past(e) & enjoy(e j e') & ?read(e' j y) & book(y) We follow Hobbs (1985), Alshawi et al. (1989), Moens et at. (1989) and others in using an event-based calculus for reasons of computationa"
C90-2008,P89-1012,0,0.0197511,"ating an approach whereby lexical entries inherit some of their structure from higher nodes in the taxonomy. Qualia structure could thus be inherited from word senses rather than abstract templates; for example Burgundy would inherit its telic role from the noun drink. If abstract templates were still needed they could be inserted into the inheritance hierarchy at the appropriate points. The approaches above only specify how the qualia structure is inherited, rather than how it is initially det~'mined. In recent work, the IBM lexical systems group have used their lexical database system (e.g. Neff & Boguraev, 1989) with a number of MRDs to generate lists of pre~licates which are applied to books by searching through definition fields for the occurrence of book in a position denoting 'typical object' of the headword. For instance, LDOCE defines sag with '(of a book, performance, etc.) to become uninteresting during part of the length'. Using these techniques with three dictionaries resulted in the following list of verbs: abridge, abstract, annotate, appreciate, autograph, ban, bang about, borrow, bring out, burlesque, bowdlerize, call in, castigate, castrate, catalogue, censor, chuck away, churn out, cl"
C90-2008,C88-2110,0,0.0322556,"Missing"
D07-1130,P06-2006,1,0.828265,"as to map the DR scheme to our GR scheme. Issues concerning 1171 this mapping are discussed in section 4. Given this mapping, we determined the subset of sentences in the (PTB) training data for which there was a single derivation in the grammar compatible with the set of mapped GRs. These derivations were used to create the initial trained model (B) from the uniform model (A). To evaluate the performance of these and subsequent models, we tested them using our own GR-based evaluation scheme over 560 sentences from our reannotated version of DepBank, a subset of section 23 of the WSJ PTB (see Briscoe & Carroll, 2006). Table 1 gives the unlabelled precision, recall and microaveraged F1 score of these models over this data. Model B was used to rerank derivations compatible with the mapped GRs recovered for the PTB training data. Model C was built from the weighted counts of actions in the initial set of unambiguous data and from the highest-ranked derivations over the training data (i.e. we do not include duplicate counts from the unambiguous data). Counts were weighted with scores ranging [0 − 1] corresponding to the overall probability of the relevant derivation. The evaluation shows a steady increase in"
D07-1130,1997.iwpt-1.16,0,0.718868,"sanne Corpora (see Briscoe, 2006 for further details). Accordingly, we submitted our results in the open class. 2 Training and Adaptation The RASP parser is a generalized LR parser which builds a non-deterministic generalized LALR(1) parse table from the grammar (Tomita, 1987). A context-free ‘backbone’ is automatically derived from a unification grammar. The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail. The parser creates a packed parse forest represented as a graphstructured stack. Inui et al. (1997) describe the probability model 1170 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1170–1174, c Prague, June 2007. 2007 Association for Computational Linguistics utilized in the system where a transition is represented by the probability of moving from one stack state, σi−1 , (an instance of the graph structured stack) to another, σi . They estimate this probability using the stack-top state si−1 , next input symbol li and next action ai . This probability is conditioned on the type of state si−1 . Ss and Sr are mutually exclusive sets of states which represent those st"
D07-1130,W07-2416,0,0.0241589,"n finding was that it was very difficult to map from the annotation scheme used to prepare training and development data to one that could be used to effectively train and adapt the RASP system unlexicalized parse ranking model. Nevertheless, we were able to demonstrate a significant improvement in performance utilizing bootstrapping over the PBIOTB data. 1 Introduction The CoNLL07 domain adaptation task was created to explore how a parser trained in one domain might be adapted to a new one. The training data were drawn from the PTB (Marcus et al., 1993) reannotated with dependency relations (Johansson and Nugues, 2007, hereafter DRs). The test data were taken from a corpus of biomedical articles (Kulick et al., 2004) and the CHILDES database (Brown, 1973; MacWhinney, 2000) also reannotated with DRs (see Nivre et al., 2007) for further details of the task, annotation format, and evaluation scheme. The development data consisted of a small amount of annotated and unannotated biomedical and conversational data. The RASP system (Briscoe et al., 2006) utilizes a manually-developed grammar and outputs grammatical bilexical dependency relations (see Briscoe, 2006 for a detailed description, hereafter GRs). Watson"
D07-1130,W04-3111,0,0.0922286,"elopment data to one that could be used to effectively train and adapt the RASP system unlexicalized parse ranking model. Nevertheless, we were able to demonstrate a significant improvement in performance utilizing bootstrapping over the PBIOTB data. 1 Introduction The CoNLL07 domain adaptation task was created to explore how a parser trained in one domain might be adapted to a new one. The training data were drawn from the PTB (Marcus et al., 1993) reannotated with dependency relations (Johansson and Nugues, 2007, hereafter DRs). The test data were taken from a corpus of biomedical articles (Kulick et al., 2004) and the CHILDES database (Brown, 1973; MacWhinney, 2000) also reannotated with DRs (see Nivre et al., 2007) for further details of the task, annotation format, and evaluation scheme. The development data consisted of a small amount of annotated and unannotated biomedical and conversational data. The RASP system (Briscoe et al., 2006) utilizes a manually-developed grammar and outputs grammatical bilexical dependency relations (see Briscoe, 2006 for a detailed description, hereafter GRs). Watson et al. (2007) describe a semi-supervised, bootstrapping approach to training the parser which utiliz"
D07-1130,J93-2004,0,0.0296153,"the open class for systems using external resources. Our main finding was that it was very difficult to map from the annotation scheme used to prepare training and development data to one that could be used to effectively train and adapt the RASP system unlexicalized parse ranking model. Nevertheless, we were able to demonstrate a significant improvement in performance utilizing bootstrapping over the PBIOTB data. 1 Introduction The CoNLL07 domain adaptation task was created to explore how a parser trained in one domain might be adapted to a new one. The training data were drawn from the PTB (Marcus et al., 1993) reannotated with dependency relations (Johansson and Nugues, 2007, hereafter DRs). The test data were taken from a corpus of biomedical articles (Kulick et al., 2004) and the CHILDES database (Brown, 1973; MacWhinney, 2000) also reannotated with DRs (see Nivre et al., 2007) for further details of the task, annotation format, and evaluation scheme. The development data consisted of a small amount of annotated and unannotated biomedical and conversational data. The RASP system (Briscoe et al., 2006) utilizes a manually-developed grammar and outputs grammatical bilexical dependency relations (se"
D07-1130,J87-1004,0,0.072546,"the development data. We then tried unsupervised bootstrapping from the unannotated development data based on these initial models. As the parser requires input to consist of a sequence of one of 150 CLAWS PoS tags, we also utilize a first-order HMM PoS tagger which has been trained on manually-annotated data from the LOB, BNC and Susanne Corpora (see Briscoe, 2006 for further details). Accordingly, we submitted our results in the open class. 2 Training and Adaptation The RASP parser is a generalized LR parser which builds a non-deterministic generalized LALR(1) parse table from the grammar (Tomita, 1987). A context-free ‘backbone’ is automatically derived from a unification grammar. The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail. The parser creates a packed parse forest represented as a graphstructured stack. Inui et al. (1997) describe the probability model 1170 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1170–1174, c Prague, June 2007. 2007 Association for Computational Linguistics utilized in the system where a transition is represented by the probabili"
D07-1130,W07-2203,1,0.872632,", 2007, hereafter DRs). The test data were taken from a corpus of biomedical articles (Kulick et al., 2004) and the CHILDES database (Brown, 1973; MacWhinney, 2000) also reannotated with DRs (see Nivre et al., 2007) for further details of the task, annotation format, and evaluation scheme. The development data consisted of a small amount of annotated and unannotated biomedical and conversational data. The RASP system (Briscoe et al., 2006) utilizes a manually-developed grammar and outputs grammatical bilexical dependency relations (see Briscoe, 2006 for a detailed description, hereafter GRs). Watson et al. (2007) describe a semi-supervised, bootstrapping approach to training the parser which utilizes unlabelled partially-bracketed input with respect to the system derivations. For the domain adaptation task we retrained RASP by mapping our GR scheme to the DR scheme and annotation format, and used this mapping to select a derivation to train the unlexicalized parse ranking model from the annotated PTB training data. We also performed similar partially-supervised bootstrapping over the 200 annotated biomedical sentences in the development data. We then tried unsupervised bootstrapping from the unannotat"
D07-1130,P06-4020,1,\N,Missing
D07-1130,D07-1096,0,\N,Missing
E85-1025,P84-1018,0,\N,Missing
E85-1025,P84-1095,0,\N,Missing
E85-1025,P84-1075,0,\N,Missing
E85-1025,P81-1030,0,\N,Missing
E85-1025,P84-1096,0,\N,Missing
E87-1011,P85-1030,0,0.0205152,"proaches reduce the complexity of the problem by limiting themselves to applying the machine readable source of a dictionary to a small class of similar tasks, and building customlsed interfaces offering relatively narrow access channels into the on-line data. Thus IBM&apos;s WordSmith system (Byrd and Chodorow, 1985) is concerned primarily with providing a browsing functionality which supports retrieval of words ~close~ to a given word along the dimensions of spelling, meaning and sound, while a group at Bell Labs has several large dictionaries on-line used only for research on stress assignment (Church, 1985). Alshawi et al. (1985) have used a machine-readable source directly for syntactic analysis of texts; however, the approach taken there -- namely that of simple pro-indexing by orthography -- does not generalise easily for applications which require the rapid locating and retrievalof entries satisfying more than one selection criterion. The n a t u r e of the p r o b l e m Several factors put the task of mounting a machinereadable dictionary as a proper development tool beyond the scope of current DBMS practice and make its conversion into a database of e.g. a standard relational kind quite di"
E87-1011,J87-3002,1,\N,Missing
E87-1011,J87-3001,0,\N,Missing
E87-1011,C86-1066,0,\N,Missing
E87-1011,P85-1034,0,\N,Missing
E87-1011,P84-1096,0,\N,Missing
E87-1011,E85-1025,1,\N,Missing
E87-1035,P83-1017,0,0.190381,"omain in the left context and to two grammatical symbols in the right context. (6) Who does Kim want e? to think that the boss will replace Sandy (with e_.) Who does Kim want e? to think that the boss expects the directors to replace Sandy (with e_e) In (6) the ~ t point of attachment cannot be determined until the end of the sentence which can be arbitrarily far away (in terms of lexical material) in the fight context. To date, little attention has been given to alternative deterministic techniques as models of natural language parsing in their own right, though. One exception is the work of Shieber (1983) and Pereira (1985), who have proposed that a simple extension of the LALR(1) technique can be used to model human natural language parsing strategies. The LALR(1) technique is a more efficient variant of the LR(1) technique. Since our implementation of the Shieber/Pereira model uses the latter technique, we will refer throughout to LR(1). With the grammar discussed below, the behaviour of a parser using either technique should be identical (see Aho & Ullrnan, 1972). In addition, Briscoe & Boguraev (1984) and Briscoe (in press) propose that a bounded context, deterministic parser in conjunctio"
E87-1035,P84-1056,0,\N,Missing
E89-1035,J82-3004,0,0.0996764,"Missing"
E89-1035,C86-1066,0,0.0623422,"Missing"
J87-3002,E87-1011,1,0.901049,"cess system to be significantly enhanced by caching most of the working subset of the dictionary at any given turn in main memory. It turns out that for a single user workstation, specially tuned for Lisp and operations optimised at the microcode level for random file access and s-expression I/O, this strategy offers remarkably good results. More recently, a dictionary server, of the kind described by Kay (1984b), was implemented and installed as a background process on a Xerox workstation networked together with the rest of the equipment dedicated to natural language processing applications (Boguraev et al., 1987). Again, the same lispified form of the machine readable source of LDOCE was used. From the point of view of providing a centralised service to more than one client, efficiently over a packet switching network, disc space on the server processor was not an issue. This made it possible to construct a larger, but more comprehensive, index for the dictionary, which now allows the recovery of a word in guaranteed time (typically less than a second). The main access route into LDOCE for most of our current applications is via the homograph fields (see Figure 1). Options exist in the access software"
J87-3002,P84-1095,0,0.0231396,"onetic, lexical, syntactic, and semantic information (Boguraev et al., 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample. 3 THE FORMAT OF THE GRAMMAR CODES The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields - - head word, pronunciation, grammar codes, definitions, examples, and so forth. However, each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require (see Calzolari (1984) for further discussion). For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character. In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1&apos;&apos;, a"
J87-3002,P84-1018,0,0.0368165,"Missing"
J87-3002,P84-1096,0,0.162284,"Missing"
J87-3002,A83-1020,0,0.0246977,"nstrained in any way by the method of access, as we do not have a very clear idea what form the restructured dictionary may eventually take. Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry. Our task was made possible by the fact that while far from being a database in the accepted sense of the word, the LDOCE typesetting tape is the only truly computerised dictionary of English (Michiels, 1983). Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Bran Boguraev and Ted Briscoe The logical structure of a dictionary entry is reflected on the tape as a sequence of typed records (see Figure 1), each with additional internal segmentation, where records and fields correspond to separate units in an entry, such as headword, pronunciation, grammar code, word senses, and so forth. (Record-type homograph (Seq-numberE-codeI-code)) (Record-type headword (Serial-no Main-entry)) (Record-type pronunciation (Phonetic)) (Record-type variant (SpellingPronunciation)) (Record-type part"
J87-3002,C86-1066,0,0.221729,"anguage. These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987; Russell et al., 1986; Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English. One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser. The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications"
J87-3002,J81-4005,0,0.0250683,"to republish, requires a fee and/or specificpermission. 0362-613X/87/030203-218503.00 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982; Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide a"
J87-3002,P84-1075,0,0.193757,"operties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics, and especially on grammatical theory - - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) - - and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language. These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary vi"
J87-3002,J87-3001,0,\N,Missing
J87-3002,E85-1025,1,\N,Missing
J93-1002,C88-1012,1,0.66618,"sing system incorporating the ANLT grammar (described above), we have implemented a breadth-first, nondeterministic LR parser for unification grammars. This parser is integrated with the Grammar Development Environment (GDE; Carroll et al. 1988) in the ANLT system, and provided as an alternative parser for use with stable grammars for batch parsing of large bodies of text. The existing chart parser, although slower, has been retained since it is more suited to grammar development, because of the speed with which modifications to the grammar can be compiled and its better debugging facilities (Boguraev et al. 1988). Our nondeterministic LR parser is based on Kipps' (1989) reformulation of Tomita's (1987) parsing algorithm and uses a graph-structured stack in the same way. Our parser is driven by the LALR(1) state table computed from the backbone grammar, but in addition on each reduction the parser performs the unifications appropriate to the unification grammar version of the backbone rule involved. The analysis being pursued fails if one of the unifications fails. The parser performs sub-analysis sharing (where if two or more trees have a common sub-analysis, that sub-analysis is represented only once"
J93-1002,P91-1027,0,0.0261499,"Missing"
J93-1002,P89-1010,0,0.0302128,"s John saw the man on the bus again, in which the possibility of a locative interpretation creates a mild preference for the adjectival reading and local attachment. To select the correct analysis in such cases it will be necessary to integrate information concerning word sense collocations into the probabilistic analysis. In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified. In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified (e.g. Church and Hanks 1989), because these apply to adjacent words in unanalyzed text. Hindle and Rooth (1991) report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly. One way of integrating 'structural' collocational information into the system presented above would be to make use of the semantic component of the (ANLT) grammar. This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input. I"
J93-1002,J82-3004,0,0.0130765,"the set returned (or reject them all). However, this approach places a great load on the analyst, who will routinely need to examine large numbers of parses for given sentences. In addition, computation of all possible analyses is likely to be expensive and, in the limit, intractable. Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient. For example, in noun compounds analyzed using a recursive binary-branching rule (N --* N N) the number of analyses correlates with the Catalan series (Church and Patil, 1982), 4 so a 3-word compound has 2 analyses, 4 has 5, 5 has 14, 9 has 1430, and so forth. However, Briscoe (1987:154f) shows that with a simple bounded context parser (with one word lookahead) set up to request help whenever a parse indeterminacy arises, it is possible to select any of the 14 analyses of a 5-word compound with a maximum of 5 interactions and any of the 1430 analyses of a 9-word compound with around 13 interactions. In general, resolution of the first indeterminacy in the input will rule out approximately half the potential analyses, resolution of the next, half of the remaining on"
J93-1002,A92-1018,0,0.0398481,"Missing"
J93-1002,J88-1003,0,0.0337293,"Missing"
J93-1002,P81-1022,0,0.20879,"Missing"
J93-1002,W89-0209,0,0.643236,"129f for an introduction). The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972). These algorithms provide polynomial solutions to the tasks of finding the most probable derivation for a given input and a stochastic regular grammar, and of performing iterative re-estimation of the parameters of a (hidden) stochastic regular grammar by considering all possible derivations over a corpus of inputs, respectively. Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF). Fujisaki et al. (1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input. Kupiec (1991) extends Baum-Welch re-estimation to arbitrary (nonCNF) CFGs. Baum-Welch re-estimation can be used with restricted or unrestricted grammars/models in the sense that some of the parameters corresponding to possible productions over a given (non-)terminal category set/set of states can be given an initial probability of zero. Unrestricted grammars/models quickly become impractical because the number of"
J93-1002,J83-3002,0,0.0752601,"Missing"
J93-1002,W89-0221,0,0.0172819,"ries following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes. An analysis is only packed into one that has already been found if its top node is subsumed by, or is equal to that of the one already found. An analysis, once packed, will thus never need to be unpacked during parsing (as in Tomita's system) since the value of each feature will always be uniquely determined. Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf. Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses. We have implemented a unification version of Schabes' (1991a) chart-based LR-like parser (which is polynomial in sentence length for CF grammars), but experiments with the ANLT grammar suggest that it offers no practical advantages over our Tomita-style parser, and Schabes' table construction algorithm yields less fine-grained and, therefore, less predictive parse tables. Nevertheless, searching the parse forest exhaustively to recover each distinct analysis proved computat"
J93-1002,W89-0220,0,0.0315058,"Missing"
J93-1002,E91-1004,0,0.267758,". (1989) propose a rather inelegant solution for the noun compound case, which involves creating 5582 instances of 4 morphosyntactically identical rules for classes of word forms with distinct bracketing behavior in noun-noun compounds. However, we would like to avoid enlarging the grammar and eventually to integrate probabilistic lexical information with probabilistic structural information in a more modular fashion. Probabilistic CFGs also will not model the context dependence of rule use; for example, an NP is more likely to be expanded as a pronoun in subject position than elsewhere (e.g. Magerman and Marcus 1991), but only one global probability can be associated with the relevant CF production. Thus the probabilistic CFG model predicts (incorrectly) that a) and f) will have the same probability of occurrence. These considerations suggest that we need a technique that allows use of a more adequate grammatical formalism than CFG and a more context-dependent probabilistic model. Our approach is to use the LR parsing technique as a natural way to obtain a finitestate representation of a non-finite-state grammar incorporating information about parse context. In the following sections, we introduce the LR"
J93-1002,E91-1013,0,0.0431095,"difficultto m a k e them in an optimal w a y for the purposes of L R parsing, since both steps involve consideration and comparison of all the categories mentioned in each rule of the grammar. 32 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Constructing the LR parse table directly and automatically from a unification grammar would avoid these drawbacks. In this case, the LR parse table would be based on complex categories, with unification of complex categories taking the place of equality of atomic ones in the standard LR parse table construction algorithm (Osborne 1990; Nakazawa 1991). However, this approach is computationally prohibitively expensive: Osborne (1990:26) reports that his implementation (in HP Common Lisp on a Hewlett Packard 9000/350) takes almost 24 hours to construct the LR(0) states for a unification grammar of just 75 productions. 3.2 Constructing a CF B a c k b o n e f r o m a Unification G r a m m a r Our approach, described below, not only extracts unification information from complex categories, but is computationally tractable for realistic sized grammars and also safe from inconsistency. We start with a unification grammar and automatically constru"
J93-1002,1991.iwpt-1.18,0,0.0567747,"irst search using probabilistic information associated with transitions might not yield the desired result given this property. For example, it is not possible to use Viterbi-style optimization of search for the maximally probable parse because this derivation may contain a sub-analysis that will be pruned locally before a subsequent unification failure renders the current most probable analysis impossible. In general, the current breadth-first probabilistic parser is more efficient than its nonprobabilistic counterpart described in the previous section. In contrast to the parser described by Ng and Tomita (1991), our probabilistic parser is able to merge (state and stack) configurations and in all cases still maintain a full record of all the probabilities computed up to that point, since it associates probabilities with partial analyses of the input so far rather than with nodes in the graph-structured stack. We are currently 6 Although we define our probabilistic model relative to the LR parsing technique, it is likely that there is an equivalent encoding in purely grammatical terms. In general our approach corresponds to making the probability of rule application conditional on other rules having"
J93-1002,P92-1017,0,0.0491443,"Missing"
J93-1002,P88-1010,0,0.0168536,"Missing"
J93-1002,J87-3008,0,0.0176419,"us sections, we undertook a preliminary experiment using a subset of LDOCE noun definitions as our test corpus. (The reasons for choosing this corpus are discussed in the introduction.) A corpus of approximately 32,000 noun definitions was created from LDOCE by extracting the definition fields and normalizing the definitions to remove punctuation, font control information, and so forth, s A lexicon was created for this corpus by extracting the appropriate lemmas and matching these against entries in the ANLT lexicon. The 10,600 resultant entries were loaded into the ANLT morphological system (Ritchie et al. 1987) and this sublexicon and the full ANLT grammar formed the starting point for the training process. A total of 246 definitions, selected without regard for their syntactic form, were parsed semi-automatically using the parser described in Section 5. During this process, further rules and lexical entries were created for some definitions that failed to parse. Of the total number, 150 were successfully parsed and 63 lexical entries and 14 rules were added. Some of the rules required reflected general inadequacies in the ANLT grammar; for example, we added rules to deal with new partitives and pre"
J93-1002,P91-1014,0,0.0199574,"g these points, since the parser is as predictive as the backbone grammar and LR technique allow, and the LALR(1) parse table allows one word lookahead to resolve some ambiguities (although, of course, the resolution of a local ambiguity may potentially involve an unlimited amount of lookahead; e.g. Briscoe 1987:125ff). In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g. Schabes 1991b). Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991). An LR parser faces an indeterminacy when it enters a state in which there is more than one possible action, given the current lookahead. In a particular state there cannot be more than one shift or accept action, but there can be several reduce actions, each specifying a reduction with a differ"
J93-1002,1991.iwpt-1.4,0,0.00854874,"g these points, since the parser is as predictive as the backbone grammar and LR technique allow, and the LALR(1) parse table allows one word lookahead to resolve some ambiguities (although, of course, the resolution of a local ambiguity may potentially involve an unlimited amount of lookahead; e.g. Briscoe 1987:125ff). In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g. Schabes 1991b). Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991). An LR parser faces an indeterminacy when it enters a state in which there is more than one possible action, given the current lookahead. In a particular state there cannot be more than one shift or accept action, but there can be several reduce actions, each specifying a reduction with a differ"
J93-1002,P83-1017,0,0.120745,"refore, not f o u n d it necessary to use Schabes' (1991a) LR-like tables (with n u m b e r of states guaranteed to be polynomial even in the worst case). 1 of the 3,710 states, 2,200 contain at least 1 action conflict, with a median of 34 conflicts per state. There are a total of 230,000 shift-reduce conflicts and 220,000 reduce-reduce conflicts, fairly uniformly distributed across the terminal lookahead symbols. In half of the latter conflicts, the rules involved have an identical number of daughters. One implication of this finding is that an approach to conflict resolution such as that of Shieber (1983) where reduce-reduce conflicts are resolved in favor of the longer reduction may not suffice to select a unique analysis for realistic NL grammars. 38 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Table 1 Sizes of grammar and LALR(1) parse tables. Grammar Number of CFG rules/categories Number of LR(0) states Number of kernel items Total number of actions Pascal2 Modula-23 Tomita, Japanese ANLT (689 PS rules) 158 / 124 227 / 194 800 / ? 1641 / 496 275 373 ? 3710 ? 420 ? 34836 2883 3238 ? 1258451 Table 2 Timings for LALR(1) parse table construction (in seconds of CPU time on"
J93-1002,P84-1075,0,0.00908517,"aining Kleene star daughters is treated as two rules: one omitting the daughters concerned and one with the daughters being Kleene plus. A new nonterminal category is created for each distinct Kleene plus category, and two extra rules are added to the backbone grammar to form a right-branching binary tree structure for it; a parser can easily be modified to flatten this out during processing into the intended flat sequence of categories. Figure 6 gives an example of what such a backbone tree looks like. Grammars written in other, more low-level unification grammar formalisms, such as PATR-I1 (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding. However, this method both allows the grammar writer to continue to use the full facilities of the ANLT formalism and allows the algorithmic derivation of an appropriate backbone grammar to support LR parsing. The major task of the backbone grammar is to encode sufficient information (in the atomic categoried CF rules) from the unification grammar to constrain the application of the latter's rules at parse time. The nearly one-to-one mapping of unification grammar rules"
J93-1002,P85-1018,0,0.0241389,"ar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987). In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by Shieber (1985). In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism (Briscoe et al. 1987). This formalism is a notational variant of Definite Clause Grammar (e.g. Pereira and Warren 1980), in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations. Categories consist of sets of feature name-value pairs, with the possibility of variable values, which may be bound within a rule, and of category-valued features. Categories are combined using fixed-arity"
J93-1002,E89-1035,1,0.845304,"Missing"
J93-1002,P84-1073,0,0.0248036,"feature w h e n parsing interactively with them (see next section). Table 1 compares the size of the LALR(1) parse table for the ANLT g r a m m a r with others reported in the literature. From these figures, the ANLT g r a m m a r is more than twice the size of Tomita's (combined morphological and syntactic) g r a m m a r for Japanese (Tomita 1987:45). The g r a m m a r itself is about one order of m a g n i t u d e bigger than that of a typical p r o g r a m m i n g language, but the LALR(1) parse table, in terms of n u m b e r of actions, is two orders of m a g n i t u d e bigger. Although Tomita (1984:357) anticipates LR parsing techniques being applied to large N L grammars written in formalisms such as GPSG, the sizes of parse tables for such grammars grow more rapidly than he predicts. However, for large real-world NL grammars such as the ANLT, the table size is still quite manageable despite Johnson's (1989) worst-case complexity result of the n u m b e r of LR(0) states being exponential on g r a m m a r size (leading to a parser with exponentially bad time performance). We have, therefore, not f o u n d it necessary to use Schabes' (1991a) LR-like tables (with n u m b e r of states g"
J93-1002,J87-1004,0,0.348353,"..................................................... 12 i0 ....................................... 13 r4 ........................................................... 13 ....................................... 14 r2 ........................................................... 14 ....................................... acc 14 8 9 ....................................... 9 r9 rg/s8 r9 ........................................................... 15 VP 15 Figure 2 LALR(1) parse table for Grammar 1. 31 Computational Linguistics Volume 19, Number 1 3.1 Creating LR Parse Tables from Unification Grammars Tomita (1987) describes a system for nondeterministic LR parsing of context-free grammars consisting of atomic categories, in which each CF production may be augmented with a set of tests (which perform similar types of operations to those available in a unification grammar). At parse time, whenever a sequence of constituents is about to be reduced into a higher-level constituent using a production, the augmentation associated with the production is invoked to check syntactic or semantic constraints such as agreement, pass attribute values between constituents, and construct a representation of the higher-"
J93-1002,W89-0211,0,0.0444225,"Missing"
J93-1002,1991.iwpt-1.12,0,0.130937,"Missing"
J93-1002,J90-1003,0,\N,Missing
J93-1002,J93-1005,0,\N,Missing
J93-1002,H91-1067,0,\N,Missing
J93-1002,H90-1054,0,\N,Missing
J93-1002,H91-1046,0,\N,Missing
J93-1002,1991.iwpt-1.22,0,\N,Missing
J93-1002,H90-1056,0,\N,Missing
J93-1014,C69-0101,0,0.478652,"2)) and contrasting it to the LSP system (Sager 1981) and Parsifal (Marcus 1980). The comparison is brief and the choice odd since more general broad-coverage grammars, such as DIAGRAM (Robinson 1982), PEG (Jensen et al. 1986) and ANLT (Grover et al. 1989), and more corpus-oriented parsing systems, such as FIDDITCH (Hindle 1983, 1993) or MITFP (de Marcken 1990), have been developed within the field, but are not discussed anywhere. A similar suspicion of isolationism recurs in the sections dealing with the grammatical formalism used; 210 Book Reviews this is based on (extended) affix grammar (Koster 1971) and, although only described informally, the variant of affix grammar adopted is probably similar in generative and expressive capacity to unification-based formalisms, such as PATR-II (Shieber 1986) or the ANLT formalism (Briscoe et al. 1987), with some interesting extensions making it more adequate to phenomena such as agreement in coordinate structures. Unfortunately, no comparison is offered. More discussion is devoted to comparison with the approach to corpus analysis taken by the Lancaster group (Garside et al. 1987); Oostdijk argues that because their espousal of probabilistic methods"
J99-4002,J96-2001,0,0.0215072,"ly have no evidence that a more complex approach is justified, given that our main aim is to rank unseen senses by plausibility. Another problem is the need to ensure that classes have comparable frequency distributions. This could matter if there were competing lexical rules, defined on different but overlapping classes, since if one class has a high percentage of low-frequency words compared to the other, the estimate of its productivity will tend to be lower. The productivity figure could be adjusted to allow for item frequency within classes. We will not discuss this further here, but see Baayen and Sproat (1996) for discussion of the related phenomenon of ambiguous derivational affixes. Schiitze (1997, 133f.) argues, in the context of a detailed critique of Pinker (1989), that accounts of lexical rules that do not include a quantitative component cannot form the basis for a satisfactory theory of the acquisition of lexical rules by language learners. The seed for the formation of a specific lexical rule must be comparison of the semantics and alternation/derivation behavior of a class of lexical items, but since there will always be noise in the form of exceptions because of the inherent semiproducti"
J99-4002,J92-2003,0,0.0640116,"Missing"
J99-4002,P94-1021,0,0.0325076,"Missing"
J99-4002,E95-1012,0,0.027227,"nt not to utilize abnormal or rare means of conveying particular messages. We can model this aspect of language use as a conditional probability that a word form will be associated to a specific lexical entry, derived using a maximum likelihood estimator: 7 Pr°b(lexical-entry I w ° r d - f ° r m ) = freq(lexical-entry with word-form) freq(word-form) This proposal is not novel and is the analogue of proposals to associate probabilities with initial trees in, for example, a lexicalized tree adjoining grammar (Resnik 1992; Schabes 1992). However, it differs from recent proposals by, for example, Brew (1995), to associate probabilities with values on paths in a TFS formalism underlying HPSG, as the probabilistic information is much less fine-grained. We associate a single probability with each complete TDFS that represents a lexical entry. In a probabilistic grammar based on this approach, the probability of a derivation must depend in part on details of the grammatical approach adopted. In a categorial framework it may be there are only mutually exclusive schemata for combining lexical entries into phrasal and clausal signs, so the probability of a given derivation can be treated as the product"
J99-4002,W96-0303,1,0.841797,"n Copestake and Briscoe [1995]; see Section 7), the relative productivity of each rule will be estimated in the manner described above, but the more specialized rule is likely to be more productive since it will apply to fewer entries than the more general rule. Similarly, in Figure 21, we assumed a Use-Substance lexical rule, but a more accurate estimation of probabilities might be obtained by considering specialized subclasses. This approach to deriving estimates of the productivity of lexical rules is applied to four denominal verb forma516 Briscoe and Copestake Lexical Rules tion rules in Briscoe and Copestake (1996), where the probabilities of the basic and derived word forms are estimated from part-of-speech tagged textual corpora. The probabilistic approach we have presented is part of a theory of language use or performance rather than one of competence or grammatical representation. As such it is not a part of the T(D)FS representation language, which is intended as a general formalism in which paradigmatic (lexical) and syntagmatic (syntactic and semantic) theories can be encoded or embedded. This probabilistic approach to lexical rules integrates neatly with extant proposals to control application"
J99-4002,J91-3003,0,0.276349,"nct Introduction, which can recursively add adverbial categories to the SUBCAT list of a verbal category. There are three main problems with the treatment of lexical, or what might be better termed unary, rules as a h o m o g e n e o u s class. * Computer Laboratory, University of Cambridge, Pembroke Street, Cambridge CB2 3QG, UK. E-maih ejb@cl.cam.ac.uk t Center for the Study of Language and Information, Stanford University, Ventura Hall, Stanford, CA 94305. E-mail: aac@csli.stanford.edu (~) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 4 Firstly, Carpenter (1991) demonstrates that if lexical rules are able to perform arbitrary manipulations (deletion, addition, and permutation) of potentially unbounded lists, any recursively enumerable language can be generated, even if the nonderived lexicon and grammar only generate context-free languages. However, once we are committed to treating rules such as Passive and Adjunct Introduction in a homogeneous way, restrictions that prevent lexical rules from increasing generative capacity, such as constraining the use of category variables, bounding the length of the suBCAT list, or limiting recursive application,"
J99-4002,J93-1001,0,0.0199297,"Missing"
J99-4002,1995.tmi-1.2,1,0.876318,"25, Number 4 intrans-verb PHON: [] SYN r RESULT : ssign ] : [ ACTIVE: n p s i g n • J r . • ~ r v-a~t-cause-move 1 SEM . &lt; verD-reJ . ~V~NT: e [EVENT:e] [ARG: • ] &gt; LALTS: [] [ TRANS-ALT : caus-inchoat ] • trans-caus-verb PHON: [] SYN [RESULT [ RESUcw:ssign ]] : : ACTIVE npslgn m ACTIVE npsign [~ r p&apos;agt&apos;cause ] . [ p&apos;pat&apos;m°ve ] SEM :&lt; [,verb&apos;relEvENT:e]j.|EVENT:e EVENT:e / &gt; LARG: [~ ARG: [] J •ALTS : [] Figure 4 The Causative-Inchoative lexical rule: Sanfilippo&apos;s approach. we are using an abbreviated version of the &quot;minimally-recursive&quot; style of encoding for the semantics (MRS) described by Copestake et al. (1995). The semantics for the causative form of gallop described is equivalent in linearized notation to: [gallop(e) A p-agt-cause(e, x) A p-pat-move(e, y)] However, the rule can only apply if the transitive entry for gallop specifies caus-inchoat as the value of ALTS TRANS-ALT.An immediate problem arises, because as Pinker (1989) and others have argued, the rule is semiproductive rather than purely abbreviatory, in the sense that nonce usages are clearly interpreted conventionally as being novel (mis)applications of such rules. As in for example, Kim subscribed his friend to Byte for a year or Don&apos;"
J99-4002,P97-1018,1,0.891387,"Missing"
J99-4002,E89-1009,0,0.0899967,"Missing"
J99-4002,J96-2002,0,0.0194019,"Missing"
J99-4002,P95-1014,0,0.0176077,"be subsumed by one of the basic descriptions or by a description derived via one or more lexical rule applications (i.e., any description tagged [] ). This latter step integrates the interpretation of lexical rules into the underlying constraint logic of TFS descriptions by structure sharing information between descriptions of basic and derived lexical entries. In view of the unrestricted generative power of conventional HPSG-style lexical rules (Carpenter 1991), naive generative application of recursive or cyclic rules can lead to nontermination during parsing. Bouma and van Noord (1994) and Johnson and Dorre (1995) propose techniques for delayed evaluation of lexical rules so that they apply &quot;on demand&quot; at parse time. Meurers and Minnen (1997) present a computational framework for efficient application of Meurers&apos; (1995) formalization of lexical rules. In their covariation approach, a finite-state machine for the application of lexical rules is derived by computing possible &quot;follow&quot; relations between the set of rules. Next, pruned finite-state machines are associated with classes of actual lexical entries representing the restricted set of rules that can apply to those entries. Finally, entries themselv"
J99-4002,P91-1008,0,0.0108986,"Missing"
J99-4002,J99-1002,1,0.88576,"he full range of rules proposed shows that Carpenter&apos;s (1991) postulated upper bound on the length of list-valued attributes such as SUBCATin the lexicon cannot be maintained, leading to unrestricted generative capacity in constraint-based formalisms utilizing HPSG-style lexical rules. We argue that it is preferable to subdivide such rules into a class of semiproductive lexically governed genuinely lexical rules, and a class offully productive unary syntactic rules. We develop a restricted approach to lexical rules in a typed default feature structure (TDFS) framework (Lascarides et al. 1995; Lascarides and Copestake 1999), which has enough expressivity to state, for example, rules of verb diathesis alternation, but which does not allow arbitrary manipulation of list-valued features. An interpretation of such lexical rules within a probabilistic version of a TDFS-based linguistic (lexical and grammatical) theory allows us to capture the semiproductive nature of genuinely lexical rules, steering an intermediate course between fully generative or purely abbreviatory rules. We illustrate the utility of this approach with a treatment of dative constructions within a linguistic framework that borrows insights from t"
J99-4002,J92-2002,0,0.0276146,"features that are omitted have their values copied over, giving the notation shown in Figure 2. A number of modifications of this original proposal within the HPSG framework have been proposed. Copestake and Briscoe (1992) and Copestake (1992) represent lexical rules as TFSs containing 1 and 0 attributes representing input and output descriptions of the lexical rule, respectively. This enables lexical rules to be encoded in a type hierarchy and for relationships between rules to be expressed via type inheritance. The interpretation of lexical rules is analogous to that of grammar rules (e.g., Shieber 1992), and such rules can be thought of as equivalent to unary grammar rules. 489 Computational Linguistics Volume 25, Number 4 3rdsng-lr IN: [base] [ 3rdsn~: ] OUT : |PHOI~ :f3rdsng( [], []) [ 3RDSNG: [] Figure 3 Reformulated Third Singular Verb Formation lexical rule. Calcagno (1995) develops an algorithm for improving the notation for lexical rules by eliminating the need to specify what is copied from input to output. Meurers (1995) also develops a similar algorithm but, although he augments the description language to allow lexical rules to be written in this abbreviated notation, he interpret"
J99-4002,W91-0209,1,\N,Missing
J99-4002,J97-4003,0,\N,Missing
J99-4002,J92-2004,0,\N,Missing
J99-4002,J94-3010,0,\N,Missing
J99-4002,C92-2066,0,\N,Missing
J99-4002,J92-3003,0,\N,Missing
J99-4002,P93-1028,0,\N,Missing
korhonen-etal-2006-large,J87-3002,1,\N,Missing
korhonen-etal-2006-large,W98-1505,0,\N,Missing
korhonen-etal-2006-large,A00-2034,0,\N,Missing
korhonen-etal-2006-large,rose-etal-2002-reuters,0,\N,Missing
korhonen-etal-2006-large,C94-1042,0,\N,Missing
korhonen-etal-2006-large,W02-0907,1,\N,Missing
korhonen-etal-2006-large,A97-1052,1,\N,Missing
korhonen-etal-2006-large,J93-2002,0,\N,Missing
korhonen-etal-2006-large,P05-1038,0,\N,Missing
korhonen-etal-2006-large,P03-1007,1,\N,Missing
korhonen-etal-2006-large,P04-2007,0,\N,Missing
korhonen-etal-2006-large,briscoe-carroll-2002-robust,1,\N,Missing
korhonen-etal-2006-large,P03-1002,0,\N,Missing
korhonen-etal-2006-large,P03-1009,1,\N,Missing
korhonen-etal-2006-large,P02-1029,0,\N,Missing
korhonen-etal-2006-large,J03-4004,0,\N,Missing
korhonen-etal-2006-large,W02-2014,1,\N,Missing
N10-2001,P06-4020,1,0.739372,"search. Named Entity Recognition NER in the biomedical domain was implemented as described in Vlachos (2007). Gene Mention tagging was performed using Conditional Random Fields and syntactic parsing, using features derived from grammatical relations to augment the tagging. We also use a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model focuses on biomedical entities and seeks to find the antecedents of anaphora, both coreferent and associative ones, and also to identify discoursenew expressions (Gasperin and Briscoe, 2008). Parsing The RASP toolkit (Briscoe et al., 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text. GRs are triplets consisting of a relation-type and two arguments and also encode morphology, word position and part-of-speech; for example, parsing “John likes Mary.” gives us a subject relation and a direct object relation: (|ncsubj ||like+s:2 VVZ ||John:1 NP1|) (|dobj ||like+s:2 VVZ ||Mary:3 NP1|) Representing a parse as a set of flat triplets allows us to index on grammatical relations, thus enabling complex relational queries. 4.3 Image Processing We build a"
N10-2001,C08-1033,1,0.822796,"put of these systems is then indexed, enabling semantic search. Named Entity Recognition NER in the biomedical domain was implemented as described in Vlachos (2007). Gene Mention tagging was performed using Conditional Random Fields and syntactic parsing, using features derived from grammatical relations to augment the tagging. We also use a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model focuses on biomedical entities and seeks to find the antecedents of anaphora, both coreferent and associative ones, and also to identify discoursenew expressions (Gasperin and Briscoe, 2008). Parsing The RASP toolkit (Briscoe et al., 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text. GRs are triplets consisting of a relation-type and two arguments and also encode morphology, word position and part-of-speech; for example, parsing “John likes Mary.” gives us a subject relation and a direct object relation: (|ncsubj ||like+s:2 VVZ ||John:1 NP1|) (|dobj ||like+s:2 VVZ ||Mary:3 NP1|) Representing a parse as a set of flat triplets allows us to index on grammatical relations, thus enabling complex re"
N10-2001,E99-1015,0,0.0516161,"re (such as Ariadne Genomics, Temis or Linguamatics). This option is only available to a tiny minority of researchers working for large wellfunded corporations. 4 Summary of Technology 4.1 PDF to SciXML The PDF format represents a document in a manner designed to facilitate printing. In short, it provides information on font and position for textual and graphical units. To enable information retrieval and extraction, we need to convert this typographic representation into a logical one that reflects the structure of scientific documents. We use an XML schema called SciXML (first introduced in Teufel et al. (1999)) that we extend to include images. We linearise the textual elements in the PDF, representing these as <div&gt; elements in XML and classify these divisions as {Title|Author|Affiliation|Abstract|Footnote|Caption| 2 Heading|Citation |References|Text} in a constraint satisfaction framework. In addition, we identify all graphics in the PDF, including lines and images. We then identify tables by looking for specific patterns of text and lines. A bounding box is identified for a table and an image is generated that overlays the text on the lines. Similarly we overlay text onto images that have been i"
N13-1040,W10-0216,0,0.0604882,"Missing"
N13-1040,P11-1070,0,0.0491511,"Missing"
N13-1040,P06-2006,1,0.830229,"node in graph g, e ∈ Eg is an edge in graph g, isDep(e, n) is a function returning 1.0 if n is the dependent in edge e, and 0.0 otherwise. NScore(n) is set to 0 if the node does not appear as a dependent in any edges. We found this metric performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marc"
N13-1040,P06-4020,1,0.81925,"Missing"
N13-1040,D07-1101,0,0.0180584,"dence scores for bilexical relations. Finally, we describe methods for combining together these scores and calculating an overall score for a dependency graph. We make publically available all the code developed for performing these steps in the parse reranking system.1 3.1 Graph modifications For every dependency graph gr the graph expansion procedure creates a modified representation gr0 which contains a wider range of bilexical relations. The motivation for this graph expansion step is similar to that motivating the move from first-order to higher-order dependency path feature types (e.g., Carreras (2007)). However, compared to using all nth-order paths, these rules are chosen to maximise the utility and minimise the sparsity of the resulting bilexical features. In addition, the cascading nature of the expansion steps means in some cases the expansion captures useful 3rd and 4th order dependencies. Similar approaches to graph modifications have been successfully used for several NLP tasks (van Noord, 2007; Arora et al., 2010). For any edge e we also use notation (rel, w1 , w2 ), referring to an edge from w1 to w2 with the label rel. We perform the following modifications on every dependency gr"
N13-1040,P05-1022,0,0.278708,"Missing"
N13-1040,P07-1032,0,0.0457012,"Missing"
N13-1040,W01-0521,0,0.169205,"Missing"
N13-1040,W03-2401,0,0.040173,"s a function returning 1.0 if n is the dependent in edge e, and 0.0 otherwise. NScore(n) is set to 0 if the node does not appear as a dependent in any edges. We found this metric performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sent"
N13-1040,P03-1054,0,0.0187076,"ic analysis in ambiguous contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to depen"
N13-1040,J93-2004,0,0.0429583,"Missing"
N13-1040,N06-1020,0,0.190011,"Missing"
N13-1040,U10-1014,0,0.0399222,"Missing"
N13-1040,P06-1055,0,0.0450946,"contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to dependency parsing. As many"
N13-1040,W08-1302,0,0.0420778,"Missing"
N13-1040,A97-1015,0,0.225961,"Missing"
N13-1040,tateisi-etal-2008-genia,0,0.0368155,"Missing"
N13-1040,W07-2201,0,0.0575934,"Missing"
N13-1040,W07-2203,1,0.830065,"performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sentences) and test data (560 sentences). We made use of the development data to experiment with a wider selection of edge and graph scoring methods, and report the final results on the"
N13-1040,P11-1156,0,0.0283641,"Missing"
N15-1060,C12-1038,0,0.581048,"tion and is sensitive to different types of edit operations. 1 You have missed word. You have missed a word. ( → a) (word → a word) or (word → words) Figure 1: Mismatch between system and gold standard edits producing the same corrected sentence. Introduction A range of methods have been applied to evaluation of grammatical error correction, but no entirely satisfactory method has emerged as yet. Standard metrics (such as accuracy, precision, recall and F -score) have been used, but they can lead to different results depending on the criteria used for their computation (Leacock et al., 2014; Chodorow et al., 2012). Accuracy, for example, can only be computed in cases where we can enumerate all true negatives, which is why it has been mostly used for article and preposition errors (De Felice and Pulman, 2008; Rozovskaya and Roth, 2010). Extending this approach to other error types involves the identification of all relevant instances or positions where an error can occur, which is not always easy and renders the evaluation process costly, languagedependent, and possibly inexact. Accuracy has also been criticised as being a poor indicator of predictive power, especially on unbalanced datasets (Manning an"
N15-1060,N12-1067,0,0.276479,"ndicator of predictive power, especially on unbalanced datasets (Manning and Sch¨utze, 1999). Alternatively, we can compute precision (P ), recall (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting true negatives. This was the official evaluation scheme adopted for the HOO 2011 (Dale and Kilgarriff, 2011) and HOO 2012 (Dale et al., 2012) shared tasks. However, these metrics can fail when edits are not identical and therefore underestimate system performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction systems and has been used as the official scorer in the subsequent CoNLL 2013 (Ng et al., 2013), CoNLL 2014 (Ng et al., 2014) and EMNLP 2014 (Mohit et al., 2014) shared tasks. In 2014, system ranking was based on F0.5 score, weighting precision twice as highl"
N15-1060,W11-2838,0,0.249858,"r types involves the identification of all relevant instances or positions where an error can occur, which is not always easy and renders the evaluation process costly, languagedependent, and possibly inexact. Accuracy has also been criticised as being a poor indicator of predictive power, especially on unbalanced datasets (Manning and Sch¨utze, 1999). Alternatively, we can compute precision (P ), recall (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting true negatives. This was the official evaluation scheme adopted for the HOO 2011 (Dale and Kilgarriff, 2011) and HOO 2012 (Dale et al., 2012) shared tasks. However, these metrics can fail when edits are not identical and therefore underestimate system performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction s"
N15-1060,W12-2006,0,0.364338,"l relevant instances or positions where an error can occur, which is not always easy and renders the evaluation process costly, languagedependent, and possibly inexact. Accuracy has also been criticised as being a poor indicator of predictive power, especially on unbalanced datasets (Manning and Sch¨utze, 1999). Alternatively, we can compute precision (P ), recall (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting true negatives. This was the official evaluation scheme adopted for the HOO 2011 (Dale and Kilgarriff, 2011) and HOO 2012 (Dale et al., 2012) shared tasks. However, these metrics can fail when edits are not identical and therefore underestimate system performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction systems and has been used as the o"
N15-1060,C08-1022,0,0.354543,"Missing"
N15-1060,W14-3605,0,0.0186414,"em performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction systems and has been used as the official scorer in the subsequent CoNLL 2013 (Ng et al., 2013), CoNLL 2014 (Ng et al., 2014) and EMNLP 2014 (Mohit et al., 2014) shared tasks. In 2014, system ranking was based on F0.5 score, weighting precision twice as highly as recall. Nevertheless, this method also suffers from a number of limitations: 578 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 578–587, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Source This machines is designed for help people . Annotator 1 (This → These), (is → are), (help → helping) Annotator 2 (machines → machine), (for → to) System hypothesis These machines are designed to help people"
N15-1060,W13-3601,0,0.310542,"l when edits are not identical and therefore underestimate system performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction systems and has been used as the official scorer in the subsequent CoNLL 2013 (Ng et al., 2013), CoNLL 2014 (Ng et al., 2014) and EMNLP 2014 (Mohit et al., 2014) shared tasks. In 2014, system ranking was based on F0.5 score, weighting precision twice as highly as recall. Nevertheless, this method also suffers from a number of limitations: 578 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 578–587, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Source This machines is designed for help people . Annotator 1 (This → These), (is → are), (help → helping) Annotator 2 (machines → machine), (for"
N15-1060,W14-1701,1,0.462639,"and therefore underestimate system performance (see Figure 1). This problem was later addressed by the MaxMatch or M2 Scorer (Dahlmeier and Ng, 2012), which is able to identify equivalent edits by applying a transitive rule (e.g. ( → a) + (word → word) ⇒ (word → a word)). The scorer also allows for multiple gold standard annotations of each sentence, choosing the ones that maximise overall F -score. So far, the M2 Scorer has been the most reliable tool for evaluating error correction systems and has been used as the official scorer in the subsequent CoNLL 2013 (Ng et al., 2013), CoNLL 2014 (Ng et al., 2014) and EMNLP 2014 (Mohit et al., 2014) shared tasks. In 2014, system ranking was based on F0.5 score, weighting precision twice as highly as recall. Nevertheless, this method also suffers from a number of limitations: 578 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 578–587, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Source This machines is designed for help people . Annotator 1 (This → These), (is → are), (help → helping) Annotator 2 (machines → machine), (for → to) System hypothesis These"
N15-1060,N10-1018,0,0.0170306,"e same corrected sentence. Introduction A range of methods have been applied to evaluation of grammatical error correction, but no entirely satisfactory method has emerged as yet. Standard metrics (such as accuracy, precision, recall and F -score) have been used, but they can lead to different results depending on the criteria used for their computation (Leacock et al., 2014; Chodorow et al., 2012). Accuracy, for example, can only be computed in cases where we can enumerate all true negatives, which is why it has been mostly used for article and preposition errors (De Felice and Pulman, 2008; Rozovskaya and Roth, 2010). Extending this approach to other error types involves the identification of all relevant instances or positions where an error can occur, which is not always easy and renders the evaluation process costly, languagedependent, and possibly inexact. Accuracy has also been criticised as being a poor indicator of predictive power, especially on unbalanced datasets (Manning and Sch¨utze, 1999). Alternatively, we can compute precision (P ), recall (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting true negatives. This was the official evalu"
N16-1042,W05-0909,0,0.00983252,"nstead of first re-annotating training data, and then building new NMT models using this newly annotated data as proposed by Luong et al. (2015). Our approach is much simpler as we avoid re-annotating any data and train only one NMT model. Due to the nature of error correction (i.e. both source and target sentences are in the same language), most words translate as themselves, and errors are often similar to their correct forms. Thus, unsupervised aligners can be successfully used to align the unknown target words. Two automatic alignment tools are used: GIZA++ (Och and Ney, 2003) and METEOR (Banerjee and Lavie, 2005). GIZA++ is an implementation of IBM Models 1-5 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996), which can align two sentences from any languages. Unlike GIZA++, METEOR aligns two sentences from the same language. The latest METEOR 1.5 only supports a few languages, and English is one of them. METEOR identifies not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases. This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share identical stems, as well as word choice"
N16-1042,P06-4020,1,0.266474,"(Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used t"
N16-1042,P06-1032,0,0.604514,"015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used t"
N16-1042,J93-2003,0,0.0592599,"els using this newly annotated data as proposed by Luong et al. (2015). Our approach is much simpler as we avoid re-annotating any data and train only one NMT model. Due to the nature of error correction (i.e. both source and target sentences are in the same language), most words translate as themselves, and errors are often similar to their correct forms. Thus, unsupervised aligners can be successfully used to align the unknown target words. Two automatic alignment tools are used: GIZA++ (Och and Ney, 2003) and METEOR (Banerjee and Lavie, 2005). GIZA++ is an implementation of IBM Models 1-5 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996), which can align two sentences from any languages. Unlike GIZA++, METEOR aligns two sentences from the same language. The latest METEOR 1.5 only supports a few languages, and English is one of them. METEOR identifies not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases. This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share identical stems, as well as word choice corrections with synonyms or unigram paraphrases. To build a word le"
N16-1042,P11-1092,0,0.0160304,"Missing"
N16-1042,N12-1067,0,0.186542,"to build good MT systems, we add training examples extracted from the CLC. Overall, there are 1,965,727 pairs of parallel sentences in our training set. The source side contains 28,823,615 words with 248,028 unique words, and the target side contains 29,219,128 words with 143,852 unique words. As we can see, the source side vocabulary size is much larger than that of the target side. Training and test data is pre-processed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three automatic evaluation metrics: I-measure (Felice and Briscoe, 2015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice"
N16-1042,W13-1703,0,0.556378,"Missing"
N16-1042,N15-1060,1,0.436733,"nce the FCE training set is too small to build good MT systems, we add training examples extracted from the CLC. Overall, there are 1,965,727 pairs of parallel sentences in our training set. The source side contains 28,823,615 words with 248,028 unique words, and the target side contains 29,219,128 words with 143,852 unique words. As we can see, the source side vocabulary size is much larger than that of the target side. Training and test data is pre-processed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three automatic evaluation metrics: I-measure (Felice and Briscoe, 2015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Broc"
N16-1042,W14-1702,1,0.585898,"t, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively. 1 Introduction Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in text written by non-native English writers. Unlike building machine learning classifiers for specific error types (e.g. determiner or preposition errors) (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), the idea of ‘translating’ a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large n"
N16-1042,P15-1001,0,0.145661,"n model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively (Luong et al., 2015). NMTbased systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC. However, NMT models typically limit vocabulary size on both source and target sides due to the complexity of training (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Jean et al., 2015). Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are replaced with UNK symbol. This problem is more serious for GEC as non-native text contains not only rare words (e.g. proper nouns), but also misspelled words (i.e. spelling errors). By replacing all the OOV words with the same UNK symbol, useful information is discarded, resulting in systems that are not able to correct misspelled words or even keep some of the error-free original words, as in the following examples (OOV words are underlined): Original sentence ... I am goign to make a plan ... System h"
N16-1042,W14-1703,0,0.477562,"state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively. 1 Introduction Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in text written by non-native English writers. Unlike building machine learning classifiers for specific error types (e.g. determiner or preposition errors) (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), the idea of ‘translating’ a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and"
N16-1042,D13-1176,0,0.135923,"pes (e.g. determiner or preposition errors) (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), the idea of ‘translating’ a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively (Luong et al., 2015). NMTbased systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC."
N16-1042,J10-4005,0,0.00766537,"types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively (Luong et al., 2015). NMTbased systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC. However, NMT models typically limit vocabulary size on both source and target sides due to the complexity of training (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Jean et al., 2015). Therefore, they are u"
N16-1042,P15-1002,0,0.519429,"several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively (Luong et al., 2015). NMTbased systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC. However, NMT models typically limit vocabulary size on both source and target sides due to the complexity of training (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Jean et al., 2015). Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are replaced with UNK symbol. This problem is more serious for GEC as non-native text contains not only rare words (e.g. proper nouns), but also misspelled words (i.e. spelling errors). By replacing all th"
N16-1042,P15-2097,0,0.362329,"raining examples extracted from the CLC. Overall, there are 1,965,727 pairs of parallel sentences in our training set. The source side contains 28,823,615 words with 248,028 unique words, and the target side contains 29,219,128 words with 143,852 unique words. As we can see, the source side vocabulary size is much larger than that of the target side. Training and test data is pre-processed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three automatic evaluation metrics: I-measure (Felice and Briscoe, 2015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based"
N16-1042,P11-1064,0,0.0265242,"d by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used to buid a 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). 4.4 NMT training details Our traini"
N16-1042,W13-3601,0,0.0810781,"As we can see, the source side vocabulary size is much larger than that of the target side. Training and test data is pre-processed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three automatic evaluation metrics: I-measure (Felice and Briscoe, 2015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felic"
N16-1042,W14-1701,1,0.829876,"errors in text written by non-native English writers. Unlike building machine learning classifiers for specific error types (e.g. determiner or preposition errors) (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), the idea of ‘translating’ a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more e"
N16-1042,J03-1002,0,0.030873,"use only the NMT model output instead of first re-annotating training data, and then building new NMT models using this newly annotated data as proposed by Luong et al. (2015). Our approach is much simpler as we avoid re-annotating any data and train only one NMT model. Due to the nature of error correction (i.e. both source and target sentences are in the same language), most words translate as themselves, and errors are often similar to their correct forms. Thus, unsupervised aligners can be successfully used to align the unknown target words. Two automatic alignment tools are used: GIZA++ (Och and Ney, 2003) and METEOR (Banerjee and Lavie, 2005). GIZA++ is an implementation of IBM Models 1-5 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996), which can align two sentences from any languages. Unlike GIZA++, METEOR aligns two sentences from the same language. The latest METEOR 1.5 only supports a few languages, and English is one of them. METEOR identifies not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases. This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share id"
N16-1042,P02-1040,0,0.120166,"essed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three automatic evaluation metrics: I-measure (Felice and Briscoe, 2015), M2 Scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the co"
N16-1042,P11-1093,0,0.0651309,"Missing"
N16-1042,D14-1102,0,0.199485,"Missing"
N16-1042,C08-1109,0,0.0167915,"Missing"
N16-1042,C96-2141,0,0.101876,"(2015). Our approach is much simpler as we avoid re-annotating any data and train only one NMT model. Due to the nature of error correction (i.e. both source and target sentences are in the same language), most words translate as themselves, and errors are often similar to their correct forms. Thus, unsupervised aligners can be successfully used to align the unknown target words. Two automatic alignment tools are used: GIZA++ (Och and Ney, 2003) and METEOR (Banerjee and Lavie, 2005). GIZA++ is an implementation of IBM Models 1-5 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996), which can align two sentences from any languages. Unlike GIZA++, METEOR aligns two sentences from the same language. The latest METEOR 1.5 only supports a few languages, and English is one of them. METEOR identifies not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases. This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share identical stems, as well as word choice corrections with synonyms or unigram paraphrases. To build a word level translation model for translating the source words that are"
N16-1042,P11-1019,1,0.724892,"s not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases. This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share identical stems, as well as word choice corrections with synonyms or unigram paraphrases. To build a word level translation model for translating the source words that are responsible for the target unknown words, we need word-aligned data. The IBM Models are used to learn word alignment from parallel sentences. 382 4 4.1 Experiments Dataset We use the publicly available FCE dataset (Yannakoudakis et al., 2011), which is a part of the Cambridge Learner Corpus (CLC) (Nicholls, 2003). The FCE dataset contains 1,244 scripts produced by learners taking the First Certificate in English (FCE) examination between 2000 and 2001. The texts have been manually annotated by linguists using a taxonomy of approximately 80 error types. The publicly available FCE dataset contains about 30,995 pairs of parallel sentences for training (approx. 496,567 tokens on the target side) and about 2,691 pairs of parallel sentences for testing (approx. 41,986 tokens on the target side). Since the FCE training set is too small t"
N16-1042,W13-3607,1,0.645524,"er and Ng, 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source). The M2 Scorer was the official scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F0.5 being the reported metric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pialign (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used to buid a 5-gram language"
N16-1042,P07-2045,0,\N,Missing
N16-1042,W14-1704,0,\N,Missing
N16-1042,D14-1179,0,\N,Missing
N18-1024,P16-1068,1,0.917429,"oach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models. 1 i. We examine the robustness of state-of-the-art neural AES models to adversarially crafted input,1 and specifically focus on input related to local coherence; that is, grammatical but incoherent sequences of sentences.2 In addition to the superiority in performance of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches t"
N18-1024,W13-1704,1,0.853287,"ce of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches to AES achieve state-of-the-art results (Alikaniotis et al., 2016; Taghipour and Ng, 2016) with the additional advantage of utilizing features that are automatically learned from the data. In order to facilitate interpretability of neural models, a number of visualization techniques have been proposed to identify textual (superficial) features that contribute to model performance (Alikaniotis et al., 2016). To the best of our knowledge, however, no prior work has investigated the robustness of neural AES systems to adversaria"
N18-1024,J08-1001,0,0.679771,"connectedness features between sentences. 1 We use the terms ‘adversarially crafted input’ and ‘adversarial input’ to refer to text that is designed with the intention to trick the system. 2 Coherence can be assessed locally in terms of transitions between adjacent sentences. 263 Proceedings of NAACL-HLT 2018, pages 263–271 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics iii. A local coherence model is typically evaluated based on its ability to rank coherently ordered sequences of sentences higher than their incoherent / permuted counterparts (e.g., Barzilay and Lapata (2008)). We focus on a stricter evaluation setting in which the model is tested on its ability to rank coherent sequences of sentences higher than any incoherent / permuted set of sentences, and not just its own permuted counterparts. This supports a more rigorous evaluation that facilitates development of more robust models. 12 texts in total. Higgins and Heilman (2014) proposed a framework for evaluating the susceptibility of AES systems to gaming behavior. Neural AES Models Alikaniotis et al. (2016) developed a deep bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) ne"
N18-1024,P98-1032,0,0.0781803,"AES approaches with respect to adversarial input related to local aspects of coherence. For our experiments, we use the Automated Student Assessment Prize (ASAP) dataset,3 which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4). 2 Related Work AES Evaluation against Adversarial Input One of the earliest attempts at evaluating AES models against adversarial input was by Powers et al. (2002) who asked writing experts – that had been briefed on how the e-Rater scoring system works – to write essays to trick e-Rater (Burstein et al., 1998). The participants managed to fool the system into assigning higher-than-deserved grades, most notably by simply repeating a few wellwritten paragraphs several times. Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012) created and used an adversarial dataset of well-written texts and their random sentence permutations, which they released in the public domain, together with the grades assigned by a human expert to each piece of text. Unfortunately, however, the dataset is quite small, consisting of 3 https://www.kaggle.com/c/asap-aes/ 264 Figure 1: Local Coherence (LC) model archi"
N18-1024,D16-1193,0,0.211424,"aselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models. 1 i. We examine the robustness of state-of-the-art neural AES models to adversarially crafted input,1 and specifically focus on input related to local coherence; that is, grammatical but incoherent sequences of sentences.2 In addition to the superiority in performance of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches to AES achieve state-of-th"
N18-1024,D16-1115,0,0.164171,"bility of AES systems to gaming behavior. Neural AES Models Alikaniotis et al. (2016) developed a deep bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network, augmented with score-specific word embeddings that capture both contextual and usage information for words. Their approach outperformed traditional feature-engineered AES models on the ASAP dataset. Taghipour and Ng (2016) investigated various recurrent and convolutional architectures on the same dataset and found that an LSTM layer followed by a Mean over Time operation achieves state-of-the-art results. Dong and Zhang (2016) showed that a twolayer Convolutional Neural Network (CNN) outperformed other baselines (e.g., Bayesian Linear Ridge Regression) on both in-domain and domainadaptation experiments on the ASAP dataset. iv. We propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens AES validity. Neural Coherence Mo"
N18-1024,P17-1121,0,0.020069,"oring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level language model, and evaluated their approach on the sentence ordering task. Tien Nguyen and Joty (2017) built a CNN over entity grid representations, and trained the network in a pairwise ranking fashion. Their model outperformed other graph-based and distributed sentence models. We note that our goal is not to identify the “best” model of local coherence on randomly permuted grammatical sentences in the domain of AES, but rather to propose a framework that strengthens the validity of AES approaches with respect to adversarial input related to local aspects of coherence. At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neural AES approaches wit"
N18-1024,P11-2022,0,0.0212531,"tored on the development sets – we select the model that yields the highest PRA value.12 We use as a baseline the LC model that is based on the multiplication of the clique scores (similarly to Li and Hovy (2014)), and compare the results (LCmul ) to our averaged approach. As another baseline, we use the entity grid (EGrid) (Barzilay and Lapata, 2008) that models transitions between sentences based on sequences of entity mentions labeled with their grammatical role. EGrid has been shown to give competitive results on similar coherence tasks in other domains. Using the Brown Coherence Toolkit (Eisner and Charniak, 2011),13 we construct the entity transition probabilities with length = 3 and salience = 2. The transition probabilities are then used as features that are fed as input to an SVM classifier with an RBF kernel and penalty parameter C = 1.5 to predict a coherence score. where M is the number of synthetic essays in the development set. We furthermore evaluate a baseline where the joint model is trained without sharing the word embedding layer between the two submodels, and report the effect on performance (Joint Learningno layer sharing ). Finally, we evaluate a baseline where for the joint model we s"
N18-1024,W12-2004,1,0.914003,"g from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4). 2 Related Work AES Evaluation against Adversarial Input One of the earliest attempts at evaluating AES models against adversarial input was by Powers et al. (2002) who asked writing experts – that had been briefed on how the e-Rater scoring system works – to write essays to trick e-Rater (Burstein et al., 1998). The participants managed to fool the system into assigning higher-than-deserved grades, most notably by simply repeating a few wellwritten paragraphs several times. Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012) created and used an adversarial dataset of well-written texts and their random sentence permutations, which they released in the public domain, together with the grades assigned by a human expert to each piece of text. Unfortunately, however, the dataset is quite small, consisting of 3 https://www.kaggle.com/c/asap-aes/ 264 Figure 1: Local Coherence (LC) model architecture using a window of size 3. All hsnt representations are computed the same way as hsnt 1 . The figure depicts the process of predicting the first clique score, which is applied to all the cliques in the text. The output coher"
N18-1024,P11-1019,1,0.922093,"ing for Adversarially Crafted Input Youmna Farag Helen Yannakoudakis Ted Briscoe Department of Computer Science and Technology The ALTA Institute University of Cambridge United Kingdom {youmna.farag,helen.yannakoudakis,ted.briscoe}@cl.cam.ac.uk Abstract missclassifications; for instance, a high score to a low quality text. Examining and addressing such validity issues is critical and imperative for AES deployment. Previous work has primarily focused on assessing the robustness of “standard” machine learning approaches that rely on manual feature engineering; for example, Powers et al. (2002); Yannakoudakis et al. (2011) have shown that such AES systems, unless explicitly designed to handle adversarial input, can be susceptible to subversion by writers who understand something of the systems’ workings and can exploit this to maximize their score. In this paper, we make the following contributions: We demonstrate that current state-of-theart approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences,"
N18-1024,D14-1218,0,0.281075,"(e.g., Bayesian Linear Ridge Regression) on both in-domain and domainadaptation experiments on the ASAP dataset. iv. We propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens AES validity. Neural Coherence Models A number of approaches have investigated neural models of coherence on news data. Li and Hovy (2014) used a window approach where a sliding kernel of weights was applied over neighboring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level"
N18-1024,D17-1019,0,0.0291831,", j ∈ {1, ..., N − m + 1}, N is the number of sentences in the text, and ∗ is the linear convolutional operation. Scoring The cliques’ predicted scores are calculated via a linear operation followed by a sigmoid function to project the predictions to a [0, 1] probability space: (1) yˆjclq = sigmoid(hclq j .V ) (3) where V ∈ Rdcnn is a learned weight. The network optimizes its parameters to minimize the negative log-likelihood of the cliques’ gold scores y clq , given the network’s predicted scores: Clique Representation Each window of sentences in a text represents a clique q = 4 We note that Li and Jurafsky (2017) also present an extended version of the work by Li and Hovy (2014), evaluated on different domains. 5 LSTMs have been shown to produce state-of-the-art results in AES (Alikaniotis et al., 2016; Taghipour and Ng, 2016). Llocal T 1X = [−yjclq log(ˆ yjclq ) T j=1 −(1 − yjclq )log(1 − yˆjclq )] 265 (4) 3.3 Combined Models We propose a framework for integrating the LSTMT&N model with the Local Coherence (LC) one. Our goal is to have a robust AES system that is able to correctly flag adversarial input while maintaining a high performance on essay scoring. 3.3.1 The baseline model simply concatenate"
N18-1024,D13-1141,0,0.0116969,"e is larger than the threshold. We experimentally demonstrate that this approach enables the model to perform well on both original ASAP and synthetic essays. During model evaluation, the texts flagged as adversarial by the model are assigned a score of zero, while the rest are assigned the predicted essay score (ˆ y esy in Figure 3). 4 5 Model Parameters and Baselines Coherence models We train and test the LC model described in Section 3.1 on the synthetic dataset and evaluate it using PRA and TPRA. During pre-processing, words are lowercased and initialized with pre-trained word embeddings (Zou et al., 2013). Words that occur only once in the training set are mapped to a special UNK embedData and Evaluation We use the ASAP dataset, which contains 12, 976 essays written by students ranging from Grade 7 to 10 We note that this threshold is different than the one mentioned in Section 3.3.2. 11 This is primarily done to keep the data balanced: initial experiments showed that training with all 10 permutations per essay harms AES performance, but has negligible effect on adversarial input detection. 9 We note that, during training, the scores are mapped to a range between 0 and 1 (similarly to Taghipou"
N18-1024,D15-1106,0,0.0214758,"the development of an approach that strengthens AES validity. Neural Coherence Models A number of approaches have investigated neural models of coherence on news data. Li and Hovy (2014) used a window approach where a sliding kernel of weights was applied over neighboring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level language model, and evaluated their approach on the sentence ordering task. Tien Nguyen and Joty (2017) built a CNN over entity grid representations, and trained the network in a pairwise ranking fashion. Their model outperformed other graph-based and distributed sentence models. We note that our goal is not to identify the “best” model of local coherence on randomly permuted grammatical sentences in the domain of AES, but rat"
N18-1024,C98-1032,0,\N,Missing
N19-1261,W11-2107,0,0.0403299,"of the learner summary: (1) verbatim features, (2) semantic similarity features, (3) features based on distributed vector representations of the summary, and (4) features that describe discourse and other textual characteristics of the summary. 4.1.1 Verbatim features Verbatim similarity is the most straightforward measure that indicates content similarity. Verbatim features measure the lexical overlap of the text units between the candidate summary and the ref2534 erence. We use the following metrics to measure verbatim similarity: ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Denkowski and Lavie, 2011). The three metrics are commonly used to assess automated summarization systems. ROUGE and BLEU are based on exact word match of N-grams, and METEOR extends the exact word match with stem, synonym, and paraphrase matches extracted from the WordNet (Miller, 1995) and a background dictionary, which allows for more flexible expressions. 4.1.2 Semantic similarity features Although verbatim overlap metrics prove to be effective in various tasks, they fail to capture the content similarity when paraphrasing and higher levels of abstraction are used in the summary. To compensate for this, word embedd"
N19-1261,W04-1013,0,0.399312,"is paper, we present a summarization task for evaluating non-native reading comprehension and propose three novel machine learning approaches to assessing learner summaries. First, we extract features to measure the content similarity between the reading passage and the summary. Secondly, we calculate a similarity matrix based on sentence-to-sentence similarity between ∗ The work by the first author was done at the University of Cambridge prior to joining Amazon Research. 2 Related Work Most of the previous studies on summary assessment are aimed at evaluating automated summarization systems (Lin, 2004; Lin and Hovy, 2003; Nenkova et al., 2007). In contrast to this line of work, our goal is to assess human-written summaries rather than machine-generated ones. Within the educational domain, several applications have been developed to help students with their writing summarization skills. Summary Street (Wade-Stein and Kintsch, 2004) is an educational software designed for children to develop summarization skills. It asks students to write a summary to a reading passage, and scores the summary by using Latent Semantic Analysis (LSA) to construct semantic representations of the text. This syst"
N19-1261,N03-1020,0,0.609113,"Missing"
N19-1261,W13-1722,0,0.0482226,"Missing"
N19-1261,P02-1040,0,0.110726,"ic features to evaluate the quality of the learner summary: (1) verbatim features, (2) semantic similarity features, (3) features based on distributed vector representations of the summary, and (4) features that describe discourse and other textual characteristics of the summary. 4.1.1 Verbatim features Verbatim similarity is the most straightforward measure that indicates content similarity. Verbatim features measure the lexical overlap of the text units between the candidate summary and the ref2534 erence. We use the following metrics to measure verbatim similarity: ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Denkowski and Lavie, 2011). The three metrics are commonly used to assess automated summarization systems. ROUGE and BLEU are based on exact word match of N-grams, and METEOR extends the exact word match with stem, synonym, and paraphrase matches extracted from the WordNet (Miller, 1995) and a background dictionary, which allows for more flexible expressions. 4.1.2 Semantic similarity features Although verbatim overlap metrics prove to be effective in various tasks, they fail to capture the content similarity when paraphrasing and higher levels of abstraction are used in the summ"
N19-1261,radev-etal-2004-mead,0,0.123935,"y to the set of key sentences. Ideally, an extractive summarizer extracts a subset of sentences from the passage that are highly representative of the original text. Although the extracted key sentences are not necessarily coherent among themselves, they provide a representation of the main ideas of the text. Comparing the candidate summary against the key sentences allows us to examine the content relevance and the coverage of the main ideas in the candidate summary. We compare two popular summarizers in selecting the key sentences for reference: TextRank (Mihalcea and Tarau, 2004) and MEAD (Radev et al., 2004). We also compare the extractive summarizers against the baseline of using a random selection of sentences as the reference. After obtaining the reference, we derive four types of linguistic features to evaluate the quality of the learner summary: (1) verbatim features, (2) semantic similarity features, (3) features based on distributed vector representations of the summary, and (4) features that describe discourse and other textual characteristics of the summary. 4.1.1 Verbatim features Verbatim similarity is the most straightforward measure that indicates content similarity. Verbatim feature"
N19-1261,W12-2018,0,0.0158964,"r representations of words from a large corpus of text. We use embeddings pre-trained on Wikipedia to compute word-to-word cosine similarity between the candidate summary and the reference. We experiment with three scoring functions to construct the textlevel semantic similarity measures from the word-to-word scores: (1) average word similarity on every word pair in the candidate summary and the reference; (2) a greedy method (Mihalcea et al., 2006) that finds the best-matching word with maximum similarity scores and computes the average over the greedily selected pairs; (3) optimal matching (Rus and Lintean, 2012) that finds the optimal alignment of word pairs and then takes the average over the alignment. 2. Sentence similarity: Skip-thought (Kiros et al., 2015) is a model for learning distributed representations of sentences. It uses an RNN-encoder to compose the sentence vector, and a decoder conditioned on the resulting vector that tries to predict the previous and the next sentences in the context. We use the model pre-trained on the BookCorpus (Zhu et al., 2015) to generate our sentence vectors. Additionally, we experiment with composing the sentence vectors using word embedding summation and tak"
N19-1261,W16-0502,1,0.851717,"Missing"
P05-1076,P87-1027,1,0.735614,"can do it” “such a young man”; “so young a man” “the younger of them”; “the young” “he died young” “Young, he was plain in appearance” “When young, he was lonely” Figure 1: Fundamental adjectival frames ing such features as the mood of the complement (mandative, interrogative, etc.), preferences for particular prepositions and whether the subject is extraposed. Even ignoring preposition preference, there are more than 30 distinguishable adjectival SCFs. Some fairly extensive frame sets can be found in large syntax dictionaries, such as COMLEX (31 SCFs) (Wolff et al., 1998) and ANLT (24 SCFs) (Boguraev et al., 1987). While such resources are generally accurate, they are disappointingly incomplete: none of the proposed frame sets in the well-known resources subsumes the others, the coverage of SCF types for individual adjectives is low, and (accurate) information on the relative frequency of SCFs for each adjective is absent. The inadequacy of manually-created dictionaries and the difficulty of adequately enhancing and maintaining the information by hand was a central motivation for early research into automatic subcategorization acquisition. The focus heretofore has remained firmly on verb subcategorizat"
P05-1076,P91-1027,0,0.259506,"is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less mul"
P05-1076,A97-1052,1,0.888084,"evelopment of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost"
P05-1076,briscoe-carroll-2002-robust,1,0.848006,"c Ann Arbor, June 2005. 2005 Association for Computational Linguistics (from tagging to syntactic and semantic analysis). Automatic SCF acquisition techniques are particularly important for adjectives because extant syntax dictionaries provide very limited coverage of adjective subcategorization. In this paper we propose a method for automatic acquisition of adjectival SCFs from English corpus data. Our method has been implemented using a decision-tree classifier which tests for the presence of grammatical relations (GRs) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). It uses a powerful taskspecific pattern-matching language which enables the frames to be classified hierarchically in a way that mirrors inheritance-based lexica. As reported later, the system is capable of detecting 30 SCFs with an accuracy comparable to that of best state-ofart verbal SCF acquisition systems (e.g. (Korhonen, 2002)). Additionally, we present a novel tool for linguistic annotation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to signi"
P05-1076,C02-1013,1,0.909127,"Missing"
P05-1076,W98-1505,0,0.0759918,"exicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer th"
P05-1076,W98-1114,1,0.955193,"ed to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of ∗ Part of this research was conducted while this author was at the University of Edinburgh Laboratory for Foundations of Computer Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting compreh"
P05-1076,P04-2007,0,0.0223169,"ion (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of the ACL, pages 614–621, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics (from t"
P05-1076,J92-3002,0,0.0440088,"some cases result in an inconsistent or misleading combination of GRs. The parser uses a scheme of GRs between lemmatised lexical heads (Carroll et al., 1998a; Briscoe et al., 2002). The relations are organized as a multipleinheritance subsumption hierarchy where each subrelation extends the meaning, and perhaps the argument structure, of its parents (figure 2). For descriptions and examples of each relation, see (Carroll et al., 1998a). The dependency relationships which the GRs embody correspond closely to the head-complement 1 Compare the cogent argument for a inheritance-based lexicon in (Flickinger and Nerbonne, 1992), much of which can be applied unchanged to the taxonomy of SCFs. 616 mod arg mod ncmod xmod cmod detmod arg aux subj or dobj subj ncsubj conj xsubj csubj comp obj dobj obj2 clausal iobj xcomp ccomp Figure 2: The GR hierarchy used by RASP 2 Obtaining Grammatical Relations In contrast to almost all earlier work, there was no filtering stage involved in SCF acquisition. The classifier was designed to operate with high precision, so filtering was less necessary. 2 SUBJECT NP 1 , 6 6 6 * "" 6 PVAL 6 6ADJ-COMPS 4 NP PP Figure 3: “for” 3 # Feature , 2 MOOD 6 6SUBJECT 4 OMISSION VP 3 3 7 7 to-infiniti"
P05-1076,C02-1122,0,0.0309122,"ture (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of the ACL, pages 614–621, c Ann Arbor, June 2005. 2005 Association for Computational Ling"
P05-1076,P03-1009,1,0.870014,"ny application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of t"
P05-1076,P02-1029,0,0.061693,"h for enhancing the performance of ∗ Part of this research was conducted while this author was at the University of Edinburgh Laboratory for Foundations of Computer Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carro"
P05-1076,P03-1002,0,0.0585437,"puter Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has"
P05-1076,H91-1067,0,\N,Missing
P05-1076,P93-1032,0,\N,Missing
P06-2006,C02-1013,1,0.456801,"nto a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB. Therefore, although Kaplan et al. demonstrate an improvement in accuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available. The parser we deploy, like the XLE one, is based on a manually-defined feature-based unification grammar"
P06-2006,C04-1041,0,0.029928,"Missing"
P06-2006,W01-0521,0,0.312595,"Missing"
P06-2006,1997.iwpt-1.16,0,0.547879,"Missing"
P06-2006,N04-1013,0,0.27193,"Missing"
P06-2006,P99-1061,1,0.343253,"Missing"
P06-2006,P03-1054,0,0.0811338,"Missing"
P06-2006,P05-1011,0,0.0262634,"Missing"
P06-2006,W05-1517,1,0.816218,"grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB. Therefore, although Kaplan et al. demonstrate an improvement in accuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available. The parser we deploy, like the XLE one, is based on a manually-defined feature-based unification grammar. However, the approa"
P06-2006,briscoe-carroll-2002-robust,1,0.676823,"ic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB. Therefore, although Kaplan et al. demonstrate an improvement in accuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available. The parser we deploy, like the XLE one, is based on a manually-defined featur"
P06-2006,J04-4004,0,\N,Missing
P06-2006,J03-4003,0,\N,Missing
P06-4020,1997.iwpt-1.16,0,0.120691,"ation and of the proportion of such derivations in the forest produced by the parser. A weighted set of GRs from the parse forest is now computed efficiently using a variant of the inside-outside algorithm (Watson et al., 2005). 2.5 Generalised LR Parser A non-deterministic LALR(1) table is constructed automatically from a CF ‘backbone’ compiled from the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This process backtracks occasionally since unifications are required during the unpacking process and they occasionally fail (see Oepen and Carroll, 2000). The probabilities of actions in the LR table are computed using bootstrapping methods which utilise an unlabelled bracketing of the Susanne Treebank (Watson et al., 2006). This makes the system more easily retrainable after changes in the gram"
P06-4020,P99-1061,1,0.152854,"Missing"
P06-4020,A00-2022,1,0.59298,"rom the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This process backtracks occasionally since unifications are required during the unpacking process and they occasionally fail (see Oepen and Carroll, 2000). The probabilities of actions in the LR table are computed using bootstrapping methods which utilise an unlabelled bracketing of the Susanne Treebank (Watson et al., 2006). This makes the system more easily retrainable after changes in the grammar and opens up the possibility of quicker tuning to in-domain data. In addition, the structural ranking induced by the parser can be reranked using (in-domain) lexical data which pro3 Evaluation The new system has been evaluated using our reannotation of the PARC dependency bank (DepBank; King et al., 2003)—consisting of 560 sentences chosen randomly"
P06-4020,briscoe-carroll-2002-robust,1,0.223171,"ised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information. Tokeniser ? PoS Tagger ? Lemmatiser ? Parser/Grammar ? Parse Ranking Model Figure 1: RASP Pipeline 1 Introduction Fourthly, the grammatical relations output has been redesigned to better support further processing. Finally, the training and tuning of the parse ranking model has been made more flexible. The first public release of the RASP system (Briscoe & Carroll, 2002) has been downloaded by over 120 sites and used in diverse natural language processing tasks, such as anaphora resolution, word sense disambiguation, identifying rhetorical relations, resolving metonymy, detecting compositionality in phrasal verbs, and diverse applications, such as topic and sentiment classification, text anonymisation, summarisation, information extraction, and open domain question answering. Briscoe & Carroll (2002) give further details about the first release. Briscoe (2006) provides references and more information about extant use of RASP and fully describes the modificati"
P06-4020,P06-2006,1,0.798254,"Missing"
P06-4020,W05-1517,1,0.615132,"ations to enable appropriate semantic inferences, and addition of a text adjunct (punctuation) relation to the scheme. Factoring rooted, directed graphs of GRs into a set of bilexical dependencies makes it possible to compute the transderivational support for a particular relation and thus compute a weighting which takes account both of the probability of derivations yielding a specific relation and of the proportion of such derivations in the forest produced by the parser. A weighted set of GRs from the parse forest is now computed efficiently using a variant of the inside-outside algorithm (Watson et al., 2005). 2.5 Generalised LR Parser A non-deterministic LALR(1) table is constructed automatically from a CF ‘backbone’ compiled from the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This pro"
P06-4020,A94-1009,0,0.142719,"Missing"
P07-1115,P87-1027,1,0.69118,"gure 3. Each instantiated GR in Figure 3 corresponds to one or more parts of the feature structure in Figure 2. xcomp( be[6] easy[8]) establishes be[6] as the head of the VP in which easy[8] occurs as a complement. The first (PP)-complement is for us, as indicated by ncmod(for[9] easy[8] we+[10]), with for as PFORM and we+ (us) as NP. The second complement is represented by xcomp(to[11] be+[6] comprehend[12]): a to-infinitive VP. The 914 SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and specific predicate selectiona"
P07-1115,P91-1027,0,0.149439,"orization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has foc"
P07-1115,A97-1052,1,0.926335,"f successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution dat"
P07-1115,briscoe-carroll-2002-robust,1,0.485691,"coe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GR s — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCF s. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCF s, 3) building lexical entries from the classified data, and 4) filtering those entries to obtain a more accurate lexicon. The details of these steps are provided in the subsequent sections. b) Kim (VP persuaded (NP the judge) (Scomp that Sandy was present)) However, both a) and b) consist"
P07-1115,P06-4020,1,0.215678,"N one | judge). In this paper we present the first system for largescale acquisition of SCFs from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The classifier incorporates 168 verbal, 37 adjectival and 31 nominal SCF distinctions. An improved acquisition technique is used which expands on the ideas Yallop et al. (2005) recently explored for a small experiment on adjectival SCF acquisition. It involves identifying SCF s on the basis of grammatical relations ( GR s) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe et al., 2006). As detailed later, the system performs better with verbs than previous comparable state-of-art systems, achieving 68.9 F-measure in detecting SCF types. It achieves similarly good performance with nouns and adjectives (62.2 and 71.9 F-measure, respectively). Additionally, we have developed a tool for linguistic annotation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to significantly reduce the number of frames presented to the user for each sentence."
P07-1115,W98-1505,0,0.06437,"tal for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and althoug"
P07-1115,W98-1114,1,0.889783,"eb, corpora of published text, etc.) is starting to produce large scale lexical resources which include frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detect"
P07-1115,C94-1042,0,0.187709,"r this sentence is shown in Figure 3. Each instantiated GR in Figure 3 corresponds to one or more parts of the feature structure in Figure 2. xcomp( be[6] easy[8]) establishes be[6] as the head of the VP in which easy[8] occurs as a complement. The first (PP)-complement is for us, as indicated by ncmod(for[9] easy[8] we+[10]), with for as PFORM and we+ (us) as NP. The second complement is represented by xcomp(to[11] be+[6] comprehend[12]): a to-infinitive VP. The 914 SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and"
P07-1115,W02-2014,1,0.876333,"type recall (the percentage of SCF types in the gold standard that the system proposes) and the F-measure which is the harmonic mean of type precision and recall. We also compared the similarity between the acquired unfiltered3 SCF distributions and gold standard SCF distributions using various measures of distributional similarity: the Spearman rank correlation (RC), Kullback-Leibler distance (KL), JensenShannon divergence (JS), cross entropy (CE), skew divergence (SD) and intersection (IS). The details of these measures and their application to subcategorization acquisition can be found in (Korhonen and Krymolowski, 2002). Finally, we recorded the total number of gold standard SCFs unseen in the system output, i.e. the type of false negatives which were never detected by the classifier. 3.4 Results Table 1 includes the average results for the 183 verbs. The first column shows the results for Briscoe and Carroll’s (1997) (B&C) system when this system is run with the original classifier but a more recent version of the parser (Briscoe and Carroll, 2002) and the same filtering technique as our new system (thresholding based on the relative frequencies of SCFs). The classifier of B&C system is comparable to our cl"
P07-1115,P03-1009,1,0.719202,"on which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at"
P07-1115,korhonen-etal-2006-large,1,0.894004,"rization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at several levels (from tagging to syntactic and semantic analysis). Furthermore the selection of the correct analysis from the set returned by a parser which does not initially utilize fine-grained lexico-syntactic inform"
P07-1115,P02-1029,0,0.0669307,"ude frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks"
P07-1115,P03-1002,0,0.0578166,"performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which co"
P07-1115,P05-1076,1,0.432878,"our results and future work, and section 5 concludes. a) Kim (VP believes (NP the evidence (Scomp that Sandy was present))) A common strategy in existing large-scale SCF acquisition systems (e.g. (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GR s — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCF s. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCF s, 3) building lexical entries from the classified data, and 4) filtering those en"
P07-1115,H91-1067,0,\N,Missing
P07-1115,P93-1032,0,\N,Missing
P07-1125,P01-1005,0,0.1409,"d improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and agreement to measure confidence on unlabelled samples, and more recently McClosky et al. (2006) use selftraining for improving parse reranking. Other relevant recent work includes (Zhang, 2004), in which random feature projection and a committee of SVM classifiers is used in a hybrid co/self-training strategy for weakly supervised relation classification and (Chen et al., 2006) where a graph based algorithm called label propagation is employed to perform weakly supervised relation extraction. 3 The Hedge Classification Task Given a collection of sentences, S, the task is to"
P07-1125,P06-1017,0,0.0591815,"le-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and agreement to measure confidence on unlabelled samples, and more recently McClosky et al. (2006) use selftraining for improving parse reranking. Other relevant recent work includes (Zhang, 2004), in which random feature projection and a committee of SVM classifiers is used in a hybrid co/self-training strategy for weakly supervised relation classification and (Chen et al., 2006) where a graph based algorithm called label propagation is employed to perform weakly supervised relation extraction. 3 The Hedge Classification Task Given a collection of sentences, S, the task is to label each sentence as either speculative or nonspeculative (spec or nspec henceforth). Specifically, S is to be partitioned into two disjoint sets, one representing sentences that contain some form of hedging, and the other representing those that do not. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the wor"
P07-1125,W99-0613,0,0.363821,". scientific text) and they do not address the problem of hedge classification directly. 2.2 Weakly Supervised Learning Recent years have witnessed a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate multiple redundant (or semi-redundant) ‘views’ of a data sample and perform mutual bootstrapping. This idea was formalised by Blum and Mitchell (1998) in their presentation of co-training. Co-training has also been used for named entity recognition (NER) (Collins and Singer, 1999), coreference resolution (Ng and Cardie, 2003), text categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for wh"
P07-1125,W04-3103,0,0.838946,"extual evidence of such an interaction 992 2 Related Work 2.1 Hedge Classification While there is a certain amount of literature within the linguistics community on the use of hedging in 1 available from www.cl.cam.ac.uk/∼bwm23/ Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics scientific text, eg. (Hyland, 1994), there is little of direct relevance to the task of classifying speculative language from an NLP/ML perspective. The most clearly relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. We will draw on this work throughout our presentation of the task. Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (Wiebe et al., 2004). Riloff et al. (2003) e"
P07-1125,N06-1020,0,0.0189051,"rtition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and agreement to measure confidence on unlabelled samples, and more recently McClosky et al. (2006) use selftraining for improving parse reranking. Other relevant recent work includes (Zhang, 2004), in which random feature projection and a committee of SVM classifiers is used in a hybrid co/self-training strategy for weakly supervised relation classification and (Chen et al., 2006) where a graph based algorithm called label propagation is employed to perform weakly supervised relation extraction. 3 The Hedge Classification Task Given a collection of sentences, S, the task is to label each sentence as either speculative or nonspeculative (spec or nspec henceforth). Specifically, S is to be p"
P07-1125,N03-1023,0,0.0112555,"lem of hedge classification directly. 2.2 Weakly Supervised Learning Recent years have witnessed a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate multiple redundant (or semi-redundant) ‘views’ of a data sample and perform mutual bootstrapping. This idea was formalised by Blum and Mitchell (1998) in their presentation of co-training. Co-training has also been used for named entity recognition (NER) (Collins and Singer, 1999), coreference resolution (Ng and Cardie, 2003), text categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work"
P07-1125,W03-0404,0,0.0224958,"is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. We will draw on this work throughout our presentation of the task. Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (Wiebe et al., 2004). Riloff et al. (2003) explore bootstrapping techniques to identify subjective nouns and subsequently classify subjective vs. objective sentences in newswire text. Their work bears some relation to ours; however, our domains of interest differ (newswire vs. scientific text) and they do not address the problem of hedge classification directly. 2.2 Weakly Supervised Learning Recent years have witnessed a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate multiple redundant (or"
P07-1125,W05-1301,0,0.00864938,"a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate multiple redundant (or semi-redundant) ‘views’ of a data sample and perform mutual bootstrapping. This idea was formalised by Blum and Mitchell (1998) in their presentation of co-training. Co-training has also been used for named entity recognition (NER) (Collins and Singer, 1999), coreference resolution (Ng and Cardie, 2003), text categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and a"
P07-1125,J04-3002,0,0.0129486,"early relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. We will draw on this work throughout our presentation of the task. Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (Wiebe et al., 2004). Riloff et al. (2003) explore bootstrapping techniques to identify subjective nouns and subsequently classify subjective vs. objective sentences in newswire text. Their work bears some relation to ours; however, our domains of interest differ (newswire vs. scientific text) and they do not address the problem of hedge classification directly. 2.2 Weakly Supervised Learning Recent years have witnessed a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate"
P07-1125,P03-1044,0,0.00942395,"0.64 0.62 0.6 0.58 0 20 Prob (Prob) Prob (SVM) SVM (Prob) SVM (SVM) Baseline 40 60 80 Iteration 100 120 140 denotes our probabilistic learning model and classifier (§9) denotes probabilistic learning model with SVM classifier denotes committee-based model (§10.4) with probabilistic classifier denotes committee-based model with SVM classifier denotes substring matching classifier of (Light et al., 2004) An important issue in incremental learning scenarios is identification of the optimum stopping point. Various methods have been investigated to address this problem, such as ‘counter-training’ (Yangarber, 2003) and committee agreement (Zhang, 2004); how such ideas can be adapted for this task is one of many avenues for future research. Figure 1: Learning curves 10.4 Baselines As a baseline classifier we use the substring matching technique of (Light et al., 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose. To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at"
P07-1125,P95-1026,0,0.051666,"ext categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space. Perhaps the most well known of such approaches is expectation maximization (EM), used by Nigam et al. (2000) for text categorization and by Ng and Cardie (2003) in combination with a meta-level feature selection procedure. Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples 993 for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and agreement to measure confidence on unlabelled samples, and more recently McClosky et al. (2006) use selftraining for improving parse reranking. Other relevant recent work includes (Zhang, 2004), in which random feature projection and a committee of SVM classifiers is used in a hybrid co/self-training strategy for weakly supervised relation classification and (Chen et al., 2006) where a graph based algorithm called label propagation is employed to perform weakly supervised relation extraction. 3 The Hedge Classification Task"
P11-1019,P06-4020,1,0.551746,"mising the differences between closely-ranked data pairs. The principal advantage of applying rank preference learning to the AA task is that we explicitly model the grade relationships between scripts and do not need to apply a further regression step to fit the classifier output to the scoring scheme. The results reported in this paper are obtained by learning a linear classification function. 3.2 Feature set We parsed the training and test data (see Section 2) using the Robust Accurate Statistical Parsing (RASP) system with the standard tokenisation and sentence boundary detection modules (Briscoe et al., 2006) in order to broaden the space of candidate features suitable for the task. The features used in our experiments are mainly motivated by the fact that lexical and grammatical features should be highly discriminative for the AA task. Our full feature set is as follows: i. Lexical ngrams (a) Word unigrams (b) Word bigrams ii. Part-of-speech (PoS) ngrams (a) PoS unigrams (b) PoS bigrams (c) PoS trigrams iii. Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distance measures iv. Other features (a) Script length (b) Error-rate Word unigrams and bigrams are"
P11-1019,P98-1032,0,0.0645038,"s of the scripts, such as discourse cohesion or relevance to the prompt eliciting the text, that examiners will take into account. 5 Validity tests The practical utility of an AA system will depend strongly on its robustness to subversion by writers who understand something of its workings and attempt to exploit this to maximise their scores (independently of their underlying ability). Surprisingly, there is very little published data on the robustness of existing systems. However, Powers et al. (2002) invited writing experts to trick the scoring capabilities of an earlier version of e-Rater (Burstein et al., 1998). e-Rater (see Section 6 for more details) assigns a score to a text based on linguistic feature types extracted using relatively domain-specific techniques. Participants were given a description of these techniques as well as of the cue words that the system uses. The results showed that it was easier to fool the system into assigning higher than lower scores. Our goal here is to determine the extent to which knowledge of the feature types deployed poses a threat to the validity of our system, where certain text generation strategies may give rise to large positive discrepancies. As mentioned"
P11-1019,W03-0209,0,0.316874,"re labelled with a grade and unlabelled test texts are fitted to the same grade point scale via a regression step applied to the classifier output (see Section 6 for more details). Different techniques have been used, including cosine similarity of vectors representing text in various ways (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), generative machine learning models (Rudner and Liang, 2002), domain-specific feature extraction (Attali and Burstein, 2006), and/or modified syntactic parsers (Lonsdale and Strong-Krause, 2003). A recent review identifies twelve different automated free-text scoring systems (Williamson, 2009). Examples include e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003), IntelliMetric (Elliot, 2003; Rudner et al., 2006) and Project Essay Grade (PEG) (Page, 2003). Several of these are now deployed in highstakes assessment of examination scripts. Although there are many published analyses of the perfor180 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180–189, c Portland, Oregon, June 19-24, 2011. 2011 As"
P11-1019,C98-1032,0,\N,Missing
P16-1075,W13-1704,0,0.012505,"4 0.769 0.851 0.753 0.776 0.721 0.629 System Performance (QW-κ) BLRR SVM TAP Phandi Phandi 0.761 0.781 0.815 0.606 0.621 0.674 0.621 0.630 0.642 0.742 0.749 0.789 0.784 0.782 0.801 0.775 0.771 0.793 0.730 0.727 0.772 0.617 0.534 0.688 Table 2: Details of ASAP dataset and a preliminary evaluation of the performance of our TAP baseline against previous work (Phandi et al., 2015). All models used only task-specific data and 5-fold crossvalidation. Best result is in bold. 5 is not found in the language model. Spelling errors are detected using a dictionary lookup, while a rule-based error module (Andersen et al., 2013) with rules generated from the Cambridge Learner Corpus (CLC) (Nicholls, 2003) is used to detect further errors. Finally, the unigrams, bigrams and trigrams are weighted by tf-idf (Sparck Jones, 1972), while all other features are weighted by their actual frequency in the essay. 4 Multi-Task Learning For multi-task learning we use EA encoding (Daume III, 2007) extended over k tasks Tj=1..k where each essay xi is associated with one task xi ∈ Tj . The transfer-learning algorithm takes a set of input vectors associated with the essays, and for each vector xi ∈ RF maps it via Φ(xi ) to a higher d"
P16-1075,P06-4020,1,0.583907,"Missing"
P16-1075,D13-1180,0,0.328502,"says are written in response to prompts which are carefully designed to elicit answers according to a number of dimensions (e.g. register, topic, and genre). For example, Table 1 shows extracts from two prompts from a publicly available dataset2 that aim to elicit different genres of persuasive/argumentative responses on different topics. Most previous work on AES has either ignored the differences between essays written in response to different prompts (Yannakoudakis et al., 2011) with the aim of building general AES systems, or has built prompt-specific models for each prompt independently (Chen and He, 2013; Persing and Ng, 2014). One of the problems hindering the wide-scale adoption and deployment of AES systems is the dependence on prompt-specific training data, i.e. substantial model retraining is often needed when a new prompt is released. Therefore, systems that can adapt to new writing tasks (i.e. prompts) with relatively few new task-specific training examples are particularly appealing. For example, a system that is trained using only responses from prompt #1 in Table 1 may not generalise well to essays written in response to prompt #2, and vice versa. Even more complications arise when"
P16-1075,D15-1085,0,0.0466674,"e is limited prompt-specific training data available. Furthermore, we perform a detailed study using varying amounts of task-specific training data and varying numbers of tasks. First, we review some related work. 2 Using training data from natural language tasks to boost performance of related tasks, for which there is limited training data, has received much attention of late (Collobert and Weston, 2008; Duh et al., 2010; Cheng et al., 2015). However, there have been relatively few attempts to apply transfer learning to automated assessment tasks. Notwithstanding, Napoles and Callison-Burch (2015) use a multi-task approach to model differences in assessors, while Heilman and Madnani (2013) specifically focus on domain-adaptation for short answer scoring over common scales. Most relevant is the work of Phandi et al. (2015), who applied domain-adaptation to the AES task using EasyAdapt (EA) (Daume III, 2007). They showed that supplementing a Bayesian linear ridge regression model (BLRR) with data from one other source domain is beneficial when there is limited target domain data. However, it was shown that simply using the source domain data as extra training data outperformed the EA dom"
P16-1075,W13-1703,0,0.0185495,"ttali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby one has access to multiple source domains) was not considered. The main difference between our work and previous work is that our model incorporates multiple source tasks and introduces a learning mechanism that enables us to combine these tasks even when the scores across tasks are not directly comparable. This has not been achieved before. This"
P16-1075,P07-1033,0,0.0916044,"Missing"
P16-1075,W15-0629,0,0.0120003,"g baseline system when there is limited prompt-specific training data available. Furthermore, we perform a detailed study using varying amounts of task-specific training data and varying numbers of tasks. First, we review some related work. 2 Using training data from natural language tasks to boost performance of related tasks, for which there is limited training data, has received much attention of late (Collobert and Weston, 2008; Duh et al., 2010; Cheng et al., 2015). However, there have been relatively few attempts to apply transfer learning to automated assessment tasks. Notwithstanding, Napoles and Callison-Burch (2015) use a multi-task approach to model differences in assessors, while Heilman and Madnani (2013) specifically focus on domain-adaptation for short answer scoring over common scales. Most relevant is the work of Phandi et al. (2015), who applied domain-adaptation to the AES task using EasyAdapt (EA) (Daume III, 2007). They showed that supplementing a Bayesian linear ridge regression model (BLRR) with data from one other source domain is beneficial when there is limited target domain data. However, it was shown that simply using the source domain data as extra training data outperformed the EA dom"
P16-1075,W14-1701,1,0.825345,"ay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby one has access to multiple source domains) was not considered. The main difference between our work and previous work is that our model incorporates multiple source tasks and introduces a learning mechanism that enables us to combine these tasks even when the scores across tasks are not directly comparable. This has not been achieved before. This is non-trivial as it is difficult to see"
P16-1075,W14-1702,0,0.0259115,"Missing"
P16-1075,S13-2046,0,0.0127478,"orm a detailed study using varying amounts of task-specific training data and varying numbers of tasks. First, we review some related work. 2 Using training data from natural language tasks to boost performance of related tasks, for which there is limited training data, has received much attention of late (Collobert and Weston, 2008; Duh et al., 2010; Cheng et al., 2015). However, there have been relatively few attempts to apply transfer learning to automated assessment tasks. Notwithstanding, Napoles and Callison-Burch (2015) use a multi-task approach to model differences in assessors, while Heilman and Madnani (2013) specifically focus on domain-adaptation for short answer scoring over common scales. Most relevant is the work of Phandi et al. (2015), who applied domain-adaptation to the AES task using EasyAdapt (EA) (Daume III, 2007). They showed that supplementing a Bayesian linear ridge regression model (BLRR) with data from one other source domain is beneficial when there is limited target domain data. However, it was shown that simply using the source domain data as extra training data outperformed the EA domain adaptation approach in three out of four cases. One major limitation to their approach was"
P16-1075,N04-1024,0,0.03256,"rmined by the fact that the gold scores are not comparable across domains, as the essays were written by students of different educational levels. A further limitation is that multi-domain adaptaRelated Work A number of commercially available systems for AES, have been developed using machine learning techniques. These include PEG (Project Essay Grade) (Page, 2003), e-Rater (Attali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been develop"
P16-1075,P14-1144,0,0.0590889,"response to prompts which are carefully designed to elicit answers according to a number of dimensions (e.g. register, topic, and genre). For example, Table 1 shows extracts from two prompts from a publicly available dataset2 that aim to elicit different genres of persuasive/argumentative responses on different topics. Most previous work on AES has either ignored the differences between essays written in response to different prompts (Yannakoudakis et al., 2011) with the aim of building general AES systems, or has built prompt-specific models for each prompt independently (Chen and He, 2013; Persing and Ng, 2014). One of the problems hindering the wide-scale adoption and deployment of AES systems is the dependence on prompt-specific training data, i.e. substantial model retraining is often needed when a new prompt is released. Therefore, systems that can adapt to new writing tasks (i.e. prompts) with relatively few new task-specific training examples are particularly appealing. For example, a system that is trained using only responses from prompt #1 in Table 1 may not generalise well to essays written in response to prompt #2, and vice versa. Even more complications arise when the scoring scale, mark"
P16-1075,P15-1053,0,0.0353263,"domain adaptaRelated Work A number of commercially available systems for AES, have been developed using machine learning techniques. These include PEG (Project Essay Grade) (Page, 2003), e-Rater (Attali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby one has access to multiple source domains) was not considered. The main difference between our work and previous work is that our model incorporates mu"
P16-1075,D15-1049,0,0.125312,"Using training data from natural language tasks to boost performance of related tasks, for which there is limited training data, has received much attention of late (Collobert and Weston, 2008; Duh et al., 2010; Cheng et al., 2015). However, there have been relatively few attempts to apply transfer learning to automated assessment tasks. Notwithstanding, Napoles and Callison-Burch (2015) use a multi-task approach to model differences in assessors, while Heilman and Madnani (2013) specifically focus on domain-adaptation for short answer scoring over common scales. Most relevant is the work of Phandi et al. (2015), who applied domain-adaptation to the AES task using EasyAdapt (EA) (Daume III, 2007). They showed that supplementing a Bayesian linear ridge regression model (BLRR) with data from one other source domain is beneficial when there is limited target domain data. However, it was shown that simply using the source domain data as extra training data outperformed the EA domain adaptation approach in three out of four cases. One major limitation to their approach was that in many instances the source domain and target domain pairs were from different grade levels. This means that any attempt to reso"
P16-1075,P11-1093,0,0.0274485,"s for AES, have been developed using machine learning techniques. These include PEG (Project Essay Grade) (Page, 2003), e-Rater (Attali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby one has access to multiple source domains) was not considered. The main difference between our work and previous work is that our model incorporates multiple source tasks and introduces a learning mechanism that enables us"
P16-1075,C14-1090,0,0.0205615,"r limitation is that multi-domain adaptaRelated Work A number of commercially available systems for AES, have been developed using machine learning techniques. These include PEG (Project Essay Grade) (Page, 2003), e-Rater (Attali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby one has access to multiple source domains) was not considered. The main difference between our work and previous work is that ou"
P16-1075,W12-2004,1,0.810976,"t the gold scores are not comparable across domains, as the essays were written by students of different educational levels. A further limitation is that multi-domain adaptaRelated Work A number of commercially available systems for AES, have been developed using machine learning techniques. These include PEG (Project Essay Grade) (Page, 2003), e-Rater (Attali and Burstein, 2006), and Intelligent Essay Assessor (IEA) (Landauer et al., 1998). Beyond commercial systems, there has been much research into varying aspects involved in automated assessment, including coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012), prompt-relevance (Persing and Ng, 2014; Higgins et al., 2006), argumentation (Labeke et al., 2013; Somasundaran et al., 2014; Persing and Ng, 2015), grammatical error detection and correction (Rozovskaya and Roth, 2011; Felice et al., 2014), and the development of publicly available resources (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Persing and Ng, 2014; Ng et al., 2014). While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference790 tion (whereby on"
P16-1075,P11-1019,1,0.816127,"earning models only perform well when the training and test instances are from similar distributions. However, it is usually the case that essays are written in response to prompts which are carefully designed to elicit answers according to a number of dimensions (e.g. register, topic, and genre). For example, Table 1 shows extracts from two prompts from a publicly available dataset2 that aim to elicit different genres of persuasive/argumentative responses on different topics. Most previous work on AES has either ignored the differences between essays written in response to different prompts (Yannakoudakis et al., 2011) with the aim of building general AES systems, or has built prompt-specific models for each prompt independently (Chen and He, 2013; Persing and Ng, 2014). One of the problems hindering the wide-scale adoption and deployment of AES systems is the dependence on prompt-specific training data, i.e. substantial model retraining is often needed when a new prompt is released. Therefore, systems that can adapt to new writing tasks (i.e. prompts) with relatively few new task-specific training examples are particularly appealing. For example, a system that is trained using only responses from prompt #1"
P16-1075,C00-2137,0,0.00982787,"means statistically greater than All-MTL-TAP on both folds (? for one fold). System Tgt-TAP All-TAP All-MTL-TAP All-MTL-cTAP 1 2 3 0.813 0.803 0.825‡ 0.816‡ 0.667 0.598 0.658‡ 0.667‡? 0.626 0.583 0.643‡ 0.654‡?? Target Tasks/Prompts 4 5 6 0.779 0.648 0.702‡ 0.783‡?? 0.789 0.747 0.784‡ 0.801‡?? 0.763 0.741 0.759‡ 0.778‡?? 7 8 0.758 0.674 0.778‡ 0.787‡? 0.665 0.462 0.692‡ 0.692‡ Table 4: Average QW-κ of systems over two-folds on the ASAP dataset. The best approach per prompt is in bold. ‡ (†) means that κ is statistically (p < 0.05) greater than All-TAP using an approximate randomisation test (Yeh, 2000) using 50,000 samples. ?? means statistically greater than All-MTL-TAP on both folds (? for one fold). 6.3 sion step in cTAP only uses original target task scores and therefore predicts scores on the correct scoring scale for the task. We study the three different learning approaches, TAP, MTL-TAP, and MTL-cTAP, in the following scenarios: All: where the approach uses data from both the target task and the available source tasks. Tgt: where the approach uses data from the target task only. Src: where the approach uses data from only the available source tasks. 6.2 Evaluation Metrics We use bot"
P17-1074,N12-1037,0,\N,Missing
P17-1074,P15-1068,1,\N,Missing
P17-1074,W14-1703,0,\N,Missing
P17-1074,D15-1052,0,\N,Missing
P17-1074,W14-1706,0,\N,Missing
P17-1074,Q16-1013,0,\N,Missing
P17-1074,C16-1079,1,\N,Missing
P87-1027,E85-1025,1,0.86709,"ated in Figure 4 can clearly be directly mapped into a feature d u s t e r within the features and feature set declarations used by the dictionary and grammar projects. A colnparison of the existing entries for ~oelieve~ in the hand crafted lexicon (Figure 1) and the third word sense for ~believem extracted from LDOCE demonstrates that much of the information available from L D O C E is of direct utility - - for example the SUBCAT values can be derived by an analysis of the T a k e s values and the O R a i e i n g logical type specification above. Indeed, we have demonstrated the feasibility (Alshawi et al., 1985) of driving a parsing system directly from the information av~lable in LDOCE by constructing dictionary entries for the PATR-H system (Shieber, Figure 4: A lexical template derived from LDOCE This resulting structure is a lexical template, designed as a formal representation for the kind of syntacrico-semantic information which can be extracted from the dictionary and which is relevant to a system for automatic morphological and syntactic analysis of English texts. The overall transformation strategy employed by our system attempts to derive both subcategorisation frames relevant to a particul"
P87-1027,J87-3002,1,\N,Missing
P87-1027,J87-3008,0,\N,Missing
P87-1027,E87-1011,1,\N,Missing
P87-1027,C86-1066,0,\N,Missing
P87-1027,P84-1075,0,\N,Missing
P89-1011,E87-1011,1,0.893077,"Missing"
P89-1011,C88-1046,0,0.0467052,"Missing"
P89-1011,H90-1095,0,0.0594923,"Missing"
P89-1011,C86-1143,0,0.0248494,"d effective for isolated word recognition of small vocabularies with the system trained to an individual speaker, as, for example, Zue & Huuonlocher (1983) argue. Furthermore, any direct access model of this type which does not incorporate a pre-lexical symbolic representation of the input will have di£ficulty capturing many rule-governed phonological processes which affect the ~onunciation of words in fluent speech. since these processes can only be chazacteris~ adequately in terms of operations on a symbolic, phonological representation of the speech input (e.g. Church. 1987; Frazier, 1987; Wiese, 1986). The research reported here forms part of an ongoing programme to develop a computationally explicit account of lexical access and word recognition in connected s1~e-~_~, which is at least informed by experimental results concerning the psychological processes and mechanisms which underlie this task. To guide research. we make use of a substantial lexical database of English derived from machine-readable versions of the Longman Dictionary of Contonporary English (see Boguracv et aL, 1987; Boguraev & Briscoe, 1989) and of the Medical Research Council's psycholinguistic database (Wilson, 1988),"
P97-1054,P94-1021,0,0.0681797,"Missing"
P97-1054,P94-1024,0,\N,Missing
preiss-etal-2002-subcategorization,J87-3002,1,\N,Missing
preiss-etal-2002-subcategorization,S01-1009,0,\N,Missing
preiss-etal-2002-subcategorization,S01-1031,0,\N,Missing
preiss-etal-2002-subcategorization,W98-1505,0,\N,Missing
preiss-etal-2002-subcategorization,C00-2100,0,\N,Missing
preiss-etal-2002-subcategorization,C94-1042,0,\N,Missing
preiss-etal-2002-subcategorization,A97-1052,1,\N,Missing
preiss-etal-2004-anaphoric,briscoe-carroll-2002-robust,1,\N,Missing
preiss-etal-2004-anaphoric,J94-4002,0,\N,Missing
preiss-etal-2004-anaphoric,J00-4003,0,\N,Missing
R13-1047,D10-1115,0,0.175652,"he corpus considered, some content word combinations will remain unattested as a consequence of their Zipf-like distributions. For example, Vecchi et al. (2011) have shown that both semantically acceptable and semantically deviant word combinations will be absent from large English corpora. A promising alternative is to use compositional models which combine distributional vectors for the component words in some way, for example, using a direct vector combination function (Kintsch, 2001; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010) or linear transformations on vectors (Baroni and Zamparelli, 2010). In spite of the spate of recent work in this area, the question of how to combine word representations is far from answered. Compositional models can be assessed by their ability both to provide a solid theoretical basis for meaning composition and to represent composite meaning for relevant practical tasks. Promising results have been shown with such models on similarity detection and paraphrase ranking (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010), adjective– noun vector prediction (Baroni and Zamparelli, 2010) and semantic anomaly detection (Vecchi et al., 2011). O"
R13-1047,P06-4020,1,0.748951,"ctly in others we use the most frequent occurrence counts. We estimate the word coannotation from the data. occurrence statistics using the BNC only, and Our test set contains 4681 correct and 530 inleave it for future research to explore the impact of correct combinations. In contrast to Vecchi et estimating them from larger corpora, for example, al. (2011), who have used a limited set of conthe ukWaC or the concatenated corpus mentioned stituent adjectives and nouns and an approxiabove. We lemmatise, tag and parse the data with mately equal number of semantically acceptable the RASP system (Briscoe et al., 2006; Andersen and deviant combinations, our test set is more et al., 2008), and extract all statistics at the lemma skewed towards correct combinations and consists level. of a wider range of constituent words. It also in4 The target elements are selected as follows: we cludes ANs occurring in the BNC – 3294 of the first select the 4K adjectives and 8K nouns which correct test ANs and 256 of the incorrect ones are are most frequent in the concatenated corpus. In corpus-attested. The set of corpus-attested ANs each case, we exclude the top 50 most frequent annotated as incorrect in our data includ"
R13-1047,D11-1010,0,0.0603092,"them easier to detect. Function words constitute a closed class, so the set of possible corrections is also limited. By comparison, errors in content word combinations pose a bigger challenge. Since content words primarily express meaning rather than encode syntax, detection and correction of such errors depend on a system’s ability, in the limit, to recognise the communicative intent of the writer. Moreover, the set of possible corrections is much larger than for function words. Previous work has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008). In the former case error detection, which is a difficult task in itself, is not addressed, while in the latter case it is integrated into that of suggesting alternatives according to some metric (for example, frequency or mutual information). In some cases, a database of typical errors in word combinations is collected from learner texts and suggestions are only made for these errorprone combinations. Otherwise suggestions"
R13-1047,W01-0513,0,0.20685,"Missing"
R13-1047,W12-2006,0,0.0511695,"Missing"
R13-1047,J98-1004,0,0.304169,"Missing"
R13-1047,D08-1094,0,0.06047,"Missing"
R13-1047,P10-1097,0,0.0363378,"Missing"
R13-1047,W11-1301,0,0.0852638,".ac.uk Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk Abstract thesaurus extraction (Grefenstette, 1994), word sense induction (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001) and others. In contrast to single words, the distribution of phrases cannot be used as a reliable approximation of their meaning, as phrase vectors are much sparser. Irrespective of the size of the corpus considered, some content word combinations will remain unattested as a consequence of their Zipf-like distributions. For example, Vecchi et al. (2011) have shown that both semantically acceptable and semantically deviant word combinations will be absent from large English corpora. A promising alternative is to use compositional models which combine distributional vectors for the component words in some way, for example, using a direct vector combination function (Kintsch, 2001; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010) or linear transformations on vectors (Baroni and Zamparelli, 2010). In spite of the spate of recent work in this area, the question of how to combine word representations is far from answered. Compo"
R13-1047,P11-1019,1,0.903747,"Missing"
R13-1047,W09-2107,0,0.0686733,"ematic which makes them easier to detect. Function words constitute a closed class, so the set of possible corrections is also limited. By comparison, errors in content word combinations pose a bigger challenge. Since content words primarily express meaning rather than encode syntax, detection and correction of such errors depend on a system’s ability, in the limit, to recognise the communicative intent of the writer. Moreover, the set of possible corrections is much larger than for function words. Previous work has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008). In the former case error detection, which is a difficult task in itself, is not addressed, while in the latter case it is integrated into that of suggesting alternatives according to some metric (for example, frequency or mutual information). In some cases, a database of typical errors in word combinations is collected from learner texts and suggestions are only made for these errorprone combinatio"
R13-1047,I08-2082,0,0.030833,"ger challenge. Since content words primarily express meaning rather than encode syntax, detection and correction of such errors depend on a system’s ability, in the limit, to recognise the communicative intent of the writer. Moreover, the set of possible corrections is much larger than for function words. Previous work has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008). In the former case error detection, which is a difficult task in itself, is not addressed, while in the latter case it is integrated into that of suggesting alternatives according to some metric (for example, frequency or mutual information). In some cases, a database of typical errors in word combinations is collected from learner texts and suggestions are only made for these errorprone combinations. Otherwise suggestions will be made for many acceptable phrases. In this work, we treat error detection in the choice of content words as an independent task and assess the ability of compositio"
R13-1047,P04-1036,0,0.0917727,"Missing"
R13-1047,andersen-etal-2008-bnc,1,\N,Missing
R13-1047,P08-1028,0,\N,Missing
R13-1047,2014.lilt-9.5,0,\N,Missing
W01-1808,J93-2002,0,0.0733008,"Missing"
W01-1808,N01-1013,0,0.0377513,"Missing"
W01-1808,1998.amta-tutorials.1,0,0.0191971,"ly-o urring text. 1 Introdu tion Head-dependent relationships (possibly labelled with a relation type) have been advo ated as a useful level of representation for grammati al stru ture in a number of di erent large-s ale languagepro essing tasks. For instan e, in re ent work on statisti al treebank grammar parsing (e.g. Collins, 1999) high levels of a ura y have been rea hed using lexi alised probabilisti models over headdependent tuples. Bouma, van Noord and Malouf (2000) reate dependen y treebanks semi-automati ally in order to indu e dependen y-based statisti al models for parse sele tion. Lin (1998), Srinivas (2000) and others have evaluated the a ura y of both phrase stru ture-based and dependen y parsers by mat hing head-dependent relations against `gold standard' relations, rather than the more established method of evaluation in terms of (labelled) phrase stru ture bra ketings. Resear h on unsupervised a quisition of lexi al information from orpora, su h as argument stru ture of predi ates (Bris oe and Carroll, 1997; M Carthy, 2000), word lasses for disambiguation (Clark and Weir, 2001), and ollo ations (Lin 1999; Pear e, 2001), has used grammati al relation/head/dependent tuples. Su"
W01-1808,P00-1017,0,0.169447,"Missing"
W04-2606,P87-1027,1,0.766622,"lated to each other via alternations and thus warrant creation of a new class. properties have not been systematically studied in terms of diathesis alternations, and therefore re-examination is warranted. In what follows, we will describe these steps in detail. 3.2.2 Rudanko’s Classification 3.1 Novel Diathesis Alternations When constructing novel diathesis alternations, we took as a starting point the subcategorization classification of Briscoe (2000). This fairly comprehensive classification incorporates 163 different subcategorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994). The SCFs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially instantiated when the a frame is associated with a specific verb. As most diathesis alternations are only semi-predictable on a verb-by-verb basis, a distinct SCF is defined for every such construction, and thus all alternations can be represented as mappings between such SCFs. We considered possible alternations between pairs of SCF s in thi"
W04-2606,W02-1016,0,0.0866251,"Missing"
W04-2606,A97-1052,1,0.81912,"lieve accuse, condemn retreat, retire handle, deal hear, learn pair, mix coincide, alternate imagine, remember notice, feel like, hate focus, concentrate mind, worry debate, argue fight, communicate agree, contract demonstrate, quote allow, permit write, read comment, remark propose, recommend happen, occur count, weight miss, boycott loiter, hesitate continue, resume terminate, finish overlook, neglect commit, charge arrive, hit assume, adopt Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single"
W04-2606,briscoe-carroll-2002-robust,1,0.384232,"e mind, worry debate, argue fight, communicate agree, contract demonstrate, quote allow, permit write, read comment, remark propose, recommend happen, occur count, weight miss, boycott loiter, hesitate continue, resume terminate, finish overlook, neglect commit, charge arrive, hit assume, adopt Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single predominating sense in balanced corpus data (Korhonen and Preiss, 2003). The back-off estimates were obtained by the following method: (i) A few individ"
W04-2606,P98-1046,0,0.668211,"introduce 106 novel diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon. 1 Introduction Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcate"
W04-2606,W02-2014,1,0.8406,"Missing"
W04-2606,P03-1007,1,0.818762,"ach test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single predominating sense in balanced corpus data (Korhonen and Preiss, 2003). The back-off estimates were obtained by the following method: (i) A few individual verbs were chosen from a new verb class whose predominant sense according to the WordNet frequency data belongs to this class, (ii) SCF distributions were built for these verbs by manually analysing c. 300 occurrences of each verb in the BNC, (iii) the resulting SCF distributions were merged. An empirically-determined threshold was finally set on the probability estimates from smoothing to reject noisy SCF s caused by errors during the statistical parsing phase. This method for SCF acquisition is highly sensit"
W04-2606,P03-1009,1,0.833242,"most widely deployed classification in English, Levin’s (1993) taxonomy, mainly deals with verbs taking noun and prepositional phrase complements, and does not provide large numbers of exemplars of the classes. The fact that no comprehensive classification is available limits the usefulness of the classes for practical NLP. Some experiments have been reported recently which indicate that it should be possible, in the future, to automatically supplement extant classifications with novel verb classes and member verbs from corpus data (Brew and Schulte im Walde, 2002; Merlo and Stevenson, 2001; Korhonen et al., 2003). While the automatic approach will avoid the expensive overhead of manual classification, the very development of the technology capable of large-scale automatic classification will require access to a target classification and gold standard exemplification of it more extensive than that available currently. In this paper, we address these problems by introducing a substantial extension to Levin’s classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. These classes, many of them drawn initially from linguistic resources, were created semi-automati"
W04-2606,J01-3003,0,0.488659,"l diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon. 1 Introduction Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korh"
W04-2606,1997.mtsummit-workshop.4,0,0.0293955,"me extensions have recently been proposed to this resource. Dang et al. (1998) have supplemented the taxonomy with intersective classes: special classes for verbs which share membership of more than one Levin class because of regular polysemy. Bonnie Dorr (University of Maryland) has provided a reformulated and extended version of Levin’s classification in her LCS database (http://www.umiacs.umd.edu/  bonnie/verbsEnglish.lcs). This resource groups 4,432 verbs (11,000 senses) into 466 Levin-based and 26 novel classes. The latter are Levin classes refined according to verbal telicity patterns (Olsen et al., 1997), while the former are additional classes for non-Levin verbs which do not fall into any of the Levin classes due to their distinctive syntactic behaviour (Dorr, 1997). As a result of this work, the taxonomy has gained considerably in depth, but not to the same extent in breadth. Verbs taking ADJP, ADVP, ADL, particle, predicative, control and sentential complements are still largely excluded, except where they show interesting behaviour with respect to NP and PP complementation. As many of these verbs are highly frequent in language, NLP applications utilizing lexical-semantic classes would b"
W04-2606,C00-2094,0,0.0306267,"cs (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of"
W04-2606,C94-1042,0,0.0295931,"reation of a new class. properties have not been systematically studied in terms of diathesis alternations, and therefore re-examination is warranted. In what follows, we will describe these steps in detail. 3.2.2 Rudanko’s Classification 3.1 Novel Diathesis Alternations When constructing novel diathesis alternations, we took as a starting point the subcategorization classification of Briscoe (2000). This fairly comprehensive classification incorporates 163 different subcategorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994). The SCFs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially instantiated when the a frame is associated with a specific verb. As most diathesis alternations are only semi-predictable on a verb-by-verb basis, a distinct SCF is defined for every such construction, and thus all alternations can be represented as mappings between such SCFs. We considered possible alternations between pairs of SCF s in this classification, focusing in particular on those SCF s"
W04-2606,J98-3003,0,0.0250277,"acted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack o"
W04-2606,P98-1112,0,0.0336065,"1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of back-off smoothing or similar techniques. Althoug"
W04-2606,J81-4005,0,0.55639,"yntactic behaviour. These classes were originally created by an automatic verb classification algorithm described in (Dorr, 1997). Although they appear semantically meaningful, their syntactic-semantic Rudanko (1996, 2000) provides a semantically motivated classification for verbs taking various types of sentential complements (including predicative and control constructions). His relatively fine-grained classes, organized into sets of independent taxonomies, have been created in a manner similar to Levin’s. We took 43 of Rundanko’s verb classes for consideration. 3.2.3 Sager’s Classification Sager (1981) presents a small classification consisting of 13 classes, which groups verbs (mostly) on the basis of their syntactic alternations. While semantic properties are largely ignored, many of the classes appear distinctive also in terms of semantics. 3.2.4 Levin’s Classification At least 20 (broad) Levin classes involve verb senses which take sentential complements. Because full treatment of these senses requires considering sentential complementation, we re-evaluated these classes using our method. 3.3 Method for Creating Classes Each candidate class was evaluated as follows: 1. We extracted from"
W04-2606,C98-1046,0,\N,Missing
W04-2606,C98-1108,0,\N,Missing
W05-1517,briscoe-carroll-2002-robust,1,0.820048,"sed parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of ‘weighted GRs’ to be computed directly from the forest. The approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output. 1 Introduction RASP is a robust statistical analysis system for English developed by Briscoe and Carroll (2002). It contains a syntactic parser which can output analyses in a number of formats, including (nbest) syntactic trees, robust minimal recursion semantics (Copestake, 2003), grammatical relations (GRs), and weighted GRs. The weighted GRs for a sentence comprise the set of grammatical relations in all parses licensed for that sentence, each GR is weighted based on the probabilities of the parses in which it occurs. This weight is normalised to fall within the range  0,1 where  indicates that all parses contain the GR. Therefore, high precision GR sets can be determined by thresholding on t"
W05-1517,C02-1013,1,0.959465,"a syntactic parser which can output analyses in a number of formats, including (nbest) syntactic trees, robust minimal recursion semantics (Copestake, 2003), grammatical relations (GRs), and weighted GRs. The weighted GRs for a sentence comprise the set of grammatical relations in all parses licensed for that sentence, each GR is weighted based on the probabilities of the parses in which it occurs. This weight is normalised to fall within the range  0,1 where  indicates that all parses contain the GR. Therefore, high precision GR sets can be determined by thresholding on the GR weight (Carroll and Briscoe, 2002). Carroll and Briscoe compute weighted GRs by first unpacking all parses or the n-best subset from the parse forest. Hence, this approach is either (a) inefficient (and for some examples impracticable) if a large number of parses are licensed by the grammar, or (b) inaccurate if the number of parses unpacked is less than the number licensed by the grammar. In this paper, we show how to obviate the need to trade off efficiency and accuracy by extracting weighted GRs directly from the parse forest using a dynamic programming approach based on the Inside-Outside algorithm (IOA) (Baker, 1979; Lari"
W05-1517,P04-1014,0,0.0663335,"parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter their packing algorithm so that nodes in the packed chart have the same semantic head and ‘unfilled’ GRs. Our ap160 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 160–170, c Vancouver, October 2005. 2005 Association for Computational Linguistics proach is novel in that while calculating inside probabilities we allow any node in the parse forest to have multiple semantic heads. Clark and Curran (2004) apply Miyao and Tsujii’s (2002) dynamic programming approach to determine weighted GRs. They outline an alternative parse selection method based on th"
W05-1517,P02-1036,0,0.0650185,"amic programming approach based on the Inside-Outside algorithm (IOA) (Baker, 1979; Lari and Young, 1990). This approach enables efficient calculation of weighted GRs over all parses and substantially improves the throughput and memory usage of the parser. Since the parser is unificationbased, we also modify the parsing algorithm so that local ambiguity packing is based on feature structure equivalence rather than subsumption. Similar dynamic programming techniques that are variants of the IOA have been applied for related tasks, such as parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter their packing algorithm so that nodes in the pack"
W05-1517,P01-1042,0,0.0202823,"ectly from the parse forest using a dynamic programming approach based on the Inside-Outside algorithm (IOA) (Baker, 1979; Lari and Young, 1990). This approach enables efficient calculation of weighted GRs over all parses and substantially improves the throughput and memory usage of the parser. Since the parser is unificationbased, we also modify the parsing algorithm so that local ambiguity packing is based on feature structure equivalence rather than subsumption. Similar dynamic programming techniques that are variants of the IOA have been applied for related tasks, such as parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter"
W05-1517,N04-1013,0,0.253442,"side algorithm (IOA) (Baker, 1979; Lari and Young, 1990). This approach enables efficient calculation of weighted GRs over all parses and substantially improves the throughput and memory usage of the parser. Since the parser is unificationbased, we also modify the parsing algorithm so that local ambiguity packing is based on feature structure equivalence rather than subsumption. Similar dynamic programming techniques that are variants of the IOA have been applied for related tasks, such as parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter their packing algorithm so that nodes in the packed chart have the same semantic head and ‘unf"
W05-1517,A00-2022,1,0.91779,"Missing"
W05-1517,P01-1060,0,0.0276824,"parse forest using a dynamic programming approach based on the Inside-Outside algorithm (IOA) (Baker, 1979; Lari and Young, 1990). This approach enables efficient calculation of weighted GRs over all parses and substantially improves the throughput and memory usage of the parser. Since the parser is unificationbased, we also modify the parsing algorithm so that local ambiguity packing is based on feature structure equivalence rather than subsumption. Similar dynamic programming techniques that are variants of the IOA have been applied for related tasks, such as parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter their packing algorithm"
W05-1517,W04-3201,0,0.0352823,"(Baker, 1979; Lari and Young, 1990). This approach enables efficient calculation of weighted GRs over all parses and substantially improves the throughput and memory usage of the parser. Since the parser is unificationbased, we also modify the parsing algorithm so that local ambiguity packing is based on feature structure equivalence rather than subsumption. Similar dynamic programming techniques that are variants of the IOA have been applied for related tasks, such as parse selection (Johnson, 2001; Schmid and Rooth, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Kaplan et al., 2004; Taskar et al., 2004). The approach we take is similar to Schmid and Rooth’s (2001) adaptation of the algorithm, where ‘expected governors’ (similar to our ‘GR specifications’) are determined for each tree, and alternative nodes in the parse forest have the same lexical head. Initially, they create a packed parse forest and during a second pass the parse forest nodes are split if multiple lexical heads occur. The IOA is applied over this split data structure. Similarly, Clark and Curran (2004) alter their packing algorithm so that nodes in the packed chart have the same semantic head and ‘unfilled’ GRs. Our ap160"
W05-1517,1995.iwpt-1.8,1,\N,Missing
W05-1517,P06-2006,1,\N,Missing
W07-2203,P04-1014,0,0.0210531,"nformation is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser"
W07-2203,A94-1009,0,0.550829,"bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent t"
W07-2203,W01-0521,0,0.0221584,"nces (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing). In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth). The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts. All but one category (reportage text) is drawn from different domains than the WSJ. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data. 4 The Evaluation Scheme The parser’s output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1 . Relations are organized into a hierarchy with the root node specifying an unlabeled dependency. The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output. The microaveraged F1 score for the baseline system using this evalua"
W07-2203,P99-1010,0,0.0317151,"findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. These findings may not hold if the level of bracketing available does not adequately constrain the parses considered – see Hwa (1999) for a related investigation with EM. In future work we intend to further investigate the problem of tuning to a new domain, given that minimal manual effort is a major priority. We hope to develop methods which required no manual annotation, for example, high precision automatic partial bracketing using phrase chunking and/or named entity recognition techniques might yield enough information to support the training methods developed here. Finally, further experiments on weighting the contribution of each dataset might be beneficial. For instance, Bacchiani et al. (2006) demonstrate imrpovemen"
W07-2203,1997.iwpt-1.16,0,0.702988,"have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM tec"
W07-2203,N06-1020,0,0.0496329,"tive results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm. As we utilize an initial model to annotate additional training data, our methods are closely related to self-training methods described in the literature (e.g. McClosky et al. 2006, Bacchi24 ani et al. 2006). However these methods have been applied to fully-annotated training data to create the initial model, which is then used to annotate further training data derived from unannotated text. Instead, we train entirely from partially-bracketed data, starting from the small proportion of ‘unambiguous’ data whereby a single parse is consistent with the annotation. Therefore, our methods are better described as semi-supervised and the main focus of this work is the flexible re-use of existing treebanks to train a wider variety of statistical parsing models. While many stati"
W07-2203,J94-2001,0,0.493249,"re the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of a"
W07-2203,C02-2025,0,0.0141029,"th EM applied to the same PGLR parse selection model. Indeed, a bracketed corpus provides flexibility as existing treebanks can be utilized despite the incompatibility between the system grammar and the underlying grammar of the treebank. Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus. An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward – see, for example Oepen et al. (2002) for a good discussion of the problem. Furthermore, it suggests that it may be possible to usefully tune a parser to a new domain with less annotation effort. Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a h"
W07-2203,P92-1017,0,0.575714,"trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain. Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is eas"
W07-2203,W01-1829,0,0.0497079,"Missing"
W07-2203,P02-1035,0,0.0284737,"h is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method"
W07-2203,I05-1018,0,0.0552412,"Missing"
W07-2203,J87-1004,0,0.328283,"utilizes a manually written feature-based unification grammar over POS tag sequences. 2.1 The Parse Selection Model A context-free ‘backbone’ is automatically derived from the unification grammar1 and a generalized or non-deterministic LALR(1) table is 1 This backbone is determined by compiling out the values of prespecified attributes. For example, if we compile out the attribute PLURAL which has 2 possible values (plural or not) we will create 2 CFG rules for each rule with categories that contain PLURAL. Therefore, no information is lost during this process. constructed from this backbone (Tomita, 1987). The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail. The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997). Estimating action probabilities, consists of a) recording an action history for the correct derivation in the pa"
W07-2203,D07-1130,1,0.813886,"09 |α| 4138 15152 19248 No Match 1322 15749 16946 Timeout 191 1094 1475 Table 1: Corpus split for S, W and SW . eting. However, a preliminary investigation of no matches didn’t yield any clear patterns of inconsistency that we could quickly ameliorate by simple modifications of the PTB bracketing. We leave for the future a more extensive investigation of these cases which, in principle, would allow us to make more use of this training data. An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007). 5.1 Confidence-Based Approaches We use γ to build an initial model. We then utilize this initial model to derive derivations (compatible with the unlabeled partial bracketing) for α from which we select additional training data. We employ two types of selection methods. First, we select the top-ranked derivation only and weight actions which resulted in this derivation equally with those of the initial model (C1 ). This method is similar to ‘Viterbi training’ of HMMs though we do not weight the corresponding actions using the top parse’s probability. Secondly, we select more than one derivat"
W07-2203,W05-1517,1,0.858062,"77.05 76.02 77.05 77.51 77.73 76.45 77.01 76.90 77.85 77.88 77.40 77.09 76.86 77.88 78.01 77.54 Rec 74.22 73.40 74.22 74.80 74.98 73.91 74.31 74.23 75.07 75.04 74.75 74.35 74.21 75.05 75.13 74.95 F1 75.61 74.69 75.61 76.13 76.33 75.16 75.64 75.55 76.43 76.43 76.05 75.70 75.51 76.44 76.54 76.23 P (z)‡ 0.0294 0.4960 0.0655 0.0154 0.2090 0.1038 0.2546 0.0017 0.0011 0.1335 0.1003 0.2483 0.0048 0.0007 0.0618 Table 2: Performance of all models on DepBank. the statistical significance of the system against the baseline model. ‡ represents corresponding normalized inside-outside weight for each node (Watson et al., 2005). We perform EM starting from two initial models; either a uniform probability model, I L (), or from models derived from unambiguous training data, γ. Figure 1 shows the cross entropy decreasing monotonically from iteration 2 (as guaranteed by the EM method) for different corpora and initial models. Some models show an initial increase in cross-entropy from iteration 1 to iteration 2, because the models are initialized from a subset of the data which is used to perform maximisation. Cross-entropy increases, by definition, as we incorporate ambiguous data with more than one consistent derivati"
W07-2203,W03-2401,0,\N,Missing
W07-2203,J03-4003,0,\N,Missing
W07-2203,P06-4020,1,\N,Missing
W07-2203,P06-2006,1,\N,Missing
W09-1405,P06-4020,1,0.857071,"inally, the triggers connected with appropriate arguments are postprocessed to generate the final set of events. Each of these stages are described in detail in subsequent sections, followed by experiments and discussion. 2 Trigger identification We perform trigger identification using the assumption that events are triggered in text either by verbal or nominal prdicates (Cohen et al., 2008). To build a dictionary of verbs and their associated event classes we use the triggers annotated in the training data. We lemmatize and stem the triggers with the morphology component of the RASP toolkit (Briscoe et al., 2006)1 and the Porter stemmer2 respectively. We sort the trigger stem - event class pairs found according to their frequency in the training data and we keep only those pairs that appear at least 10 times. The trigger stems are then mapped to verbs. This excludes some relatively common triggers, which will reduce recall, but, given that we rely exclusively on the parser for 1 2 http://www.cogs.susx.ac.uk/lab/nlp/rasp/ http://www.tartarus.org/˜martin/PorterStemmer Proceedings of the Workshop on BioNLP: Shared Task, pages 37–40, c Boulder, Colorado, June 2009. 2009 Association for Computational Lingu"
W09-1405,W05-0623,0,0.0684239,"ndependent unlexicalized RASP parser, which generates parses over the part-of-speech (PoS) tags of the tokens generated by an HMM-based tagger trained on balanced English text. While we expect that a parser adapted to the biomedical domain may perform better, we want to preserve the domain-independence of the system and explore its potential. The only adjustment we make is to change the PoS tags of tokens that are part of a protein name to proper names tags. We consider such an adjustment domain-independent given that NER is available in many domains (Lewin, 2007). Following 38 Haghighi et al (2005), in order to ameliorate parsing errors, we use the top-10 parses and return a set of bilexical head-dependent grammatical relations (GRs) weighted according to the proportion and probability of the top parses supporting that GR. The GRs produced by the parser define directed graphs between tokens in the sentence, and a partial event is formed when a path that connects a trigger with an appropriate argument is identified. GR paths that are likely to generate events are selected using the development data, which does not contradict the goals of our approach because we do not require annotated t"
W09-1405,C08-1057,0,\N,Missing
W09-1405,W07-1022,0,\N,Missing
W10-2809,briscoe-carroll-2002-robust,1,0.73689,"formative link as that on which the model is most uncertain, more ∗ that maxiformally the link between instances lij mizes the following entropy: ∗ lij = arg max H(zi = zj ) i,j 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs (2) If we consider clustering as binary classification of links into must-links and cannot-links, it is equivalent to selecting the pair with the highest label entropy. During the sampling process used for parameter inference, component assignments vary 58 sampling. The performances were averaged across the collected samples. Random selection was r"
W10-2809,N09-1062,0,0.0294167,"n employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specifi"
W10-2809,P06-1124,0,0.0421635,"traint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are"
W10-2809,W09-0210,1,0.674898,"Missing"
W10-2809,P06-1044,0,0.0158003,"d by a human expert. In the context of the DPMMs, the model chooses a pair of instances for which a must-link or a cannot-link must be provided. To select the pair, we employ the simple but effective idea of uncertainty based sampling. We consider the most informative link as that on which the model is most uncertain, more ∗ that maxiformally the link between instances lij mizes the following entropy: ∗ lij = arg max H(zi = zj ) i,j 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs (2) If we consider clustering as binary classification of links into must-links and can"
W10-2809,D07-1043,0,0.0161767,"om 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did not affect the performance substantially. We evaluate our results using three information theoretic measures: Variation of Information (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering. Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard. V-beta balances these proper"
W10-3008,P06-4020,1,0.80229,"Missing"
W10-3008,W10-3001,0,0.460822,"Missing"
W10-3008,W08-0607,0,0.697422,"Missing"
W10-3008,W04-3103,0,0.466187,"Missing"
W10-3008,P07-1125,1,0.85684,"Missing"
W10-3008,W09-1304,0,0.817847,"supervised CRF classifier was used to refine these predictions. As a final step, scopes were constructed from the classifier output using a small set of post-processing rules. Development of the system revealed a number of issues with the annotation scheme adopted by the organisers. 1 1. Detecting the cues using a token-level supervised classifier. 2. Finding the scopes with a combination of manual rules and a second supervised tokenlevel classifier. 3. Applying postprocessing rules to convert the token-level annotation into predictions about scope. Parts of the system are similar to that of Morante and Daelemans (2009) — both make use of machine learning to tag tokens as being in a cue or a scope. The most important differences are the use of manually defined rules and the inclusion of grammatical relations from a parser as critical features. Introduction Speculative or, more generally, “hedged” language is a way of weakening the strength of a statement. It is usually signalled by a word or phrase, called a hedge cue, which weakens some clauses or propositions. These weakened portions of a sentence form the scope of the hedge cues. Hedging is an important tool in scientific language allowing scientists to g"
W10-3008,P08-1033,0,0.466444,"Missing"
W10-3008,W08-0606,0,0.349839,"Missing"
W11-0202,bentivogli-etal-2010-building,0,0.027228,"sis), (larval fat body → larval tissue) and (the synthesis of x in y ↔ x is synthesised in y). Pattern (2) entails pattern (1) and would also return results matching the information need. (2) the overexpression of x in the larval fat body A system for entailment detection can automatically extract a database of entailing fragments from a large corpus and use them to modify any query given by the user. Recent studies have also investigated how complex sentence-level entailment relations can be broken down into smaller consecutive steps involving fragment-level entailment (Sammons et al., 2010; Bentivogli et al., 2010). For example: (3) Text: The mitogenic effects of the B beta chain of fibrinogen are mediated through cell surface calreticulin. Hypothesis: Fibrinogen beta chain interacts with CRP55. To recognise that the hypothesis is entailed by the text, it can be decomposed into five separate steps involving text fragments: 1. B beta chain of fibrinogen → Fibrinogen beta chain In order to cover a wide variety of language phenomena, a fragment is defined in the following way: Definition 1. A fragment is any connected subgraph of a directed dependency graph containing one or more words and the grammatical"
W11-0202,P10-1124,0,0.0199709,"ean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation. In contrast to current work, each of the approaches described above only focuses on detecting entailment be"
W11-0202,P06-4020,1,0.695974,"Missing"
W11-0202,W09-0215,0,0.0206492,"various schemes for feature weighting and found the best one to be a variation of Dice’s coefficient (Dice, 1945), described by Curran (2003): wA (f ) = 2P (A, f ) P (A, ∗) + P (∗, f ) where wA (f ) is the weight of feature f for fragment A, P (∗, f ) is the probability of the feature appearing in the corpus with any fragment, P (A, ∗) is the probability of the fragment appearing with any feature, and P (A, f ) is the probability of the fragment and the feature appearing together. Different measures of distributional similarity, both symmetrical and directonal, were also tested and ClarkeDE (Clarke, 2009) was used for the final system as it achieved the highest performance on graph fragments: P ClarkeDE(A → B) = min(wA (f ), wB (f )) f ∈FA ∩FB P f ∈FA wA (f ) where FA is the set of features for fragment A and wA (f ) is the weight of feature f for fragment A. It quantifies the weighted coverage of the features of A by the features of B by finding the sum of minimum weights. 2 http://www.biomedcentral.com/info/about/datamining/ The ClarkeDE similarity measure is designed to detect whether the features of A are a proper subset of the features of B. This works well for finding more general versio"
W11-0202,W10-3001,0,0.042392,"Missing"
W11-0202,H05-1049,0,0.0586057,"Missing"
W11-0202,P98-2127,0,0.0602462,"Missing"
W11-0202,C08-1066,0,0.0246429,"rds is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation. In contrast to current work, each of the approaches described above only focuses on detecting entailment between specific subtypes of fragments (either sentences, relations or words) and optimising the system for a single scenario. This means only limited types of entailment relations are found and they cannot be used for entailment generation or compositional entailment detection as described in Section 2. MacCartney and Manning (2008) approach sentence-level entailment detection by breaking the problem into a sequence of atomic edits linking the premise to the hypothesis. Entailment relations are then predicted for each edit, propagated up through a syntax tree and then used to compose the resulting entailment decision. However, their system focuses more on natural logic and uses a predefined set of compositional rules to capture a subset of valid inferences with high precision but low recall. It also relies on a supervised classifier and information from WordNet to reach the final entailment decision. Androutsopoulos and"
W11-0202,P10-1122,0,0.0138678,"verexpression → synthesis), (larval fat body → larval tissue) and (the synthesis of x in y ↔ x is synthesised in y). Pattern (2) entails pattern (1) and would also return results matching the information need. (2) the overexpression of x in the larval fat body A system for entailment detection can automatically extract a database of entailing fragments from a large corpus and use them to modify any query given by the user. Recent studies have also investigated how complex sentence-level entailment relations can be broken down into smaller consecutive steps involving fragment-level entailment (Sammons et al., 2010; Bentivogli et al., 2010). For example: (3) Text: The mitogenic effects of the B beta chain of fibrinogen are mediated through cell surface calreticulin. Hypothesis: Fibrinogen beta chain interacts with CRP55. To recognise that the hypothesis is entailed by the text, it can be decomposed into five separate steps involving text fragments: 1. B beta chain of fibrinogen → Fibrinogen beta chain In order to cover a wide variety of language phenomena, a fragment is defined in the following way: Definition 1. A fragment is any connected subgraph of a directed dependency graph containing one or more"
W11-0202,C08-1107,0,0.0202487,"(2006) combine lexico-syntactic features and automatically acquired paraphrases to classify entailing sentences. Lintean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisat"
W11-0202,W04-3206,0,0.0277677,"ies when calculating similarity, which supports incorporation of extra syntactic information. Hickl et al. (2006) combine lexico-syntactic features and automatically acquired paraphrases to classify entailing sentences. Lintean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures base"
W11-0202,W05-1202,0,0.0152433,"d to a truly recursive method where fragments consist of smaller fragments. 3.2 Extrinsic similarity The extrinsic similarity between two fragments or words is modelled using measures of directional distributional similarity. We define a context relation as a tuple (a, d, r, a0 ) where a is the main word, a0 is a word connected to it through a dependency relation, r is the label of that relation and d shows the direction of the relation. The tuple f : (d, r, a0 ) is referred to as a feature of a. To calculate the distributional similarity between two fragments, we adopt an approach similar to Weeds et al. (2005). Using the previous notation, (d, r, a0 ) is a feature of fragment A if (d, r, a0 ) is a feature for a word which is contained in A. The general algorithm for feature collection is as follows: 13 1. Find the next instance of a fragment in the background corpus. 2. For each word in the fragment, find dependency relations which connect to words not contained in the fragment. 3. Count these dependency relations as distributional features for the fragment. For example, in Figure 1 the fragment (* induces * in *) has three features: (1, subj, B61), (1, dobj, autophosphorylation) and (1, dobj, cell"
W11-0202,morante-2010-descriptive,0,\N,Missing
W11-0202,C98-2122,0,\N,Missing
W12-0206,P06-4020,1,0.882403,"of language attainment at different stages of learning. The English Profile (EP)2 research programme aims to enhance the learning, teaching and assessment of English as an additional language by creating detailed reference level descriptions of the language abilities expected at each level. As part of our research within that framework, we modify and combine techniques developed for information visualisation with methodologies from computational linguistics to support a novel and more empirical perspective on CEFR 3 Briscoe et al. (2010) POS tagged and parsed the data using the RASP toolkit (Briscoe et al., 2006). POS tags are based on the CLAWS tagset. 1 http://www.coe.int/t/dg4/linguistic/cadre en.asp 2 http://www.englishprofile.org/ 35 Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 35–43, c Avignon, France, April 23 - 24 2012. 2012 Association for Computational Linguistics Feature Example VM RR (POS bigram: +) could clearly , because (word bigram: −) , because of necessary (word unigram: +) it is necessary that the people (word bigram: −) *the people are clever VV∅ VV∅ (POS bigram: −) *we go see film NN2 VVG (POS bigram: +) children smiling tween features can rapidly grow and"
W12-0206,P11-2053,0,0.0181983,"ary collections of items, such as a linguistic parse tree. VisLink provides a general platform within which multiple visualisations of language (e.g., a force-directed graph and a radial graph) can be connected, cross-queried and compared. Moreover, he explores the space of content analysis. DocuBurst is an interactive visualisation of document content, which spatially organizes words using an expert-created ontology (e.g., WordNet). Parallel Tag Clouds combine keyword extraction and coordinated visualisations to provide comparative overviews across subsets of a faceted text corpus. Recently, Rohrdantz et al. (2011) proposed a new approach to detecting and investigating changes in word senses by visually modelling and plotting aggregated views about the diachronic development in word contexts. Visualisation techniques have been successfully used in other areas including the humanities (e.g., Plaisant et al. (2006) and Don et al. (2007)), as well as genomics (e.g., Meyer et al. (2010a) and Meyer et al. (2010b)). For example, Meyer 41 the development of the tool and was eager to use and test it. There were dozens of meetings over a period of seven months, and the feedback on early interfaces was incorporat"
W12-0206,P11-1019,1,0.849506,"weight; + and − show their association with either passing or failing scripts. tions we describe in detail the visualiser, illustrate how it can support the investigation of individual features, and discuss how such investigations can shed light on the relationships between features and developmental aspects of learner grammars. To the best of our knowledge, this is the first attempt to visually analyse as well as perform a linguistic interpretation of discriminative features that characterise learner English. We also apply our visualiser to a set of 1,244 publicallyavailable FCE ESOL texts (Yannakoudakis et al., 2011) and make it available as a web service to other researchers5 . 2 Dataset We use texts produced by candidates taking the FCE exam, which assesses English at an upperintermediate level. The FCE texts, which are part of the Cambridge Learner Corpus6 , are produced by English language learners from around the world sitting Cambridge Assessment’s ESOL examinations7 . The texts are manually tagged with information about linguistic errors (Nicholls, 2003) and linked to meta-data about the learners (e.g., age and native language) and the exam (e.g., grade). 3 The English Profile visualiser 3.1 Basic"
W12-2004,W07-0607,0,0.0685571,"tom /item3646603/ texts under the framework of AA. Most of the methods we investigate require syntactic analysis. As in Yannakoudakis et al. (2011), we analyze all texts using the RASP toolkit (Briscoe et al., 2006)4 . vocabulary. We thus assess the minimum, maximum and average word length as a superficial proxy for coherence. 4.1 We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture second-order effects in common with other dimensionality-reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes. Utilizing the S-Space package (Jurgens and Stevens, 2010), we trained an ISA model5 using a subset of ukWaC (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens. We used the POS"
W12-2004,J08-1001,0,0.0608695,"tein (2007) use RI to determine the semantic similarity between sentences of same/different discourse segments (e.g., from the essay thesis and conclusion, or between sentences and the essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid ca"
W12-2004,N04-1015,0,0.0945124,"Missing"
W12-2004,P06-4020,1,0.814619,"us on the development and evaluation of (automated) methods for assessing coherence in learner 1 Powers et al. (2002) report the results of a related experiment with the AA system e-Rater, in which experts tried to subvert the system by submitting essays they believed would be inaccurately scored. 2 http://ilexir.co.uk/applications/clc-fce-dataset/ 3 http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom /item3646603/ texts under the framework of AA. Most of the methods we investigate require syntactic analysis. As in Yannakoudakis et al. (2011), we analyze all texts using the RASP toolkit (Briscoe et al., 2006)4 . vocabulary. We thus assess the minimum, maximum and average word length as a superficial proxy for coherence. 4.1 We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture"
W12-2004,N10-1099,0,0.191056,"ence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 on the same ukWaC data. Below we only report results for the fully-incremental ISA model. 8 https://bitbucket.org/melsner/browncoherence 9 The tool does not per"
W12-2004,E09-1017,0,0.0262547,"si+1 as the maximum cosine similarity between the history vectors of the words they contain. The overall coherence of a text T is then measured by taking the mean of all sentence-pair scores: Pn−1 maxk,j sim(ski , sji+1 ) coherence(T ) = i=1 (1) n−1 ‘Superficial’ Proxies In this section we introduce diverse classes of ‘superficial’ cohesive features that serve as proxies for coherence. Surface text properties have been assessed in the framework of automatic summary evaluation (Pitler et al., 2010), and have been shown to significantly correlate with the fluency of machinetranslated sentences (Chae and Nenkova, 2009). 4.1.1 Part-of-Speech (POS) Distribution The AA system described in Yannakoudakis et al. (2011) exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades. In coherent texts, constituent clauses and sentences are related and depend on each other for their interpretation. Anaphors such as pronouns link the current sentence to those where the entities were previously mentioned. Pronouns can be directly related to (lack of) coherence and make intuitive sense as cohesive devices. We compute the number of pronouns in a text and use it as a shall"
W12-2004,E09-1018,0,0.0615917,"e trained a RI model with 2,000 dimensions and a context window of 3 on the same ukWaC data. Below we only report results for the fully-incremental ISA model. 8 https://bitbucket.org/melsner/browncoherence 9 The tool does not perform full coreference resolution; instead, coreference is approximated by linking entities that share a head noun. 10 We represent entities with specified roles (Subject, Object, Neither, Absent), use transition probabilities of length 2, 3 and 4, and a salience option of 2. 36 4.4 Pronoun Coreference Model Pronominal anaphora is another important aspect of coherence. Charniak and Elsner (2009) present an unsupervised generative model of pronominal anaphora for coherence modeling. In their implementation, they model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a ‘good’ antecedent is found, the probability of a pronoun will be high; otherwise, the probability will be low. The overall probability of a text is then calculated as the probability of the resulting sequence of pronoun assignments. In our experiments, we use the pre-trained model distributed by Charniak and Elsner (2009) for news text to estimate the probability of a text and includ"
W12-2004,P08-2011,0,0.462645,"ntential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pag"
W12-2004,P11-1118,0,0.160474,"l grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 o"
W12-2004,P11-2022,0,0.339282,"l grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 o"
W12-2004,P11-1030,0,0.0476069,"Missing"
W12-2004,J95-2003,0,0.398328,"essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA te"
W12-2004,N04-1024,0,0.192101,"ng this coherence score, as well as the maximum 5 The parameters of our ISA model are fairly standard: 1800 dimensions, a context window of 3 words, impact rate i = 0.0003 and decay rate km = 50. 6 We exclude articles, conjunctions, prepositions and auxiliary verbs from the calculation of sentence similarity. sim value found over the entire text, to the vectors of features associated with a text. The hypothesis is that the degree of semantic relatedness between adjoining sentences serves as a proxy for local discourse coherence; that is, coherent text units contain semantically-related words. Higgins et al. (2004) and Higgins and Burstein (2007) use RI to determine the semantic similarity between sentences of same/different discourse segments (e.g., from the essay thesis and conclusion, or between sentences and the essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherenc"
W12-2004,P10-4006,0,0.0121557,"eatures used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture second-order effects in common with other dimensionality-reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes. Utilizing the S-Space package (Jurgens and Stevens, 2010), we trained an ISA model5 using a subset of ukWaC (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens. We used the POS tagger lexicon provided with the RASP system to discard documents whose proportion of valid English words to total words is less than 0.4; 78,000 documents were extracted in total and were then preprocessed replacing URLs, email addresses, IP addresses, numbers and emoticons with special markers. To measure local coherence we define the similarity between two sentences si and si+1 as the maximum cosine similarity between the history vecto"
W12-2004,P11-1100,0,0.0850322,"l primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building E"
W12-2004,P00-1056,0,0.0155456,"idered to be coreferent. semantic space models such as ISA or RI (discussed above), this method models the intuition that local coherence is signaled by the identification of word co-occurrence patterns across adjacent sentences. We compute two features introduced by Soricut and Marcu (2006): the forward likelihood and the backward likelihood. The first refers to the likelihood of observing the words in sentence si+1 conditioned on si , and the latter to the likelihood of observing the words in si conditioned on si+1 . We extract 3 million adjacent sentences from ukWaC12 , and use the GIZA++ (Och and Ney, 2000) implementation of IBM model 1 to obtain the probabilities of recurring patterns. The forward and backward probabilities are calculated over the entire text, and their values are used as features in our feature vectors13 . We further extend the above model and incorporate syntactic aspects of text coherence by training on POS tags instead of lexical items. We try to model the intuition that local coherence is signaled by the identification of POS co-occurrence patterns across adjacent sentences, where the use of certain POS tags in a sentence tends to trigger the use of other POS tags in an ad"
W12-2004,P10-1056,0,0.568577,"n, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33–43, c Montr´eal, Canada, June 3-8, 2012. 2012 Association"
W12-2004,P06-2103,0,0.359085,"ween textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33–43, c Montr´eal, Canada,"
W12-2004,P11-1019,1,0.0513038,"systems of English learner text assign grades based on textual features which attempt to balance evidence of writing competence against evidence of performance errors. Previous work has mostly treated AA as a supervised text classification or regression task. A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al., 2011). As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from m"
W12-2004,J93-2003,0,\N,Missing
W12-2028,W12-2006,0,0.0909018,"Missing"
W12-2028,C08-1022,0,0.0481647,"Missing"
W12-2028,I08-1059,0,0.0341095,"ifiers and develop one model which maximizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn. 1 Introduction The task of detecting and correcting writing errors made by learners of English as a Second Language (ESL) has recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a mo"
W12-2028,N10-1019,0,0.0154786,"age (ESL) has recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer’s first or native language (L1) is less restricted by the amount of training data. Rozovskaya and Rot"
W12-2028,han-etal-2010-using,0,0.0199568,"recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer’s first or native language (L1) is less restricted by the amount of training data. Rozovskaya and Roth (2010b; 2010c) sh"
W12-2028,P03-2026,0,0.0366898,"imizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn. 1 Introduction The task of detecting and correcting writing errors made by learners of English as a Second Language (ESL) has recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is"
W12-2028,C10-2103,0,0.0210832,"the Innovative Use of NLP for Building Educational Applications, pages 242–250, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics low. This is a disadvantage for applications such as self-tutoring or writing assistance, which require feedback to the user. A high proportion of errorful suggestions is likely to further confuse learners and/or non-native writers rather than improve their writing or assist learning. Instead a system which maximizes precision over recall returning accurate suggestions for a small proportion of errors is likely to be more helpful (Nagata and Nakatani, 2010). In section 2 we describe the data used for training and testing the systems we developed. In section 3 we describe the preprocessing of the ESL text undertaken to provide a source of features for the classifiers. We also discuss the feature types that we exploit in our classifiers. In section 4 we describe and report results for a high precision system which makes no attempt to generalize from training data. In section 5 we describe our approach to adapting multiclass NB classifiers to characteristic errors and L1s. We also report the performance of some of these NB classifiers on the traini"
W12-2028,D10-1094,0,0.0391267,"his area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer’s first or native language (L1) is less restricted by the amount of training data. Rozovskaya and Roth (2010b; 2010c) show that adapting error corrections to the writer’s L1 and incorporating artificial"
W12-2028,N10-1018,0,0.0213953,"his area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer’s first or native language (L1) is less restricted by the amount of training data. Rozovskaya and Roth (2010b; 2010c) show that adapting error corrections to the writer’s L1 and incorporating artificial"
W12-2028,P11-1093,0,0.182618,"dels being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer’s first or native language (L1) is less restricted by the amount of training data. Rozovskaya and Roth (2010b; 2010c) show that adapting error corrections to the writer’s L1 and incorporating artificial errors, in a way that mimics the typical error rates and"
W12-2028,C08-1109,0,0.306093,"Missing"
W12-2028,P10-2065,0,0.0184782,"r-coded Cambridge Learner Corpus, from which the HOO data is drawn. 1 Introduction The task of detecting and correcting writing errors made by learners of English as a Second Language (ESL) has recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text (Eeg-Olofsson and Knutsson, 2003; De Felice and Pulman, 2008; Gamon et al., 2008; Han et al., 2006; Izumi et al., 2003; Tetreault and Chodorow, Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk 2008; Tetreault et al., 2010). However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process (Gamon, 2010; Han et al., 2010), or by using native language-specific error statistics (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010c; Rozovskaya and Roth, 2011). Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using nati"
W12-2028,P11-1019,1,0.830582,"ins about 20M words of error-annotated scripts from a wide variety of examinations. The HOO training and test datasets are drawn from the CLC. The training dataset is a reformatted 1000-script subset of a publically-available subset of CLC scripts produced by learners sitting the First Certficate in English (FCE) examination.3 This examination assesses English at an upper-intermediate level, so many learners sitting this exam still manifest a number of errors motivated by the conventions of their L1s. The CLC-FCE subcorpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al. (2011).4 The HOO training dataset contains scripts from FCE examinations undertaken in the years 2000 and 2001 written by speakers of 16 L1s. These scripts can be divided into two broad L1 typological groups, Asian (Chinese, Thai, Korean, Japanese) and European (French, Spanish, Italian, Portuguese, Catalan, Greek, Russian, Polish). The latter can be further subdivided into Slavic (Russian, Polish) and Romance. In turn, the Romance languages differ in typological relatedness with, for example, Portuguese and Spanish being closer than Spanish and French. Error coding which is not relevant to preposit"
W12-2028,andersen-etal-2008-bnc,1,\N,Missing
W12-2028,W10-1004,0,\N,Missing
W12-2028,P06-4020,1,\N,Missing
W14-1608,andersen-etal-2008-bnc,1,0.788635,"occur more than once in the dataset, and weight them using pointwise mutual information to construct feature vectors for every term. Features with negative weights were retained, as they proved to be beneficial for some similarity measures. The window-based, dependency-based and word2vec vector sets were all trained on 112M words from the British National Corpus, with preprocessing steps for lowercasing and lemmatising. Any numbers were grouped and substituted by more generic tokens. For constructing the dependency-based vector representations, we used the parsed version of the BNC created by Andersen et al. (2008) with the RASP toolkit (Briscoe et al., 2006). When saved as plain text, the 500dimensional word2vec vectors and dependencybased vectors are comparable in size (602MB and 549MB), whereas the window-based vectors are twice as large (1,004MB). We make these vector 1. Systematic evaluation of different vector space models and similarity measures on the task of hyponym generation. 2. Proposal of new properties for modelling the directional hyponymy relation. 3. Release of three lexical vector datasets, trained using neural network, window-based, and dependency-based features. 2 Vector space models"
W14-1608,W11-2501,0,0.100829,"inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Aut"
W14-1608,C92-2082,0,0.297896,"ions. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. Howe"
W14-1608,E12-1004,0,0.0727088,"to discard WordNet hypernyms that are very rare in practical use, and would not have enough examples for learning informative vector representations. The final dataset contains the remaining terms, together with all of their hyponyms, including the rare/unseen hyponyms. As expected, some general terms, such as group or location, have a large number of inherited hyponyms. On average, each hypernym in the dataset has 233 hyponyms, but the distribution is roughly exponential, and the median is only 36. In order to better facilitate future experiments with supervised methods, such as described by Baroni et al. (2012), we randomly separated the data into training (1230 hypernyms), validation (922), and test (922) sets, and we make these datasets publically available online.4 function decreasing linearly as the rank number increases, but the weights for the shared features always remain higher compared to the non-shared features. Tied feature values are handled by assigning them the average rank value. Adding 1 to the denominator of the relative rank calculation avoids exceptions with empty vectors, and also ensures that the value will always be strictly greater than C. While the basic function is still the"
W14-1608,P98-2127,0,0.0911649,", but this can lead to unreliable results when the number of features is very small. The motivation behind combining these measures is that the symmetric Lin measure will decrease the final score for such word pairs, thereby balancing the results. Similarity measures We compare the performance of a range of similarity measures, both directional and symmetrical, on the task of hyponym generation. Cosine similarity is defined as the angle between two feature vectors and has become a standard measure of similarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similari"
W14-1608,P06-4020,1,0.745272,"Missing"
W14-1608,N13-1090,0,0.0222916,"rk that takes the concatenated vectors of context words as input, and is trained to predict the vector representation of the next word, which is then transformed into a probability distribution over possible words. To speed up training and testing, they use a hierarchical data structure for filtering down the list of candidates. Both CW and HLBL vectors were trained using 37M words from RCV1. Word2vec: We created word representations using the word2vec2 toolkit. The tool is based on a feedforward neural network language model, with modifications to make representation learning more efficient (Mikolov et al., 2013a). We make use of the skip-gram model, which takes each word in a sequence as an input to a log-linear classifier with a continuous projection layer, and predicts words within a certain range before and after the input word. The window size was set to 5 and vectors were trained with both 100 and 500 dimensions. Dependencies: Finally, we created vector representations for words by using dependency relations from a parser as features. Every incoming and outgoing dependency relation is counted as a feature, together with the connected term. For example, given the dependency relation (play, dobj,"
W14-1608,P99-1016,0,0.0247043,"tionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly"
W14-1608,W09-0215,0,0.0276972,"is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these"
W14-1608,N04-1041,0,0.0116134,"words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-"
W14-1608,C08-1107,0,0.00872041,"time, and the result is compared to hyponym candidates using cosine similarity. For sparse high-dimensional vector space models it was not feasible to use the full offset vector during experiments, therefore we retain only the top 1,000 highest-weighted features. 3 term a is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity"
W14-1608,P10-1040,0,0.0133961,"co-occurrences in a fixed context window. Every word that occurs within a window of three words before or after is counted as a feature for the target word. Pointwise mutual information is then used for weighting. CW: Collobert and Weston (2008) constructed a neural network language model that is trained to predict the next word in the sequence, and simultaneously learns vector representations for each word. The vectors for context words are concatenated and used as input for the neural network, which uses a sample of possible outputs for gradient calculation to speed up the training process. Turian et al. (2010) recreated their experiments and made the vectors available online.1 HLBL: Mnih and Hinton (2007) created word representations using the hierarchical log-bilinear 1 2 http://metaoptimize.com/projects/wordreprs/ 69 https://code.google.com/p/word2vec/ sets publically available for download.3 Recently, Mikolov et al. (2013b) published interesting results about linguistic regularities in vector space models. They proposed that the relationship between two words can be characterised by their vector offset, for example, we could find the vector for word “queen” by performing the operation “king - ma"
W14-1608,W96-0103,0,0.0249265,"ten make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, The task of detecting and generating hyponyms is at the core of semantic understanding of language, and has numerous practical a"
W14-1608,J05-4002,0,0.0172427,"Missing"
W14-1608,C04-1146,0,0.79769,"returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these measures is that the symmetric Lin measure will decrease the final score for such word pairs, thereby balancing the res"
W14-1608,J09-3004,0,0.0525115,"marek@swiftkey.net Abstract the input text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of"
W14-1608,C98-2122,0,\N,Missing
W14-1701,D11-1010,1,0.82865,"bmit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt with. In the CoNLL-2014"
W14-1701,P11-1092,1,0.545955,"bmit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt with. In the CoNLL-2014"
W14-1701,D12-1052,1,0.571584,"systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical error"
W14-1701,N12-1067,1,0.528923,"systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical error"
W14-1701,W14-1705,0,0.0380739,"Missing"
W14-1701,W13-1703,1,0.793382,"examples of the 28 error types in the CoNLL-2014 shared task. Since there are 28 error types in our shared task compared to two in HOO 2012 and five in CoNLL2013, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task. To illustrate, consider the following sentence: 3 Data This section describes the training and test data released to each participating team in our shared task. 3.1 Training Data The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). As noted by (Leacock et al., 2010), the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The g"
W14-1701,W13-3601,1,0.494294,"ote that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. NUCLE release version 3.2 was used in the CoNLL-2014 shared task. In this version, 17 essays were removed from the first release of NUCLE since these essays were duplicates with multiple annotations. In addition, in order to facilitate the detection and correction of article/determiner errors and preposition errors, we performed some automatic mapping of error types in the original NUCLE corpus to arrive at release version 3.2. Ng et al. (2013) gives more details of how the mapping was carried out. The statistics of the NUCLE corpus (release 3.2 version) are shown in Table 2. The distribution of errors among all error types is shown in Table 3. While the NUCLE corpus is provided in our shared task, participating teams are free to not use NUCLE, or to use additional resources and tools in building their grammatical error correction systems, as long as these resources and tools are pub1 # essays # sentences # word tokens Training data (NUCLE) 1,397 57,151 1,161,567 Test data 50 1,312 30,144 Table 2: Statistics of training and test dat"
W14-1701,W11-2838,0,0.158186,"o independently annotated the test essays, compared to just one human annotator in CoNLL-2013. 1 Introduction Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014 (CoNLL-2014). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012), and a CoNLL 2 Task Definition The goal of the CoNLL-2014 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for th"
W14-1701,W12-2006,0,0.112084,"he test essays, compared to just one human annotator in CoNLL-2013. 1 Introduction Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014 (CoNLL-2014). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012), and a CoNLL 2 Task Definition The goal of the CoNLL-2014 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The abil"
W14-1701,de-marneffe-etal-2006-generating,0,0.051769,"Missing"
W14-1701,D10-1094,0,0.00847095,"able to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and N"
W14-1701,W14-1702,0,0.477963,"Bombay Instituto Polit´ecnico Nacional Nara Institute of Science and Technology National Tsing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in"
W14-1701,W14-1704,0,0.500627,"ypes are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, the number of a subject agrees with the number of a verb. In contrast, it is a lot harder to write a rule to consistently correct Wci (wrong collocation/idiom) errors. As such, RB methods were often, but not always, used as a preliminary or supplementary stage in a larger hybrid system. Finally, although there were fewer machinelearnt classifier (ML) approaches than last year, some teams still used various classifiers to correct specific error types. In fact, CUUI (Rozovskaya et al., 2014) only built classifiers for specific error types and did not attempt to tackle the whole range of errors. SJTU (Wang et al., 2014a) also preprocessed the training data into more precise error categories using rules (e.g., verb tense (Vt) team’s approach can be found in Table 6. While machine-learnt classifiers for specific error types proved popular in last year’s CoNLL-2013 shared task, since this year’s task required the correction of all 28 error types, teams tended to prefer methods that could deal with all error types simultaneously. In fact, most teams built hybrid systems that made use"
W14-1701,N10-1019,0,0.0148891,"so made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word cho"
W14-1701,W08-1205,0,0.00769315,"Missing"
W14-1701,P10-2065,0,0.0173654,"eams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt"
W14-1701,W14-1707,0,0.00737204,"s that made use of a combination of different approaches to identify and correct errors. One of the most popular approaches to nonspecific error type correction, incorporated to various extents in many teams’ systems, was the Language Model (LM) based approach. Specifically, the probability of a learner n-gram is compared with the probability of a candidate corrected ngram, and if the difference is greater than some threshold, an error was perceived to have been detected and a higher scoring replacement n-gram could be suggested. Some teams used this approach only to detect errors, e.g., IPN (Hernandez and Calvo, 2014), which could then be corrected by other methods, whilst other teams used other methods to detect errors first, and then made corrections based on the alternative highest n-gram probability score, e.g., RAC (Boros¸ et al., 2014). No single team used a uniquely LM-based solution and the LM approach was always a component in a hybrid system. An alternative solution to correcting all errors was to use a phrase-based statistical machine translation (MT) system to “translate” learner English into correct English. Teams that followed the 7 Team ID CAMB CUUI AMU POST NTHU RAC UMC PKU∗ NARA SJTU UFC∗"
W14-1701,W14-1710,0,0.061916,"ing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, th"
W14-1701,W14-1711,0,0.0400407,"ing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, th"
W14-1701,W14-1703,0,0.263527,"Missing"
W14-1701,P13-1143,1,0.816741,"ecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many diffe"
W14-1701,P03-1054,0,0.0268268,"ed in (Dahlmeier and Ng, 2011b), and has been publicly available for research purposes since June 20111 . All instances of grammatical errors are annotated in NUCLE. To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NUCLE essays include sentence segmentation and word tokenization using the NLTK toolkit (Bird et al., 2009), and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. NUCLE release version 3.2 was used in the CoNLL-2014 shared task. In this version, 17 essays were removed from the first release o"
W14-1701,P11-1019,1,0.389773,"e shown in Table 2. The distribution of errors among all error types is shown in Table 3. While the NUCLE corpus is provided in our shared task, participating teams are free to not use NUCLE, or to use additional resources and tools in building their grammatical error correction systems, as long as these resources and tools are pub1 # essays # sentences # word tokens Training data (NUCLE) 1,397 57,151 1,161,567 Test data 50 1,312 30,144 Table 2: Statistics of training and test data. licly available and not proprietary. For example, participating teams are free to use the Cambridge FCE corpus (Yannakoudakis et al., 2011; Nicholls, 2003) (the training data provided in HOO 2012 (Dale et al., 2012)) as additional training data. 3.2 Test Data Similar to CoNLL-2013, 25 NUS students, who are non-native speakers of English, were recruited to write new essays to be used as blind test data in the shared task. Each student wrote two essays in response to the two prompts shown in Table 4, one essay per prompt. The first prompt was also used in the NUCLE training data, but the second prompt is entirely new and not used previously. As a result, 50 new test essays were collected. The statistics of the test essays are also"
W14-1701,W14-1708,0,0.0161278,"f Science and Technology National Tsing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make"
W15-0627,P06-4020,1,0.588853,"ave adjective–noun combinations as corrections. 3.3 NUCLE dataset We have also used the training and development sets from the CoNLL-2014 Shared Task on Grammatical Error Correction (Ng et al., 2014) to extract the incorrect AN combinations. The data for the shared task has been extracted from the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). Unlike the other two datasets it represents a smaller range of L1s, and similarly to the CLC-FCE dataset the errors are not further annotated with respect to their subtypes. We have preprocessed the data using the RASP parser (Briscoe et al., 2006), and used the error annotation provided to extract the AN combinations that contain errors in the choice of either one or both words. Additionally, we have also checked that the suggested corrections are represented by AN combinations. The extracted dataset contains 369 ANs. Table 2 reports the distribution of the errors with respect to the incorrect choice of an adjective, noun or both words within AN combinations in all three datasets. 236 Error Correction Algorithm First, we implement a basic error correction algorithm that replicates the previous approaches to error correction overviewed"
W15-0627,P06-1032,0,0.0127625,"lar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Previous work on EDC for content words has also demonstrated that since these error types are substantially different from errors with function words, they require different approaches. The most widely adopted approach to E"
W15-0627,D11-1010,0,0.270639,"ith a state-of-the-art content word error detection system and discuss the results. 1 The major difficulty is that correct word choice is not governed by any strictly defined rules: native speakers know that powerful computer is preferred over strong computer, while strong tea is preferred over powerful tea (Leacock et al., 2014), but language learners often find themselves unsure of how to choose an appropriate word. As a result, they often confuse words that are similar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014"
W15-0627,W12-2006,0,0.0180979,"uter, while strong tea is preferred over powerful tea (Leacock et al., 2014), but language learners often find themselves unsure of how to choose an appropriate word. As a result, they often confuse words that are similar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Pr"
W15-0627,I08-1059,0,0.027569,"words that are similar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Previous work on EDC for content words has also demonstrated that since these error types are substantially different from errors with function words, they require different approaches. The most widel"
W15-0627,C14-1164,1,0.736347,"r error correction system with a state-of-the-art content word error detection system and discuss the results. 1 The major difficulty is that correct word choice is not governed by any strictly defined rules: native speakers know that powerful computer is preferred over strong computer, while strong tea is preferred over powerful tea (Leacock et al., 2014), but language learners often find themselves unsure of how to choose an appropriate word. As a result, they often confuse words that are similar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised ("
W15-0627,W09-2107,0,0.0238359,"ranslations. Madnani and Cahill (2014) use a corpus of Wikipedia revisions containing annotated errors in the use of prepositions and their corrections to improve the ranking of the suggestions. Finally, we note that a number of previous approaches to errors in content words have combined error detection and correction, flagging an original choice as an error if an EDC algorithm is able to find ¨ a more frequent or fluent combination (Ostling and Knutsson, 2009; Chang et al., 2008; Futagi et al., 2008; Shei and Pain, 2000), while some focussed on error correction only (Dahlmeier and Ng, 2011; Liu et al., 2009). Kochmar and Briscoe (2014) argue that error detection and correction should be performed separately. They show that an EDC algorithm is prone to overcorrection, flagging originally correct combinations as errors, if error detection is dependent on the set of alternatives and if some of these alternatives are judged to be more fluent than the original combination. We follow Kochmar and Briscoe (2014) and treat error detection and error correction in content words as separate steps. We focus on the correction step, and first implement a simple error correction algorithm that replicates previou"
W15-0627,W14-1810,0,0.0747313,"es gives access to the types of confusions which cannot be captured by any L2 resources. Learner corpora and databases of text revisions can be used to similar effect. For example, Rozovskaya and Roth (2011) show that performance of an EDC algorithm applied to articles and prepositions can be improved if the classifier uses L1-specific priors, with the priors being set using the distribution of confusion pairs in learner texts. Sawai et al. (2013) show that an EDC system that uses a large learner corpus to extract confusion sets outperforms systems that use WordNet and roundtrip translations. Madnani and Cahill (2014) use a corpus of Wikipedia revisions containing annotated errors in the use of prepositions and their corrections to improve the ranking of the suggestions. Finally, we note that a number of previous approaches to errors in content words have combined error detection and correction, flagging an original choice as an error if an EDC algorithm is able to find ¨ a more frequent or fluent combination (Ostling and Knutsson, 2009; Chang et al., 2008; Futagi et al., 2008; Shei and Pain, 2000), while some focussed on error correction only (Dahlmeier and Ng, 2011; Liu et al., 2009). Kochmar and Briscoe"
W15-0627,P11-1093,0,0.166801,"appropriate word. As a result, they often confuse words that are similar in meaning or spelling, overuse words with general meaning, or select words based on their L1s (Kochmar and Briscoe, 2014; Dahlmeier and Ng, 2011). Introduction The task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Previous work on EDC for content words has also demonstrated that since these error types are substantially different from errors with function words, they r"
W15-0627,E14-1038,0,0.0200086,"task of error detection and correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Previous work on EDC for content words has also demonstrated that since these error types are substantially different from errors with function words, they require different approaches. The most widely adopted approach to EDC for function words relies on availability of finite confusion sets. The task can then be cast as multi-class classification with the number of classes equal to the numb"
W15-0627,P13-2124,0,0.0631573,"nd correction (EDC) on non-native texts, as well as research on learner language in general, has attracted much attention recently (Leacock et al., 2014; Ng et al., 2014; Ng et al., 2013; Dale et al., 2012). The field has been dominated by EDC for grammatical errors and errors in the use of articles and prepositions (Ng et al., 2013; Rozovskaya and Roth, 2011; Chodorow et al., 2010; Gamon et al., 2008; Brockett et al., 2006; Han et al., 2006). More recently, however, the need to address other error types has been recognised (Kochmar and Briscoe, 2014; Ng et al., 2014; Rozovskaya et al., 2014; Sawai et al., 2013; Dahlmeier and Ng, 2011). Among these, errors in content words are the third most frequent error type after errors in articles and prepositions (Leacock et al., 2014; Ng et al., 2014). Previous work on EDC for content words has also demonstrated that since these error types are substantially different from errors with function words, they require different approaches. The most widely adopted approach to EDC for function words relies on availability of finite confusion sets. The task can then be cast as multi-class classification with the number of classes equal to the number of possible alter"
W15-0627,P11-1019,1,0.897138,"Missing"
W15-0627,W11-2838,0,\N,Missing
W15-0627,W14-1701,1,\N,Missing
W15-0627,W13-1703,0,\N,Missing
W15-0627,W13-3601,0,\N,Missing
W16-0502,J08-1001,0,0.0131453,"pair of the nouns in the text, we check whether they are semantically related. Finally, lexical chains are built by linking semantically related nouns in text. A set of 7 lexical chain-based features are computed, including total number of lexical chains per document, total number of lexical chains normalized with text length, average/maximum lexical chain length, average/maximum lexical chain span, and the number of lexical chains that span more than half of the document.5 (3) Entity grid features Another entity-based approach to measure text coherence is the entity grid model introduced by Barzilay and Lapata (2008). They represented each text by an entity grid, which is a two-dimensional array that captures the distribution of discourse entities across text sentences. Each grid cell contains the grammatical role of a particular entity in the specified sentence: whether it is a subject (S), object (O), neither a subject nor an object (X), or absent from the sentence (-). A local entity transition is defined as the transition of the grammatical role of an entity from one sentence to the following sentence. In our experiments, we used the Brown Coreference Toolkit v1.0 (Eisner and Charniak, 2011) to genera"
W16-0502,P06-4020,1,0.253323,"written by learners from all over the world (Capel, 2012). It provides a more fine-grained lexical complexity measure that captures the relative difficulty of each word by assigning the word difficulty to one of the six CEFR levels. Additionally, the EVP indicates the word difficulty for L2 learners rather than native speakers, which makes it more informative in non-native readability analysis. In our experiments, the proportion of words at each CEFR level is calculated and added to the feature set. Parse Tree Syntactic Features A number of syntactic measures based on the RASP parser output (Briscoe et al., 2006) are used to describe the grammatical complexity of text, including average parse tree depth, and average number of noun, verb, adjective, adverb, prepositional phrases and clauses per sentence. Grammatical relations (GR) between constituents in a sentence may also affect the judgement of syntactic difficulty. Yannakoudakis (2013) applied 24 GR-based complexity measures in essay scoring and showed good results. These complexity measures capture the grammatical sophistication of the text through the representation of the distance be4 http://www.englishprofile.org/ tween the sentence constituent"
W16-0502,N04-1025,0,0.367018,"Missing"
W16-0502,P07-1033,0,0.159703,"Missing"
W16-0502,P11-2022,0,0.0162308,"oduced by Barzilay and Lapata (2008). They represented each text by an entity grid, which is a two-dimensional array that captures the distribution of discourse entities across text sentences. Each grid cell contains the grammatical role of a particular entity in the specified sentence: whether it is a subject (S), object (O), neither a subject nor an object (X), or absent from the sentence (-). A local entity transition is defined as the transition of the grammatical role of an entity from one sentence to the following sentence. In our experiments, we used the Brown Coreference Toolkit v1.0 (Eisner and Charniak, 2011) to generate the entity grid for the documents. The probabilities of the 16 types of local entity transition patterns are calculated to represent the coherence of the text. 4.2 Implementation and Evaluation In our experiments, we cast readability assessment as a supervised machine learning problem. In particular, a pairwise ranking approach is adopted and compared with a classification method. We believe that the reading difficulty of text is a continuous rather than discrete variable. Text difficulty within a level can also vary. Instead of assigning an abso5 The length of a chain is the numb"
W16-0502,E09-1027,0,0.0279672,"over-fitting to the WeeBit data, two types of language modeling based features are extracted using the SRILM toolkit (Stolcke, 2002): (1) word token n-gram models, with n ranging from 1 to 5, trained on the British National Corpus (BNC), and (2) POS n-grams, with n ranging from 1 to 5, trained on the five levels in the WeeBit corpus itself. The LMs are used to score the text with loglikelihood and perplexity. Discourse-based Features Discourse features measure the cohesion and coherence of the text. Three types of discourse-based features are used. (1) Entity density features Previous work by Feng et al. (2009; 2010) has shown that entity density is strongly associated with text comprehension. An entity set is a union of named entities and general nouns (including nouns and proper nouns) contained in a text, with overlapping general nouns removed. Based on this, 9 entity density features, including the total number of all/unique entities per document, the average number of all/unique entities per sentence, percentage of named entities per sentence/document, percentage of named entities in all entities, percentage of overlapping nouns removed, and percentage of unique named entities in all unique en"
W16-0502,C10-2032,0,0.255852,"ter results in terms of accuracy than the traditional readability formulae, such as the the Flesch-Kincaid score (Kincaid et al., 1975). Schwarm and Ostendorf (2005) extended this method to multiple language models. They combined traditional reading metrics with statistical language models as well as some basic parse tree features and then applied an SVM classifier. Heilman et al. (2007; 2008) expanded the feature set to include certain lexical and grammatical features extracted from parse trees while using a linear regression model to predict the grade level. 13 Pitler and Nenkova (2008) and Feng et al. (2010) were the first to introduce discourse-based features into the framework. The experiments with discourse features demonstrated promising results in predicting the readability level of text for both classification and regression approaches. Kate et al. (2010) looked at both the effect of the feature choice and the machine learning framework choice on performance, and found that the improvement resulting from changing the framework is smaller than that from changing the features. 2.2 Readability Assessment for L2 Learners Most previous work on readability assessment is directed at predicting rea"
W16-0502,D12-1043,0,0.50422,"Missing"
W16-0502,N07-1058,0,0.759474,"en depends upon the need and characteristics of the target readers. Most of the studies so far have evaluated text difficulty as judged by native speakers, despite the fact that text comprehensibility can be perceived very differently by L2 learners. In the case of L2 learners, due to the difference in the pace of language acquisition, the focus in readability measures often differs from that for native readers. For example, the grammatical aspects of readability usually contribute more to text comprehensibility for L2 learners than the conceptual cognition difficulty of the reading material (Heilman et al., 2007). A system that is tailored towards learner’s perception of reading difficulty can produce more accurate estimation of text reading difficulty for non-native readers and thus better facilitate language learning. One of the major challenges for a data-driven approach to text readability assessment for L2 learners is that there is not enough significantly sized, properly annotated data for this task. At the same time, text readability assessment in general has been previously studied by many researchers and there are a number of existing corpora aimed at native speakers that can be used. To addr"
W16-0502,W08-0909,0,0.0948737,"Missing"
W16-0502,C10-1062,0,0.285432,"with statistical language models as well as some basic parse tree features and then applied an SVM classifier. Heilman et al. (2007; 2008) expanded the feature set to include certain lexical and grammatical features extracted from parse trees while using a linear regression model to predict the grade level. 13 Pitler and Nenkova (2008) and Feng et al. (2010) were the first to introduce discourse-based features into the framework. The experiments with discourse features demonstrated promising results in predicting the readability level of text for both classification and regression approaches. Kate et al. (2010) looked at both the effect of the feature choice and the machine learning framework choice on performance, and found that the improvement resulting from changing the framework is smaller than that from changing the features. 2.2 Readability Assessment for L2 Learners Most previous work on readability assessment is directed at predicting reading difficulty for native readers. Several efforts in developing automated readability assessment that take L2 learners into consideration have emerged since 2007. Heilman et al. (2007) tested the effect of grammatical features for both L1 (first language)"
W16-0502,D15-1049,0,0.0185447,"where the WeeBit corpus is taken as the source domain, and the L2 data as the target domain. The idea behind this is to use out-of-domain training data to boost the performance on limited in-domain data. EasyAdapt (Daum´e III, 2007) is one of the best performing domain adaptation algorithms. It has previously been applied to essay scoring and showed 6 A 4th order polynomial function is adopted because it yields better results compared to other orders. 7 Throughout this paper, we test significance using t-test for ACC and Williams’ test (Williams, 1959) for P CC. 19 testing data good results (Phandi et al., 2015). In a two domain case, EasyAdapt expands the input feature space from RF to R3F , and then applies two mapping functions ΦS (x) = hx, x, 0i and ΦT (x) = hx, 0, xi on source domain data and target domain data input vectors respectively. Here, 0 = h0, ...0i ∈ RF is the zero vector. In this manner, the instance feature vectors from the WeeBit corpus and Cambridge Exams datases are augmented to three times their original dimensionality. The augmented feature space captures both general and domain specific information and is thus capable of generalizing source domain knowledge to facilitate estima"
W16-0502,D08-1020,0,0.211871,"modelling approach yields better results in terms of accuracy than the traditional readability formulae, such as the the Flesch-Kincaid score (Kincaid et al., 1975). Schwarm and Ostendorf (2005) extended this method to multiple language models. They combined traditional reading metrics with statistical language models as well as some basic parse tree features and then applied an SVM classifier. Heilman et al. (2007; 2008) expanded the feature set to include certain lexical and grammatical features extracted from parse trees while using a linear regression model to predict the grade level. 13 Pitler and Nenkova (2008) and Feng et al. (2010) were the first to introduce discourse-based features into the framework. The experiments with discourse features demonstrated promising results in predicting the readability level of text for both classification and regression approaches. Kate et al. (2010) looked at both the effect of the feature choice and the machine learning framework choice on performance, and found that the improvement resulting from changing the framework is smaller than that from changing the features. 2.2 Readability Assessment for L2 Learners Most previous work on readability assessment is dir"
W16-0502,P05-1065,0,0.229535,"vious studies on text readability assessment have used machine learning based approaches, which enable investigation of a broader set of linguistic features. Si and Callan (2001) and CollinsThompson and Callan (2004) were among the early works on statistical readability assessment. They applied unigram language models and na¨ıve Bayes classification to estimate the grade level of a given text. Experiments showed that the language modelling approach yields better results in terms of accuracy than the traditional readability formulae, such as the the Flesch-Kincaid score (Kincaid et al., 1975). Schwarm and Ostendorf (2005) extended this method to multiple language models. They combined traditional reading metrics with statistical language models as well as some basic parse tree features and then applied an SVM classifier. Heilman et al. (2007; 2008) expanded the feature set to include certain lexical and grammatical features extracted from parse trees while using a linear regression model to predict the grade level. 13 Pitler and Nenkova (2008) and Feng et al. (2010) were the first to introduce discourse-based features into the framework. The experiments with discourse features demonstrated promising results in"
W16-0502,W13-2904,0,0.0161826,"ders and found that grammatical features play a more important role in L2 readability prediction than in L1 readability prediction. Vajjala and Meurers (2012) combined measures from Second Language Acquisition research with traditional readability features and showed that the use of lexical and syntactic features for measuring language development of L2 learners has a substantial positive impact on readability classification. They observed that lexical features perform better than syntactic features, and that the traditional features have a good predictive power when used with other features. Shen et al. (2013) developed a language-independent approach to automatic text difficulty assessment for L2 learners. They treated the task of reading level assessment as a discriminative problem and applied a regression approach using a set of features that they claim to be language-independent. However, most of these studies have used textual data annotated with the readability levels for native speakers of English rather than L2 learners specifically. While the majority of work on automated readability assessment are for English, studies on L2 readability in other languages, including French (Franc¸ois and F"
W16-0502,W12-2019,0,0.749959,"lting from changing the framework is smaller than that from changing the features. 2.2 Readability Assessment for L2 Learners Most previous work on readability assessment is directed at predicting reading difficulty for native readers. Several efforts in developing automated readability assessment that take L2 learners into consideration have emerged since 2007. Heilman et al. (2007) tested the effect of grammatical features for both L1 (first language) and L2 readers and found that grammatical features play a more important role in L2 readability prediction than in L1 readability prediction. Vajjala and Meurers (2012) combined measures from Second Language Acquisition research with traditional readability features and showed that the use of lexical and syntactic features for measuring language development of L2 learners has a substantial positive impact on readability classification. They observed that lexical features perform better than syntactic features, and that the traditional features have a good predictive power when used with other features. Shen et al. (2013) developed a language-independent approach to automatic text difficulty assessment for L2 learners. They treated the task of reading level a"
W16-0510,P06-4020,1,0.803516,"Missing"
W16-0510,N04-1024,0,0.272002,"mber of existing automated textscoring systems (sometimes referred to as essay scoring systems). For an overview, the interested reader is directed to reviews and advances in the area (Shermis and Burstein, 2003; Landauer, 2003; Valenti et al., 2003; Dikli, 2006; Phillips, 2007; Briscoe et al., 2010; Shermis and Burstein, 2013). In this section, we review related research on topicalrelevance detection for automated writing assessment, and outline the key differences between our approach and that of existing work. A wide variety of computational approaches (Miller, 2003; Landauer et al., 2003; Higgins et al., 2004; Higgins and Burstein, 2007; Chen et al., 2010) have been used to automatically assess L2 texts. Early work on topical relevance (Higgins et al., 2006) posed the problem as one of binary classification and aimed to identify whether a text was either on or off-topic. The main motivation of the research was to detect off-topic text, text submitted mistakenly (within an online assessment setting), or 96 text submitted in bad faith (i.e., possibly memorised on an unrelated topic). They adopted an unsupervised approach to the problem, where they matched each text to its corresponding prompt using"
W16-0510,Q15-1016,0,0.0362934,"a lemmatised version of Wikipedia from 2013. We removed from the corpus all words that appeared less than 200 times and used the 96,811 remaining words as both potential expansion words w and as contexts c. We used a 5 word context window (2 words either side of the target word) and reduced the size of the resultant vectors by only storing dimensions that had a PPMI greater than 2.0 (Turney et al., 2010). The resultant vectors are competitive with the best reported results for traditional word vectors on a word–word similarity task (Spearman-ρ = 0.732 on 3000 word-pairs from the MEN dataset) (Levy et al., 2015). We create a vector representation for the prompt p in Rn by summing the PPMI word-vectors of the words occurring in the prompt. Finally, the |e |closest words to the prompt vector p, as measured by cosine similarity, can then be selected as expansion terms. 3.3 Random Indexing Random Indexing (RI) (Kanerva et al., 2000) is an approach which incrementally builds word vectors in a dimensionally-reduced space. Words are initially assigned a unique random index vector in a space Zn , where n is user-defined. These nearorthogonal vectors are updated by iterating over a corpus of text. In particul"
W16-0510,W10-1013,0,0.0313589,"es employed in that work was to calculate the similarity of an essay to a number of unrelated prompts. If the essay was closer to an unrelated prompt than the relevant one, the essay was deemed to be off-topic. Briscoe et al. (2010) tackle the problem of offtopic detection using more complex distributional semantic models that tend to overcome the problem of vocabulary mismatch. However, they frame the task as binary classification and evaluate their approach by determining if it can associate a learner text with the correct prompt. The work which is closest in spirit to that of our own is by Louis and Higgins (2010), who expand prompts using morphological variations, synonyms, and words that are distributionally similar to those that appear in the prompt. Their work builds on the earlier work by Higgins et al. (2006), and again pose the problem as one of binary classification. The most recent work of Persing and Ng (2014) involves scoring L2 learner texts for relevance on a seven-point scale using a feature-rich linear regression approach. While they demonstrate that learning one linear regression model per prompt is a useful supervised approach, it means that substantial training data is needed for each"
W16-0510,D09-1045,0,0.0471189,"Missing"
W16-0510,P14-1144,0,0.593295,"er the learner has understood the prompt and attempted a response with appropriate vocabulary. Other reasons for measuring the topical relevance of a text include the detection of malicious submissions, that is, detecting submissions that have been rote-learned or memorised specifically for assessment situations (Higgins et al., 2006). In this paper, we employ techniques from the area of distributional semantics and information retrieval (IR) to develop unsupervised promptrelevance models, and demonstrate that they correlate well with human judgements. In particu2 We note that a recent paper (Persing and Ng, 2014) has referred to this task as prompt adherence, while we use the terms prompt-relevance and topical-relevance interchangeably throughout this paper. 95 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 95–104, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics lar, we study four different methods of expanding a prompt with with topically-related words and show that some are more beneficial than others at overcoming the ‘vocabulary mismatch’ problem which is typically present in free-text learner writing. T"
W16-0510,W09-3927,0,0.0198121,"r different methods of expanding a prompt with with topically-related words and show that some are more beneficial than others at overcoming the ‘vocabulary mismatch’ problem which is typically present in free-text learner writing. To the best of our knowledge, there have been no attempts at a comparative study investigating the effectiveness of such techniques on the automatic prediction of a topical-relevance score in the noisy domain of learner texts, where grammatical errors are common. In addition, we perform an external evaluation to measure the extent to which prompt-relevance informs (Rotaru and Litman, 2009) the holistic score. The remainder of the paper is outlined as follows: Section 2 discusses related work and outlines our contribution. Section 3 presents our framework and four unsupervised approaches to measuring semantic similarity. Section 4 presents both quantitative and qualitative evaluations for all of the methods employed in this paper. Section 5 performs an external evaluation by incorporating the best promptrelevance model as features into a supervised preference ranking approach. Finally, Section 6 concludes with a discussion and outline of future work. 2 Related Research There are"
W16-0510,W12-2004,1,0.904618,"y style) to those in the ICLE dataset. Candidates are assigned an overall score on a scale from 1 to 9. Prompt relevance is an aspect that is present in the marking criteria, and it is identified as a determinant of the overall score. We therefore hypothesise that adding prompt-relevance measures to the feature set of a prompt-independent essay scoring system (i.e. that is designed to assess linguistic competence only) would better reflect the evaluation performed by examiners and improve system performance. The baseline system is a linear preference ranking model (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012) and is trained to predict an overall essay score based on the following set of features: - word unigrams, bigrams, and trigrams POS (part-of-speech) counts grammatical relations essay length (# of unique words) 101 - counts of cohesive devices - max-word length and min-sentence length - number of errors based on a presence/absence trigram language model We divided the dataset into 5-folds in two separate ways. First, we created prompt-dependent folds, where essays associated with all 22 prompts appear in both the training and test data in the appropriate proportions. This scenario allows the"
W16-0510,W15-0625,1,0.851147,"ch on many of the prompts. However, to measure the topical quality of the expansion words selected by each approach in isolation, we removed the original prompt words from the expanded prompts and again calculated the performance of the different approaches. This more rigorous evaluation in Table 2 (Bottom) shows that the topical quality of the expansion words from the PRF approach tends to be better than the other approaches. We next look at the actual expansion words selected for two prompts. time, assessors graded within a point of each other. Furthermore, correlation is affected by scale (Yannakoudakis and Cummins, 2015). 9 The two remaining prompts have only three essays associated with them. Prompt # of essays length cos(p, s) dsp+e RIp+e cbow p+e skipp+e PRFp+e 1 237 -0.113 0.324 0.328 0.372 0.345 0.359 0.348 2 53 -0.026 0.120 0.141 0.098 0.125 0.160 0.188 3 64 -0.062 0.195 0.182 0.103 0.131 0.183 0.126 4 58 0.211 0.122 0.114 0.214 0.114 0.139 0.145 5 131 -0.023 0.205 0.208 0.192 0.209 0.245 0.260 6 43 -0.111 -0.019 -0.011 0.093 0.068 0.026 0.034 7 80 0.103 0.333 0.340 0.398 0.328 0.363 0.340 8 28 -0.115 0.511 0.519 0.720 0.581 0.571 0.598 9 49 -0.056 0.268 0.280 0.259 0.265 0.278 0.335 10 71 0.171 0.064 0"
W16-0510,P11-1019,1,0.899291,"similar in style (i.e. essay style) to those in the ICLE dataset. Candidates are assigned an overall score on a scale from 1 to 9. Prompt relevance is an aspect that is present in the marking criteria, and it is identified as a determinant of the overall score. We therefore hypothesise that adding prompt-relevance measures to the feature set of a prompt-independent essay scoring system (i.e. that is designed to assess linguistic competence only) would better reflect the evaluation performed by examiners and improve system performance. The baseline system is a linear preference ranking model (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012) and is trained to predict an overall essay score based on the following set of features: - word unigrams, bigrams, and trigrams POS (part-of-speech) counts grammatical relations essay length (# of unique words) 101 - counts of cohesive devices - max-word length and min-sentence length - number of errors based on a presence/absence trigram language model We divided the dataset into 5-folds in two separate ways. First, we created prompt-dependent folds, where essays associated with all 22 prompts appear in both the training and test data in the appropriate prop"
W16-0530,P06-4020,1,0.69678,"GEC (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014). To overcome this problem, JunczysDowmunt and Grundkiewicz (2014) introduced examples collected from the language exchange social 260 networking website Lang-8, and were able to improve system performance by 6 F-score points. As noticed by them, Lang-8 data may be too noisy and error-prone, so we decided to add examples from the fully annotated learner corpus CLC to our training set (approx. 1,965,727 pairs of parallel sentences and 29,219,128 tokens on the target side). Segmentation and tokenisation are performed using RASP (Briscoe et al., 2006), which is expected to perform better on learner data than a system developed exclusively from high quality copy-edited text such as the Wall Street Journal. 3.2 Evaluation System performance is evaluated using the Imeasure proposed by Felice and Briscoe (2015), which is designed to address problems with previous evaluation methods and reflect any improvement on the original sentence after applying a system’s corrections. An I score is computed by comparing system performance (WAccsys ) with that of a baseline that leaves the original text uncorrected (WAccbase ):  bWAccsys c"
W16-0530,P06-1032,0,0.4637,"e report results on two well-known publicly available test sets that can be used for cross-system comparisons. 2 Approach Our re-ranking approach is defined as follows: 1. an SMT system is first used to generate an nbest list of candidates for each input sentence; 257 2. features that are potentially useful to discriminate between good and bad corrections are extracted from the n-best list; 3. these features are then used to determine a new ranking for the n-best list; 4. the new highest-ranked candidate is finally output. 2.1 SMT for grammatical error correction Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013), Junczys-Dowmunt and Grundkiewicz (2014)), we approach GEC as a translation problem from incorrect into correct English. Our training data comprises parallel sentences extracted from the Cambridge Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using"
W16-0530,J93-2003,0,0.0759515,"ng for the n-best list; 4. the new highest-ranked candidate is finally output. 2.1 SMT for grammatical error correction Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013), Junczys-Dowmunt and Grundkiewicz (2014)), we approach GEC as a translation problem from incorrect into correct English. Our training data comprises parallel sentences extracted from the Cambridge Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA++, Pialign creates a phrase table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language models used during decoding are built from the corrected sentences in the learner corpus, to make sure that the final system outputs f"
W16-0530,P15-1068,0,0.139261,"Missing"
W16-0530,P02-1034,0,0.117466,"(2014) introduced Levenshtein distance and sparse features to their SMT systems, and reported better performance. In addition, Felice et al. (2014) used a LM to re-rank the 10-best candidates after they noticed that better corrections were in the n-best list. Similarly, for Chinese GEC, Zhao et al. (2015) confirmed that their system included correct predictions in its 10-best list not selected during decoding, so a reranking of the n-best list was clearly needed. Re-ranking has been widely used in many natural language processing tasks such as parsing, tagging and sentence boundary detection (Collins and Duffy, 2002; Collins and Koo, 2005; Roark et al., 2006; Huang et al., 2007). Various machine learning algorithms have been adapted to these re-ranking tasks, including boosting, perceptrons and SVMs. In machine translation, generative models have been widely used. Over the last decade, re-ranking techniques have shown significant improvement. Discriminative re-ranking (Shen et al., 2004), one of 264 the best-performing strategies, used two perceptronlike re-ranking algorithms that improved translation quality over a baseline system when evaluating with BLEU. Goh et al. (2010) employed an online training"
W16-0530,J05-1003,0,0.0686116,"tein distance and sparse features to their SMT systems, and reported better performance. In addition, Felice et al. (2014) used a LM to re-rank the 10-best candidates after they noticed that better corrections were in the n-best list. Similarly, for Chinese GEC, Zhao et al. (2015) confirmed that their system included correct predictions in its 10-best list not selected during decoding, so a reranking of the n-best list was clearly needed. Re-ranking has been widely used in many natural language processing tasks such as parsing, tagging and sentence boundary detection (Collins and Duffy, 2002; Collins and Koo, 2005; Roark et al., 2006; Huang et al., 2007). Various machine learning algorithms have been adapted to these re-ranking tasks, including boosting, perceptrons and SVMs. In machine translation, generative models have been widely used. Over the last decade, re-ranking techniques have shown significant improvement. Discriminative re-ranking (Shen et al., 2004), one of 264 the best-performing strategies, used two perceptronlike re-ranking algorithms that improved translation quality over a baseline system when evaluating with BLEU. Goh et al. (2010) employed an online training algorithm for SVM-based"
W16-0530,D12-1052,0,0.186623,"the NLTK toolkit, whereas the CLC was tokenised with RASP. System GLEU Baseline 64.19 CAMB + SVM re-ranker 65.68 Susanto et al. (2014) n/a Top 3 systems in CoNLL-2014 CAMB (Felice et al., 2014) 64.32 CUUI (Rozovskaya et al., 2014) 64.64 AMU (Junczys-Dowmunt and 64.56 Grundkiewicz, 2014) F0.5 0 38.08 39.39 I 0 -1.71 n/a 37.33 36.79 35.01 -5.58 -3.91 -3.31 Table 6: System performance on the CoNLL-2014 test set without alternative answers (in percentages). proposed the use of a noisy channel SMT model for correcting a set of 14 countable/uncountable nouns which are often confusing for learners. Dahlmeier and Ng (2012a) developed a beam-search decoder to iteratively generate candidates and score them using individual classifiers and a general LM. Their decoder focused on five types of errors: spelling, articles, prepositions, punctuation insertion, and noun number. Three classifiers were used to capture three of the common error types: article, preposition and noun number. Yuan and Felice (2013) trained phrase-based and POS-factored SMT systems to correct 5 error types using learner and artificial data. Later, researchers realised the need for new features in SMT for GEC. Felice et al. (2014) and Junczys-D"
W16-0530,N12-1067,0,0.153752,"the NLTK toolkit, whereas the CLC was tokenised with RASP. System GLEU Baseline 64.19 CAMB + SVM re-ranker 65.68 Susanto et al. (2014) n/a Top 3 systems in CoNLL-2014 CAMB (Felice et al., 2014) 64.32 CUUI (Rozovskaya et al., 2014) 64.64 AMU (Junczys-Dowmunt and 64.56 Grundkiewicz, 2014) F0.5 0 38.08 39.39 I 0 -1.71 n/a 37.33 36.79 35.01 -5.58 -3.91 -3.31 Table 6: System performance on the CoNLL-2014 test set without alternative answers (in percentages). proposed the use of a noisy channel SMT model for correcting a set of 14 countable/uncountable nouns which are often confusing for learners. Dahlmeier and Ng (2012a) developed a beam-search decoder to iteratively generate candidates and score them using individual classifiers and a general LM. Their decoder focused on five types of errors: spelling, articles, prepositions, punctuation insertion, and noun number. Three classifiers were used to capture three of the common error types: article, preposition and noun number. Yuan and Felice (2013) trained phrase-based and POS-factored SMT systems to correct 5 error types using learner and artificial data. Later, researchers realised the need for new features in SMT for GEC. Felice et al. (2014) and Junczys-D"
W16-0530,W13-1703,0,0.13017,"s Dataset We use the publicly available FCE dataset (Yannakoudakis et al., 2011), which is a part of the CLC. The FCE dataset is a set of 1,244 scripts written by learners of English taking the First Certificate in English (FCE) examination around the world between 2000 and 2001. The texts have been manually error-annotated with a taxonomy of approximately 80 error types (Nicholls, 2003). The FCE dataset covers a wide variety of L1s and was used in the HOO-2012 error correction shared task (Dale et al., 2012). Compared to the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) used in the CoNLL 2013 and 2014 shared tasks, which contains essays written by students at the National University of Singapore, the FCE dataset is a more representative test set of learner writing, which is why we use it for our experiments. The performance of our model on the CoNLL-2014 shared task test data is also presented in Section 3.7. Following Yannakoudakis et al. (2011), we split the publicly available FCE dataset into training and test sets: we use the 1,141 scripts from the year 2000 and the 6 validation scripts for training, and the 97 scripts from the year 2001 for testing. The"
W16-0530,W12-2006,0,0.0267201,"the n-best list for that source sentence and N (Hmin ) is the minimum candidate length. 3 3.1 Experiments Dataset We use the publicly available FCE dataset (Yannakoudakis et al., 2011), which is a part of the CLC. The FCE dataset is a set of 1,244 scripts written by learners of English taking the First Certificate in English (FCE) examination around the world between 2000 and 2001. The texts have been manually error-annotated with a taxonomy of approximately 80 error types (Nicholls, 2003). The FCE dataset covers a wide variety of L1s and was used in the HOO-2012 error correction shared task (Dale et al., 2012). Compared to the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) used in the CoNLL 2013 and 2014 shared tasks, which contains essays written by students at the National University of Singapore, the FCE dataset is a more representative test set of learner writing, which is why we use it for our experiments. The performance of our model on the CoNLL-2014 shared task test data is also presented in Section 3.7. Following Yannakoudakis et al. (2011), we split the publicly available FCE dataset into training and test sets: we use the 1,141 scripts from th"
W16-0530,N15-1060,1,0.883337,"system trained on the whole CLC (Nicholls, 2003). More details about the datasets and system are presented in Section 3. only needs to be performed once, which allows for fast experimentation. Most previous work on GEC has used evaluation methods based on precision (P), recall (R), and Fscore (e.g. the CoNLL 2013 and 2014 shared tasks). However, they do not provide an indicator of improvement on the original text so there is no way to compare GEC systems with a ‘do-nothing’ baseline. Since the aim of GEC is to improve text quality, we use the Improvement (I) score calculated by the Imeasure (Felice and Briscoe, 2015), which tells us whether a system improves the input. The main contributions of our work are as follows. First, to the best of our knowledge, we are the first to use a supervised discriminative re-ranking model in SMT for GEC, showing that n-best list re-ranking can be used to improve sentence quality. Second, we propose and investigate a range of easily computed features for GEC re-ranking. Finally, we report results on two well-known publicly available test sets that can be used for cross-system comparisons. 2 Approach Our re-ranking approach is defined as follows: 1. an SMT system is first"
W16-0530,W14-1702,1,0.937753,"Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA++, Pialign creates a phrase table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language models used during decoding are built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used to build ngram language models (up to 5-grams) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Previous work has shown that adding bigger language models based on larger corpora improves performance (Yuan and Felice, 2013; JunczysDowmunt and Grundkiewicz, 2014). The use of bigger language models will be inv"
W16-0530,D15-1052,0,0.0307897,"Missing"
W16-0530,W10-1744,0,0.0256816,"candidate combination. MBR was first proposed by Kumar and Byrne (2004) to minimise the expected loss of translation errors under loss functions that measure translation performance. Instead of using the model’s best output, the one that is most similar to the most likely translations is selected. We use the same n-best list as the candidate set and the likely translation set. MBR re-ranking can then be considered as selecting a consensus candidate: the least ‘risky’ candidate which is closest on average to all the likely candidates. The MEMT system combination technique was first proposed by Heafield and Lavie (2010) and was successfully applied to GEC by Susanto et al. (2014). A confusion network is created by aligning the candidates, on which a beam search is later performed to find the best candidate. The 10-best list from the best SMT system in Table 2 is used for re-ranking and results of using MBR re-ranking and MEMT candidate combination are System Source Reference SMT best SVM re-ranker Source Reference SMT best SVM re-ranker Source Reference SMT best SVM re-ranker Example sentences I meet a lot of people on internet and it really interest me. I meet a lot of people on the Internet and it really i"
W16-0530,2008.amta-srw.3,0,0.0202306,"he n-best list’s n-gram probabilities. N-gram counts are collected using the entries in the n-best list for each source sentence. N-grams repeated more often than others in the n-best list get higher scores, thus ameliorating incorrect lexical choices and word order. The n-gram probability for a target word ei given its history ei−1 i−n+1 is defined as: pn−best (ei |ei−1 i−n+1 ) = countn−best (ei , ei−1 i−n+1 ) countn−best (ei−1 i−n+1 ) (6) The sentence score for the sth candidate Hs is calculated as: score(Hs ) = log( Y pn−best (ei |ei−1 i−n+1 )) used, from 2 to 6. This feature is taken from Hildebrand and Vogel (2008). C) Statistical word lexicon feature set: We use the word lexicon learnt by the IBM Model 4, which contains translation probabilities for word-to-word mappings. The statistical word translation lexicon is used to calculate the translation probability Plex (e) for each word e in the target sentence. Plex (e) is the sum of all translation probabilities of e for each word fj in the source sentence f1J . Specifically, this can be defined as: Plex (e|f1J ) J 1 X = p(e|fj ) J +1 where f1J is the source sentence and J is the source sentence length. p(e|fj ) is the word-to-word translation probabilit"
W16-0530,D07-1117,0,0.0356116,"SMT systems, and reported better performance. In addition, Felice et al. (2014) used a LM to re-rank the 10-best candidates after they noticed that better corrections were in the n-best list. Similarly, for Chinese GEC, Zhao et al. (2015) confirmed that their system included correct predictions in its 10-best list not selected during decoding, so a reranking of the n-best list was clearly needed. Re-ranking has been widely used in many natural language processing tasks such as parsing, tagging and sentence boundary detection (Collins and Duffy, 2002; Collins and Koo, 2005; Roark et al., 2006; Huang et al., 2007). Various machine learning algorithms have been adapted to these re-ranking tasks, including boosting, perceptrons and SVMs. In machine translation, generative models have been widely used. Over the last decade, re-ranking techniques have shown significant improvement. Discriminative re-ranking (Shen et al., 2004), one of 264 the best-performing strategies, used two perceptronlike re-ranking algorithms that improved translation quality over a baseline system when evaluating with BLEU. Goh et al. (2010) employed an online training algorithm for SVM-based structured prediction. Various global fe"
W16-0530,W14-1703,0,0.351183,"ance, demonstrating that there is considerable room for improvement in the re-ranking component developed here, such as incorporating features able to capture long-distance dependencies. 1 Since SMT was not originally designed for GEC, many standard features do not perform well on this task. It is necessary to add new local and global features to help the decoder distinguish good from bad corrections. Felice et al. (2014) used Levenshtein distance to limit the changes made by their SMT system, given that most words translate into themselves and errors are often similar to their correct forms. Junczys-Dowmunt and Grundkiewicz (2014) also augmented their SMT system with Levenshtein distance and other sparse features that were extracted from edit operations. Introduction Grammatical error correction (GEC) has attracted considerable interest in recent years. Unlike classifiers built for specific error types (e.g. determiner or preposition errors), statistical machine translation (SMT) systems are trained to deal with all error types simultaneously. An SMT system thus learns to translate incorrect English into correct English using a parallel corpus of corrected sentences. The SMT framework has been successfully used for GEC"
W16-0530,P07-2045,0,0.00816889,"nment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA++, Pialign creates a phrase table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language models used during decoding are built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used to build ngram language models (up to 5-grams) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Previous work has shown that adding bigger language models based on larger corpora improves performance (Yuan and Felice, 2013; JunczysDowmunt and Grundkiewicz, 2014). The use of bigger language models will be investigated at the reranking stage, as it allows us to com"
W16-0530,N04-1022,0,0.0619988,"score is about 41 points higher than the standard SMT score in terms of I, and about 5 points higher in terms of WAcc, suggesting that there are alternative candidates in the 10-best list that are not chosen by the SMT model. Our re-ranker improves the I score from 2.87 to 9.78, and the WAcc score from 87.21 to 88.12, a significant improvement over the standard SMT model. However, there 262 Benchmark results We also compare our ranking model with two other methods: Minimum Bayes-Risk (MBR) re-ranking and Multi-Engine Machine Translation (MEMT) candidate combination. MBR was first proposed by Kumar and Byrne (2004) to minimise the expected loss of translation errors under loss functions that measure translation performance. Instead of using the model’s best output, the one that is most similar to the most likely translations is selected. We use the same n-best list as the candidate set and the likely translation set. MBR re-ranking can then be considered as selecting a consensus candidate: the least ‘risky’ candidate which is closest on average to all the likely candidates. The MEMT system combination technique was first proposed by Heafield and Lavie (2010) and was successfully applied to GEC by Susant"
W16-0530,P15-2097,0,0.139722,"Missing"
W16-0530,W14-1701,1,0.887037,"tures that were extracted from edit operations. Introduction Grammatical error correction (GEC) has attracted considerable interest in recent years. Unlike classifiers built for specific error types (e.g. determiner or preposition errors), statistical machine translation (SMT) systems are trained to deal with all error types simultaneously. An SMT system thus learns to translate incorrect English into correct English using a parallel corpus of corrected sentences. The SMT framework has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014). However, the integration of additional models/features into the decoding process may affect the dynamic programming algorithm used in SMT, because it does not support some complex features, such as those computed from an n-best list. An alternative to performing integrated decoding is to use additional information to re-rank an SMT decoder’s output. The aim of n-best list re-ranking is to re-rank the translation candidates produced by the SMT system using a rich set of features that are not used by the SMT decoder, so that better candidates can be selected as ‘optimal’ translations. This has"
W16-0530,J03-1002,0,0.0140785,"ections are extracted from the n-best list; 3. these features are then used to determine a new ranking for the n-best list; 4. the new highest-ranked candidate is finally output. 2.1 SMT for grammatical error correction Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013), Junczys-Dowmunt and Grundkiewicz (2014)), we approach GEC as a translation problem from incorrect into correct English. Our training data comprises parallel sentences extracted from the Cambridge Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA++, Pialign creates a phrase table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language models used during decoding are b"
W16-0530,P02-1040,0,0.103045,"em’s candidate, and a gold-standard reference:1 1 TP: true positives, TN: true negatives, FP: false positives, FN: false negatives, FPN: both a FP and a FN (see Felice and Briscoe (2015)) 3.4 w · TP + TN w · (TP + FP) + TN + FN − (w + 1) · FPN 2 (15) In Section 3.3 and 3.7, we also report results using another two evaluation metrics for comparison: F0.5 from M2 Scorer (Dahlmeier and Ng, 2012b) and GLEU (Napoles et al., 2015). The M2 Scorer was the official scorer in the CoNLL 2013 and 2014 shared tasks, with the latter using F0.5 as the system ranking metric. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL2014 shared task test set. WAcc = 3.3 SMT system We train several SMT systems and select the best one for our re-ranking experiments. These systems use different configurations, defined as follows: • GIZA++: uses GIZA++ for word alignment; • Pialign: uses Pialign to learn a phrase table; • FCE: uses the publicly available FCE as training data; • + LD: limits edit distance by adding the character-level Levenshtein distance as a new feature; • + CLC: incorporates additional training examples extracted from the CLC. Evaluation resu"
W16-0530,N04-1023,0,0.0908681,"41 points higher than the standard SMT score in terms of I, and about 5 points higher in terms of WAcc, suggesting that there are alternative candidates in the 10-best list that are not chosen by the SMT model. Our re-ranker improves the I score from 2.87 to 9.78, and the WAcc score from 87.21 to 88.12, a significant improvement over the standard SMT model. However, there 262 Benchmark results We also compare our ranking model with two other methods: Minimum Bayes-Risk (MBR) re-ranking and Multi-Engine Machine Translation (MEMT) candidate combination. MBR was first proposed by Kumar and Byrne (2004) to minimise the expected loss of translation errors under loss functions that measure translation performance. Instead of using the model’s best output, the one that is most similar to the most likely translations is selected. We use the same n-best list as the candidate set and the likely translation set. MBR re-ranking can then be considered as selecting a consensus candidate: the least ‘risky’ candidate which is closest on average to all the likely candidates. The MEMT system combination technique was first proposed by Heafield and Lavie (2010) and was successfully applied to GEC by Susant"
W16-0530,D14-1102,0,0.209227,"(2004) to minimise the expected loss of translation errors under loss functions that measure translation performance. Instead of using the model’s best output, the one that is most similar to the most likely translations is selected. We use the same n-best list as the candidate set and the likely translation set. MBR re-ranking can then be considered as selecting a consensus candidate: the least ‘risky’ candidate which is closest on average to all the likely candidates. The MEMT system combination technique was first proposed by Heafield and Lavie (2010) and was successfully applied to GEC by Susanto et al. (2014). A confusion network is created by aligning the candidates, on which a beam search is later performed to find the best candidate. The 10-best list from the best SMT system in Table 2 is used for re-ranking and results of using MBR re-ranking and MEMT candidate combination are System Source Reference SMT best SVM re-ranker Source Reference SMT best SVM re-ranker Source Reference SMT best SVM re-ranker Example sentences I meet a lot of people on internet and it really interest me. I meet a lot of people on the Internet and it really interests me. I meet a lot of people on the internet and it re"
W16-0530,J07-1003,0,0.0195669,"xicon learnt by the IBM Model 4, which contains translation probabilities for word-to-word mappings. The statistical word translation lexicon is used to calculate the translation probability Plex (e) for each word e in the target sentence. Plex (e) is the sum of all translation probabilities of e for each word fj in the source sentence f1J . Specifically, this can be defined as: Plex (e|f1J ) J 1 X = p(e|fj ) J +1 where f1J is the source sentence and J is the source sentence length. p(e|fj ) is the word-to-word translation probability of the target word e from one source word fj . As noted by Ueffing and Ney (2007), the sum in Equation (8) is dominated by the maximum lexicon probability, which we also use as an additional feature: Plex−max (e|f1J ) = max p(e|fj ) j=0,...,J 259 (9) For both lexicon scores, we sum over all words ei in the target sentence and normalise by sentence length to get sentence translation scores. Lexicon scores are calculated in both directions. This feature is also taken from Hildebrand and Vogel (2008). D) Length feature set: These features are used to make sure that the final system does not make unnecessary deletions or insertions. This set contains four length ratios: score("
W16-0530,C96-2141,0,0.608776,"finally output. 2.1 SMT for grammatical error correction Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013), Junczys-Dowmunt and Grundkiewicz (2014)), we approach GEC as a translation problem from incorrect into correct English. Our training data comprises parallel sentences extracted from the Cambridge Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA++, Pialign creates a phrase table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language models used during decoding are built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2"
W16-0530,P11-1019,1,0.948113,"re some information you asked me for. There were some information you have asked me about. There is some information you have asked me. Table 1: In this example, there are two errors in the sentence (marked in bold): an agreement error (are → is) and a mass noun error (informations → information). The best output is the one with highest probability, which only corrects the mass noun error, but misses the agreement error. However, the 2nd-ranked candidate corrects both errors and matches the reference (marked in italics). The source sentence and error annotation are taken from the FCE dataset (Yannakoudakis et al., 2011), and the 10-best list is from an SMT system trained on the whole CLC (Nicholls, 2003). More details about the datasets and system are presented in Section 3. only needs to be performed once, which allows for fast experimentation. Most previous work on GEC has used evaluation methods based on precision (P), recall (R), and Fscore (e.g. the CoNLL 2013 and 2014 shared tasks). However, they do not provide an indicator of improvement on the original text so there is no way to compare GEC systems with a ‘do-nothing’ baseline. Since the aim of GEC is to improve text quality, we use the Improvement ("
W16-0530,W13-3607,1,0.951048,"well-known publicly available test sets that can be used for cross-system comparisons. 2 Approach Our re-ranking approach is defined as follows: 1. an SMT system is first used to generate an nbest list of candidates for each input sentence; 257 2. features that are potentially useful to discriminate between good and bad corrections are extracted from the n-best list; 3. these features are then used to determine a new ranking for the n-best list; 4. the new highest-ranked candidate is finally output. 2.1 SMT for grammatical error correction Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013), Junczys-Dowmunt and Grundkiewicz (2014)), we approach GEC as a translation problem from incorrect into correct English. Our training data comprises parallel sentences extracted from the Cambridge Learner Corpus (CLC) (Nicholls, 2003). Two automatic alignment tools are used for word alignment: GIZA++ (Och and Ney, 2003) and Pialign (Neubig et al., 2011). GIZA++ is an implementation of IBM Models 15 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996). Word alignments learnt by GIZA++ are used to extract phrase-to-phrase translations using heuristics. Unlike GIZA"
W16-0530,W15-4417,0,0.0138405,"error types: article, preposition and noun number. Yuan and Felice (2013) trained phrase-based and POS-factored SMT systems to correct 5 error types using learner and artificial data. Later, researchers realised the need for new features in SMT for GEC. Felice et al. (2014) and Junczys-Dowmunt and Grundkiewicz (2014) introduced Levenshtein distance and sparse features to their SMT systems, and reported better performance. In addition, Felice et al. (2014) used a LM to re-rank the 10-best candidates after they noticed that better corrections were in the n-best list. Similarly, for Chinese GEC, Zhao et al. (2015) confirmed that their system included correct predictions in its 10-best list not selected during decoding, so a reranking of the n-best list was clearly needed. Re-ranking has been widely used in many natural language processing tasks such as parsing, tagging and sentence boundary detection (Collins and Duffy, 2002; Collins and Koo, 2005; Roark et al., 2006; Huang et al., 2007). Various machine learning algorithms have been adapted to these re-ranking tasks, including boosting, perceptrons and SVMs. In machine translation, generative models have been widely used. Over the last decade, re-rank"
W16-0530,P11-1064,0,\N,Missing
W17-5016,P16-1068,1,0.395662,"commercial use, including Project Essay Grade (PEG) (Page, 2003), e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writin"
W17-5016,D14-1162,0,0.0764291,"ce not only reduce grader workload, but also bypass grader inconsistencies as only one system would be responsible for the 1 https://www.kaggle.com/c/asap-aes/ 149 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–158 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics into neural networks to determine what minimum useful information they can utilize to enhance their predictive power. Initializing neural models with contextually rich word embeddings pre-trained on large corpora (Mikolov et al., 2013; Pennington et al., 2014; Turian et al., 2010) has been used to feed the networks with meaningful embeddings rather than random initialization. Those embeddings are generic and widely employed in Natural Language Processing (NLP) tasks, yet few attempts have been made to learn more task-specific embeddings. For instance, Alikaniotis et al. (2016) developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset. Their embeddings are constructed by ranking correct ngrams against their “noisy” counterparts, in addition to capturing words’ informativeness measured by their contribution to the ov"
W17-5016,W13-1704,0,0.0139616,"s. Subsequently, a second filter is applied over sentence representations followed by a pooling operation then a fully-connected layer to predict the final score. Their CNN was applied to the ASAP dataset and its efficacy in in-domain and domain-adaptation essay evaluation was demonstrated in comparison to traditional state-of-the-art baselines. Several AA approaches in the literature have exploited the “quality” or “correctness” of ngrams as a feature to discriminate between good and poor essays. Phandi et al. (2015) defined good essays Figure 1: Error-specific Word Embeddings (ESWE). (SAT) (Andersen et al., 2013) that applied a supervised ranking perceptron to rich linguistic features. Adding their correctness probability feature successfully enhanced the predictive power of the SAT. as the ones with grades above or equal to the average score and the rest as poor ones. They calculated the Fisher scores (Fisher, 1922) of ngrams and selected 201 with the highest scores as “useful ngrams”. Similarly, they generated correct POS ngrams from grammatically correct texts, classified the rest as “bad POS ngrams” and used them along with the useful ngrams and other shallow lexical features as bag-of-words featu"
W17-5016,D15-1049,0,0.249167,"onal systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a challenge to neural models and they still fail to beat state-of-the-art methods when evaluated on these sets. An example of such datasets is the First Certificate in English (FCE) set where applying a rank preference Support Vector Machine (SVM) trained on various lexical and grammatical features achieved the best results (Yannakoudakis et al., 2011). This motivates further investigation We p"
W17-5016,P16-1112,1,0.926663,"other initialization methods and augmenting the model with error corrections helps alleviate the effects of data sparsity. Finally, we further analyse the pre-trained representations and demonstrate that our embeddings are better at detecting errors which is inherent for AA. 2 number of erroneous script words script length This correlation could even be higher if error severity is accounted for as some errors could be more serious than others. Therefore, it seems plausible to exploit writing errors and integrate them into AA systems, as was successfully done by Yannakoudakis et al. (2011) and Rei and Yannakoudakis (2016), but not by capturing this information directly in word embeddings in a neural AA model. Our pre-training model learns to predict a score for each ngram based on the errors it contains and modifies the word vectors accordingly. The idea is to arrange the embedding space in a way that discriminates between “good” and “bad” ngrams based on their contribution to writing errors. Bootstrapping the assessment neural model with those learned embeddings could help detect wrong pat150 Related Work There have been various attempts to employ neural networks to assess the essays in the ASAP dataset. Tagh"
W17-5016,P06-4020,1,0.66906,"to examine the effects of training with extra data, we conduct experiments where we augment the public set with additional FCE scripts and refer to this extended version as FCEext , which contains 9, 822 scripts. We report the results of both datasets on the released test set. The public FCE dataset is divided into 1, 061 scripts for training and 80 for development while for FCEext , 8, 842 scripts are used for training and 980 are held out for development. The only data preprocessing employed is word tokenization which is achieved using the Robust Accurate Statistical Parsing (RASP) system (Briscoe et al., 2006). Evaluation. We replicate the SSWE model, implement our ESWE and ECSWE models, use Google and GloVe embeddings and conduct a comparison between the 5 initilization approaches by feeding their output embeddings to the AA system from Section 3.2. All the models are implemented using the open-source Python library Theano (Al-Rfou et al., 2016). For evaluation, we calculate Spearman’s rank correlation coefficient (ρ), Pearson’s product-moment correlation coefficient (r) and root mean square error (RM SE) between the final predicted script scores and the ground-truth values (Yannakoudakis and Cumm"
W17-5016,D16-1193,0,0.199001,"Project Essay Grade (PEG) (Page, 2003), e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recours"
W17-5016,D16-1115,0,0.294813,"tein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a ch"
W17-5016,P10-1040,0,0.0628994,"workload, but also bypass grader inconsistencies as only one system would be responsible for the 1 https://www.kaggle.com/c/asap-aes/ 149 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–158 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics into neural networks to determine what minimum useful information they can utilize to enhance their predictive power. Initializing neural models with contextually rich word embeddings pre-trained on large corpora (Mikolov et al., 2013; Pennington et al., 2014; Turian et al., 2010) has been used to feed the networks with meaningful embeddings rather than random initialization. Those embeddings are generic and widely employed in Natural Language Processing (NLP) tasks, yet few attempts have been made to learn more task-specific embeddings. For instance, Alikaniotis et al. (2016) developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset. Their embeddings are constructed by ranking correct ngrams against their “noisy” counterparts, in addition to capturing words’ informativeness measured by their contribution to the overall score of the ess"
W17-5016,P11-1019,1,0.811718,"ineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a challenge to neural models and they still fail to beat state-of-the-art methods when evaluated on these sets. An example of such datasets is the First Certificate in English (FCE) set where applying a rank preference Support Vector Machine (SVM) trained on various lexical and grammatical features achieved the best results (Yannakoudakis et al., 2011). This motivates furt"
W17-5016,W15-0625,0,0.0150669,"(Briscoe et al., 2006). Evaluation. We replicate the SSWE model, implement our ESWE and ECSWE models, use Google and GloVe embeddings and conduct a comparison between the 5 initilization approaches by feeding their output embeddings to the AA system from Section 3.2. All the models are implemented using the open-source Python library Theano (Al-Rfou et al., 2016). For evaluation, we calculate Spearman’s rank correlation coefficient (ρ), Pearson’s product-moment correlation coefficient (r) and root mean square error (RM SE) between the final predicted script scores and the ground-truth values (Yannakoudakis and Cummins, 2015). Dataset. For our experiments, we use the FCE dataset (Yannakoudakis et al., 2011) which consists of exam scripts written by English learners of upper-intermediate proficiency and graded with scores ranging from 1 to 40.8 Each script contains two answers corresponding to two different Training. Hyperparameter tuning is done for each model separately. The SSWE, ESWE and ECSWE models are initialized with GloVe (dwrd = 50) vectors, trained for 20 epochs and the learning rate is set to 0.01. For SSWE, α is set to 0.1, batch size to 128, the number of randomly gen6 https://code.google.com/archive/"
W17-5032,P06-4020,1,0.581269,"total of 35,625 patterns from our training data. For each input sentence, we first decide how many errors will be generated (using probabilities from the background corpus) and attempt to cre288 ate them by sampling from the collection of applicable patterns. This process is repeated until all the required errors have been generated or the sentence is exhausted. During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and correct sentences as in the background corpus (Felice, 2016). The required POS tags were generated with RASP (Briscoe et al., 2006), using the CLAWS2 tagset. 3 Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data. We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014). Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for"
W17-5032,P06-1032,0,0.779673,"orrected version ‘We went shopping on Saturday’ would produce the following pattern: Machine Translation We treat AEG as a translation task – given a correct sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levensh"
W17-5032,N13-1055,0,0.030868,"ore translations from the n-best list of the SMT system. These results reveal that allowing the model to see multiple alternative versions of the same file gives a distinct improvement – showing the model both correct and incorrect variations of the same sentences likely assists in learning a discriminative model. 5 F0.5 0.4 0.3 EVP FCE 0.2 0 2 4 6 8 Number of versions 10 12 Figure 1: F0.5 on FCE development set with increasing amounts of artificial data from SMT. an annotated corpus. However, their method uses a limited number of edit operations and is thus unable to generate complex errors. Cahill et al. (2013) compared different training methodologies and showed that artificial errors helped correct prepositions. Felice and Yuan (2014) learned error type distributions for generating five types of errors, and the system in Section 2.2 is an extension of this model. While previous work focused on generating a specific subset of error types, we explored two holistic approaches to AEG and showed that they are able to significantly improve error detection performance. 6 Related Work Conclusion This paper investigated two AEG methods, in order to create additional training data for error detection. First"
W17-5032,W12-2005,0,0.0604515,"k – given a correct sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inse"
W17-5032,P11-1064,0,0.016646,"ike errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency &gt;= 5, which yields a total of 35,625 patterns from our training data. For each input sentence, we first decide how many errors will be generated (using"
W17-5032,C16-1079,1,0.850945,"ey, 1995). We investigate two alternative methods for AEG. The models receive grammatically correct text as input and modify certain tokens to produce incorrect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify specific words that need to be marked as errors. While these alignments are not always perfect, we found them to be sufficient for practical purposes, since alternative alignments of similar sentences often result in the same binary labeling. Future work could explore more advanced alignment methods, such as proposed by Felice et al. (2016). In Section 4, this automatically labeled data is then used for training error detection models. 2.1 2.2 Pattern Extraction We also describe a method for AEG using patterns over words and part-of-speech (POS) tags, extracting known incorrect sequences from a corpus of annotated corrections. This approach is based on the best method identified by Felice and Yuan (2014), using error type distributions; while they covered only 5 error types, we relax this restriction and learn patterns for generating all types of errors. The original and corrected sentences in the corpus are aligned and used to"
W17-5032,P17-1194,1,0.820995,"s) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014). Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system. Table 1 contains example sentences from the error generation systems, highlighting each of the edits that are marked as errors. Error Detection Model We construct a neural sequence labeling model for error detection, following the previous work (Rei and Yannakoudakis, 2016; Rei, 2017). The model receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current context. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings. Next, the embeddings are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), in order to create context-dependent representations for every token. The hidden states from forward- and backward-LSTMs are concatenated for each word position, resulting in representations that are conditioned on the whole"
W17-5032,D10-1094,0,0.141496,"itional training data for error detection. First, we explored a method using textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences. The addition of artificial data to the training process was evaluated on three error detection annoOur work builds on prior research into AEG. Brockett et al. (2006) constructed regular expressions for transforming correct sentences to contain noun number errors. Rozovskaya and Roth (2010) learned confusion sets from an annotated corpus in order to generate preposition errors. Foster and Andersen (2009) devised a tool for generating errors for different types using patterns provided by the user or collected automatically from 290 tations, using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice and Yuan (2014). The combination of the pattern-based method with the machine t"
W17-5032,P11-1093,0,0.166943,"Missing"
W17-5032,P11-1019,1,0.842843,"see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the patternbased approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection. Evaluation We trained our error generation models on the public FCE training set (Yannakoudakis et al., 2011) and used them to generate additional artificial training data. Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).1 . While there are other text corpora that could be used (e.g., 1 We used the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) to calculate statistical significance and found that the improvement http://www.englis"
W17-5032,W13-3607,1,0.876962,"ate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, lookin"
W17-5032,W09-2112,0,\N,Missing
W17-5032,P07-2045,0,\N,Missing
W17-5032,P16-1112,1,\N,Missing
W18-0529,N12-1067,0,0.78198,"guage modelling in GEC is that low probability sequences are more likely to contain grammatical errors than high probability sequences. For example, *discuss about the problem is expected to be a low probability sequence because it contains an error while discuss the problem or talk about the problem are expected to be higher probability sequences because they do not contain errors. The goal of LM-based GEC is hence to determine how to transform the former into the latter based on LM probabilities.1 With this in mind, our approach is fundamentally a simplification of the algorithm proposed by Dahlmeier and Ng (2012a). It consists of 5 steps and is illustrated in Table 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining"
W18-0529,W14-1702,0,0.164157,"Missing"
W18-0529,P17-1074,1,0.703658,"ed GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official development set, we use the same split as Rei and Yannakoudakis (2016),6 which we tokenize with spaCy7 v1.9.0. We also reprocess all the datasets with the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) in an effort to standardise them. This standardisation is especially important for JFLEG which is not explicitly annotated and so otherwise cannot be evaluated in terms of F-score. Note that results on CoNLL2014 and JFLEG are typically higher than on other datasets because they contain more than one reference. See Table 2 for more information about each of the development and test sets. 4 JFLEG-dev 50 Table 2: Various stats about the learner corpora we use. 3 FCE-dev 60 ERRANT F0.5 Dataset CoNLL-2013 CoNLL-2014 FCE-dev FCE-test JFLEG-dev JFLEG-test 40 30 20 10 0 0 2 4 6 8 Improvement Threshol"
W18-0529,W11-2123,0,0.0907132,"ly. 2 Note that targeting other error types may be more appropriate in other languages; e.g. Mandarin Chinese contains very little morphology. 3 https://pypi.python.org/pypi/ CyHunspell 4 https://sourceforge.net/projects/ wordlist/files/speller/2017.08.24/ 5 248 http://wordlist.aspell.net/other/ Tokenizer NLTK NLTK spaCy spaCy NLTK NLTK Sents 1381 1312 2371 2805 754 747 Coders 1 2 1 1 4 4 Edits 3404 6104 4419 5556 10576 10082 CoNLL-2013 Data and Resources In all our experiments, we used a 5-gram language model trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011). While a neural model would likely result in better performance, efficient training on such a large amount of data is still an active area of research (Grave et al., 2017). Although LM-based GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official d"
W18-0529,D16-1195,0,0.0336906,"rrection (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable 1. Calculate the normalised log probability of an input sentence. 2. Build a confusion set, if any"
W18-0529,W17-5037,0,0.330247,"combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable 1. Calculate the normalised log probability of an input sentence. 2. Build a confusion set, if any, for each token in that sentence. 3. Re-score the sentence substitu"
W18-0529,W14-1704,0,0.0170869,"ey do not contain errors. The goal of LM-based GEC is hence to determine how to transform the former into the latter based on LM probabilities.1 With this in mind, our approach is fundamentally a simplification of the algorithm proposed by Dahlmeier and Ng (2012a). It consists of 5 steps and is illustrated in Table 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014),"
W18-0529,D16-1161,0,0.16036,"Missing"
W18-0529,P16-1208,0,0.0917822,"ble 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable 1. Calculate the normalised"
W18-0529,W14-1709,0,0.402445,"vskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable 1. Calculate the normalised log probability of an input sentence. 2. Build a confusion set, if any, for each token in that sentence. 3. Re-score the sentence substituting each candidate in each confusion set. 4. Apply the single best correction that increases the probability above a threshold. 5. Iterate step"
W18-0529,Q16-1013,0,0.0360964,"Missing"
W18-0529,P15-2097,0,0.153645,"Missing"
W18-0529,I17-2062,0,0.357055,"Missing"
W18-0529,E17-2037,0,0.559296,"ne Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011). While a neural model would likely result in better performance, efficient training on such a large amount of data is still an active area of research (Grave et al., 2017). Although LM-based GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official development set, we use the same split as Rei and Yannakoudakis (2016),6 which we tokenize with spaCy7 v1.9.0. We also reprocess all the datasets with the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) in an effort to standardise them. This standardisation is especially important for JFLEG which is not explicitly annotated and so otherwise cannot be evaluated in terms of F-score. Note that results on CoNLL2014 and JFLEG are typically higher than on other datasets because they contain more than one reference. See Table 2 for"
W18-0529,D14-1102,0,0.014719,"d is illustrated in Table 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable"
W18-0529,W14-1701,1,0.912116,"t the problem is expected to be a low probability sequence because it contains an error while discuss the problem or talk about the problem are expected to be higher probability sequences because they do not contain errors. The goal of LM-based GEC is hence to determine how to transform the former into the latter based on LM probabilities.1 With this in mind, our approach is fundamentally a simplification of the algorithm proposed by Dahlmeier and Ng (2012a). It consists of 5 steps and is illustrated in Table 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et"
W18-0529,P11-1019,1,0.822309,"a 5-gram language model trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011). While a neural model would likely result in better performance, efficient training on such a large amount of data is still an active area of research (Grave et al., 2017). Although LM-based GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official development set, we use the same split as Rei and Yannakoudakis (2016),6 which we tokenize with spaCy7 v1.9.0. We also reprocess all the datasets with the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) in an effort to standardise them. This standardisation is especially important for JFLEG which is not explicitly annotated and so otherwise cannot be evaluated in terms of F-score. Note that results on CoNLL2014 and JFLEG are typically higher than on other datasets because they contain more th"
W18-0529,W13-3601,0,0.137342,"76 10082 CoNLL-2013 Data and Resources In all our experiments, we used a 5-gram language model trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011). While a neural model would likely result in better performance, efficient training on such a large amount of data is still an active area of research (Grave et al., 2017). Although LM-based GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official development set, we use the same split as Rei and Yannakoudakis (2016),6 which we tokenize with spaCy7 v1.9.0. We also reprocess all the datasets with the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) in an effort to standardise them. This standardisation is especially important for JFLEG which is not explicitly annotated and so otherwise cannot be evaluated in terms of F-score. Note that results on CoNLL2014"
W18-0529,P16-1112,0,0.0508713,"t in better performance, efficient training on such a large amount of data is still an active area of research (Grave et al., 2017). Although LM-based GEC does not require annotated training data, a small amount of annotated data is still required for development and testing. We hence make use of several popular GEC corpora, including: CoNLL-2013 and CoNLL-2014 (Ng et al., 2013, 2014), the public First Certificate in English (FCE) (Yannakoudakis et al., 2011), and JFLEG (Napoles et al., 2017). Since the FCE was not originally released with an official development set, we use the same split as Rei and Yannakoudakis (2016),6 which we tokenize with spaCy7 v1.9.0. We also reprocess all the datasets with the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) in an effort to standardise them. This standardisation is especially important for JFLEG which is not explicitly annotated and so otherwise cannot be evaluated in terms of F-score. Note that results on CoNLL2014 and JFLEG are typically higher than on other datasets because they contain more than one reference. See Table 2 for more information about each of the development and test sets. 4 JFLEG-dev 50 Table 2: Various stats about the learner corpora we us"
W18-0529,N16-1042,1,0.887292,"ree teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competitive results even with very simple systems. In fact, a notable 1. Calculate the normalised log probability of an input sentence. 2. Build a confusion set, if any, for each token in that sentence. 3. Re-s"
W18-0529,W16-0530,1,0.853006,"sed by Dahlmeier and Ng (2012a). It consists of 5 steps and is illustrated in Table 1: Introduction In the CoNLL-2014 shared task on Grammatical Error Correction (GEC) (Ng et al., 2014), the top three teams all employed a combination of statistical machine translation (SMT) or classifierbased approaches (Junczys-Dowmunt and Grundkiewicz, 2014; Felice et al., 2014; Rozovskaya et al., 2014). These approaches have since come to dominate the field, and a lot of recent research has focused on fine-tuning SMT systems (JunczysDowmunt and Grundkiewicz, 2016), reranking SMT output (Hoang et al., 2016; Yuan et al., 2016), combining SMT and classifier systems (Susanto et al., 2014; Rozovskaya and Roth, 2016), and developing various neural architectures (Chollampatt et al., 2016; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt and Ng, 2017; Sakaguchi et al., 2017; Yannakoudakis et al., 2017). Despite coming a fairly competitive fourth in the shared task however (Lee and Lee, 2014), research into language model (LM) based approaches to GEC has largely stagnated. The main aim of this paper is hence to re-examine language modelling in the context of GEC and show that it is still possible to achieve competiti"
W18-0536,D13-1180,0,0.018539,"ect is scored as an integer in the range 0-5. We add the scores of these four aspects of a text together to obtain a total score in the range 0-20, and we use this total score as the score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-"
W18-0536,W16-0510,1,0.847487,"integer in the range 0-5. We add the scores of these four aspects of a text together to obtain a total score in the range 0-20, and we use this total score as the score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-20 MEAN 13.92 14.51 13"
W18-0536,P16-1075,1,0.811691,"integer in the range 0-5. We add the scores of these four aspects of a text together to obtain a total score in the range 0-20, and we use this total score as the score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-20 MEAN 13.92 14.51 13"
W18-0536,P16-1068,0,0.0242448,"odels have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-20 MEAN 13.92 14.51 13.20 5.78 13.72 12.77 STD 2.92 2.18 2.69 0.96 2.41 2.73 # prompts 31 37 50 58 67 35 # scripts 1212 2047 2088 1604 6584 1910 # train 822 1447 1488 1004 5984 1310 # dev 293 300 300 300 300 300 # test 97 300 300 300 300 300 Table 1: The details of the six datasets. FCE is the dataset released by Yannakoudakis et al.. For the other five datasets, the name of each dataset encodes its target CEFR level learners with"
W18-0536,D16-1115,0,0.0248985,"Missing"
W18-0536,D16-1193,0,0.0255476,"Missing"
W18-0536,W17-5016,1,0.813874,"RSE is the CLC version with extra discourse features. In the DISCOURSE version, Yannakoudakis and Briscoe (2012) investigated different features to measure the coherence of a text and how these features affect the overall score of the texts in the FCE dataset. They showed that the coherence feature based on incremental semantic analysis (Baroni et al., 2007) measuring average adjacent sentence similarity can help their ATS system improve in terms of the Pearson and Spearman correlations. Table 4 does not include any recent neural model on the FCE dataset, because the neural model developed by Farag et al. (2017) shows that there is still a performance gap between the neural model and the models built on hand-crafted features. Our models achieve relatively good performance, and we also found that by selecting appropriate features and hyper-parameters, the difference between using ranking and regression to train an ATS model is relatively small. This contrasts with Yannakoudakis et al. (2011)’s finding that ranking is much better than regression on this task. Therefore, we use SVR (BASE) in the following experiments. Benchmark Yannakoudakis et al. (2011) only built an overalllevel model and evaluated i"
W18-0536,W12-2004,1,0.86561,"score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-20 MEAN 13.92 14.51 13.20 5.78 13.72 12.77 STD 2.92 2.18 2.69 0.96 2.41 2.73 # prompts 31 37 50 58 67 35 # scripts 1212 2047 2088 1604 6584 1910 # train 822 1447 1488 1004 5984 1310 # dev 293 300 3"
W18-0536,N04-1024,0,0.0873287,"is total score as the score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-U C1-U AL-U B2-S C1-S CEFR B2 B2 C1 A1-C2 B2 C1 Score Range 0-20 0-20 0-20 0-9 0-20 0-20 MEAN 13.92 14.51 13.20 5.78 13.72 12.77 STD 2.92 2.18 2.69 0.96 2.41 2.73 # prompts 31 37 50 58 67 35 # scripts 1212 2047 2088 1604 6584 1910 # train 822 1447 14"
W18-0536,P11-1019,1,0.883477,"learner may share some commonalities, such as preferential word usages and common mistakes, and should also approximately 305 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 305–314 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics Zhang, 2016; Taghipour and Ng, 2016). However, no previous work has investigated the utility of authorship knowledge in ATS. One possible reason is that most datasets only have one text written by each learner. The First Certificate in English (FCE) dataset released by Yannakoudakis et al. (2011), on the other hand, contains two texts per learner. We primarily focus on the FCE dataset in this paper, but also utilise other datasets to corroborate our results. Yannakoudakis et al. defined all the texts written by a learner as a script. They extracted features from each text and then combined the features of the two texts within the same script together. A support vector machine (SVM) ranking model was trained to learn the relationship between features and overall scores. equally reflect their writing skills. We suggest that when an individual-level model predicts the score of a text wri"
W18-0536,C00-2137,0,0.244346,"0.341 0.343 0.378+ 0.389+ 0.381+ ρprs AL-U 0.684 0.704 0.689 0.698 0.720+ B2-S 0.476 0.501+ 0.510+ 0.541+ 0.506 C1-S 0.504 0.511 0.567+ 0.529 0.548+ ρspr α/β 0.659 0.687+ 0.667 0.680 0.710+ X 0.34 0.33 0.50 0.70 0.442 0.463 0.476+ 0.511+ 0.481 X 0.23 0.33 0.33 0.80 0.471 0.480 0.523+ 0.498 0.513+ X 0.02 0.78 0.40 0.67 The best setup per dataset is in bold. GREEN means improvement and RED means degradation over BASE. The optimal interpolation hyperparameters for each fusion approach are reported as α/β. + means significantly better (p < 0.05) than BASE using the permutation randomisation test (Yeh, 2000) with 2,000 samples. No metric is found significantly worse than BASE. Table 6: The results of different setups on the test sets. FCE B2-U C1-U 1.990 2.550 1.980 2.500 2.400 1.960 1.950 2.400 2.380 1.940 2.350 1.920 0.00 0.50 1.00 0.50 1.00 2.050 2.400 0.700 2.025 2.375 0.690 2.000 2.350 1.975 2.325 1.950 2.300 1.925 2.275 0.670 0.00 0.50 1.00 C1-S 2.425 2.075 2.340 0.00 B2-S 2.100 0.680 2.360 1.930 2.300 AL-U 0.710 2.420 1.970 2.450 0.720 2.440 0.00 dev test 0.50 1.00 0.00 0.50 1.00 0.00 0.50 1.00 Figure 1: How RMSE (y-axis) changes with β (x-axis) in FF-CT. The vertical RED and GREEN dashedd"
W18-0536,D15-1049,0,0.0156856,"aspects: content, communicative achievement, language quality and organisation. Each aspect is scored as an integer in the range 0-5. We add the scores of these four aspects of a text together to obtain a total score in the range 0-20, and we use this total score as the score for Related Work In most previous work, text features are defined manually and automatically extracted from each text. A machine learning model is then applied to learn the mapping from features to scores. Many different machine learning models have been used, including regression (Page, 2003; Attali and Burstein, 2006; Phandi et al., 2015), classification (Larkey, 1998; Rudner and Liang, 2002) and ranking (Chen and He, 2013; Cummins et al., 2016b). The features used in previous work range from shallow textual features to discourse structure and semantic coherence (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Somasundaran et al., 2014), and from prompt independent to dependent features (Cummins et al., 2016a). Some recent models have dispensed with feature engineering and utilised word embeddings and neural networks (Alikaniotis et al., 2016; Dong and 3 306 http://www.coe.int/t/dg4/linguistic/Cadre1 en.asp Exam FCE B2-"
W19-4406,D18-1399,0,0.0155086,"ack also dominated in the Low Resource Track. The UEDIN-MS system even outperformed 14 of the Restricted Track submissions despite the limited training data. This is most likely because UEDINMS and Kakao&Brain both made effective use of artificial data. The CAMB-CUED system also achieved a fairly competitive score despite not using any parallel training data. This contrasts with LAIX, who scored higher by 1 F0.5 using a complicated system of classifiers, CNNs and transformer NMT models. The TMU system is also notable for applying techniques from unsupervised SMT to GEC for the first time (cf. Artetxe et al., 2018). Although it performed poorly overall, it took several years to adapt supervised SMT to GEC (Junczys-Dowmunt and Grundkiewicz, 2016), so we hope researchers will continue to explore unsupervised SMT in future work. 8 • All systems still struggle most with content word errors. • Systems are significantly better at correcting multi token errors than they were 5 years ago. • The GLEU metric (Napoles et al., 2015) strongly correlates with recall and seems to be less discriminative than other metrics. We ultimately hope that the results and corpus statistics we report will serve as useful benchmar"
W19-4406,I17-2058,0,0.020022,"approximately two-thirds of all teams in the BEA2019 shared task5 used transformer-based neural machine translation (NMT) (Vaswani et al., 2017), while the remainder used convolutional neural networks (CNN), or both. Although they were most likely inspired by Junczys-Dowmunt et al. (2018) and Chollampatt and Ng (2018a), who previously reported state-of-the-art results on the CoNLL2014 test set, the main consequence of this is that systems could only be differentiated based on lower-level system properties, such as: Metric Justification Since robust evaluation is still a hot topic in GEC (cf. Asano et al., 2017; Choshen and Abend, 2018), we also wanted to provide some additional evidence that ERRANT F0.5 is as reliable as MaxMatch F0.5 and other popular metrics (Felice and Briscoe, 2015; Napoles et al., 2015). We evaluated ERRANT in relation to human judgements on the CoNLL-2014 test set using the same setup as Chollampatt and Ng (2018b), and found similar correlation coefficients (Table 6). Although this table shows that no metric is superior in all settings, the main advantage of ERRANT is that it can also provide much more detailed feedback than the alternatives; e.g. in terms of error types. We"
W19-4406,W19-4418,0,0.0968329,"Missing"
W19-4406,P17-1074,1,0.666119,"ide a platform where systems can be reevaluated under more controlled conditions. 52 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Input Output Travel by bus is exspensive , bored and annoying . Travelling by bus is expensive , boring and annoying . Train Texts Sentences Tokens Dev Texts Sentences Tokens Test Texts Sentences Tokens Total Texts Sentences Tokens Table 1: An example input and output sentence. is calculated using the ERRANT scorer (Bryant et al., 2017), rather than the M2 scorer (Dahlmeier and Ng, 2012), because the ERRANT scorer can provide much more detailed feedback, e.g. in terms of performance on specific error types. Official evaluation is carried out on the Codalab competition platform, where a separate competition is created for each track. More details and links can be found on the official shared task website.1 The remainder of this report is structured as followed. Section 2 first summarises the task instructions and lists exactly what participants are asked to do. Section 3 next introduces the new W&I+LOCNESS corpus and describe"
W19-4406,N15-1060,1,0.877066,"onvolutional neural networks (CNN), or both. Although they were most likely inspired by Junczys-Dowmunt et al. (2018) and Chollampatt and Ng (2018a), who previously reported state-of-the-art results on the CoNLL2014 test set, the main consequence of this is that systems could only be differentiated based on lower-level system properties, such as: Metric Justification Since robust evaluation is still a hot topic in GEC (cf. Asano et al., 2017; Choshen and Abend, 2018), we also wanted to provide some additional evidence that ERRANT F0.5 is as reliable as MaxMatch F0.5 and other popular metrics (Felice and Briscoe, 2015; Napoles et al., 2015). We evaluated ERRANT in relation to human judgements on the CoNLL-2014 test set using the same setup as Chollampatt and Ng (2018b), and found similar correlation coefficients (Table 6). Although this table shows that no metric is superior in all settings, the main advantage of ERRANT is that it can also provide much more detailed feedback than the alternatives; e.g. in terms of error types. We hope that researchers can make use of this information to build better systems. • How much artificial data was used, if any, and how it was generated. • How much over-sampled data"
W19-4406,C16-1079,1,0.930634,"Missing"
W19-4406,C18-1231,0,0.291053,"Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC) continues the tradition of the previous Helping Our Own (HOO) and Conference on Natural Language Learning (CoNLL) shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014) and was motivated by the need to re-evaluate the field after a five year hiatus. Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora (Sakaguchi et al., 2017; Yannakoudakis et al., 2017; Chollampatt and Ng, 2018a; Ge et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018; Zhao et al., 2019). Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems can be reevaluated under more controlled conditions. 52 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Input Output Travel by bus is exspensive , bored and annoying . Travelling by bus is expensive , boring and a"
W19-4406,P18-1097,0,0.322275,"ared Task on Grammatical Error Correction (GEC) continues the tradition of the previous Helping Our Own (HOO) and Conference on Natural Language Learning (CoNLL) shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014) and was motivated by the need to re-evaluate the field after a five year hiatus. Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora (Sakaguchi et al., 2017; Yannakoudakis et al., 2017; Chollampatt and Ng, 2018a; Ge et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018; Zhao et al., 2019). Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems can be reevaluated under more controlled conditions. 52 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Input Output Travel by bus is exspensive , bored and annoying . Travelling by bus is expensive , boring and annoying . Train Te"
W19-4406,P18-1059,0,0.0740211,"hirds of all teams in the BEA2019 shared task5 used transformer-based neural machine translation (NMT) (Vaswani et al., 2017), while the remainder used convolutional neural networks (CNN), or both. Although they were most likely inspired by Junczys-Dowmunt et al. (2018) and Chollampatt and Ng (2018a), who previously reported state-of-the-art results on the CoNLL2014 test set, the main consequence of this is that systems could only be differentiated based on lower-level system properties, such as: Metric Justification Since robust evaluation is still a hot topic in GEC (cf. Asano et al., 2017; Choshen and Abend, 2018), we also wanted to provide some additional evidence that ERRANT F0.5 is as reliable as MaxMatch F0.5 and other popular metrics (Felice and Briscoe, 2015; Napoles et al., 2015). We evaluated ERRANT in relation to human judgements on the CoNLL-2014 test set using the same setup as Chollampatt and Ng (2018b), and found similar correlation coefficients (Table 6). Although this table shows that no metric is superior in all settings, the main advantage of ERRANT is that it can also provide much more detailed feedback than the alternatives; e.g. in terms of error types. We hope that researchers can"
W19-4406,W19-4427,0,0.304223,"Missing"
W19-4406,D16-1161,0,0.204468,"issions despite the limited training data. This is most likely because UEDINMS and Kakao&Brain both made effective use of artificial data. The CAMB-CUED system also achieved a fairly competitive score despite not using any parallel training data. This contrasts with LAIX, who scored higher by 1 F0.5 using a complicated system of classifiers, CNNs and transformer NMT models. The TMU system is also notable for applying techniques from unsupervised SMT to GEC for the first time (cf. Artetxe et al., 2018). Although it performed poorly overall, it took several years to adapt supervised SMT to GEC (Junczys-Dowmunt and Grundkiewicz, 2016), so we hope researchers will continue to explore unsupervised SMT in future work. 8 • All systems still struggle most with content word errors. • Systems are significantly better at correcting multi token errors than they were 5 years ago. • The GLEU metric (Napoles et al., 2015) strongly correlates with recall and seems to be less discriminative than other metrics. We ultimately hope that the results and corpus statistics we report will serve as useful benchmarks and guidance for future work. Acknowledgements We are extremely grateful to Diane Nicholls and her team of annotators for annotati"
W19-4406,C12-2084,0,0.307729,"Missing"
W19-4406,N18-1055,0,0.503201,"the tradition of the previous Helping Our Own (HOO) and Conference on Natural Language Learning (CoNLL) shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014) and was motivated by the need to re-evaluate the field after a five year hiatus. Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora (Sakaguchi et al., 2017; Yannakoudakis et al., 2017; Chollampatt and Ng, 2018a; Ge et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018; Zhao et al., 2019). Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems can be reevaluated under more controlled conditions. 52 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Input Output Travel by bus is exspensive , bored and annoying . Travelling by bus is expensive , boring and annoying . Train Texts Sentences Tokens Dev Texts Sentences Tokens Test Texts Sentences T"
W19-4406,P15-2097,0,0.660785,"ks (CNN), or both. Although they were most likely inspired by Junczys-Dowmunt et al. (2018) and Chollampatt and Ng (2018a), who previously reported state-of-the-art results on the CoNLL2014 test set, the main consequence of this is that systems could only be differentiated based on lower-level system properties, such as: Metric Justification Since robust evaluation is still a hot topic in GEC (cf. Asano et al., 2017; Choshen and Abend, 2018), we also wanted to provide some additional evidence that ERRANT F0.5 is as reliable as MaxMatch F0.5 and other popular metrics (Felice and Briscoe, 2015; Napoles et al., 2015). We evaluated ERRANT in relation to human judgements on the CoNLL-2014 test set using the same setup as Chollampatt and Ng (2018b), and found similar correlation coefficients (Table 6). Although this table shows that no metric is superior in all settings, the main advantage of ERRANT is that it can also provide much more detailed feedback than the alternatives; e.g. in terms of error types. We hope that researchers can make use of this information to build better systems. • How much artificial data was used, if any, and how it was generated. • How much over-sampled data was used, if any, and"
W19-4406,W19-4422,0,0.0584278,"Missing"
W19-4406,W14-1701,1,0.837726,"ory University of Cambridge Cambridge, UK {cjb255,mf501,oa223,ejb}@cam.ac.uk Abstract With this in mind, another significant contribution of the BEA-2019 shared task is the introduction of a new annotated dataset, the Cambridge English Write & Improve (W&I) and LOCNESS corpus, which is designed to represent a much wider range of English levels and abilities than previous corpora. This is significant because systems have traditionally only been tested on the CoNLL-2014 test set, which only contains 50 essays (1,312 sentences) on 2 different topics written by 25 South-East Asian undergraduates (Ng et al., 2014). In contrast, the W&I+LOCNESS test set contains 350 essays (4,477 sentences) on approximately 50 topics written by 334 authors from around the world (including native English speakers). We hope that this diversity will encourage the development of systems that can generalise better to unseen data. Another difference to the previous shared tasks is the introduction of tracks; namely the Restricted, Unrestricted and Low Resource track. While annotated data was comparatively scarce five years ago, it has since become more available, so we can now control what resources participants have access t"
W19-4406,W19-4414,0,0.155019,"Missing"
W19-4406,W13-3601,0,0.223622,"tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F0.5 , which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set. 1 Ted Briscoe Introduction The Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC) continues the tradition of the previous Helping Our Own (HOO) and Conference on Natural Language Learning (CoNLL) shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014) and was motivated by the need to re-evaluate the field after a five year hiatus. Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora (Sakaguchi et al., 2017; Yannakoudakis et al., 2017; Chollampatt and Ng, 2018a; Ge et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018; Zhao et al., 2019). Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems ca"
W19-4406,W19-4413,0,0.0550243,"Missing"
W19-4406,W19-4425,0,0.0296414,"Missing"
W19-4406,W19-4420,0,0.0839174,"Missing"
W19-4406,Q16-1013,0,0.0595332,"Missing"
W19-4406,I17-2062,0,0.13295,"Missing"
W19-4406,W19-4424,0,0.0509291,"Missing"
W19-4406,W19-4417,0,0.204379,"Missing"
W19-4406,N19-1014,0,0.283197,"nd Conference on Natural Language Learning (CoNLL) shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014) and was motivated by the need to re-evaluate the field after a five year hiatus. Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora (Sakaguchi et al., 2017; Yannakoudakis et al., 2017; Chollampatt and Ng, 2018a; Ge et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018; Zhao et al., 2019). Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems can be reevaluated under more controlled conditions. 52 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Input Output Travel by bus is exspensive , bored and annoying . Travelling by bus is expensive , boring and annoying . Train Texts Sentences Tokens Dev Texts Sentences Tokens Test Texts Sentences Tokens Total Texts Sentences Tokens Table 1:"
W19-4406,P12-2039,0,0.465292,"e shared task. Since these corpora were previously only available in different formats, we make new standardised versions available with the shared task (Table 3). FCE The First Certificate in English (FCE) corpus is a subset of the Cambridge Learner Corpus (CLC) that contains 1,244 written answers to FCE exam questions (Yannakoudakis et al., 2011). Lang-8 Corpus of Learner English Lang-8 is an online language learning website which encourages users to correct each other’s grammar. The Lang-8 Corpus of Learner English is a somewhat-clean, English subset of this website (Mizumoto et al., 2012; Tajiri et al., 2012). It is distinct from the raw, multilingual Lang-8 Learner Corpus. LOCNESS Since most GEC research has traditionally focused on non-native errors, we also wanted to incorporate some native errors into the shared task. To do this, we used the LOCNESS corpus, a collection of approximately 400 essays written by native British and American undergraduates on various topics (Granger, 1998).4 Since these essays were typically much longer than the texts submitted to Write & Improve, we first filtered them to remove essays longer than 550 words. We also removed essays that contained transcription issue"
W96-0209,1995.iwpt-1.8,1,0.391588,"Missing"
W96-0209,P94-1040,1,0.864533,"which a syntactic analysis should, in 2Briscoe & Carroll (1995) note that &quot;coverage&quot; is a weak measure since discovery of one or more global analyses does not entail that the correct analysis is recovered. 93 principle, be found; for example, in (3), the absence of dashes would mislead a parser into seeking a syntactic relationship between three and the following names, whilst in fact there is only a discourse relation of elaboration between this text adjunct and pronominal three. or build a modular semantics. Our less-tightly integrated g r a m m a r is described in more detail in Briscoe & Carroll (1994). 5. P A R S I N G T H E S U S A N N E SEC CORPORA (3) The three - Miles J. Cooperman, Sheldon Teller, and Richard Austin - and eight other defendants were charged in six indictments with conspiracy to violate federal narcotic law. AND We have used the integrated g r a m m a r to parse the Susanne corpus and the quite distinct Spoken English Corpus (SEC; Taylor ~ Knowles, 1988), a 50K word treebanked corpus of transcribed British radio programmes punctuated by the corpus compilers. Both corpora were retagged using the Acquilex HMM tagger (Elworthy, 1993, 1994) trained on text tagged with a sli"
W96-0209,A88-1019,0,0.0763159,"nced subset of the Brown corpus. Many of the 'failures' are due to the root S(entence) requirement enforced by the parser when dealing with fragments from dialogue and so forth. We have not relaxed this requirement since it increases ambiguity, our primary interest at this point being the extraction of subcategorisation information from full clauses in corpus data. 2. P A R T : O F - S P E E C H TAG SEQUENCE GRAMMAR 3. T E X T GRAMMAR PUNCTUATION We utilised the ANLT metagrammatical formalism to develop a feature-based, declarative description of part-of-speech (PoS) label sequences (see e.g. Church, 1988) for English. This grammar compiles into a DCG-like grammar of approximately 400 rules. It has been designed to enumerate possible valencies for predicates (verbs, adjectives and nouns) by including separate rules for each pattern of possible complementation in English. The distinction between arguments and adjuncts is expressed, following Xbar theory (e.g. Jackendoff, 1977), by Chomskyadjunction of adjuncts to maximal projections (XP ~ XP Adjunct) as opposed to government of arguments (i.e. arguments are sisters within X1 projections; X1 --~ X0 A r g l . . . ArgN). Although the grammar enumer"
W96-0209,P89-1018,0,0.0486341,"Missing"
W96-0209,A94-1009,0,0.268552,"Missing"
W96-0209,C94-1042,0,0.124071,"Missing"
W96-0209,P95-1037,0,0.132203,"Missing"
W96-0209,P90-1031,0,0.0796417,"Missing"
W96-0209,E93-1040,0,0.0788724,"Missing"
W96-0209,E89-1035,1,0.87064,"Missing"
W96-0209,J93-2006,0,0.0457786,"Missing"
W96-0209,A97-1052,1,\N,Missing
W96-0209,P89-1015,0,\N,Missing
W96-0303,P93-1005,0,0.0175841,"which was derived with much greater amounts of human effort, has a slightly better performance, the difference is not great. Automatic acquisition of information from corpora is a partial answer to this problem, and one which is in many respects complementary to the approach assumed here, but successful acquisition of a broad-coverage lexicon from a really large corpus would lead to a similar problem of massive ambiguity as we see in the case of productive lexical rules. Control of syntactic ambiguity by the use of lexical and other probabilities has been demonstrated by several authors (e.g. Black et al., 1993; Schabes, 1992; Resnik, 1992), but the difficulty of acquisition means that the validity of utilizing lexical probabilities of the type assumed here has not yet been demonstrated on a large scale. This approach fits in most naturally with systems where probabilistic information is incorporated systematically. However it could be useful with more traditional systems. Different applications could utilize probabilistic information in different ways. For word choice in generation, it would be appropriate to take the highest-probability suitable entry, and, if none are attested, to construct a phr"
W96-0303,P94-1021,0,0.077893,"Missing"
W96-0303,A88-1019,0,0.0763999,"Missing"
W96-0303,C94-1042,0,0.0417729,"ncy statements. In the introductory section, we argued that this approach cannot be correct in principle, because of the problem of nonce senses. But it is also demonstrably inadequate, at least for systems which are not limited to a narrow domain. In an experiment with a wide-coverage parsing system (Alvey NL Tools, ANLT) Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate subcategorization information in the lexicon. The ANLT lexicon was derived semi-automatically from a machine readable dictionary (LDOCE), and although the COMLEX syntax dictionary (Grishman et al., 1994), which was derived with much greater amounts of human effort, has a slightly better performance, the difference is not great. Automatic acquisition of information from corpora is a partial answer to this problem, and one which is in many respects complementary to the approach assumed here, but successful acquisition of a broad-coverage lexicon from a really large corpus would lead to a similar problem of massive ambiguity as we see in the case of productive lexical rules. Control of syntactic ambiguity by the use of lexical and other probabilities has been demonstrated by several authors (e.g"
W96-0303,P95-1014,0,0.138039,"al rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora. 1 Introduction Lexicalist linguistic theories, such as HPSG, LFG and categorial grammar, rely heavily on lexical rules. Recently, techniques have been described which address the efficiency issues that this raises for fully productive rules, such as inflectional rules and 'syntactic rules' (such as the HPSG complement extraction lexical rule). For example, Bouma & van Noord (1994) and Johnson & Dorre (1995) propose techniques for delayed evaluation of lexical rules so that they apply 'on demand' at parse time. Meurers ~ Minnen (1995) present a covariation approach, in which a finite-state machine for the application of lexical rules is derived by computing possible follow relations between the set of rules and then pruned FSMs are associated with classes of actual lexical entries representing the restricted set of rules which can apply to those entries. Finally, entries themselves are extended with information common to all their derived variants. These techniques achieve most of the advantages"
W96-0303,P93-1032,0,0.0147151,"or limited domains this may well be the best approach. We are more interested in incorporating probabilities in a large, reusable, lexical knowledge base. Recent developments in corpus processing techniques have made this more feasible. For instance, work on word sense disambiguation in corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. Many lexical rules involve changes in subcategorization, and automatic techniques for extracting subcategorization from corpora (e.g. Briscoe and Carroll, 1995; Manning, 1993) could eventually be exploited to give frequency information. In some cases, a combination of large corpora and sense taxonomies can be used to provide a rough estimate of lexical rule productivity suitable for instantiating the formulae given in the 12 previous section. For example, we examined verbs derived from several classes of noun from the 90 million word written portion of the British National Corpus, using the wordlists compiled by A d a m Kilgarriff. We looked at four classes of nouns: vehicles, dances, hitting weapons (e.g. club, whip) and decOrative coatings (e.g. lacquer, varnish)"
W96-0303,W95-0105,0,0.0280426,"to acquire probabilities for attested senses, and to derive appropriate estimates of lexical rule productivity. Probabilities of different word senses can be learned by a running analyzer, to the extent that lexical ambiguities are resolved either during processing or by an external oracle, and for limited domains this may well be the best approach. We are more interested in incorporating probabilities in a large, reusable, lexical knowledge base. Recent developments in corpus processing techniques have made this more feasible. For instance, work on word sense disambiguation in corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. Many lexical rules involve changes in subcategorization, and automatic techniques for extracting subcategorization from corpora (e.g. Briscoe and Carroll, 1995; Manning, 1993) could eventually be exploited to give frequency information. In some cases, a combination of large corpora and sense taxonomies can be used to provide a rough estimate of lexical rule productivity suitable for instantiating the formulae given in the 12 previous section. For example, we examined ver"
W96-0303,C92-2066,0,\N,Missing
W96-0303,J93-1002,1,\N,Missing
W97-1010,C92-2066,0,0.0517135,"he basis of many popular language learning systems, examples of which include the Baum-Welch algorithm for estimating hidden Markov models (Baum, 1972) and the InsideOutside algorithm for estimating CFGs (Baker, 1990). As is well known, Bayes&apos; theorem takes the following form: P(H I D) = P(H)P(D I H) P(D) Smooth the resulting parameters in the hope that they back-off from the training data and apportion more of the probability mass to account for unseen material. Examples of the first approach can be seen most clearly with the usage of CNF grammars by the Inside-Outside algorithm (Pereira and Schabes, 1992, Lari and Young, 1990). A grammar in CNF does not contain rules of an arbitrary arity, and so when learning CNF grammars, the Inside-Outside algorithm cannot find the maximal likelihood estimation of some training set. The problem with this language restriction is that there is no a priori reason why one should settle with any particular limit on rule arity; some grammars mainly contain binary rules, but others (for example those implicitly within tree-banks) sometimes contain rules with many right-hand side categories. Any language restriction, in lieu of some theory of rule arity, must rema"
W97-1010,P92-1017,0,\N,Missing
W97-1010,H92-1024,0,\N,Missing
W97-1010,P96-1025,0,\N,Missing
W98-1114,P87-1027,1,0.265911,"Missing"
W98-1114,P91-1027,0,0.618493,"Missing"
W98-1114,J93-2002,0,0.465894,"Missing"
W98-1114,A97-1052,1,0.929049,"ernatives for individual verbal predicates. However, the empirical question of whether this type of frequency information can in practice improve the accuracy of a statistical parser has not yet been answered. In this paper we describe an experiment with a widecoverage statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge"
W98-1114,1997.iwpt-1.18,0,0.0195461,"divide into clausal, and into non-clausal direct object (dobj), second (non-clausal) complement in ditransitive constructions (obj2), and indirect object complement introduced by a preposition (iobj). In general the parser returns the most specific (leaf) relations in the G R hierarchy, except when it is unable to determine whether clausal s u b j e c t s / o b j e c t s are controlled from within or without (i.e. csubj vs. zsubj, and ccomp vs. zcomp respectively), in which case it 4Shortcomings of this combination of annotation and evaluation scheme have been noted previously by Lin (1996), Carpenter & Manning (1997) and others. Car(3) a ... (VP will hear (NP a greeting) (PP from (NP Gov. Mark garfield))) ... b ... (VP will hear (NP a greeting (PP from ( Y P Gov. Mark Hatfield)))) ... roll, Briscoe & Sanfilippo (1998) summarise the various criticisms that have been made. 122 dependent rood ncmod xmod c arg..mod m o a~ ~ ncsu3Zs~ubbjjcsubj~ a u s a l dobj obj2 iobj xcompccomp Figure 1: Portions of GR hierarchy used. (Relations in italics are not returned by the parser). returns subj or clausal as appropriate. Each relation is parameterised with a head (lemma) and a dependent (lemma)--also optionally a type"
W98-1114,W96-0209,1,0.837679,"ter Science, Tokyo Institute of Technology, and at CSLI, Stanford University; the author wishes to thank researchers at these institutionsfor many stimulating conversations. 118 ing, no wide-coverage parser has yet been constructed which explicitly incorporates probabilities of different subcategorisation alternatives for individual predicates. It is therefore an open question whether this type of information can actually improve parser accuracy in practice. In this paper we address this issue, describing an experiment with an existing wide-coverage statistical grammar and parser for English (Carroll & Briscoe, 1996) in conjunction with subcategorisation frequencies acquired from 10 million words of text from the British National Corpus (BNC; Leech, 1992). Our results show conclusively that this information can improve parse accuracy. 2 Background 2.1 S u b c a t e g o r i s a t i o n A c q u i s i t i o n Several substantial machine-readable subcategorisation dictionaries exist for English, either built semi-automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the C"
W98-1114,1997.iwpt-1.6,1,0.757881,"er for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering', and by an E P S R C Advanced Fellowship to the first author. Some of the work was carried out while the firstauthor was a visitor at the Tanaka Laboratory, Department of Computer Science, Tokyo Institute of Technology, and at CSLI, Stanford University; the a"
W98-1114,W98-1505,0,0.055878,"cates. However, the empirical question of whether this type of frequency information can in practice improve the accuracy of a statistical parser has not yet been answered. In this paper we describe an experiment with a widecoverage statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering'"
W98-1114,P96-1025,0,0.152394,"Missing"
W98-1114,A94-1009,0,0.0356313,"Missing"
W98-1114,P98-1071,0,0.0280851,"verbal predicates. However, the empirical question of whether this type of frequency information can in practice improve the accuracy of a statistical parser has not yet been answered. In this paper we describe an experiment with a widecoverage statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction f"
W98-1114,C94-1042,0,0.0950826,"Missing"
W98-1114,A92-1022,0,0.0705477,"Missing"
W98-1114,C92-2099,0,0.0514336,"Missing"
W98-1114,1997.iwpt-1.16,0,0.0704025,"Missing"
W98-1114,P95-1037,0,0.0539086,"Missing"
W98-1114,P93-1032,0,0.556183,"Missing"
W98-1114,J93-2004,0,0.0462928,"Missing"
W98-1114,W97-0808,0,0.0568636,"Missing"
W98-1114,W93-0108,0,0.0603925,"Missing"
W98-1114,W97-0301,0,0.0474814,"Missing"
W98-1114,C92-2065,0,0.0443408,"ge statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering', and by an E P S R C Advanced Fellowship to the first author. Some of the work was carried out while the firstauthor was a visitor at the Tanaka Laboratory, Department of Computer Science, Tokyo Institute of Technology,"
W98-1114,C94-2123,0,0.050874,"Missing"
W98-1114,C92-2066,0,0.0607765,"grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy 1. 1 Introduction Recent work on the automatic acquisition of lexical information from substantial amounts of machine-readable text (e.g. Briscoe & Carroll, 1997; Gahl, 1998; Carroll & Rooth, 1998) has opened up the possibility of producing largescale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, although Resnik (1992), Schabes (1992), Carroll & Weir (1997) and others have proposed 'lexicalised' probabilistic grammars to improve the accuracy of parse rank~This work was funded by U K E P S R C project GR/L53175 'PSET: Practical Simplification of English Text', C E C Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering', and by an E P S R C Advanced Fellowship to the first author. Some of the work was carried out while the firstauthor was a visitor at the Tanaka Laboratory, Department of Computer Science, Tokyo Institute of Technology, and at CSLI, Sta"
W98-1114,W93-0109,0,0.185618,"Missing"
W98-1114,C98-1068,0,\N,Missing
